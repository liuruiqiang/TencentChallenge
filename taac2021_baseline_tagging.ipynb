{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 腾讯广告算法大赛 2021: 多模态视频广告标签\n",
    "* 赛道二: 多模态视频广告标签（Multimodal Video Ads Tagging）  \n",
    "    * 赛题简介: 对于给定的测试视频样本，通过算法预测出视频在呈现形式、场景、风格等三个维度上的标签，使用Global Average Precision(GAP)进行评分。  \n",
    "* 官网报名 - https://algo.qq.com/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赛题解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多模态视频广告标签是一种多模式学习任务，旨在通过理解包括视频、音频、文本等在内的多模态信息来预测语义标签。具体地，对于我们给定的视频广告，参赛者需要从4个维度来预测视频标签: 呈现、场景、样式、叙述。  \n",
    "![avatar](tagging.png)\n",
    "如上图所示，每个视频广告样本都包含视频和音频，还可以通过OCR和ASR提取出文本信息。基于这些特征作为输入，使用多模态标签学习来预测标签。  \n",
    "* 补充说明: 允许参赛者使用外部训练数据优化模型，包括合成数据以及其它公共数据集。此外，参赛者也可以使用NLP纠错、知识图谱和其它策略进行优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个广告视频，您将提交一份预测标签及其对应的置信度得分的列表。  \n",
    "评估采用具有最高k(i)的预测标签，其中i为索引，每个视频在每个索引下的置信度得分，然后将每个预测和置信度得分视为一长串的全局数据中的单个数据点预测，以计算上述各方面的所有预测和所有视频的平均精度。如果提交视频具有k(i)个预测（标记/置信对），并按其置信度得分排序，则GAP\n",
    "$$\n",
    "GAP = \\sum_{i=1}^{3} \\sum_{j=1}^{k(i)} p(j)\\Delta r(j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交结果文件为json格式，示例如下:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"03121d15e3cb0354c478576ec12c1f56.mp4\": {\n",
    "        \"result\": [{\n",
    "                \"labels\": [\"情景剧\",\"BGM\",\"办公室\",\"人行道\",\"悬念\",\"中景\",\"特写\",\"现代\",\"背景交代\",\"疑问悬念\",\"附加赠礼\",\"游戏界面\",\"红包\",\"游戏原声\",\"手机屏幕\",\"轻快\",\"产品展示\",\"金币/红包激动\",\"服务优势\",\"品牌强化\",\"行动指引\"],\n",
    "                \"scores\": [0.9, 0.8, 0.5, 0.5, ...]\n",
    "                  }]\n",
    "        }\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初赛阶段会提供初赛数据集，参赛者在报名成功后，可在腾讯云TI-ONE中启动一个Notebook，初赛数据集已经预置在了/home/tione/notebook/algo-2021/dataset/目录下。  \n",
    "* 腾讯云TI-ONE如何启动一个Notebook - https://cloud.tencent.com/document/product/851/44434  \n",
    "* /home/tione/notebook/algo-2021/dataset/目录是只读目录，为了提升性能或便于数据处理，用户可以在/home/tione/notebook/目录下新建文件夹dataset，并将/home/tione/notebook/algo-2021/dataset/下的内容复制到/home/tione/notebook/dataset/下\n",
    "\n",
    "初赛数据集的目录结构如下:  \n",
    "```\n",
    "- dataset/\n",
    "    - label_id.txt                                                  // 标签字典文件，每一行是(标签名称, 标签编号)\n",
    "    - structuring/\n",
    "        - GroundTruth/                                              // structuring任务的训练集标注信息和数据集文件\n",
    "            - datafile/                                             // structuring任务的训练集数据集文件，包括训练集（datafile/train.txt, shuffle后的90%）与验证集（datafile/val.txt, shuffle后的10%）\n",
    "                - train.txt\n",
    "                - val.txt\n",
    "            - structuring_tagging_info.txt                          // structuring任务训练集标注信息（csv格式）\n",
    "            - train5k.txt                                           // structuring任务训练集标注信息（json格式）\n",
    "        - structuring_dataset_test_5k/                              // structuring任务测试集特征信息\n",
    "            - aud_feat/\n",
    "            - aud_wav/\n",
    "            - shot_keyf/\n",
    "            - shot_split_video/\n",
    "            - shot_stats/\n",
    "            - shot_txt/\n",
    "        - structuring_dataset_train_5k/                             // structuring任务训练集特征信息\n",
    "            - aud_feat/\n",
    "            - labels/\n",
    "            - meta/\n",
    "            - place_feat/\n",
    "            - shot_stats/\n",
    "            - shot_txt/ \n",
    "        - train5k_split_video/                                      // 切分后的视频\n",
    "        - train5k_split_video_feats/                                // 切分后的视频特征，包括视频特征、音频特征、封面图、文本特征等\n",
    "    - tagging/\n",
    "        - GroundTruth/                                              // tagging任务的训练集标注信息和数据集文件\n",
    "            - datafile/                                             // tagging任务的训练集数据集文件，包括训练集（datafile/train.txt，shuffle后的90%）与验证集（datafile/val.txt，shuffle后的10%），每一个样本包括6行，依次是video_npy、audio_npy、封面图、文本信息、多标签标注信息（文本）、空行\n",
    "                - train.txt\n",
    "                - val.txt\n",
    "            - tagging_info.txt                                      // tagging任务的训练集标注信息（csv格式）\n",
    "        - tagging_dataset_test_5k/                                  // 测试集的特征文件\n",
    "            - audio_npy/Vggish/tagging/                             // 音频特征: 测试集视频的音频特征文件（.npy）\n",
    "            - image_jpg/tagging/                                    // 封面图: 测试集视频的封面图文件\n",
    "            - text_txt/tagging/                                     // 文本特征: 测试集视频的文本特征，包括视频广告的OCR结果、ASR结果等\n",
    "            - video_npy/Youtube8M/tagging/                          // 视频特征: 测试集视频的视频特征文件（.npy）\n",
    "        - tagging_dataset_train_5k/                                 // 训练集的特征文件\n",
    "            - audio_npy/Vggish/tagging/                             // 音频特征: 训练集视频的音频特征文件（.npy）\n",
    "            - image_jpg/tagging/                                    // 封面图: 训练集视频的封面图文件\n",
    "            - text_txt/tagging/                                     // 文本特征: 训练集视频的文本特征，包括视频广告的OCR结果、ASR结果等\n",
    "            - video_npy/Youtube8M/tagging/                          // 视频特征: 训练集视频的视频特征文件（.npy）\n",
    "    - videos/\n",
    "        - test_5k_A/                                                // 测试集原始视频文件（.mp4）\n",
    "        - train_5k_A/                                               // 训练集原始视频文件（.mp4）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （彩蛋）数据展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助TI-ONE Notebook，您可以方便地展示数据集中的数据，包括视频、图片等，以帮助进一步分析。  \n",
    "以下代码片段，给出了一个简单的示例（相信你已经能感受到Notebook的强大了，尽情探索Notebook的数据展示与分析功能吧）: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 不需要跑这个视频代码\n",
    "\n",
    "# import os\n",
    "\n",
    "# dataset_root = './VideoStructuring/dataset/'\n",
    "\n",
    "# # ########## get train_5k_A video file lists\n",
    "# videos_train_5k_A_dir = os.path.join(dataset_root, 'videos/train_5k_A')\n",
    "# videos_train_5k_A_files = [os.path.join(videos_train_5k_A_dir, f) for f in os.listdir(videos_train_5k_A_dir) if os.path.isfile(os.path.join(videos_train_5k_A_dir, f))]\n",
    "\n",
    "# print(\"videos_train_5k_A_dir= {}\".format(videos_train_5k_A_dir))\n",
    "# print(\"len(videos/train_5k_A)= {}\".format(len(videos_train_5k_A_files)))\n",
    "\n",
    "# # ########## display\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# # video\n",
    "# test_video_path = videos_train_5k_A_files[3000]\n",
    "# print(test_video_path)\n",
    "# print(os.path.exists(test_video_path))\n",
    "# html_str = '''\n",
    "# <video controls width=\\\"500\\\" height=\\\"500\\\" src=\\\"{}\\\">animation</video>\n",
    "# '''.format(test_video_path)\n",
    "# print(html_str)\n",
    "# display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让参赛者更快速地熟悉赛题，我们提供了一个baseline，帮助参赛者们理解赛题，并开拓思路。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参赛者在报名成功后，可在腾讯云TI-ONE中启动一个Notebook，tagging任务的baseline已经预置在了/home/tione/notebook/algo-2021/baseline/tagging/VideoStructuring/目录下。  \n",
    "* 腾讯云TI-ONE如何启动一个Notebook - https://cloud.tencent.com/document/product/851/44434\n",
    "* /home/tione/notebook/algo-2021/baseline/tagging/VideoStructuring/目录是只读目录，为了提升性能或便于数据处理，用户可以在/home/tione/notebook/目录下新建文件夹VideoStructuring，并将/home/tione/notebook/algo-2021/baseline/tagging/VideoStructuring/下的内容复制到/home/tione/notebook/VideoStructuring/下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline（/home/tione/notebook/VideoStructuring/）的目录结构如下:  \n",
    "```\n",
    "- VideoStructuring/\n",
    "    - MultiModal-Tagging/                              // tagging任务相关的代码\n",
    "        - configs/\n",
    "        - dataset/\n",
    "        - pretrained/\n",
    "        - scripts/\n",
    "        - src/\n",
    "        - utils/\n",
    "        - ReadMe.md\n",
    "        - requirment.txt\n",
    "    - SceneSeg/                                        // Scene Segmentation相关的代码\n",
    "        - lgss/\n",
    "        - pre/\n",
    "        - run/\n",
    "        - ReadMe.md\n",
    "    - init.sh                                          // 初始化脚本，需要执行 ./init.sh run 初始化环境\n",
    "    - run.sh                                           // 运行脚本，seg_train、tag_train、test等步骤都封装成了脚本，可通过 ./run.sh 脚本执行相关任务，使用 ./run.sh help 了解如何使用\n",
    "    - ReadMe.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline代码运行所需要的环境可以直接运行 init.sh 进行准备  \n",
    "注意: init.sh 中的 JupyterRoot 需要更改为 VideoStructuring 文件夹所在的目录  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_CONFIG_ROOT_PREFIX= root_prefix: /opt/conda\n",
      "CONDA_ROOT= /opt/conda\n",
      "CONDA_NEW_ENV= taac2021-tagging\n",
      "JUPYTER_ROOT= /home/tione/notebook\n",
      "CODE_ROOT= /home/tione/notebook/VideoStructuring\n",
      "OS_ID= ubuntu\n",
      "[Info] installing system libraries in ubuntu\n",
      "Get:1 http://mirrors.tencentyun.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:2 http://mirrors.tencentyun.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:3 http://mirrors.tencentyun.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:4 http://mirrors.tencentyun.com/ubuntu bionic/multiverse Sources [216 kB]\n",
      "Get:5 http://mirrors.tencentyun.com/ubuntu bionic/main Sources [1,063 kB]\n",
      "Get:6 http://mirrors.tencentyun.com/ubuntu bionic/universe Sources [11.5 MB]   \n",
      "Get:7 http://mirrors.tencentyun.com/ubuntu bionic/restricted Sources [5,823 B] \n",
      "Get:8 http://mirrors.tencentyun.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:9 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 Packages [1,344 kB]\n",
      "Get:10 http://mirrors.tencentyun.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:11 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:12 http://mirrors.tencentyun.com/ubuntu bionic-security/universe Sources [346 kB]\n",
      "Get:13 http://mirrors.tencentyun.com/ubuntu bionic-security/restricted Sources [21.6 kB]\n",
      "Get:14 http://mirrors.tencentyun.com/ubuntu bionic-security/multiverse Sources [5,571 B]\n",
      "Get:15 http://mirrors.tencentyun.com/ubuntu bionic-security/main Sources [315 kB]\n",
      "Get:16 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
      "Get:17 http://mirrors.tencentyun.com/ubuntu bionic-security/restricted amd64 Packages [396 kB]\n",
      "Get:18 http://mirrors.tencentyun.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
      "Get:19 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
      "Get:20 http://mirrors.tencentyun.com/ubuntu bionic-updates/main Sources [638 kB]\n",
      "Get:21 http://mirrors.tencentyun.com/ubuntu bionic-updates/restricted Sources [25.0 kB]\n",
      "Get:22 http://mirrors.tencentyun.com/ubuntu bionic-updates/universe Sources [573 kB]\n",
      "Get:23 http://mirrors.tencentyun.com/ubuntu bionic-updates/multiverse Sources [13.2 kB]\n",
      "Get:24 http://mirrors.tencentyun.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
      "Get:25 http://mirrors.tencentyun.com/ubuntu bionic-updates/restricted amd64 Packages [426 kB]\n",
      "Get:26 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
      "Get:27 http://mirrors.tencentyun.com/ubuntu bionic-updates/universe amd64 Packages [2,181 kB]\n",
      "Fetched 37.2 MB in 3s (11.8 MB/s)                          \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  apt libapt-inst2.0 libapt-pkg5.0\n",
      "Suggested packages:\n",
      "  apt-doc aptitude | synaptic | wajig powermgmt-base\n",
      "The following NEW packages will be installed:\n",
      "  apt-utils libapt-inst2.0\n",
      "The following packages will be upgraded:\n",
      "  apt libapt-pkg5.0\n",
      "2 upgraded, 2 newly installed, 0 to remove and 129 not upgraded.\n",
      "Need to get 2,270 kB of archives.\n",
      "After this operation, 1,274 kB of additional disk space will be used.\n",
      "Get:1 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 libapt-pkg5.0 amd64 1.6.13 [808 kB]\n",
      "Get:2 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 apt amd64 1.6.13 [1,201 kB]\n",
      "Get:3 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 libapt-inst2.0 amd64 1.6.13 [54.6 kB]\n",
      "Get:4 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 apt-utils amd64 1.6.13 [206 kB]\n",
      "Fetched 2,270 kB in 0s (17.5 MB/s)  \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 53322 files and directories currently installed.)\n",
      "Preparing to unpack .../libapt-pkg5.0_1.6.13_amd64.deb ...\n",
      "Unpacking libapt-pkg5.0:amd64 (1.6.13) over (1.6.12) ...\n",
      "Setting up libapt-pkg5.0:amd64 (1.6.13) ...\n",
      "(Reading database ... 53322 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/apt_1.6.13_amd64.deb ...\n",
      "Unpacking apt (1.6.13) over (1.6.12) ...\n",
      "Setting up apt (1.6.13) ...\n",
      "Selecting previously unselected package libapt-inst2.0:amd64.\n",
      "(Reading database ... 53322 files and directories currently installed.)\n",
      "Preparing to unpack .../libapt-inst2.0_1.6.13_amd64.deb ...\n",
      "Unpacking libapt-inst2.0:amd64 (1.6.13) ...\n",
      "Selecting previously unselected package apt-utils.\n",
      "Preparing to unpack .../apt-utils_1.6.13_amd64.deb ...\n",
      "Unpacking apt-utils (1.6.13) ...\n",
      "Setting up libapt-inst2.0:amd64 (1.6.13) ...\n",
      "Setting up apt-utils (1.6.13) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  i965-va-driver libaacs0 libass9 libavc1394-0 libavcodec57 libavdevice57\n",
      "  libavfilter6 libavformat57 libavresample3 libavutil55 libbdplus0 libbluray2\n",
      "  libbs2b0 libcaca0 libcdio-cdda2 libcdio-paranoia2 libcdio17 libchromaprint1\n",
      "  libcrystalhd3 libdc1394-22 libfftw3-double3 libflac-dev libflite1\n",
      "  libfribidi0 libgme0 libgsm1 libiec61883-0 libjack-jackd2-0 libllvm10\n",
      "  libmp3lame0 libmpg123-0 libmysofa0 libnorm1 libnuma1 libogg-dev\n",
      "  libopenal-data libopenal1 libopenjp2-7 libopenmpt0 libopus0 libpgm-5.2-0\n",
      "  libpostproc54 libraw1394-11 librubberband2 libsamplerate0 libsdl2-2.0-0\n",
      "  libshine3 libslang2 libsnappy1v5 libsndio6.1 libsodium23 libsoxr0 libspeex1\n",
      "  libssh-gcrypt-4 libswresample2 libswscale4 libtheora0 libtwolame0\n",
      "  libusb-1.0-0 libva-drm2 libva-x11-2 libva2 libvdpau1 libvorbis-dev\n",
      "  libvorbisfile3 libvpx5 libwavpack1 libwayland-cursor0 libwayland-egl1\n",
      "  libwayland-egl1-mesa libx264-152 libx265-146 libxkbcommon0 libxvidcore4\n",
      "  libzmq5 libzvbi-common libzvbi0 mesa-va-drivers mesa-vdpau-drivers\n",
      "  va-driver-all vdpau-driver-all xkb-data\n",
      "Suggested packages:\n",
      "  ffmpeg-doc i965-va-driver-shaders libbluray-bdj firmware-crystalhd\n",
      "  libfftw3-bin libfftw3-dev jackd2 libportaudio2 opus-tools libraw1394-doc\n",
      "  sndiod speex libvdpau-va-gl1 nvidia-vdpau-driver\n",
      "  nvidia-legacy-340xx-vdpau-driver\n",
      "The following NEW packages will be installed:\n",
      "  ffmpeg i965-va-driver libaacs0 libass9 libavc1394-0 libavcodec57\n",
      "  libavdevice57 libavfilter6 libavformat57 libavresample3 libavutil55\n",
      "  libbdplus0 libbluray2 libbs2b0 libcaca0 libcdio-cdda2 libcdio-paranoia2\n",
      "  libcdio17 libchromaprint1 libcrystalhd3 libdc1394-22 libfftw3-double3\n",
      "  libflac-dev libflite1 libfribidi0 libgme0 libgsm1 libiec61883-0\n",
      "  libjack-jackd2-0 libllvm10 libmp3lame0 libmpg123-0 libmysofa0 libnorm1\n",
      "  libnuma1 libogg-dev libopenal-data libopenal1 libopenjp2-7 libopenmpt0\n",
      "  libopus0 libpgm-5.2-0 libpostproc54 libraw1394-11 librubberband2\n",
      "  libsamplerate0 libsdl2-2.0-0 libshine3 libslang2 libsnappy1v5\n",
      "  libsndfile1-dev libsndio6.1 libsodium23 libsoxr0 libspeex1 libssh-gcrypt-4\n",
      "  libswresample2 libswscale4 libtheora0 libtwolame0 libusb-1.0-0 libva-drm2\n",
      "  libva-x11-2 libva2 libvdpau1 libvorbis-dev libvorbisfile3 libvpx5\n",
      "  libwavpack1 libwayland-cursor0 libwayland-egl1 libwayland-egl1-mesa\n",
      "  libx264-152 libx265-146 libxkbcommon0 libxvidcore4 libzmq5 libzvbi-common\n",
      "  libzvbi0 mesa-va-drivers mesa-vdpau-drivers va-driver-all vdpau-driver-all\n",
      "  xkb-data\n",
      "0 upgraded, 84 newly installed, 0 to remove and 129 not upgraded.\n",
      "Need to get 53.9 MB of archives.\n",
      "After this operation, 244 MB of additional disk space will be used.\n",
      "Get:1 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libfribidi0 amd64 0.19.7-2 [24.9 kB]\n",
      "Get:2 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libslang2 amd64 2.3.1a-3ubuntu1 [424 kB]\n",
      "Get:3 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 xkb-data all 2.23.1-1ubuntu1.18.04.1 [325 kB]\n",
      "Get:4 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 libnuma1 amd64 2.0.11-2.1ubuntu0.1 [22.0 kB]\n",
      "Get:5 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libusb-1.0-0 amd64 2:1.0.21-2 [43.3 kB]\n",
      "Get:6 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libva2 amd64 2.1.0-3 [47.6 kB]\n",
      "Get:7 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libva-drm2 amd64 2.1.0-3 [6,880 B]\n",
      "Get:8 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libva-x11-2 amd64 2.1.0-3 [11.5 kB]\n",
      "Get:9 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libvdpau1 amd64 1.1.1-3ubuntu1 [25.5 kB]\n",
      "Get:10 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavutil55 amd64 7:3.4.8-0ubuntu0.2 [190 kB]\n",
      "Get:11 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libcrystalhd3 amd64 1:0.0~git20110715.fdd2f19-12 [45.8 kB]\n",
      "Get:12 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libgsm1 amd64 1.0.13-4build1 [22.4 kB]\n",
      "Get:13 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libmp3lame0 amd64 3.100-2 [136 kB]\n",
      "Get:14 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libopenjp2-7 amd64 2.3.0-2build0.18.04.1 [145 kB]\n",
      "Get:15 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libopus0 amd64 1.1.2-1ubuntu1 [159 kB]\n",
      "Get:16 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libshine3 amd64 3.1.1-1 [22.9 kB]\n",
      "Get:17 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libsnappy1v5 amd64 1.1.7-1 [16.0 kB]\n",
      "Get:18 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libspeex1 amd64 1.2~rc1.2-1ubuntu2 [52.1 kB]\n",
      "Get:19 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libsoxr0 amd64 0.1.2-3 [65.9 kB]\n",
      "Get:20 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libswresample2 amd64 7:3.4.8-0ubuntu0.2 [55.2 kB]\n",
      "Get:21 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libtheora0 amd64 1.1.1+dfsg.1-14 [170 kB]\n",
      "Get:22 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libtwolame0 amd64 0.3.13-3 [46.7 kB]\n",
      "Get:23 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libvpx5 amd64 1.7.0-3ubuntu0.18.04.1 [796 kB]\n",
      "Get:24 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libwavpack1 amd64 5.1.0-2ubuntu1.5 [76.8 kB]\n",
      "Get:25 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libx264-152 amd64 2:0.152.2854+gite9a5903-2 [609 kB]\n",
      "Get:26 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libx265-146 amd64 2.6-3 [1,026 kB]\n",
      "Get:27 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libxvidcore4 amd64 2:1.3.5-1 [200 kB]\n",
      "Get:28 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libzvbi-common all 0.2.35-13 [32.1 kB]\n",
      "Get:29 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libzvbi0 amd64 0.2.35-13 [235 kB]\n",
      "Get:30 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavcodec57 amd64 7:3.4.8-0ubuntu0.2 [4,595 kB]\n",
      "Get:31 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libraw1394-11 amd64 2.1.2-1 [30.7 kB]\n",
      "Get:32 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libavc1394-0 amd64 0.5.4-4build1 [16.1 kB]\n",
      "Get:33 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libass9 amd64 1:0.14.0-1 [88.2 kB]\n",
      "Get:34 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libbluray2 amd64 1:1.0.2-3 [141 kB]\n",
      "Get:35 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libchromaprint1 amd64 1.4.3-1 [36.8 kB]\n",
      "Get:36 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libgme0 amd64 0.6.2-1 [121 kB]\n",
      "Get:37 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libmpg123-0 amd64 1.25.10-1 [125 kB]\n",
      "Get:38 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libvorbisfile3 amd64 1.3.5-4.2 [16.0 kB]\n",
      "Get:39 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libopenmpt0 amd64 0.3.6-1 [561 kB]\n",
      "Get:40 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libssh-gcrypt-4 amd64 0.8.0~20170825.94fa1e38-1ubuntu0.7 [172 kB]\n",
      "Get:41 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavformat57 amd64 7:3.4.8-0ubuntu0.2 [953 kB]\n",
      "Get:42 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavresample3 amd64 7:3.4.8-0ubuntu0.2 [52.6 kB]\n",
      "Get:43 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libbs2b0 amd64 3.1.0+dfsg-2.2 [10.5 kB]\n",
      "Get:44 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libflite1 amd64 2.1-release-1 [12.8 MB]\n",
      "Get:45 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libmysofa0 amd64 0.6~dfsg0-3+deb10u1build1 [38.5 kB]\n",
      "Get:46 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libpostproc54 amd64 7:3.4.8-0ubuntu0.2 [50.3 kB]\n",
      "Get:47 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libfftw3-double3 amd64 3.3.7-1 [735 kB]\n",
      "Get:48 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libsamplerate0 amd64 0.1.9-1 [938 kB]\n",
      "Get:49 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 librubberband2 amd64 1.8.1-7ubuntu2 [86.7 kB]\n",
      "Get:50 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libswscale4 amd64 7:3.4.8-0ubuntu0.2 [150 kB]\n",
      "Get:51 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libnorm1 amd64 1.5r6+dfsg1-6 [224 kB]\n",
      "Get:52 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libpgm-5.2-0 amd64 5.2.122~dfsg-2 [157 kB]\n",
      "Get:53 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libsodium23 amd64 1.0.16-2 [143 kB]\n",
      "Get:54 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libzmq5 amd64 4.2.5-1ubuntu0.2 [221 kB]\n",
      "Get:55 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavfilter6 amd64 7:3.4.8-0ubuntu0.2 [874 kB]\n",
      "Get:56 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libcaca0 amd64 0.99.beta19-2ubuntu0.18.04.2 [202 kB]\n",
      "Get:57 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libcdio17 amd64 1.0.0-2ubuntu2 [58.8 kB]\n",
      "Get:58 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libcdio-cdda2 amd64 10.2+0.94+2-2build1 [17.7 kB]\n",
      "Get:59 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libcdio-paranoia2 amd64 10.2+0.94+2-2build1 [17.2 kB]\n",
      "Get:60 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libdc1394-22 amd64 2.2.5-1 [77.5 kB]\n",
      "Get:61 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libiec61883-0 amd64 1.2.0-2 [23.5 kB]\n",
      "Get:62 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libjack-jackd2-0 amd64 1.9.12~dfsg-2 [263 kB]\n",
      "Get:63 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libopenal-data all 1:1.18.2-2 [102 kB]\n",
      "Get:64 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libsndio6.1 amd64 1.1.0-3 [23.4 kB]\n",
      "Get:65 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libopenal1 amd64 1:1.18.2-2 [266 kB]\n",
      "Get:66 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libwayland-cursor0 amd64 1.16.0-1ubuntu1.1~18.04.3 [10.1 kB]\n",
      "Get:67 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libwayland-egl1 amd64 1.16.0-1ubuntu1.1~18.04.3 [5,464 B]\n",
      "Get:68 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 libwayland-egl1-mesa amd64 20.0.8-0ubuntu1~18.04.1 [6,444 B]\n",
      "Get:69 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 libxkbcommon0 amd64 0.8.2-1~ubuntu18.04.1 [97.8 kB]\n",
      "Get:70 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libsdl2-2.0-0 amd64 2.0.8+dfsg1-1ubuntu1.18.04.4 [382 kB]\n",
      "Get:71 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 libavdevice57 amd64 7:3.4.8-0ubuntu0.2 [74.9 kB]\n",
      "Get:72 http://mirrors.tencentyun.com/ubuntu bionic-security/universe amd64 ffmpeg amd64 7:3.4.8-0ubuntu0.2 [1,587 kB]\n",
      "Get:73 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libaacs0 amd64 0.9.0-1 [51.4 kB]\n",
      "Get:74 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 libbdplus0 amd64 0.1.2-2 [46.6 kB]\n",
      "Get:75 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libogg-dev amd64 1.3.2-1 [156 kB]\n",
      "Get:76 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libflac-dev amd64 1.3.2-1 [260 kB]\n",
      "Get:77 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libllvm10 amd64 1:10.0.0-4ubuntu1~18.04.2 [15.4 MB]\n",
      "Get:78 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 libvorbis-dev amd64 1.3.5-4.2 [321 kB]\n",
      "Get:79 http://mirrors.tencentyun.com/ubuntu bionic-security/main amd64 libsndfile1-dev amd64 1.0.28-4ubuntu0.18.04.1 [287 kB]\n",
      "Get:80 http://mirrors.tencentyun.com/ubuntu bionic-updates/universe amd64 mesa-va-drivers amd64 20.0.8-0ubuntu1~18.04.1 [2,376 kB]\n",
      "Get:81 http://mirrors.tencentyun.com/ubuntu bionic-updates/main amd64 mesa-vdpau-drivers amd64 20.0.8-0ubuntu1~18.04.1 [2,496 kB]\n",
      "Get:82 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 i965-va-driver amd64 2.1.0-0ubuntu1 [925 kB]\n",
      "Get:83 http://mirrors.tencentyun.com/ubuntu bionic/universe amd64 va-driver-all amd64 2.1.0-3 [4,376 B]\n",
      "Get:84 http://mirrors.tencentyun.com/ubuntu bionic/main amd64 vdpau-driver-all amd64 1.1.1-3ubuntu1 [4,674 B]\n",
      "Fetched 53.9 MB in 2s (31.1 MB/s)              \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 84.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package libfribidi0:amd64.\n",
      "(Reading database ... 53409 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libfribidi0_0.19.7-2_amd64.deb ...\n",
      "Unpacking libfribidi0:amd64 (0.19.7-2) ...\n",
      "Selecting previously unselected package libslang2:amd64.\n",
      "Preparing to unpack .../01-libslang2_2.3.1a-3ubuntu1_amd64.deb ...\n",
      "Unpacking libslang2:amd64 (2.3.1a-3ubuntu1) ...\n",
      "Selecting previously unselected package xkb-data.\n",
      "Preparing to unpack .../02-xkb-data_2.23.1-1ubuntu1.18.04.1_all.deb ...\n",
      "Unpacking xkb-data (2.23.1-1ubuntu1.18.04.1) ...\n",
      "Selecting previously unselected package libnuma1:amd64.\n",
      "Preparing to unpack .../03-libnuma1_2.0.11-2.1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libnuma1:amd64 (2.0.11-2.1ubuntu0.1) ...\n",
      "Selecting previously unselected package libusb-1.0-0:amd64.\n",
      "Preparing to unpack .../04-libusb-1.0-0_2%3a1.0.21-2_amd64.deb ...\n",
      "Unpacking libusb-1.0-0:amd64 (2:1.0.21-2) ...\n",
      "Selecting previously unselected package libva2:amd64.\n",
      "Preparing to unpack .../05-libva2_2.1.0-3_amd64.deb ...\n",
      "Unpacking libva2:amd64 (2.1.0-3) ...\n",
      "Selecting previously unselected package libva-drm2:amd64.\n",
      "Preparing to unpack .../06-libva-drm2_2.1.0-3_amd64.deb ...\n",
      "Unpacking libva-drm2:amd64 (2.1.0-3) ...\n",
      "Selecting previously unselected package libva-x11-2:amd64.\n",
      "Preparing to unpack .../07-libva-x11-2_2.1.0-3_amd64.deb ...\n",
      "Unpacking libva-x11-2:amd64 (2.1.0-3) ...\n",
      "Selecting previously unselected package libvdpau1:amd64.\n",
      "Preparing to unpack .../08-libvdpau1_1.1.1-3ubuntu1_amd64.deb ...\n",
      "Unpacking libvdpau1:amd64 (1.1.1-3ubuntu1) ...\n",
      "Selecting previously unselected package libavutil55:amd64.\n",
      "Preparing to unpack .../09-libavutil55_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavutil55:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libcrystalhd3:amd64.\n",
      "Preparing to unpack .../10-libcrystalhd3_1%3a0.0~git20110715.fdd2f19-12_amd64.deb ...\n",
      "Unpacking libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-12) ...\n",
      "Selecting previously unselected package libgsm1:amd64.\n",
      "Preparing to unpack .../11-libgsm1_1.0.13-4build1_amd64.deb ...\n",
      "Unpacking libgsm1:amd64 (1.0.13-4build1) ...\n",
      "Selecting previously unselected package libmp3lame0:amd64.\n",
      "Preparing to unpack .../12-libmp3lame0_3.100-2_amd64.deb ...\n",
      "Unpacking libmp3lame0:amd64 (3.100-2) ...\n",
      "Selecting previously unselected package libopenjp2-7:amd64.\n",
      "Preparing to unpack .../13-libopenjp2-7_2.3.0-2build0.18.04.1_amd64.deb ...\n",
      "Unpacking libopenjp2-7:amd64 (2.3.0-2build0.18.04.1) ...\n",
      "Selecting previously unselected package libopus0:amd64.\n",
      "Preparing to unpack .../14-libopus0_1.1.2-1ubuntu1_amd64.deb ...\n",
      "Unpacking libopus0:amd64 (1.1.2-1ubuntu1) ...\n",
      "Selecting previously unselected package libshine3:amd64.\n",
      "Preparing to unpack .../15-libshine3_3.1.1-1_amd64.deb ...\n",
      "Unpacking libshine3:amd64 (3.1.1-1) ...\n",
      "Selecting previously unselected package libsnappy1v5:amd64.\n",
      "Preparing to unpack .../16-libsnappy1v5_1.1.7-1_amd64.deb ...\n",
      "Unpacking libsnappy1v5:amd64 (1.1.7-1) ...\n",
      "Selecting previously unselected package libspeex1:amd64.\n",
      "Preparing to unpack .../17-libspeex1_1.2~rc1.2-1ubuntu2_amd64.deb ...\n",
      "Unpacking libspeex1:amd64 (1.2~rc1.2-1ubuntu2) ...\n",
      "Selecting previously unselected package libsoxr0:amd64.\n",
      "Preparing to unpack .../18-libsoxr0_0.1.2-3_amd64.deb ...\n",
      "Unpacking libsoxr0:amd64 (0.1.2-3) ...\n",
      "Selecting previously unselected package libswresample2:amd64.\n",
      "Preparing to unpack .../19-libswresample2_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libswresample2:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libtheora0:amd64.\n",
      "Preparing to unpack .../20-libtheora0_1.1.1+dfsg.1-14_amd64.deb ...\n",
      "Unpacking libtheora0:amd64 (1.1.1+dfsg.1-14) ...\n",
      "Selecting previously unselected package libtwolame0:amd64.\n",
      "Preparing to unpack .../21-libtwolame0_0.3.13-3_amd64.deb ...\n",
      "Unpacking libtwolame0:amd64 (0.3.13-3) ...\n",
      "Selecting previously unselected package libvpx5:amd64.\n",
      "Preparing to unpack .../22-libvpx5_1.7.0-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libvpx5:amd64 (1.7.0-3ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package libwavpack1:amd64.\n",
      "Preparing to unpack .../23-libwavpack1_5.1.0-2ubuntu1.5_amd64.deb ...\n",
      "Unpacking libwavpack1:amd64 (5.1.0-2ubuntu1.5) ...\n",
      "Selecting previously unselected package libx264-152:amd64.\n",
      "Preparing to unpack .../24-libx264-152_2%3a0.152.2854+gite9a5903-2_amd64.deb ...\n",
      "Unpacking libx264-152:amd64 (2:0.152.2854+gite9a5903-2) ...\n",
      "Selecting previously unselected package libx265-146:amd64.\n",
      "Preparing to unpack .../25-libx265-146_2.6-3_amd64.deb ...\n",
      "Unpacking libx265-146:amd64 (2.6-3) ...\n",
      "Selecting previously unselected package libxvidcore4:amd64.\n",
      "Preparing to unpack .../26-libxvidcore4_2%3a1.3.5-1_amd64.deb ...\n",
      "Unpacking libxvidcore4:amd64 (2:1.3.5-1) ...\n",
      "Selecting previously unselected package libzvbi-common.\n",
      "Preparing to unpack .../27-libzvbi-common_0.2.35-13_all.deb ...\n",
      "Unpacking libzvbi-common (0.2.35-13) ...\n",
      "Selecting previously unselected package libzvbi0:amd64.\n",
      "Preparing to unpack .../28-libzvbi0_0.2.35-13_amd64.deb ...\n",
      "Unpacking libzvbi0:amd64 (0.2.35-13) ...\n",
      "Selecting previously unselected package libavcodec57:amd64.\n",
      "Preparing to unpack .../29-libavcodec57_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavcodec57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libraw1394-11:amd64.\n",
      "Preparing to unpack .../30-libraw1394-11_2.1.2-1_amd64.deb ...\n",
      "Unpacking libraw1394-11:amd64 (2.1.2-1) ...\n",
      "Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../31-libavc1394-0_0.5.4-4build1_amd64.deb ...\n",
      "Unpacking libavc1394-0:amd64 (0.5.4-4build1) ...\n",
      "Selecting previously unselected package libass9:amd64.\n",
      "Preparing to unpack .../32-libass9_1%3a0.14.0-1_amd64.deb ...\n",
      "Unpacking libass9:amd64 (1:0.14.0-1) ...\n",
      "Selecting previously unselected package libbluray2:amd64.\n",
      "Preparing to unpack .../33-libbluray2_1%3a1.0.2-3_amd64.deb ...\n",
      "Unpacking libbluray2:amd64 (1:1.0.2-3) ...\n",
      "Selecting previously unselected package libchromaprint1:amd64.\n",
      "Preparing to unpack .../34-libchromaprint1_1.4.3-1_amd64.deb ...\n",
      "Unpacking libchromaprint1:amd64 (1.4.3-1) ...\n",
      "Selecting previously unselected package libgme0:amd64.\n",
      "Preparing to unpack .../35-libgme0_0.6.2-1_amd64.deb ...\n",
      "Unpacking libgme0:amd64 (0.6.2-1) ...\n",
      "Selecting previously unselected package libmpg123-0:amd64.\n",
      "Preparing to unpack .../36-libmpg123-0_1.25.10-1_amd64.deb ...\n",
      "Unpacking libmpg123-0:amd64 (1.25.10-1) ...\n",
      "Selecting previously unselected package libvorbisfile3:amd64.\n",
      "Preparing to unpack .../37-libvorbisfile3_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbisfile3:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libopenmpt0:amd64.\n",
      "Preparing to unpack .../38-libopenmpt0_0.3.6-1_amd64.deb ...\n",
      "Unpacking libopenmpt0:amd64 (0.3.6-1) ...\n",
      "Selecting previously unselected package libssh-gcrypt-4:amd64.\n",
      "Preparing to unpack .../39-libssh-gcrypt-4_0.8.0~20170825.94fa1e38-1ubuntu0.7_amd64.deb ...\n",
      "Unpacking libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) ...\n",
      "Selecting previously unselected package libavformat57:amd64.\n",
      "Preparing to unpack .../40-libavformat57_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavformat57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libavresample3:amd64.\n",
      "Preparing to unpack .../41-libavresample3_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavresample3:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../42-libbs2b0_3.1.0+dfsg-2.2_amd64.deb ...\n",
      "Unpacking libbs2b0:amd64 (3.1.0+dfsg-2.2) ...\n",
      "Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../43-libflite1_2.1-release-1_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.1-release-1) ...\n",
      "Selecting previously unselected package libmysofa0:amd64.\n",
      "Preparing to unpack .../44-libmysofa0_0.6~dfsg0-3+deb10u1build1_amd64.deb ...\n",
      "Unpacking libmysofa0:amd64 (0.6~dfsg0-3+deb10u1build1) ...\n",
      "Selecting previously unselected package libpostproc54:amd64.\n",
      "Preparing to unpack .../45-libpostproc54_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libpostproc54:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libfftw3-double3:amd64.\n",
      "Preparing to unpack .../46-libfftw3-double3_3.3.7-1_amd64.deb ...\n",
      "Unpacking libfftw3-double3:amd64 (3.3.7-1) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../47-libsamplerate0_0.1.9-1_amd64.deb ...\n",
      "Unpacking libsamplerate0:amd64 (0.1.9-1) ...\n",
      "Selecting previously unselected package librubberband2:amd64.\n",
      "Preparing to unpack .../48-librubberband2_1.8.1-7ubuntu2_amd64.deb ...\n",
      "Unpacking librubberband2:amd64 (1.8.1-7ubuntu2) ...\n",
      "Selecting previously unselected package libswscale4:amd64.\n",
      "Preparing to unpack .../49-libswscale4_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libswscale4:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libnorm1:amd64.\n",
      "Preparing to unpack .../50-libnorm1_1.5r6+dfsg1-6_amd64.deb ...\n",
      "Unpacking libnorm1:amd64 (1.5r6+dfsg1-6) ...\n",
      "Selecting previously unselected package libpgm-5.2-0:amd64.\n",
      "Preparing to unpack .../51-libpgm-5.2-0_5.2.122~dfsg-2_amd64.deb ...\n",
      "Unpacking libpgm-5.2-0:amd64 (5.2.122~dfsg-2) ...\n",
      "Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../52-libsodium23_1.0.16-2_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.16-2) ...\n",
      "Selecting previously unselected package libzmq5:amd64.\n",
      "Preparing to unpack .../53-libzmq5_4.2.5-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libzmq5:amd64 (4.2.5-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libavfilter6:amd64.\n",
      "Preparing to unpack .../54-libavfilter6_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavfilter6:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../55-libcaca0_0.99.beta19-2ubuntu0.18.04.2_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta19-2ubuntu0.18.04.2) ...\n",
      "Selecting previously unselected package libcdio17:amd64.\n",
      "Preparing to unpack .../56-libcdio17_1.0.0-2ubuntu2_amd64.deb ...\n",
      "Unpacking libcdio17:amd64 (1.0.0-2ubuntu2) ...\n",
      "Selecting previously unselected package libcdio-cdda2:amd64.\n",
      "Preparing to unpack .../57-libcdio-cdda2_10.2+0.94+2-2build1_amd64.deb ...\n",
      "Unpacking libcdio-cdda2:amd64 (10.2+0.94+2-2build1) ...\n",
      "Selecting previously unselected package libcdio-paranoia2:amd64.\n",
      "Preparing to unpack .../58-libcdio-paranoia2_10.2+0.94+2-2build1_amd64.deb ...\n",
      "Unpacking libcdio-paranoia2:amd64 (10.2+0.94+2-2build1) ...\n",
      "Selecting previously unselected package libdc1394-22:amd64.\n",
      "Preparing to unpack .../59-libdc1394-22_2.2.5-1_amd64.deb ...\n",
      "Unpacking libdc1394-22:amd64 (2.2.5-1) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../60-libiec61883-0_1.2.0-2_amd64.deb ...\n",
      "Unpacking libiec61883-0:amd64 (1.2.0-2) ...\n",
      "Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../61-libjack-jackd2-0_1.9.12~dfsg-2_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.12~dfsg-2) ...\n",
      "Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../62-libopenal-data_1%3a1.18.2-2_all.deb ...\n",
      "Unpacking libopenal-data (1:1.18.2-2) ...\n",
      "Selecting previously unselected package libsndio6.1:amd64.\n",
      "Preparing to unpack .../63-libsndio6.1_1.1.0-3_amd64.deb ...\n",
      "Unpacking libsndio6.1:amd64 (1.1.0-3) ...\n",
      "Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../64-libopenal1_1%3a1.18.2-2_amd64.deb ...\n",
      "Unpacking libopenal1:amd64 (1:1.18.2-2) ...\n",
      "Selecting previously unselected package libwayland-cursor0:amd64.\n",
      "Preparing to unpack .../65-libwayland-cursor0_1.16.0-1ubuntu1.1~18.04.3_amd64.deb ...\n",
      "Unpacking libwayland-cursor0:amd64 (1.16.0-1ubuntu1.1~18.04.3) ...\n",
      "Selecting previously unselected package libwayland-egl1:amd64.\n",
      "Preparing to unpack .../66-libwayland-egl1_1.16.0-1ubuntu1.1~18.04.3_amd64.deb ...\n",
      "Unpacking libwayland-egl1:amd64 (1.16.0-1ubuntu1.1~18.04.3) ...\n",
      "Selecting previously unselected package libwayland-egl1-mesa:amd64.\n",
      "Preparing to unpack .../67-libwayland-egl1-mesa_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libwayland-egl1-mesa:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libxkbcommon0:amd64.\n",
      "Preparing to unpack .../68-libxkbcommon0_0.8.2-1~ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking libxkbcommon0:amd64 (0.8.2-1~ubuntu18.04.1) ...\n",
      "Selecting previously unselected package libsdl2-2.0-0:amd64.\n",
      "Preparing to unpack .../69-libsdl2-2.0-0_2.0.8+dfsg1-1ubuntu1.18.04.4_amd64.deb ...\n",
      "Unpacking libsdl2-2.0-0:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\n",
      "Selecting previously unselected package libavdevice57:amd64.\n",
      "Preparing to unpack .../70-libavdevice57_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking libavdevice57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../71-ffmpeg_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\n",
      "Unpacking ffmpeg (7:3.4.8-0ubuntu0.2) ...\n",
      "Selecting previously unselected package libaacs0:amd64.\n",
      "Preparing to unpack .../72-libaacs0_0.9.0-1_amd64.deb ...\n",
      "Unpacking libaacs0:amd64 (0.9.0-1) ...\n",
      "Selecting previously unselected package libbdplus0:amd64.\n",
      "Preparing to unpack .../73-libbdplus0_0.1.2-2_amd64.deb ...\n",
      "Unpacking libbdplus0:amd64 (0.1.2-2) ...\n",
      "Selecting previously unselected package libogg-dev:amd64.\n",
      "Preparing to unpack .../74-libogg-dev_1.3.2-1_amd64.deb ...\n",
      "Unpacking libogg-dev:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libflac-dev:amd64.\n",
      "Preparing to unpack .../75-libflac-dev_1.3.2-1_amd64.deb ...\n",
      "Unpacking libflac-dev:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libllvm10:amd64.\n",
      "Preparing to unpack .../76-libllvm10_1%3a10.0.0-4ubuntu1~18.04.2_amd64.deb ...\n",
      "Unpacking libllvm10:amd64 (1:10.0.0-4ubuntu1~18.04.2) ...\n",
      "Selecting previously unselected package libvorbis-dev:amd64.\n",
      "Preparing to unpack .../77-libvorbis-dev_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbis-dev:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libsndfile1-dev.\n",
      "Preparing to unpack .../78-libsndfile1-dev_1.0.28-4ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsndfile1-dev (1.0.28-4ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package mesa-va-drivers:amd64.\n",
      "Preparing to unpack .../79-mesa-va-drivers_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking mesa-va-drivers:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package mesa-vdpau-drivers:amd64.\n",
      "Preparing to unpack .../80-mesa-vdpau-drivers_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking mesa-vdpau-drivers:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package i965-va-driver:amd64.\n",
      "Preparing to unpack .../81-i965-va-driver_2.1.0-0ubuntu1_amd64.deb ...\n",
      "Unpacking i965-va-driver:amd64 (2.1.0-0ubuntu1) ...\n",
      "Selecting previously unselected package va-driver-all:amd64.\n",
      "Preparing to unpack .../82-va-driver-all_2.1.0-3_amd64.deb ...\n",
      "Unpacking va-driver-all:amd64 (2.1.0-3) ...\n",
      "Selecting previously unselected package vdpau-driver-all:amd64.\n",
      "Preparing to unpack .../83-vdpau-driver-all_1.1.1-3ubuntu1_amd64.deb ...\n",
      "Unpacking vdpau-driver-all:amd64 (1.1.1-3ubuntu1) ...\n",
      "Setting up libvorbisfile3:amd64 (1.3.5-4.2) ...\n",
      "Setting up libpgm-5.2-0:amd64 (5.2.122~dfsg-2) ...\n",
      "Setting up libtwolame0:amd64 (0.3.13-3) ...\n",
      "Setting up libraw1394-11:amd64 (2.1.2-1) ...\n",
      "Setting up libx264-152:amd64 (2:0.152.2854+gite9a5903-2) ...\n",
      "Setting up libopenjp2-7:amd64 (2.3.0-2build0.18.04.1) ...\n",
      "Setting up libllvm10:amd64 (1:10.0.0-4ubuntu1~18.04.2) ...\n",
      "Setting up libwavpack1:amd64 (5.1.0-2ubuntu1.5) ...\n",
      "Setting up libaacs0:amd64 (0.9.0-1) ...\n",
      "Setting up libnuma1:amd64 (2.0.11-2.1ubuntu0.1) ...\n",
      "Setting up libflite1:amd64 (2.1-release-1) ...\n",
      "Setting up libsoxr0:amd64 (0.1.2-3) ...\n",
      "Setting up libssh-gcrypt-4:amd64 (0.8.0~20170825.94fa1e38-1ubuntu0.7) ...\n",
      "Setting up xkb-data (2.23.1-1ubuntu1.18.04.1) ...\n",
      "Setting up libbluray2:amd64 (1:1.0.2-3) ...\n",
      "Setting up libvdpau1:amd64 (1.1.1-3ubuntu1) ...\n",
      "Setting up libshine3:amd64 (3.1.1-1) ...\n",
      "Setting up libva2:amd64 (2.1.0-3) ...\n",
      "Setting up libiec61883-0:amd64 (1.2.0-2) ...\n",
      "Setting up libspeex1:amd64 (1.2~rc1.2-1ubuntu2) ...\n",
      "Setting up libfftw3-double3:amd64 (3.3.7-1) ...\n",
      "Setting up libxvidcore4:amd64 (2:1.3.5-1) ...\n",
      "Setting up libopus0:amd64 (1.1.2-1ubuntu1) ...\n",
      "Setting up libx265-146:amd64 (2.6-3) ...\n",
      "Setting up libopenal-data (1:1.18.2-2) ...\n",
      "Setting up libbs2b0:amd64 (3.1.0+dfsg-2.2) ...\n",
      "Setting up libnorm1:amd64 (1.5r6+dfsg1-6) ...\n",
      "Setting up i965-va-driver:amd64 (2.1.0-0ubuntu1) ...\n",
      "Setting up libsodium23:amd64 (1.0.16-2) ...\n",
      "Setting up libmp3lame0:amd64 (3.100-2) ...\n",
      "Setting up libusb-1.0-0:amd64 (2:1.0.21-2) ...\n",
      "Setting up libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-12) ...\n",
      "Setting up libsnappy1v5:amd64 (1.1.7-1) ...\n",
      "Setting up mesa-va-drivers:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libva-drm2:amd64 (2.1.0-3) ...\n",
      "Setting up libavc1394-0:amd64 (0.5.4-4build1) ...\n",
      "Setting up libzvbi-common (0.2.35-13) ...\n",
      "Setting up libfribidi0:amd64 (0.19.7-2) ...\n",
      "Setting up libxkbcommon0:amd64 (0.8.2-1~ubuntu18.04.1) ...\n",
      "Setting up libvpx5:amd64 (1.7.0-3ubuntu0.18.04.1) ...\n",
      "Setting up libgme0:amd64 (0.6.2-1) ...\n",
      "Setting up libbdplus0:amd64 (0.1.2-2) ...\n",
      "Setting up libzvbi0:amd64 (0.2.35-13) ...\n",
      "Setting up libva-x11-2:amd64 (2.1.0-3) ...\n",
      "Setting up libsamplerate0:amd64 (0.1.9-1) ...\n",
      "Setting up libsndio6.1:amd64 (1.1.0-3) ...\n",
      "Setting up libtheora0:amd64 (1.1.1+dfsg.1-14) ...\n",
      "Setting up libmpg123-0:amd64 (1.25.10-1) ...\n",
      "Setting up libogg-dev:amd64 (1.3.2-1) ...\n",
      "Setting up libslang2:amd64 (2.3.1a-3ubuntu1) ...\n",
      "Setting up libwayland-cursor0:amd64 (1.16.0-1ubuntu1.1~18.04.3) ...\n",
      "Setting up libgsm1:amd64 (1.0.13-4build1) ...\n",
      "Setting up libmysofa0:amd64 (0.6~dfsg0-3+deb10u1build1) ...\n",
      "Setting up libwayland-egl1:amd64 (1.16.0-1ubuntu1.1~18.04.3) ...\n",
      "Setting up libcdio17:amd64 (1.0.0-2ubuntu2) ...\n",
      "Setting up libvorbis-dev:amd64 (1.3.5-4.2) ...\n",
      "Setting up libzmq5:amd64 (4.2.5-1ubuntu0.2) ...\n",
      "Setting up libavutil55:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libopenmpt0:amd64 (0.3.6-1) ...\n",
      "Setting up mesa-vdpau-drivers:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libass9:amd64 (1:0.14.0-1) ...\n",
      "Setting up libflac-dev:amd64 (1.3.2-1) ...\n",
      "Setting up libdc1394-22:amd64 (2.2.5-1) ...\n",
      "Setting up libcdio-cdda2:amd64 (10.2+0.94+2-2build1) ...\n",
      "Setting up libswresample2:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up librubberband2:amd64 (1.8.1-7ubuntu2) ...\n",
      "Setting up libsdl2-2.0-0:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\n",
      "Setting up libswscale4:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up va-driver-all:amd64 (2.1.0-3) ...\n",
      "Setting up libwayland-egl1-mesa:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libcdio-paranoia2:amd64 (10.2+0.94+2-2build1) ...\n",
      "Setting up libpostproc54:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libjack-jackd2-0:amd64 (1.9.12~dfsg-2) ...\n",
      "Setting up libopenal1:amd64 (1:1.18.2-2) ...\n",
      "Setting up libcaca0:amd64 (0.99.beta19-2ubuntu0.18.04.2) ...\n",
      "Setting up vdpau-driver-all:amd64 (1.1.1-3ubuntu1) ...\n",
      "Setting up libsndfile1-dev (1.0.28-4ubuntu0.18.04.1) ...\n",
      "Setting up libavresample3:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libavcodec57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libchromaprint1:amd64 (1.4.3-1) ...\n",
      "Setting up libavformat57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libavfilter6:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up libavdevice57:amd64 (7:3.4.8-0ubuntu0.2) ...\n",
      "Setting up ffmpeg (7:3.4.8-0ubuntu0.2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "channels:\n",
      "  - conda-forge\n",
      "  - defaults\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/tione/notebook/envs/taac2021-tagging\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.0\n",
      "    - cudnn=7.6.0\n",
      "    - ipykernel\n",
      "    - python=3.7\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
      "    _openmp_mutex-4.5          |            1_gnu          22 KB  conda-forge\n",
      "    backcall-0.2.0             |     pyh9f0ad1d_0          13 KB  conda-forge\n",
      "    backports-1.0              |             py_2           4 KB  conda-forge\n",
      "    backports.functools_lru_cache-1.6.4|     pyhd8ed1ab_0           9 KB  conda-forge\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py37h89c1867_1         143 KB  conda-forge\n",
      "    cudatoolkit-10.0.130       |       hf841e97_8       336.5 MB  conda-forge\n",
      "    cudnn-7.6.0                |       cuda10.0_0       157.9 MB  defaults\n",
      "    decorator-5.0.7            |     pyhd8ed1ab_0          11 KB  conda-forge\n",
      "    ipykernel-5.5.3            |   py37h085eea5_0         166 KB  conda-forge\n",
      "    ipython-7.23.0             |   py37h085eea5_0         1.1 MB  conda-forge\n",
      "    ipython_genutils-0.2.0     |             py_1          21 KB  conda-forge\n",
      "    jedi-0.18.0                |   py37h89c1867_2         923 KB  conda-forge\n",
      "    jupyter_client-6.1.12      |     pyhd8ed1ab_0          79 KB  conda-forge\n",
      "    jupyter_core-4.7.1         |   py37h89c1867_0          72 KB  conda-forge\n",
      "    ld_impl_linux-64-2.35.1    |       hea4e1c9_2         618 KB  conda-forge\n",
      "    libffi-3.3                 |       h58526e2_2          51 KB  conda-forge\n",
      "    libgcc-ng-9.3.0            |      h2828fa1_19         7.8 MB  conda-forge\n",
      "    libgomp-9.3.0              |      h2828fa1_19         376 KB  conda-forge\n",
      "    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n",
      "    libstdcxx-ng-9.3.0         |      h6de172a_19         4.0 MB  conda-forge\n",
      "    matplotlib-inline-0.1.2    |     pyhd8ed1ab_2          11 KB  conda-forge\n",
      "    ncurses-6.2                |       h58526e2_4         985 KB  conda-forge\n",
      "    openssl-1.1.1k             |       h7f98852_0         2.1 MB  conda-forge\n",
      "    parso-0.8.2                |     pyhd8ed1ab_0          68 KB  conda-forge\n",
      "    pexpect-4.8.0              |     pyh9f0ad1d_2          47 KB  conda-forge\n",
      "    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n",
      "    pip-21.1.1                 |     pyhd8ed1ab_0         1.1 MB  conda-forge\n",
      "    prompt-toolkit-3.0.18      |     pyha770c72_0         244 KB  conda-forge\n",
      "    ptyprocess-0.7.0           |     pyhd3deb0d_0          16 KB  conda-forge\n",
      "    pygments-2.8.1             |     pyhd8ed1ab_0         736 KB  conda-forge\n",
      "    python-3.7.10              |hffdb5ce_100_cpython        57.3 MB  conda-forge\n",
      "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
      "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
      "    pyzmq-22.0.3               |   py37h336d617_1         524 KB  conda-forge\n",
      "    readline-8.1               |       h46c0cb4_0         295 KB  conda-forge\n",
      "    setuptools-49.6.0          |   py37h89c1867_3         947 KB  conda-forge\n",
      "    six-1.15.0                 |     pyh9f0ad1d_0          14 KB  conda-forge\n",
      "    sqlite-3.35.5              |       h74cdb3f_0         1.4 MB  conda-forge\n",
      "    tk-8.6.10                  |       h21135ba_1         3.2 MB  conda-forge\n",
      "    tornado-6.1                |   py37h5e8e339_1         646 KB  conda-forge\n",
      "    traitlets-5.0.5            |             py_0          81 KB  conda-forge\n",
      "    wcwidth-0.2.5              |     pyh9f0ad1d_2          33 KB  conda-forge\n",
      "    wheel-0.36.2               |     pyhd3deb0d_0          31 KB  conda-forge\n",
      "    xz-5.2.5                   |       h516909a_1         343 KB  conda-forge\n",
      "    zeromq-4.3.4               |       h9c3ff4c_0         352 KB  conda-forge\n",
      "    zlib-1.2.11                |    h516909a_1010         106 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       581.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-1_gnu\n",
      "  backcall           conda-forge/noarch::backcall-0.2.0-pyh9f0ad1d_0\n",
      "  backports          conda-forge/noarch::backports-1.0-py_2\n",
      "  backports.functoo~ conda-forge/noarch::backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2020.12.5-ha878542_0\n",
      "  certifi            conda-forge/linux-64::certifi-2020.12.5-py37h89c1867_1\n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-10.0.130-hf841e97_8\n",
      "  cudnn              pkgs/main/linux-64::cudnn-7.6.0-cuda10.0_0\n",
      "  decorator          conda-forge/noarch::decorator-5.0.7-pyhd8ed1ab_0\n",
      "  ipykernel          conda-forge/linux-64::ipykernel-5.5.3-py37h085eea5_0\n",
      "  ipython            conda-forge/linux-64::ipython-7.23.0-py37h085eea5_0\n",
      "  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-py_1\n",
      "  jedi               conda-forge/linux-64::jedi-0.18.0-py37h89c1867_2\n",
      "  jupyter_client     conda-forge/noarch::jupyter_client-6.1.12-pyhd8ed1ab_0\n",
      "  jupyter_core       conda-forge/linux-64::jupyter_core-4.7.1-py37h89c1867_0\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.35.1-hea4e1c9_2\n",
      "  libffi             conda-forge/linux-64::libffi-3.3-h58526e2_2\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_19\n",
      "  libgomp            conda-forge/linux-64::libgomp-9.3.0-h2828fa1_19\n",
      "  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1\n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_19\n",
      "  matplotlib-inline  conda-forge/noarch::matplotlib-inline-0.1.2-pyhd8ed1ab_2\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.2-h58526e2_4\n",
      "  openssl            conda-forge/linux-64::openssl-1.1.1k-h7f98852_0\n",
      "  parso              conda-forge/noarch::parso-0.8.2-pyhd8ed1ab_0\n",
      "  pexpect            conda-forge/noarch::pexpect-4.8.0-pyh9f0ad1d_2\n",
      "  pickleshare        conda-forge/noarch::pickleshare-0.7.5-py_1003\n",
      "  pip                conda-forge/noarch::pip-21.1.1-pyhd8ed1ab_0\n",
      "  prompt-toolkit     conda-forge/noarch::prompt-toolkit-3.0.18-pyha770c72_0\n",
      "  ptyprocess         conda-forge/noarch::ptyprocess-0.7.0-pyhd3deb0d_0\n",
      "  pygments           conda-forge/noarch::pygments-2.8.1-pyhd8ed1ab_0\n",
      "  python             conda-forge/linux-64::python-3.7.10-hffdb5ce_100_cpython\n",
      "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
      "  pyzmq              conda-forge/linux-64::pyzmq-22.0.3-py37h336d617_1\n",
      "  readline           conda-forge/linux-64::readline-8.1-h46c0cb4_0\n",
      "  setuptools         conda-forge/linux-64::setuptools-49.6.0-py37h89c1867_3\n",
      "  six                conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.35.5-h74cdb3f_0\n",
      "  tk                 conda-forge/linux-64::tk-8.6.10-h21135ba_1\n",
      "  tornado            conda-forge/linux-64::tornado-6.1-py37h5e8e339_1\n",
      "  traitlets          conda-forge/noarch::traitlets-5.0.5-py_0\n",
      "  wcwidth            conda-forge/noarch::wcwidth-0.2.5-pyh9f0ad1d_2\n",
      "  wheel              conda-forge/noarch::wheel-0.36.2-pyhd3deb0d_0\n",
      "  xz                 conda-forge/linux-64::xz-5.2.5-h516909a_1\n",
      "  zeromq             conda-forge/linux-64::zeromq-4.3.4-h9c3ff4c_0\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.11-h516909a_1010\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "cudnn-7.6.0          | 157.9 MB  | ##################################### | 100% \n",
      "python_abi-3.7       | 4 KB      | ##################################### | 100% \n",
      "jupyter_core-4.7.1   | 72 KB     | ##################################### | 100% \n",
      "libgcc-ng-9.3.0      | 7.8 MB    | ##################################### | 100% \n",
      "python-dateutil-2.8. | 220 KB    | ##################################### | 100% \n",
      "zeromq-4.3.4         | 352 KB    | ##################################### | 100% \n",
      "parso-0.8.2          | 68 KB     | ##################################### | 100% \n",
      "ipython-7.23.0       | 1.1 MB    | ##################################### | 100% \n",
      "ld_impl_linux-64-2.3 | 618 KB    | ##################################### | 100% \n",
      "python-3.7.10        | 57.3 MB   | ##################################### | 100% \n",
      "jedi-0.18.0          | 923 KB    | ##################################### | 100% \n",
      "wcwidth-0.2.5        | 33 KB     | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "wheel-0.36.2         | 31 KB     | ##################################### | 100% \n",
      "pyzmq-22.0.3         | 524 KB    | ##################################### | 100% \n",
      "setuptools-49.6.0    | 947 KB    | ##################################### | 100% \n",
      "backcall-0.2.0       | 13 KB     | ##################################### | 100% \n",
      "sqlite-3.35.5        | 1.4 MB    | ##################################### | 100% \n",
      "libgomp-9.3.0        | 376 KB    | ##################################### | 100% \n",
      "pexpect-4.8.0        | 47 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "tornado-6.1          | 646 KB    | ##################################### | 100% \n",
      "jupyter_client-6.1.1 | 79 KB     | ##################################### | 100% \n",
      "prompt-toolkit-3.0.1 | 244 KB    | ##################################### | 100% \n",
      "backports.functools_ | 9 KB      | ##################################### | 100% \n",
      "pygments-2.8.1       | 736 KB    | ##################################### | 100% \n",
      "decorator-5.0.7      | 11 KB     | ##################################### | 100% \n",
      "backports-1.0        | 4 KB      | ##################################### | 100% \n",
      "ptyprocess-0.7.0     | 16 KB     | ##################################### | 100% \n",
      "ncurses-6.2          | 985 KB    | ##################################### | 100% \n",
      "xz-5.2.5             | 343 KB    | ##################################### | 100% \n",
      "pip-21.1.1           | 1.1 MB    | ##################################### | 100% \n",
      "tk-8.6.10            | 3.2 MB    | ##################################### | 100% \n",
      "matplotlib-inline-0. | 11 KB     | ##################################### | 100% \n",
      "cudatoolkit-10.0.130 | 336.5 MB  | ##################################### | 100% \n",
      "six-1.15.0           | 14 KB     | ##################################### | 100% \n",
      "ipython_genutils-0.2 | 21 KB     | ##################################### | 100% \n",
      "ipykernel-5.5.3      | 166 KB    | ##################################### | 100% \n",
      "zlib-1.2.11          | 106 KB    | ##################################### | 100% \n",
      "_openmp_mutex-4.5    | 22 KB     | ##################################### | 100% \n",
      "openssl-1.1.1k       | 2.1 MB    | ##################################### | 100% \n",
      "readline-8.1         | 295 KB    | ##################################### | 100% \n",
      "libstdcxx-ng-9.3.0   | 4.0 MB    | ##################################### | 100% \n",
      "traitlets-5.0.5      | 81 KB     | ##################################### | 100% \n",
      "pickleshare-0.7.5    | 9 KB      | ##################################### | 100% \n",
      "libsodium-1.0.18     | 366 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "libffi-3.3           | 51 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - b'By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\\n'\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /home/tione/notebook/envs/taac2021-tagging\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "                      *  /home/tione/notebook/envs/taac2021-tagging\n",
      "base                     /opt/conda\n",
      "JupyterSystemEnv         /opt/conda/envs/JupyterSystemEnv\n",
      "mxnet_py2                /opt/conda/envs/mxnet_py2\n",
      "mxnet_py3                /opt/conda/envs/mxnet_py3\n",
      "python2                  /opt/conda/envs/python2\n",
      "python3                  /opt/conda/envs/python3\n",
      "pytorch_py2              /opt/conda/envs/pytorch_py2\n",
      "pytorch_py3              /opt/conda/envs/pytorch_py3\n",
      "tensorflow2_py3          /opt/conda/envs/tensorflow2_py3\n",
      "tensorflow_py2           /opt/conda/envs/tensorflow_py2\n",
      "tensorflow_py3           /opt/conda/envs/tensorflow_py3\n",
      "\n",
      "Installed kernelspec taac2021-tagging in /home/tione/.local/share/jupyter/kernels/taac2021-tagging\n",
      "/home/tione/notebook/VideoStructuring/MultiModal-Tagging\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting pyyaml\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 5.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting munch\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting resampy\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[K     |████████████████████████████████| 323 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting moviepy==1.0.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/18/54/01a8c4e35c75ca9724d19a7e4de9dc23f0ceb8769102c7de056113af61c3/moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[K     |████████████████████████████████| 388 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: ipython in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from -r requirement.txt (line 8)) (7.23.0)\n",
      "Collecting jupyter\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ce/63/74c0b6184b6b169b121bb72458818ee60a7d7c436d7b1907bd5874188c55/matplotlib-3.4.1-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.3 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/51/51/48f3fc47c4e2144da2806dfb6629c4dd1fa3d5a143f9652b141e979a8ca9/pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting xlrd\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a6/0c/c2a72d51fe56e08a08acc85d13013558a2d793028ae7385448a6ccdfae64/xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openpyxl\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/39/08/595298c9b7ced75e7d23be3e7596459980d63bc35112ca765ceccafbe9a4/openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tomorrow3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/83/df/9d1a0e45d25804bc88896000c875b846e8adbd406c923d305cc09d62bbed/tomorrow3-1.2.2-py3-none-any.whl (2.2 kB)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting requests<3.0,>=2.8.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting proglog<=1.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz (10 kB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio<3.0,>=2.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 83.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio_ffmpeg>=0.2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/89/0f/4b49476d185a273163fa648eaf1e7d4190661d1bbf37ec2975b84df9de02/imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.9 MB 492 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/33/34/542152297dcc6c47a9dcb0685eac6d652d878ed3cea83bf2b23cb988e857/Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r requirement.txt (line 6)) (2020.12.5)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from munch->-r requirement.txt (line 3)) (1.15.0)\n",
      "Collecting scipy>=0.13\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7d/e8/43ffca541d2f208d516296950b25fe1084b35c2881f4d444c1346ca75815/scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 27.4 MB 486 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.32\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bb/73/d9c127eddbe3c105a33379d425b88f9dca249a6eddf39ce886494d49c3f9/numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from numba>=0.32->resampy->-r requirement.txt (line 4)) (49.6.0.post20210108)\n",
      "Collecting llvmlite<0.37,>=0.36.0rc1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/54/25/2b4015e2b0c3be2efa6870cf2cf2bd969dd0e5f937476fc13c102209df32/llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 940 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cffi>=1.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/97/2d/cd29c79f2eb1384577d0662f23c89d29621152f14bef8c6b25747785744b/cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib-inline in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (3.0.18)\n",
      "Requirement already satisfied: pickleshare in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (4.8.0)\n",
      "Requirement already satisfied: backcall in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: pygments in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (2.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipython->-r requirement.txt (line 8)) (5.0.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from jedi>=0.16->ipython->-r requirement.txt (line 8)) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from pexpect>4.3->ipython->-r requirement.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirement.txt (line 8)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from traitlets>=4.2->ipython->-r requirement.txt (line 8)) (0.2.0)\n",
      "Collecting jupyter-console\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/59/cd/aa2670ffc99eb3e5bbe2294c71e4bf46a9804af4f378d09d7a8950996c9b/jupyter_console-6.4.0-py3-none-any.whl (22 kB)\n",
      "Collecting nbconvert\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/13/2f/acbe7006548f3914456ee47f97a2033b1b2f3daf921b12ac94105d87c163/nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "\u001b[K     |████████████████████████████████| 552 kB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting qtconsole\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/22/4d/94cb45a6f0c25a2693f7c8c0fe0814c3f52ba0f9c920ad75104005b31d42/qtconsole-5.0.3-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from jupyter->-r requirement.txt (line 9)) (5.5.3)\n",
      "Collecting notebook\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/5d/86/8f951abc6ac651a75a059d2b77fe99fa5df80bf4dc4700c126a0bee486b8/notebook-6.3.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipywidgets\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/11/53/084940a83a8158364e630a664a30b03068c25ab75243224d6b488800d43a/ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d2/46/231de802ade4225b76b96cffe419cf3ce52bbe92e3b092cf12db7d11c207/kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 22.5 MB/s eta 0:00:01     |████████████                    | 430 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from matplotlib->-r requirement.txt (line 10)) (2.8.1)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: jupyter-client in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirement.txt (line 9)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from ipykernel->jupyter->-r requirement.txt (line 9)) (6.1)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/18/b5/3473d275e3b2359efdf5768e9df95537308b93a31ad94fa92814ac565826/jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nbformat>=4.2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e7/c7/dd50978c637a7af8234909277c4e7ec1b71310c13fb3135f3c8f5b6e045f/nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirement.txt (line 9)) (4.7.1)\n",
      "Collecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyrsistent>=0.14.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4d/70/fd441df751ba8b620e03fd2d2d9ca902103119616f0f6cc42e6405035062/pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 116.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.4.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/8e/e2/49966924c93909d47612bb47d911448140a2f6c1390aec2f4c1afbe3748f/importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/22/f7/f6e1676521ce7e311d38563d2cf6594d09d3717d799ede7dab7b2520093e/prometheus_client-0.10.1-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7e/c2/1eece8c95ddbc9b1aeb64f5783a9e07a286de42191b7204d67b7496ddf35/Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Send2Trash>=1.5.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e0/d7/5da06217807106ed6d7b4f5ccb8ec5e3f9ec969217faad4b5d1af0b55101/argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting terminado>=0.8.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/47/ad/f7bc3e40570212ed9ccc9ac72d17315574d82fc100eb74d660c31817ddd2/terminado-0.9.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from notebook->jupyter->-r requirement.txt (line 9)) (22.0.3)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/2e/35/6c4fff5ab443b57116cb1aad46421fb719bed2825664e8fe77d66d99bcbc/typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c2/37/2e4def8ce3739a258998215df907f5815ecd1af71e62147f5eea2d12d4e8/MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/22/a6/f3a01a5c1a0e72d1d064f33d4cd9c3a782010f48f48f47f256d0b438994a/nbclient-0.5.3-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 6.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/28/78/bd59a9adb72fa139b1c9c186e6f65aebee52375a747e4b6a6dcf0880956f/pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a8/6f/c34288766797193b512c6508f5994b830fb06134fdc4ca8214daba0aa443/jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting bleach\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f0/46/2bbd92086a4c6f051214cb48df6d9132b5f32c5e881d3f4991b16ec7e499/bleach-3.3.0-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting entrypoints>=0.2.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/07/6c/aa3f2f849e01cb6a001cd8554a88d4c77c5c1a31c95bdf1cf9301e6d9ef4/defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting testpath\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nest-asyncio\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/52/e2/9b37da54e6e9094d2f558ae643d1954a0fa8215dfee4fa261f31c5439796/nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
      "Collecting async-generator\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting packaging\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/3e/89/7ea760b4daa42653ece2380531c90f64788d979110a2ab51049d92f408af/packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 11.1 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting webencodings\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting qtpy\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/cd/fd/9972948f02e967b691cc0ca1f26124826a3b88cb38f412a8b7935b8c3c72/QtPy-1.9.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: moviepy, gast, proglog, resampy, pyrsistent, pandocfilters\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110726 sha256=26661c8d39630d454c74d2e296a3033b97409751df246788e26bd82ef3bedd6e\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/93/54/14/b414559b156ddfc1b086909ec483b86e4fadb0d61778b87e8d\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=be25a8de7a43efa9926c663275c72e48065566a76ac9d75d79750ecd82b7ae10\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/c6/d6/a4/74c83546cf7e6299f4e77042c21be880d1e35dd08f249201cb\n",
      "  Building wheel for proglog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for proglog: filename=proglog-0.1.9-py3-none-any.whl size=6147 sha256=39a84a27aced28b00d7290c3693aa2a7cf85abd8e9ce88f98b3601b3fa774105\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/53/60/aa/3da639dc7620b486ff0d7ac0969dd194f8170fb30ab84c3e75\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320718 sha256=b2371852800ccfc9e04e31fb1102f07c9facdd7b3422070da555b543a5f572c8\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/63/d6/b8/3d9da69515eecf9ddea10e534e9951aef59195f631e1dbdd94\n",
      "  Building wheel for pyrsistent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp37-cp37m-linux_x86_64.whl size=115273 sha256=5b232e0f43b693916b85a5476e878e945b5137eee8da37d70b1c50e2a2cbfb68\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/4c/01/57/56eee792e9a58561c8fe444efccd32b29416c47d4e0f329401\n",
      "  Building wheel for pandocfilters (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=7992 sha256=1572eb3b1c40b2c34757b019b177b60707a9be66ade5c7ecb4607c7e4d83e9b6\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/3b/22/5e/eaa891d992b5b191a5d06afc5f8ef0ad8b841065e14bffc077\n",
      "Successfully built moviepy gast proglog resampy pyrsistent pandocfilters\n",
      "Installing collected packages: zipp, typing-extensions, pyrsistent, importlib-metadata, attrs, pyparsing, jsonschema, webencodings, pycparser, packaging, nest-asyncio, nbformat, MarkupSafe, decorator, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, entrypoints, defusedxml, cffi, bleach, terminado, Send2Trash, prometheus-client, nbconvert, argon2-cffi, notebook, widgetsnbextension, urllib3, tqdm, qtpy, pillow, numpy, llvmlite, jupyterlab-widgets, idna, chardet, scipy, requests, qtconsole, pytz, proglog, numba, kiwisolver, jupyter-console, ipywidgets, imageio-ffmpeg, imageio, et-xmlfile, cycler, xlrd, tomorrow3, soundfile, resampy, pyyaml, pandas, openpyxl, munch, moviepy, matplotlib, jupyter, gast\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.0.7\n",
      "    Uninstalling decorator-5.0.7:\n",
      "      Successfully uninstalled decorator-5.0.7\n",
      "Successfully installed MarkupSafe-1.1.1 Send2Trash-1.5.0 argon2-cffi-20.1.0 async-generator-1.10 attrs-20.3.0 bleach-3.3.0 cffi-1.14.5 chardet-4.0.0 cycler-0.10.0 decorator-4.4.2 defusedxml-0.7.1 entrypoints-0.3 et-xmlfile-1.1.0 gast-0.2.2 idna-2.10 imageio-2.9.0 imageio-ffmpeg-0.4.3 importlib-metadata-4.0.1 ipywidgets-7.6.3 jinja2-2.11.3 jsonschema-3.2.0 jupyter-1.0.0 jupyter-console-6.4.0 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.0 kiwisolver-1.3.1 llvmlite-0.36.0 matplotlib-3.4.1 mistune-0.8.4 moviepy-1.0.3 munch-2.5.0 nbclient-0.5.3 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.5.1 notebook-6.3.0 numba-0.53.1 numpy-1.20.2 openpyxl-3.0.7 packaging-20.9 pandas-1.2.4 pandocfilters-1.4.3 pillow-8.2.0 proglog-0.1.9 prometheus-client-0.10.1 pycparser-2.20 pyparsing-2.4.7 pyrsistent-0.17.3 pytz-2021.1 pyyaml-5.4.1 qtconsole-5.0.3 qtpy-1.9.0 requests-2.25.1 resampy-0.2.2 scipy-1.6.3 soundfile-0.10.3.post1 terminado-0.9.4 testpath-0.4.4 tomorrow3-1.2.2 tqdm-4.60.0 typing-extensions-3.10.0.0 urllib3-1.26.4 webencodings-0.5.1 widgetsnbextension-3.5.1 xlrd-2.0.1 zipp-3.4.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorflow-gpu==1.14\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/32/67/559ca8408431c37ad3a17e859c8c291ea82f092354074baef482b98ffb7b/tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 377.1 MB 497 kB/s eta 0:00:014     |██▊                             | 31.5 MB 107.9 MB/s eta 0:00:04     |███▍                            | 39.5 MB 107.9 MB/s eta 0:00:04     |███████████▉                    | 139.6 MB 43.6 MB/s eta 0:00:06MB 1.5 MB/s eta 0:01:18██         | 270.5 MB 1.5 MB/s eta 0:01:14��████████████████▌       | 288.7 MB 1.5 MB/s eta 0:00:59     |█████████████████████████▋      | 301.4 MB 4.1 MB/s eta 0:00:19     |██████████████████████████▌     | 312.8 MB 1.4 MB/s eta 0:00:47 | 314.7 MB 1.4 MB/s eta 0:00:45    |███████████████████████████▍    | 323.1 MB 2.2 MB/s eta 0:00:25��███████████████████▊   | 338.4 MB 1.5 MB/s eta 0:00:26��████████████████████   | 341.6 MB 1.5 MB/s eta 0:00:24█████████▉| 374.8 MB 497 kB/s eta 0:00:05\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0f/13/192104516c4a3d92dc6b5e106ffcfbf0fe35f3c4faa49650205ff652af72/opencv_python-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (50.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.4 MB 1.0 MB/s eta 0:00:013                  | 12.3 MB 1.7 MB/s eta 0:00:22     |█████████████████▍              | 27.4 MB 474 kB/s eta 0:00:49████████▋             | 29.3 MB 474 kB/s eta 0:00:450:42     |██████████████████████████▎     | 41.4 MB 1.1 MB/s eta 0:00:09�██████████████████████▎    | 43.0 MB 1.1 MB/s eta 0:00:07ta 0:00:05\n",
      "\u001b[?25hCollecting torch==1.2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/05/65/5248be50c55ab7429dd5c11f5e2f9f5865606b80e854ca63139ad1a584f2/torch-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (748.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.9 MB 478 kB/s eta 0:00:01112 MB 71 kB/s eta 2:43:1525 |████▎                           | 101.0 MB 650 kB/s eta 0:16:37�                           | 110.0 MB 568 kB/s eta 0:18:45█████▋                          | 132.0 MB 163 kB/s eta 1:02:53    |██████                          | 139.2 MB 531 kB/s eta 0:19:07a 0:18:540:18:320:17:55     |██████▉                         | 161.2 MB 549 kB/s eta 0:17:50███▍                        | 172.0 MB 530 kB/s eta 0:18:08▋                        | 178.0 MB 530 kB/s eta 0:17:57                | 186.0 MB 537 kB/s eta 0:17:272211 MB 564 kB/s eta 0:16:09.9 MB 564 kB/s eta 0:15:5812K     |█████████▏                      | 215.0 MB 553 kB/s eta 0:16:06              | 220.9 MB 553 kB/s eta 0:15:55███████▉                     | 253.5 MB 602 kB/s eta 0:13:43��██████▏                    | 260.6 MB 584 kB/s eta 0:13:56                    | 272.8 MB 575 kB/s eta 0:13:47B/s eta 0:13:20        | 293.8 MB 594 kB/s eta 0:12:46��█████████                   | 303.5 MB 653 kB/s eta 0:11:22         | 313.0 MB 616 kB/s eta 0:11:48521 kB/s eta 0:13:39564 kB/s eta 0:12:24�███████████▌                 | 340.4 MB 532 kB/s eta 0:12:47███▊                 | 345.2 MB 532 kB/s eta 0:12:38 0:12:34████                 | 352.4 MB 2.8 MB/s eta 0:02:21    |███████████████▎                | 356.6 MB 2.8 MB/s eta 0:02:20    |███████████████▍                | 361.3 MB 575 kB/s eta 0:11:14     |████████████████                | 373.3 MB 471 kB/s eta 0:13:17��████████████                | 376.4 MB 471 kB/s eta 0:13:106 MB 471 kB/s eta 0:13:01��█████▍               | 384.6 MB 551 kB/s eta 0:11:01 |████████████████▋               | 389.0 MB 551 kB/s eta 0:10:53  | 399.6 MB 820 kB/s eta 0:07:06  | 421.1 MB 155 kB/s eta 0:35:10�████▏             | 426.0 MB 155 kB/s eta 0:34:39�█████████████████▎             | 428.5 MB 3.0 MB/s eta 0:01:48████████████████▌             | 433.1 MB 3.0 MB/s eta 0:01:47     |██████████████████▋             | 436.7 MB 525 kB/s eta 0:09:55��████████▉             | 439.8 MB 525 kB/s eta 0:09:49�████████████            | 465.7 MB 256 kB/s eta 0:18:27��███████████████████            | 469.7 MB 256 kB/s eta 0:18:11��████████████████▎           | 475.2 MB 688 kB/s eta 0:06:38�████████▌           | 481.0 MB 688 kB/s eta 0:06:30██████████▉           | 486.3 MB 650 kB/s eta 0:06:44�███████████████████           | 491.8 MB 157 kB/s eta 0:27:16�██▎          | 497.7 MB 157 kB/s eta 0:26:38502.3 MB 648 kB/s eta 0:06:21�██▊          | 508.2 MB 648 kB/s eta 0:06:11512.8 MB 601 kB/s eta 0:06:33 |██████████████████████▎         | 520.5 MB 601 kB/s eta 0:06:2084 kB/s eta 0:05:25 |██████████████████████▊         | 532.1 MB 684 kB/s eta 0:05:17MB 599 kB/s eta 0:06:01MB 599 kB/s eta 0:05:54     |███████████████████████         | 537.7 MB 599 kB/s eta 0:05:53█████▏        | 543.3 MB 643 kB/s eta 0:05:20��██▍        | 546.9 MB 643 kB/s eta 0:05:15 0:21:06█████████████████▉        | 556.7 MB 153 kB/s eta 0:20:51██████████████████        | 562.8 MB 153 kB/s eta 0:20:11████████████▎       | 567.1 MB 153 kB/s eta 0:19:43B 626 kB/s eta 0:04:43��████████████████▌       | 572.2 MB 626 kB/s eta 0:04:42    |████████████████████████▊       | 577.3 MB 518 kB/s eta 0:05:32��███████████████████████       | 585.8 MB 518 kB/s eta 0:05:15█████████████▎      | 590.4 MB 46.5 MB/s eta 0:00:04███▍      | 593.1 MB 46.5 MB/s eta 0:00:04B 654 kB/s eta 0:03:29 | 613.2 MB 654 kB/s eta 0:03:28████████████████████▎     | 615.9 MB 654 kB/s eta 0:03:24███████████████████▍     | 618.6 MB 654 kB/s eta 0:03:20███████████████████▊     | 625.6 MB 4.4 MB/s eta 0:00:280:28��████████▎    | 639.3 MB 633 kB/s eta 0:02:54K     |███████████████████████████▌    | 642.4 MB 640 kB/s eta 0:02:47�██████▋    | 645.6 MB 640 kB/s eta 0:02:42█████████████████████████▊    | 648.3 MB 640 kB/s eta 0:02:37K     |███████████████████████████▊    | 649.4 MB 640 kB/s eta 0:02:36██████▉    | 651.1 MB 640 kB/s eta 0:02:33   |████████████████████████████    | 653.1 MB 640 kB/s eta 0:02:30�████████    | 653.5 MB 627 kB/s eta 0:02:33█████████    | 654.7 MB 627 kB/s eta 0:02:31███████    | 656.6 MB 627 kB/s eta 0:02:28a 0:02:25��████████████████▏   | 660.2 MB 627 kB/s eta 0:02:22��████████████████████████▌   | 666.5 MB 68.1 MB/s eta 0:00:02 |████████████████████████████▋   | 668.8 MB 68.1 MB/s eta 0:00:02��████▊   | 670.9 MB 68.1 MB/s eta 0:00:02�███████████▊   | 672.2 MB 68.1 MB/s eta 0:00:02████████████████████████   | 679.2 MB 153.8 MB/s eta 0:00:01�████████████▏  | 682.2 MB 153.8 MB/s eta 0:00:01██████████████▎  | 686.2 MB 2.0 MB/s eta 0:00:31�███████████████████████████▍  | 687.0 MB 2.0 MB/s eta 0:00:31██████████████▌  | 691.5 MB 469 kB/s eta 0:02:03�██████████████████████▋  | 693.1 MB 469 kB/s eta 0:01:59�███████████████████████████▊  | 694.6 MB 469 kB/s eta 0:01:56�██████████████████████▉  | 698.4 MB 469 kB/s eta 0:01:48    |█████████████████████████████▉  | 699.1 MB 469 kB/s eta 0:01:47�███████████████████████  | 701.3 MB 469 kB/s eta 0:01:42██████████████████████████████  | 703.9 MB 3.4 MB/s eta 0:00:14�███████████▏ | 705.2 MB 3.4 MB/s eta 0:00:13█████▎ | 707.5 MB 3.4 MB/s eta 0:00:139 MB 3.4 MB/s eta 0:00:12�█████████████▌ | 712.8 MB 3.4 MB/s eta 0:00:11eta 0:00:47��█████████████████████▋ | 716.3 MB 739 kB/s eta 0:00:45��███████████████ | 723.9 MB 68.8 MB/s eta 0:00:01��███████████████ | 727.1 MB 68.8 MB/s eta 0:00:01:00:0101�████████████████▍| 734.6 MB 72.8 MB/s eta 0:00:01█████████████▌| 737.8 MB 72.8 MB/s eta 0:00:01�████████████▋| 738.5 MB 72.8 MB/s eta 0:00:01███████████████▋| 740.8 MB 72.8 MB/s eta 0:00:01██████████████████████████▉| 745.3 MB 478 kB/s eta 0:00:0804\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/31/d8/1bfe90cc49c166dd2ec1be46fa4830c254ce702004a110830c74ec1df0c0/grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from tensorflow-gpu==1.14) (1.20.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from tensorflow-gpu==1.14) (0.36.2)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "\u001b[K     |████████████████████████████████| 488 kB 31.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/51/4e/de63de3cd9a83d3c1753a4566b11fc9d90b845f2448a132cfd36d3cb3cd1/protobuf-3.15.8-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/b3/c5/94e2444eb691f658fb8e3cf6cde3ae29540cf6d9ce76f0561afcdbb89136/h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (49.6.0.post20210108)\n",
      "Requirement already satisfied: importlib-metadata in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (4.0.1)\n",
      "Collecting cached-property\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.10.0.0)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=dac896376205221d919960a79b3ad46b94ec027d0039c7c7e0660cb3f0abfe3c\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/4f/05/b6/8f9f374f50e6ab52dd51f0271cb5301831b4da12b787e4eab0\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=71027 sha256=e9982ec3342d3abf811541d8410b9657ba8d655d8eedef7a2d858ba3d053b6c6\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/62/4e/1a/2cd61915c5fea0936c6a29c62025c92778a5eb3432fc250aa3\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: cached-property, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, google-pasta, astor, torch, tensorflow-gpu, opencv-python\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 cached-property-1.5.2 google-pasta-0.2.0 grpcio-1.37.1 h5py-3.2.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 opencv-python-4.5.1.48 protobuf-3.15.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0 termcolor-1.1.0 torch-1.2.0 werkzeug-1.0.1 wrapt-1.12.1\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2021-05-02 20:45:24.588076: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-05-02 20:45:24.604949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-05-02 20:45:24.605418: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fde6510c9b0 executing computations on platform Host. Devices:\n",
      "2021-05-02 20:45:24.605446: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-05-02 20:45:24.606597: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-02 20:45:24.621372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.622370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 20:45:24.622613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:45:24.623938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 20:45:24.625266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 20:45:24.625558: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 20:45:24.627229: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 20:45:24.628524: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 20:45:24.632447: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 20:45:24.632590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.633614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.634544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 20:45:24.634594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:45:24.719569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 20:45:24.719604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 20:45:24.719615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 20:45:24.719775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.720781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.721735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:24.722684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 30556 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "2021-05-02 20:45:24.724532: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fde68b0a410 executing computations on platform CUDA. Devices:\n",
      "2021-05-02 20:45:24.724555: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2021-05-02 20:45:26.144382: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-05-02 20:45:26.161475: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-05-02 20:45:26.161939: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3b1b9ff360 executing computations on platform Host. Devices:\n",
      "2021-05-02 20:45:26.161966: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-05-02 20:45:26.163112: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-02 20:45:26.177566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.178547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 20:45:26.178798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:45:26.180179: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 20:45:26.181489: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 20:45:26.181758: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 20:45:26.183401: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 20:45:26.184672: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 20:45:26.188552: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 20:45:26.188680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.189687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.190612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 20:45:26.190647: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:45:26.268840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 20:45:26.268884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 20:45:26.268895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 20:45:26.269055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.270044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.270993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:45:26.271918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 30556 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "2021-05-02 20:45:26.273713: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3b1f3950b0 executing computations on platform CUDA. Devices:\n",
      "2021-05-02 20:45:26.273734: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15485924808871563179\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15433416831113357709\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32040825652\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5589303978851795207\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16363166467332820553\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "[TensorFlow]\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "1.14.0\n",
      "[NumPy]\n",
      "1.20.2\n",
      "[Torch]\n",
      "1.2.0\n",
      "[OpenCV]\n",
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod a+x ./VideoStructuring/init.sh && ./VideoStructuring/init.sh run  # 加入run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好环境后，可以测试一下环境是否准备成功。  \n",
    "将 notebook 的右上角 kernel 换成 TAAC2021 (taac2021-tagging)，然后执行下述代码，如果是 tf1.14 则说明环境准备成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以运行一下下述命令，看看虚拟环境是否正常创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         /home/tione/notebook/envs/taac2021-tagging\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs | grep taac2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型之前需要链接一下数据集，将VideoStructuring/dataset链接到/home/tione/notebook/algo-2021/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -s /home/tione/notebook/algo-2021/dataset /home/tione/notebook/VideoStructuring/dataset  # 不用跑这行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于标签预测，需要完成特征提取（./run.sh extract）与数据集生成（./run.sh gt）两项任务。  \n",
    "为了简化操作，baseline已完成了前置步骤，用户可以直接使用下列脚本进行视频标签模型的训练。  \n",
    "训练完成后，模型存在于VideoStructuring/MultiModal-Tagging/checkpoints/目录下。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_CONFIG_ROOT_PREFIX= root_prefix: /opt/conda\n",
      "CONDA_ROOT= /opt/conda\n",
      "CONDA_NEW_ENV= taac2021-tagging\n",
      "JUPYTER_ROOT= /home/tione/notebook\n",
      "CODE_ROOT= /home/tione/notebook/VideoStructuring\n",
      "DATASET_ROOT= /home/tione/notebook/VideoStructuring/dataset\n",
      "OS_ID= ubuntu\n",
      "# conda environments:\n",
      "#\n",
      "                      *  /home/tione/notebook/envs/taac2021-tagging\n",
      "base                     /opt/conda\n",
      "JupyterSystemEnv         /opt/conda/envs/JupyterSystemEnv\n",
      "mxnet_py2                /opt/conda/envs/mxnet_py2\n",
      "mxnet_py3                /opt/conda/envs/mxnet_py3\n",
      "python2                  /opt/conda/envs/python2\n",
      "python3                  /opt/conda/envs/python3\n",
      "pytorch_py2              /opt/conda/envs/pytorch_py2\n",
      "pytorch_py3              /opt/conda/envs/pytorch_py3\n",
      "tensorflow2_py3          /opt/conda/envs/tensorflow2_py3\n",
      "tensorflow_py2           /opt/conda/envs/tensorflow_py2\n",
      "tensorflow_py3           /opt/conda/envs/tensorflow_py3\n",
      "\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2021-05-02 20:47:27.432489: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-05-02 20:47:27.449297: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-05-02 20:47:27.449732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5cf944bc60 executing computations on platform Host. Devices:\n",
      "2021-05-02 20:47:27.449764: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-05-02 20:47:27.450961: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-02 20:47:27.465789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.466880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 20:47:27.467120: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:47:27.468468: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 20:47:27.469742: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 20:47:27.470012: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 20:47:27.471759: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 20:47:27.473080: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 20:47:27.476939: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 20:47:27.477069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.478142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.479151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 20:47:27.479202: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 20:47:27.567653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 20:47:27.567690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 20:47:27.567702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 20:47:27.567865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.568874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.569836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 20:47:27.570798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 30556 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "2021-05-02 20:47:27.572562: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5cfce75410 executing computations on platform CUDA. Devices:\n",
      "2021-05-02 20:47:27.572587: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17825386891482782826\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10376953831072922248\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32040825652\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6880130078209263472\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14995286277825640701\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "1.14.0\n",
      "1.20.2\n",
      "1.2.0\n",
      "[Info] TYPE is train\n",
      "/home/tione/notebook/VideoStructuring/MultiModal-Tagging\n",
      "[Warning] CONFIG_FILE is not set for TYPE= train, using default: /home/tione/notebook/VideoStructuring/MultiModal-Tagging/configs/config.tagging.5k.yaml\n",
      "[Info] train with config= /home/tione/notebook/VideoStructuring/MultiModal-Tagging/configs/config.tagging.5k.yaml\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/text_head/bert_model.py:5: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "/home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:393: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open(config_path))\n",
      "{'ModelConfig': {'model_type': 'NextVladBERT', 'use_modal_drop': True, 'with_embedding_bn': False, 'modal_drop_rate': 0.3, 'with_video_head': True, 'with_audio_head': True, 'with_text_head': True, 'with_image_head': True, 'video_head_type': 'NeXtVLAD', 'video_head_params': {'nextvlad_cluster_size': 128, 'groups': 16, 'expansion': 2, 'feature_size': 1024, 'max_frames': 300}, 'audio_head_type': 'NeXtVLAD', 'audio_head_params': {'nextvlad_cluster_size': 64, 'groups': 16, 'expansion': 2, 'feature_size': 128, 'max_frames': 300}, 'text_head_type': 'BERT', 'text_head_params': {'bert_config': {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 21128}, 'bert_emb_encode_size': 1024}, 'image_head_type': 'resnet_v2_50', 'image_head_params': {}, 'fusion_head_type': 'SE', 'fusion_head_params': {'hidden1_size': 1024, 'gating_reduction': 8, 'drop_rate': {'video': 0.8, 'audio': 0.5, 'image': 0.5, 'text': 0.5, 'fusion': 0.8}}, 'tagging_classifier_type': 'LogisticModel', 'tagging_classifier_params': {'num_classes': 82}}, 'OptimizerConfig': {'optimizer': 'AdamOptimizer', 'optimizer_init_params': {}, 'clip_gradient_norm': 1.0, 'learning_rate_dict': {'video': 0.0001, 'audio': 0.0001, 'text': 1e-05, 'image': 0.0001, 'classifier': 0.01}, 'loss_type_dict': {'tagging': 'CrossEntropyLoss'}, 'max_step_num': 10000, 'export_model_steps': 1000, 'learning_rate_decay': 0.1, 'start_new_model': True, 'num_gpu': 1, 'log_device_placement': False, 'gpu_allow_growth': True, 'pretrained_model': {'text_pretrained_model': 'pretrained/bert/chinese_L-12_H-768_A-12/bert_model.ckpt', 'image_pretrained_model': 'pretrained/resnet_v2_50/resnet_v2_50.ckpt'}, 'train_dir': './checkpoints/tagging5k_temp'}, 'DatasetConfig': {'batch_size': 32, 'shuffle': True, 'train_data_source_list': {'train799': {'file': '../dataset/tagging/GroundTruth/datafile/train.txt', 'batch_size': 32}}, 'valid_data_source_list': {'val799': {'file': '../dataset/tagging/GroundTruth/datafile/val.txt', 'batch_size': 32}}, 'preprocess_root': 'src/dataloader/preprocess/', 'preprocess_config': {'feature': [{'name': 'video,video_frames_num,idx', 'shape': [[300, 1024], [], []], 'dtype': 'float32,int32,string', 'class': 'frames_npy_preprocess.Preprocess', 'extra_args': {'max_frames': 300, 'feat_dim': 1024, 'return_frames_num': True, 'return_idx': True}}, {'name': 'audio,audio_frames_num', 'shape': [[300, 128], []], 'dtype': 'float32,int32', 'class': 'frames_npy_preprocess.Preprocess', 'extra_args': {'max_frames': 300, 'feat_dim': 128, 'return_frames_num': True}}, {'name': 'image', 'shape': [[224, 224, 3]], 'dtype': 'float32', 'class': 'image_preprocess.Preprocess'}, {'name': 'text', 'shape': [[128]], 'dtype': 'int64', 'class': 'text_preprocess.Preprocess', 'extra_args': {'vocab': 'pretrained/bert/chinese_L-12_H-768_A-12/vocab.txt', 'max_len': 128}}], 'label': [{'name': 'tagging', 'dtype': 'float32', 'shape': [[82]], 'class': 'label_preprocess.Preprocess_label_sparse_to_dense', 'extra_args': {'index_dict': '../dataset/label_id.txt'}}]}}}\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:403: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:403: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:404: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:/job:master/task:0: Tensorflow version: 1.14.0.\n",
      "6\n",
      "Train Source sample_count:  train799 4500\n",
      "Train Source batch_num:  train799 140\n",
      "Valid Source:  val799 500\n",
      "Valid Source batch_num:  val799 15\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/frames_npy_preprocess.py:57: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/frames_npy_preprocess.py:60: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/frames_npy_preprocess.py:62: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/cnn_preprocessing/inception_preprocessing.py:148: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/cnn_preprocessing/inception_preprocessing.py:38: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/cnn_preprocessing/inception_preprocessing.py:228: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From src/dataloader/preprocess/cnn_preprocessing/inception_preprocessing.py:292: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/dataloader/dataloader.py:35: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/video_head/nextvlad.py:45: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/text_head/bert_base.py:354: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/text_head/bert_base.py:667: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/text_head/bert_model.py:19: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/export_model.py:81: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/export_model.py:35: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! modal_name_list: ['video', 'audio', 'text', 'image']\n",
      "INFO:tensorflow:/job:master/task:0: Removing existing train directory.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:359: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:361: The name tf.logging.error is deprecated. Please use tf.compat.v1.logging.error instead.\n",
      "\n",
      "ERROR:tensorflow:/job:master/task:0: Failed to delete directory ./checkpoints/tagging5k_temp when starting a new model. Please delete it manually and try again.\n",
      "INFO:tensorflow:Using the following GPUs to train: ['/device:GPU:0']\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/train_util.py:21: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:97: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:102: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/model/models/nextvlad_bert.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "tower/video/fully_connected/weights:0\n",
      "tower/video/fully_connected/biases:0\n",
      "tower/video/fully_connected_1/weights:0\n",
      "tower/video/fully_connected_1/biases:0\n",
      "tower/video/cluster_weights:0\n",
      "tower/video/cluster_bn/gamma:0\n",
      "tower/video/cluster_bn/beta:0\n",
      "tower/video/cluster_weights2:0\n",
      "tower/video/vlad_bn/gamma:0\n",
      "tower/video/vlad_bn/beta:0\n",
      "tower/tag_classifier/v/hidden1_weights:0\n",
      "tower/tag_classifier/v/hidden1_bn/gamma:0\n",
      "tower/tag_classifier/v/hidden1_bn/beta:0\n",
      "tower/tag_classifier/v/gating_weights_1:0\n",
      "tower/tag_classifier/v/gating_bn/beta:0\n",
      "tower/tag_classifier/v/gating_bn/gamma:0\n",
      "tower/tag_classifier/v/gating_weights_2:0\n",
      "tower/tag_classifier/v/fully_connected/weights:0\n",
      "tower/tag_classifier/v/fully_connected/biases:0\n",
      "tower/audio/fully_connected/weights:0\n",
      "tower/audio/fully_connected/biases:0\n",
      "tower/audio/fully_connected_1/weights:0\n",
      "tower/audio/fully_connected_1/biases:0\n",
      "tower/audio/cluster_weights:0\n",
      "tower/audio/cluster_bn/gamma:0\n",
      "tower/audio/cluster_bn/beta:0\n",
      "tower/audio/cluster_weights2:0\n",
      "tower/audio/vlad_bn/gamma:0\n",
      "tower/audio/vlad_bn/beta:0\n",
      "tower/tag_classifier/a/hidden1_weights:0\n",
      "tower/tag_classifier/a/hidden1_bn/gamma:0\n",
      "tower/tag_classifier/a/hidden1_bn/beta:0\n",
      "tower/tag_classifier/a/gating_weights_1:0\n",
      "tower/tag_classifier/a/gating_bn/beta:0\n",
      "tower/tag_classifier/a/gating_bn/gamma:0\n",
      "tower/tag_classifier/a/gating_weights_2:0\n",
      "tower/tag_classifier/a/fully_connected/weights:0\n",
      "tower/tag_classifier/a/fully_connected/biases:0\n",
      "tower/text/bert/embeddings/word_embeddings:0\n",
      "tower/text/bert/embeddings/token_type_embeddings:0\n",
      "tower/text/bert/embeddings/position_embeddings:0\n",
      "tower/text/bert/embeddings/LayerNorm/beta:0\n",
      "tower/text/bert/embeddings/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_0/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_0/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_0/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_0/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_0/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_0/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_0/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_0/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_0/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_0/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_1/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_1/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_1/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_1/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_1/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_1/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_1/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_1/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_1/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_1/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_2/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_2/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_2/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_2/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_2/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_2/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_2/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_2/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_2/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_2/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_3/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_3/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_3/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_3/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_3/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_3/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_3/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_3/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_3/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_3/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_4/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_4/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_4/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_4/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_4/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_4/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_4/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_4/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_4/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_4/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_5/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_5/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_5/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_5/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_5/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_5/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_5/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_5/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_5/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_5/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_6/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_6/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_6/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_6/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_6/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_6/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_6/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_6/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_6/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_6/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_7/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_7/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_7/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_7/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_7/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_7/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_7/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_7/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_7/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_7/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_8/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_8/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_8/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_8/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_8/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_8/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_8/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_8/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_8/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_8/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_9/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_9/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_9/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_9/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_9/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_9/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_9/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_9/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_9/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_9/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_10/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_10/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_10/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_10/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_10/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_10/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_10/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_10/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_10/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_10/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/query/kernel:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/query/bias:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/key/kernel:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/key/bias:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/value/kernel:0\n",
      "tower/text/bert/encoder/layer_11/attention/self/value/bias:0\n",
      "tower/text/bert/encoder/layer_11/attention/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_11/attention/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_11/attention/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0\n",
      "tower/text/bert/encoder/layer_11/intermediate/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_11/intermediate/dense/bias:0\n",
      "tower/text/bert/encoder/layer_11/output/dense/kernel:0\n",
      "tower/text/bert/encoder/layer_11/output/dense/bias:0\n",
      "tower/text/bert/encoder/layer_11/output/LayerNorm/beta:0\n",
      "tower/text/bert/encoder/layer_11/output/LayerNorm/gamma:0\n",
      "tower/text/bert/pooler/dense/kernel:0\n",
      "tower/text/bert/pooler/dense/bias:0\n",
      "tower/text/text_features/kernel:0\n",
      "tower/text/text_features/bias:0\n",
      "tower/text/batch_normalization/gamma:0\n",
      "tower/text/batch_normalization/beta:0\n",
      "tower/tag_classifier/t/hidden1_weights:0\n",
      "tower/tag_classifier/t/hidden1_bn/gamma:0\n",
      "tower/tag_classifier/t/hidden1_bn/beta:0\n",
      "tower/tag_classifier/t/gating_weights_1:0\n",
      "tower/tag_classifier/t/gating_bn/beta:0\n",
      "tower/tag_classifier/t/gating_bn/gamma:0\n",
      "tower/tag_classifier/t/gating_weights_2:0\n",
      "tower/tag_classifier/t/fully_connected/weights:0\n",
      "tower/tag_classifier/t/fully_connected/biases:0\n",
      "tower/image/resnet_v2_50/conv1/weights:0\n",
      "tower/image/resnet_v2_50/conv1/biases:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights:0\n",
      "tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases:0\n",
      "tower/image/resnet_v2_50/postnorm/gamma:0\n",
      "tower/image/resnet_v2_50/postnorm/beta:0\n",
      "tower/tag_classifier/i/hidden1_weights:0\n",
      "tower/tag_classifier/i/hidden1_bn/gamma:0\n",
      "tower/tag_classifier/i/hidden1_bn/beta:0\n",
      "tower/tag_classifier/i/gating_weights_1:0\n",
      "tower/tag_classifier/i/gating_bn/beta:0\n",
      "tower/tag_classifier/i/gating_bn/gamma:0\n",
      "tower/tag_classifier/i/gating_weights_2:0\n",
      "tower/tag_classifier/i/fully_connected/weights:0\n",
      "tower/tag_classifier/i/fully_connected/biases:0\n",
      "tower/tag_classifier/fusion/hidden1_weights:0\n",
      "tower/tag_classifier/fusion/hidden1_bn/gamma:0\n",
      "tower/tag_classifier/fusion/hidden1_bn/beta:0\n",
      "tower/tag_classifier/fusion/gating_weights_1:0\n",
      "tower/tag_classifier/fusion/gating_bn/beta:0\n",
      "tower/tag_classifier/fusion/gating_bn/gamma:0\n",
      "tower/tag_classifier/fusion/gating_weights_2:0\n",
      "tower/tag_classifier/fusion/fully_connected/weights:0\n",
      "tower/tag_classifier/fusion/fully_connected/biases:0\n",
      "INFO:tensorflow:!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "INFO:tensorflow:video vars size: 10|audio vars size: 10|text vars size: 203|image vars size: 172|classifier vars size: 45\n",
      "input_name: video, input_shape:[300, 1024], input_dtype: <dtype: 'float32'>\n",
      "input_name: video_frames_num, input_shape:[], input_dtype: <dtype: 'int32'>\n",
      "input_name: idx, input_shape:[], input_dtype: <dtype: 'string'>\n",
      "input_name: audio, input_shape:[300, 128], input_dtype: <dtype: 'float32'>\n",
      "input_name: audio_frames_num, input_shape:[], input_dtype: <dtype: 'int32'>\n",
      "input_name: image, input_shape:[224, 224, 3], input_dtype: <dtype: 'float32'>\n",
      "input_name: text, input_shape:[128], input_dtype: <dtype: 'int64'>\n",
      "input_name: tagging, input_shape:[82], input_dtype: <dtype: 'float32'>\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:237: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "assign:  bert/embeddings/LayerNorm/beta tower/text/bert/embeddings/LayerNorm/beta\n",
      "assign:  bert/embeddings/LayerNorm/gamma tower/text/bert/embeddings/LayerNorm/gamma\n",
      "assign:  bert/embeddings/position_embeddings tower/text/bert/embeddings/position_embeddings\n",
      "assign:  bert/embeddings/token_type_embeddings tower/text/bert/embeddings/token_type_embeddings\n",
      "assign:  bert/embeddings/word_embeddings tower/text/bert/embeddings/word_embeddings\n",
      "assign:  bert/encoder/layer_0/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_0/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_0/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_0/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_0/attention/output/dense/bias tower/text/bert/encoder/layer_0/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_0/attention/output/dense/kernel tower/text/bert/encoder/layer_0/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_0/attention/self/key/bias tower/text/bert/encoder/layer_0/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_0/attention/self/key/kernel tower/text/bert/encoder/layer_0/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_0/attention/self/query/bias tower/text/bert/encoder/layer_0/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_0/attention/self/query/kernel tower/text/bert/encoder/layer_0/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_0/attention/self/value/bias tower/text/bert/encoder/layer_0/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_0/attention/self/value/kernel tower/text/bert/encoder/layer_0/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_0/intermediate/dense/bias tower/text/bert/encoder/layer_0/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_0/intermediate/dense/kernel tower/text/bert/encoder/layer_0/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_0/output/LayerNorm/beta tower/text/bert/encoder/layer_0/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_0/output/LayerNorm/gamma tower/text/bert/encoder/layer_0/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_0/output/dense/bias tower/text/bert/encoder/layer_0/output/dense/bias\n",
      "assign:  bert/encoder/layer_0/output/dense/kernel tower/text/bert/encoder/layer_0/output/dense/kernel\n",
      "assign:  bert/encoder/layer_1/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_1/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_1/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_1/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_1/attention/output/dense/bias tower/text/bert/encoder/layer_1/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_1/attention/output/dense/kernel tower/text/bert/encoder/layer_1/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_1/attention/self/key/bias tower/text/bert/encoder/layer_1/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_1/attention/self/key/kernel tower/text/bert/encoder/layer_1/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_1/attention/self/query/bias tower/text/bert/encoder/layer_1/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_1/attention/self/query/kernel tower/text/bert/encoder/layer_1/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_1/attention/self/value/bias tower/text/bert/encoder/layer_1/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_1/attention/self/value/kernel tower/text/bert/encoder/layer_1/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_1/intermediate/dense/bias tower/text/bert/encoder/layer_1/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_1/intermediate/dense/kernel tower/text/bert/encoder/layer_1/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_1/output/LayerNorm/beta tower/text/bert/encoder/layer_1/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_1/output/LayerNorm/gamma tower/text/bert/encoder/layer_1/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_1/output/dense/bias tower/text/bert/encoder/layer_1/output/dense/bias\n",
      "assign:  bert/encoder/layer_1/output/dense/kernel tower/text/bert/encoder/layer_1/output/dense/kernel\n",
      "assign:  bert/encoder/layer_10/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_10/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_10/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_10/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_10/attention/output/dense/bias tower/text/bert/encoder/layer_10/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_10/attention/output/dense/kernel tower/text/bert/encoder/layer_10/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_10/attention/self/key/bias tower/text/bert/encoder/layer_10/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_10/attention/self/key/kernel tower/text/bert/encoder/layer_10/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_10/attention/self/query/bias tower/text/bert/encoder/layer_10/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_10/attention/self/query/kernel tower/text/bert/encoder/layer_10/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_10/attention/self/value/bias tower/text/bert/encoder/layer_10/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_10/attention/self/value/kernel tower/text/bert/encoder/layer_10/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_10/intermediate/dense/bias tower/text/bert/encoder/layer_10/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_10/intermediate/dense/kernel tower/text/bert/encoder/layer_10/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_10/output/LayerNorm/beta tower/text/bert/encoder/layer_10/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_10/output/LayerNorm/gamma tower/text/bert/encoder/layer_10/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_10/output/dense/bias tower/text/bert/encoder/layer_10/output/dense/bias\n",
      "assign:  bert/encoder/layer_10/output/dense/kernel tower/text/bert/encoder/layer_10/output/dense/kernel\n",
      "assign:  bert/encoder/layer_11/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_11/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_11/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_11/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_11/attention/output/dense/bias tower/text/bert/encoder/layer_11/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_11/attention/output/dense/kernel tower/text/bert/encoder/layer_11/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_11/attention/self/key/bias tower/text/bert/encoder/layer_11/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_11/attention/self/key/kernel tower/text/bert/encoder/layer_11/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_11/attention/self/query/bias tower/text/bert/encoder/layer_11/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_11/attention/self/query/kernel tower/text/bert/encoder/layer_11/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_11/attention/self/value/bias tower/text/bert/encoder/layer_11/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_11/attention/self/value/kernel tower/text/bert/encoder/layer_11/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_11/intermediate/dense/bias tower/text/bert/encoder/layer_11/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_11/intermediate/dense/kernel tower/text/bert/encoder/layer_11/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_11/output/LayerNorm/beta tower/text/bert/encoder/layer_11/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_11/output/LayerNorm/gamma tower/text/bert/encoder/layer_11/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_11/output/dense/bias tower/text/bert/encoder/layer_11/output/dense/bias\n",
      "assign:  bert/encoder/layer_11/output/dense/kernel tower/text/bert/encoder/layer_11/output/dense/kernel\n",
      "assign:  bert/encoder/layer_2/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_2/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_2/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_2/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_2/attention/output/dense/bias tower/text/bert/encoder/layer_2/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_2/attention/output/dense/kernel tower/text/bert/encoder/layer_2/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_2/attention/self/key/bias tower/text/bert/encoder/layer_2/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_2/attention/self/key/kernel tower/text/bert/encoder/layer_2/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_2/attention/self/query/bias tower/text/bert/encoder/layer_2/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_2/attention/self/query/kernel tower/text/bert/encoder/layer_2/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_2/attention/self/value/bias tower/text/bert/encoder/layer_2/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_2/attention/self/value/kernel tower/text/bert/encoder/layer_2/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_2/intermediate/dense/bias tower/text/bert/encoder/layer_2/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_2/intermediate/dense/kernel tower/text/bert/encoder/layer_2/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_2/output/LayerNorm/beta tower/text/bert/encoder/layer_2/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_2/output/LayerNorm/gamma tower/text/bert/encoder/layer_2/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_2/output/dense/bias tower/text/bert/encoder/layer_2/output/dense/bias\n",
      "assign:  bert/encoder/layer_2/output/dense/kernel tower/text/bert/encoder/layer_2/output/dense/kernel\n",
      "assign:  bert/encoder/layer_3/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_3/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_3/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_3/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_3/attention/output/dense/bias tower/text/bert/encoder/layer_3/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_3/attention/output/dense/kernel tower/text/bert/encoder/layer_3/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_3/attention/self/key/bias tower/text/bert/encoder/layer_3/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_3/attention/self/key/kernel tower/text/bert/encoder/layer_3/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_3/attention/self/query/bias tower/text/bert/encoder/layer_3/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_3/attention/self/query/kernel tower/text/bert/encoder/layer_3/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_3/attention/self/value/bias tower/text/bert/encoder/layer_3/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_3/attention/self/value/kernel tower/text/bert/encoder/layer_3/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_3/intermediate/dense/bias tower/text/bert/encoder/layer_3/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_3/intermediate/dense/kernel tower/text/bert/encoder/layer_3/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_3/output/LayerNorm/beta tower/text/bert/encoder/layer_3/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_3/output/LayerNorm/gamma tower/text/bert/encoder/layer_3/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_3/output/dense/bias tower/text/bert/encoder/layer_3/output/dense/bias\n",
      "assign:  bert/encoder/layer_3/output/dense/kernel tower/text/bert/encoder/layer_3/output/dense/kernel\n",
      "assign:  bert/encoder/layer_4/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_4/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_4/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_4/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_4/attention/output/dense/bias tower/text/bert/encoder/layer_4/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_4/attention/output/dense/kernel tower/text/bert/encoder/layer_4/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_4/attention/self/key/bias tower/text/bert/encoder/layer_4/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_4/attention/self/key/kernel tower/text/bert/encoder/layer_4/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_4/attention/self/query/bias tower/text/bert/encoder/layer_4/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_4/attention/self/query/kernel tower/text/bert/encoder/layer_4/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_4/attention/self/value/bias tower/text/bert/encoder/layer_4/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_4/attention/self/value/kernel tower/text/bert/encoder/layer_4/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_4/intermediate/dense/bias tower/text/bert/encoder/layer_4/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_4/intermediate/dense/kernel tower/text/bert/encoder/layer_4/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_4/output/LayerNorm/beta tower/text/bert/encoder/layer_4/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_4/output/LayerNorm/gamma tower/text/bert/encoder/layer_4/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_4/output/dense/bias tower/text/bert/encoder/layer_4/output/dense/bias\n",
      "assign:  bert/encoder/layer_4/output/dense/kernel tower/text/bert/encoder/layer_4/output/dense/kernel\n",
      "assign:  bert/encoder/layer_5/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_5/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_5/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_5/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_5/attention/output/dense/bias tower/text/bert/encoder/layer_5/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_5/attention/output/dense/kernel tower/text/bert/encoder/layer_5/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_5/attention/self/key/bias tower/text/bert/encoder/layer_5/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_5/attention/self/key/kernel tower/text/bert/encoder/layer_5/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_5/attention/self/query/bias tower/text/bert/encoder/layer_5/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_5/attention/self/query/kernel tower/text/bert/encoder/layer_5/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_5/attention/self/value/bias tower/text/bert/encoder/layer_5/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_5/attention/self/value/kernel tower/text/bert/encoder/layer_5/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_5/intermediate/dense/bias tower/text/bert/encoder/layer_5/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_5/intermediate/dense/kernel tower/text/bert/encoder/layer_5/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_5/output/LayerNorm/beta tower/text/bert/encoder/layer_5/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_5/output/LayerNorm/gamma tower/text/bert/encoder/layer_5/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_5/output/dense/bias tower/text/bert/encoder/layer_5/output/dense/bias\n",
      "assign:  bert/encoder/layer_5/output/dense/kernel tower/text/bert/encoder/layer_5/output/dense/kernel\n",
      "assign:  bert/encoder/layer_6/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_6/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_6/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_6/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_6/attention/output/dense/bias tower/text/bert/encoder/layer_6/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_6/attention/output/dense/kernel tower/text/bert/encoder/layer_6/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_6/attention/self/key/bias tower/text/bert/encoder/layer_6/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_6/attention/self/key/kernel tower/text/bert/encoder/layer_6/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_6/attention/self/query/bias tower/text/bert/encoder/layer_6/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_6/attention/self/query/kernel tower/text/bert/encoder/layer_6/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_6/attention/self/value/bias tower/text/bert/encoder/layer_6/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_6/attention/self/value/kernel tower/text/bert/encoder/layer_6/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_6/intermediate/dense/bias tower/text/bert/encoder/layer_6/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_6/intermediate/dense/kernel tower/text/bert/encoder/layer_6/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_6/output/LayerNorm/beta tower/text/bert/encoder/layer_6/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_6/output/LayerNorm/gamma tower/text/bert/encoder/layer_6/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_6/output/dense/bias tower/text/bert/encoder/layer_6/output/dense/bias\n",
      "assign:  bert/encoder/layer_6/output/dense/kernel tower/text/bert/encoder/layer_6/output/dense/kernel\n",
      "assign:  bert/encoder/layer_7/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_7/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_7/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_7/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_7/attention/output/dense/bias tower/text/bert/encoder/layer_7/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_7/attention/output/dense/kernel tower/text/bert/encoder/layer_7/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_7/attention/self/key/bias tower/text/bert/encoder/layer_7/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_7/attention/self/key/kernel tower/text/bert/encoder/layer_7/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_7/attention/self/query/bias tower/text/bert/encoder/layer_7/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_7/attention/self/query/kernel tower/text/bert/encoder/layer_7/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_7/attention/self/value/bias tower/text/bert/encoder/layer_7/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_7/attention/self/value/kernel tower/text/bert/encoder/layer_7/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_7/intermediate/dense/bias tower/text/bert/encoder/layer_7/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_7/intermediate/dense/kernel tower/text/bert/encoder/layer_7/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_7/output/LayerNorm/beta tower/text/bert/encoder/layer_7/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_7/output/LayerNorm/gamma tower/text/bert/encoder/layer_7/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_7/output/dense/bias tower/text/bert/encoder/layer_7/output/dense/bias\n",
      "assign:  bert/encoder/layer_7/output/dense/kernel tower/text/bert/encoder/layer_7/output/dense/kernel\n",
      "assign:  bert/encoder/layer_8/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_8/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_8/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_8/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_8/attention/output/dense/bias tower/text/bert/encoder/layer_8/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_8/attention/output/dense/kernel tower/text/bert/encoder/layer_8/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_8/attention/self/key/bias tower/text/bert/encoder/layer_8/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_8/attention/self/key/kernel tower/text/bert/encoder/layer_8/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_8/attention/self/query/bias tower/text/bert/encoder/layer_8/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_8/attention/self/query/kernel tower/text/bert/encoder/layer_8/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_8/attention/self/value/bias tower/text/bert/encoder/layer_8/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_8/attention/self/value/kernel tower/text/bert/encoder/layer_8/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_8/intermediate/dense/bias tower/text/bert/encoder/layer_8/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_8/intermediate/dense/kernel tower/text/bert/encoder/layer_8/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_8/output/LayerNorm/beta tower/text/bert/encoder/layer_8/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_8/output/LayerNorm/gamma tower/text/bert/encoder/layer_8/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_8/output/dense/bias tower/text/bert/encoder/layer_8/output/dense/bias\n",
      "assign:  bert/encoder/layer_8/output/dense/kernel tower/text/bert/encoder/layer_8/output/dense/kernel\n",
      "assign:  bert/encoder/layer_9/attention/output/LayerNorm/beta tower/text/bert/encoder/layer_9/attention/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_9/attention/output/LayerNorm/gamma tower/text/bert/encoder/layer_9/attention/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_9/attention/output/dense/bias tower/text/bert/encoder/layer_9/attention/output/dense/bias\n",
      "assign:  bert/encoder/layer_9/attention/output/dense/kernel tower/text/bert/encoder/layer_9/attention/output/dense/kernel\n",
      "assign:  bert/encoder/layer_9/attention/self/key/bias tower/text/bert/encoder/layer_9/attention/self/key/bias\n",
      "assign:  bert/encoder/layer_9/attention/self/key/kernel tower/text/bert/encoder/layer_9/attention/self/key/kernel\n",
      "assign:  bert/encoder/layer_9/attention/self/query/bias tower/text/bert/encoder/layer_9/attention/self/query/bias\n",
      "assign:  bert/encoder/layer_9/attention/self/query/kernel tower/text/bert/encoder/layer_9/attention/self/query/kernel\n",
      "assign:  bert/encoder/layer_9/attention/self/value/bias tower/text/bert/encoder/layer_9/attention/self/value/bias\n",
      "assign:  bert/encoder/layer_9/attention/self/value/kernel tower/text/bert/encoder/layer_9/attention/self/value/kernel\n",
      "assign:  bert/encoder/layer_9/intermediate/dense/bias tower/text/bert/encoder/layer_9/intermediate/dense/bias\n",
      "assign:  bert/encoder/layer_9/intermediate/dense/kernel tower/text/bert/encoder/layer_9/intermediate/dense/kernel\n",
      "assign:  bert/encoder/layer_9/output/LayerNorm/beta tower/text/bert/encoder/layer_9/output/LayerNorm/beta\n",
      "assign:  bert/encoder/layer_9/output/LayerNorm/gamma tower/text/bert/encoder/layer_9/output/LayerNorm/gamma\n",
      "assign:  bert/encoder/layer_9/output/dense/bias tower/text/bert/encoder/layer_9/output/dense/bias\n",
      "assign:  bert/encoder/layer_9/output/dense/kernel tower/text/bert/encoder/layer_9/output/dense/kernel\n",
      "assign:  bert/pooler/dense/bias tower/text/bert/pooler/dense/bias\n",
      "assign:  bert/pooler/dense/kernel tower/text/bert/pooler/dense/kernel\n",
      "not in variables: cls/predictions/output_bias\n",
      "not in variables: cls/predictions/transform/LayerNorm/beta\n",
      "not in variables: cls/predictions/transform/LayerNorm/gamma\n",
      "not in variables: cls/predictions/transform/dense/bias\n",
      "not in variables: cls/predictions/transform/dense/kernel\n",
      "not in variables: cls/seq_relationship/output_bias\n",
      "not in variables: cls/seq_relationship/output_weights\n",
      "WARNING:tensorflow:From scripts/train_tagging.py:47: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "load text_pretrained_model: pretrained/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\n",
      "not in variables: global_step\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights tower/image/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_2/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block1/unit_3/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights tower/image/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_2/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_3/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block2/unit_4/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights tower/image/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_2/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_3/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_4/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_5/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block3/unit_6/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights tower/image/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_2/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma/Momentum\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_mean tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_mean\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_variance tower/image/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_variance\n",
      "not in variables: resnet_v2_50/block4/unit_3/bottleneck_v2/preact/moving_variance/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/conv1/biases tower/image/resnet_v2_50/conv1/biases\n",
      "not in variables: resnet_v2_50/conv1/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/conv1/biases/Momentum\n",
      "assign:  resnet_v2_50/conv1/weights tower/image/resnet_v2_50/conv1/weights\n",
      "not in variables: resnet_v2_50/conv1/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/conv1/weights/Momentum\n",
      "not in variables: resnet_v2_50/logits/biases\n",
      "not in variables: resnet_v2_50/logits/biases/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/logits/biases/Momentum\n",
      "not in variables: resnet_v2_50/logits/weights\n",
      "not in variables: resnet_v2_50/logits/weights/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/logits/weights/Momentum\n",
      "assign:  resnet_v2_50/postnorm/beta tower/image/resnet_v2_50/postnorm/beta\n",
      "not in variables: resnet_v2_50/postnorm/beta/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/postnorm/beta/Momentum\n",
      "assign:  resnet_v2_50/postnorm/gamma tower/image/resnet_v2_50/postnorm/gamma\n",
      "not in variables: resnet_v2_50/postnorm/gamma/ExponentialMovingAverage\n",
      "not in variables: resnet_v2_50/postnorm/gamma/Momentum\n",
      "assign:  resnet_v2_50/postnorm/moving_mean tower/image/resnet_v2_50/postnorm/moving_mean\n",
      "not in variables: resnet_v2_50/postnorm/moving_mean/ExponentialMovingAverage\n",
      "assign:  resnet_v2_50/postnorm/moving_variance tower/image/resnet_v2_50/postnorm/moving_variance\n",
      "not in variables: resnet_v2_50/postnorm/moving_variance/ExponentialMovingAverage\n",
      "not in variables: total_loss/ExponentialMovingAverage\n",
      "load image_pretrained_model: pretrained/resnet_v2_50/resnet_v2_50.ckpt\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/base_trainer.py:273: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:/job:master/task:0: Starting managed session.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:/job:master/task:0: Entering training loop.\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:training step 1 | tagging_loss_video: 44.825|tagging_loss_audio: 57.273|tagging_loss_text: 46.510|tagging_loss_image: 46.826|tagging_loss_fusion: 63.134|total_loss: 258.567 | 1.23 Examples/sec\n",
      "INFO:tensorflow:training step 2 | tagging_loss_video: 43.635|tagging_loss_audio: 49.651|tagging_loss_text: 47.193|tagging_loss_image: 45.379|tagging_loss_fusion: 72.588|total_loss: 258.446 | 73.56 Examples/sec\n",
      "INFO:tensorflow:training step 3 | tagging_loss_video: 47.091|tagging_loss_audio: 42.946|tagging_loss_text: 29.140|tagging_loss_image: 35.606|tagging_loss_fusion: 55.457|total_loss: 210.240 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 4 | tagging_loss_video: 30.218|tagging_loss_audio: 23.741|tagging_loss_text: 22.155|tagging_loss_image: 22.076|tagging_loss_fusion: 39.232|total_loss: 137.421 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 5 | tagging_loss_video: 21.265|tagging_loss_audio: 25.895|tagging_loss_text: 24.771|tagging_loss_image: 19.273|tagging_loss_fusion: 27.742|total_loss: 118.945 | 72.00 Examples/sec\n",
      "INFO:tensorflow:training step 6 | tagging_loss_video: 16.653|tagging_loss_audio: 20.504|tagging_loss_text: 18.128|tagging_loss_image: 20.135|tagging_loss_fusion: 22.214|total_loss: 97.633 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 7 | tagging_loss_video: 20.304|tagging_loss_audio: 20.415|tagging_loss_text: 22.690|tagging_loss_image: 25.095|tagging_loss_fusion: 27.685|total_loss: 116.188 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 8 | tagging_loss_video: 18.917|tagging_loss_audio: 19.803|tagging_loss_text: 20.657|tagging_loss_image: 23.942|tagging_loss_fusion: 27.174|total_loss: 110.494 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 9 | tagging_loss_video: 24.159|tagging_loss_audio: 22.561|tagging_loss_text: 19.898|tagging_loss_image: 22.338|tagging_loss_fusion: 28.692|total_loss: 117.649 | 71.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 10 |tagging_loss_video: 22.488|tagging_loss_audio: 19.784|tagging_loss_text: 19.581|tagging_loss_image: 18.742|tagging_loss_fusion: 28.157|total_loss: 108.751 | Examples/sec: 68.52\n",
      "INFO:tensorflow:GAP: 0.53 | precision@0.1: 0.39 | precision@0.5: 0.66 |recall@0.1: 0.79 | recall@0.5: 0.51\n",
      "INFO:tensorflow:training step 11 | tagging_loss_video: 18.325|tagging_loss_audio: 17.232|tagging_loss_text: 15.110|tagging_loss_image: 18.768|tagging_loss_fusion: 25.235|total_loss: 94.669 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 12 | tagging_loss_video: 16.011|tagging_loss_audio: 16.737|tagging_loss_text: 17.389|tagging_loss_image: 16.188|tagging_loss_fusion: 34.661|total_loss: 100.987 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 13 | tagging_loss_video: 14.436|tagging_loss_audio: 14.677|tagging_loss_text: 11.565|tagging_loss_image: 19.896|tagging_loss_fusion: 22.255|total_loss: 82.829 | 71.87 Examples/sec\n",
      "INFO:tensorflow:training step 14 | tagging_loss_video: 17.046|tagging_loss_audio: 14.295|tagging_loss_text: 17.189|tagging_loss_image: 17.291|tagging_loss_fusion: 22.589|total_loss: 88.411 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 15 | tagging_loss_video: 14.206|tagging_loss_audio: 20.620|tagging_loss_text: 17.017|tagging_loss_image: 16.662|tagging_loss_fusion: 21.565|total_loss: 90.070 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 16 | tagging_loss_video: 16.194|tagging_loss_audio: 17.538|tagging_loss_text: 16.842|tagging_loss_image: 17.050|tagging_loss_fusion: 21.615|total_loss: 89.239 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 17 | tagging_loss_video: 15.621|tagging_loss_audio: 14.853|tagging_loss_text: 19.569|tagging_loss_image: 24.744|tagging_loss_fusion: 24.824|total_loss: 99.611 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 18 | tagging_loss_video: 19.997|tagging_loss_audio: 15.054|tagging_loss_text: 17.013|tagging_loss_image: 16.304|tagging_loss_fusion: 24.026|total_loss: 92.394 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 19 | tagging_loss_video: 20.554|tagging_loss_audio: 15.518|tagging_loss_text: 17.968|tagging_loss_image: 16.636|tagging_loss_fusion: 24.948|total_loss: 95.624 | 71.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 20 |tagging_loss_video: 15.045|tagging_loss_audio: 14.920|tagging_loss_text: 18.963|tagging_loss_image: 17.184|tagging_loss_fusion: 24.371|total_loss: 90.485 | Examples/sec: 68.73\n",
      "INFO:tensorflow:GAP: 0.57 | precision@0.1: 0.39 | precision@0.5: 0.77 |recall@0.1: 0.85 | recall@0.5: 0.43\n",
      "INFO:tensorflow:training step 21 | tagging_loss_video: 15.875|tagging_loss_audio: 14.825|tagging_loss_text: 16.966|tagging_loss_image: 16.221|tagging_loss_fusion: 21.519|total_loss: 85.406 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 22 | tagging_loss_video: 14.707|tagging_loss_audio: 17.575|tagging_loss_text: 17.963|tagging_loss_image: 21.171|tagging_loss_fusion: 22.330|total_loss: 93.747 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 23 | tagging_loss_video: 14.887|tagging_loss_audio: 12.838|tagging_loss_text: 15.189|tagging_loss_image: 15.673|tagging_loss_fusion: 20.981|total_loss: 79.569 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 24 | tagging_loss_video: 17.245|tagging_loss_audio: 14.618|tagging_loss_text: 18.172|tagging_loss_image: 19.172|tagging_loss_fusion: 22.165|total_loss: 91.373 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 25 | tagging_loss_video: 18.646|tagging_loss_audio: 16.585|tagging_loss_text: 16.739|tagging_loss_image: 16.999|tagging_loss_fusion: 23.225|total_loss: 92.194 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 26 | tagging_loss_video: 17.446|tagging_loss_audio: 18.829|tagging_loss_text: 14.814|tagging_loss_image: 17.051|tagging_loss_fusion: 22.507|total_loss: 90.648 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 27 | tagging_loss_video: 18.078|tagging_loss_audio: 11.711|tagging_loss_text: 16.459|tagging_loss_image: 16.009|tagging_loss_fusion: 23.581|total_loss: 85.837 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 28 | tagging_loss_video: 16.218|tagging_loss_audio: 12.638|tagging_loss_text: 14.545|tagging_loss_image: 14.712|tagging_loss_fusion: 21.159|total_loss: 79.274 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 29 | tagging_loss_video: 13.192|tagging_loss_audio: 18.925|tagging_loss_text: 17.259|tagging_loss_image: 16.913|tagging_loss_fusion: 24.342|total_loss: 90.631 | 72.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 30 |tagging_loss_video: 13.830|tagging_loss_audio: 13.335|tagging_loss_text: 15.812|tagging_loss_image: 14.409|tagging_loss_fusion: 21.367|total_loss: 78.751 | Examples/sec: 67.64\n",
      "INFO:tensorflow:GAP: 0.60 | precision@0.1: 0.37 | precision@0.5: 0.79 |recall@0.1: 0.83 | recall@0.5: 0.50\n",
      "INFO:tensorflow:training step 31 | tagging_loss_video: 15.218|tagging_loss_audio: 11.788|tagging_loss_text: 15.299|tagging_loss_image: 13.556|tagging_loss_fusion: 19.981|total_loss: 75.842 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 32 | tagging_loss_video: 14.690|tagging_loss_audio: 18.131|tagging_loss_text: 19.975|tagging_loss_image: 18.250|tagging_loss_fusion: 23.867|total_loss: 94.913 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 33 | tagging_loss_video: 14.681|tagging_loss_audio: 12.942|tagging_loss_text: 14.871|tagging_loss_image: 16.940|tagging_loss_fusion: 20.121|total_loss: 79.556 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 34 | tagging_loss_video: 17.390|tagging_loss_audio: 14.646|tagging_loss_text: 15.798|tagging_loss_image: 17.039|tagging_loss_fusion: 22.242|total_loss: 87.115 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 35 | tagging_loss_video: 14.100|tagging_loss_audio: 16.794|tagging_loss_text: 17.497|tagging_loss_image: 14.426|tagging_loss_fusion: 22.071|total_loss: 84.888 | 72.03 Examples/sec\n",
      "INFO:tensorflow:training step 36 | tagging_loss_video: 14.333|tagging_loss_audio: 15.242|tagging_loss_text: 14.074|tagging_loss_image: 17.324|tagging_loss_fusion: 21.230|total_loss: 82.203 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 37 | tagging_loss_video: 15.931|tagging_loss_audio: 16.657|tagging_loss_text: 17.323|tagging_loss_image: 14.587|tagging_loss_fusion: 20.178|total_loss: 84.677 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 38 | tagging_loss_video: 15.892|tagging_loss_audio: 11.898|tagging_loss_text: 18.592|tagging_loss_image: 19.807|tagging_loss_fusion: 21.763|total_loss: 87.953 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 39 | tagging_loss_video: 14.681|tagging_loss_audio: 15.315|tagging_loss_text: 17.002|tagging_loss_image: 15.689|tagging_loss_fusion: 22.509|total_loss: 85.195 | 71.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 40 |tagging_loss_video: 15.190|tagging_loss_audio: 15.139|tagging_loss_text: 14.670|tagging_loss_image: 15.315|tagging_loss_fusion: 22.630|total_loss: 82.945 | Examples/sec: 71.17\n",
      "INFO:tensorflow:GAP: 0.55 | precision@0.1: 0.31 | precision@0.5: 0.69 |recall@0.1: 0.86 | recall@0.5: 0.47\n",
      "INFO:tensorflow:training step 41 | tagging_loss_video: 13.364|tagging_loss_audio: 11.893|tagging_loss_text: 15.014|tagging_loss_image: 15.785|tagging_loss_fusion: 20.137|total_loss: 76.192 | 71.89 Examples/sec\n",
      "INFO:tensorflow:training step 42 | tagging_loss_video: 17.041|tagging_loss_audio: 16.027|tagging_loss_text: 15.988|tagging_loss_image: 20.187|tagging_loss_fusion: 23.637|total_loss: 92.880 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 43 | tagging_loss_video: 13.652|tagging_loss_audio: 13.889|tagging_loss_text: 15.178|tagging_loss_image: 16.789|tagging_loss_fusion: 21.301|total_loss: 80.809 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 44 | tagging_loss_video: 12.447|tagging_loss_audio: 16.075|tagging_loss_text: 17.600|tagging_loss_image: 15.291|tagging_loss_fusion: 20.365|total_loss: 81.778 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 45 | tagging_loss_video: 12.481|tagging_loss_audio: 15.605|tagging_loss_text: 17.290|tagging_loss_image: 13.951|tagging_loss_fusion: 21.944|total_loss: 81.271 | 72.31 Examples/sec\n",
      "INFO:tensorflow:training step 46 | tagging_loss_video: 13.274|tagging_loss_audio: 14.716|tagging_loss_text: 16.075|tagging_loss_image: 14.727|tagging_loss_fusion: 20.393|total_loss: 79.184 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 47 | tagging_loss_video: 18.999|tagging_loss_audio: 12.766|tagging_loss_text: 14.144|tagging_loss_image: 16.717|tagging_loss_fusion: 20.887|total_loss: 83.512 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 48 | tagging_loss_video: 13.549|tagging_loss_audio: 13.329|tagging_loss_text: 13.737|tagging_loss_image: 16.614|tagging_loss_fusion: 19.659|total_loss: 76.888 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 49 | tagging_loss_video: 17.102|tagging_loss_audio: 14.846|tagging_loss_text: 13.941|tagging_loss_image: 13.794|tagging_loss_fusion: 19.677|total_loss: 79.360 | 69.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 50 |tagging_loss_video: 14.321|tagging_loss_audio: 15.530|tagging_loss_text: 20.697|tagging_loss_image: 14.672|tagging_loss_fusion: 22.282|total_loss: 87.501 | Examples/sec: 70.29\n",
      "INFO:tensorflow:GAP: 0.62 | precision@0.1: 0.42 | precision@0.5: 0.82 |recall@0.1: 0.87 | recall@0.5: 0.43\n",
      "INFO:tensorflow:training step 51 | tagging_loss_video: 14.109|tagging_loss_audio: 18.578|tagging_loss_text: 15.170|tagging_loss_image: 13.305|tagging_loss_fusion: 18.564|total_loss: 79.727 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 52 | tagging_loss_video: 15.280|tagging_loss_audio: 17.301|tagging_loss_text: 17.639|tagging_loss_image: 17.113|tagging_loss_fusion: 22.808|total_loss: 90.141 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 53 | tagging_loss_video: 12.765|tagging_loss_audio: 14.432|tagging_loss_text: 13.130|tagging_loss_image: 11.308|tagging_loss_fusion: 18.906|total_loss: 70.541 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 54 | tagging_loss_video: 16.815|tagging_loss_audio: 17.457|tagging_loss_text: 14.657|tagging_loss_image: 17.087|tagging_loss_fusion: 21.386|total_loss: 87.403 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 55 | tagging_loss_video: 12.944|tagging_loss_audio: 13.398|tagging_loss_text: 17.561|tagging_loss_image: 16.517|tagging_loss_fusion: 19.921|total_loss: 80.342 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 56 | tagging_loss_video: 13.942|tagging_loss_audio: 14.942|tagging_loss_text: 15.841|tagging_loss_image: 14.915|tagging_loss_fusion: 20.248|total_loss: 79.889 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 57 | tagging_loss_video: 12.975|tagging_loss_audio: 13.254|tagging_loss_text: 15.677|tagging_loss_image: 10.940|tagging_loss_fusion: 18.969|total_loss: 71.814 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 58 | tagging_loss_video: 17.974|tagging_loss_audio: 16.181|tagging_loss_text: 18.513|tagging_loss_image: 16.919|tagging_loss_fusion: 21.780|total_loss: 91.366 | 67.67 Examples/sec\n",
      "INFO:tensorflow:training step 59 | tagging_loss_video: 14.393|tagging_loss_audio: 14.318|tagging_loss_text: 13.775|tagging_loss_image: 16.761|tagging_loss_fusion: 21.316|total_loss: 80.563 | 69.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 60 |tagging_loss_video: 18.987|tagging_loss_audio: 15.663|tagging_loss_text: 16.286|tagging_loss_image: 9.667|tagging_loss_fusion: 20.987|total_loss: 81.590 | Examples/sec: 69.73\n",
      "INFO:tensorflow:GAP: 0.64 | precision@0.1: 0.40 | precision@0.5: 0.76 |recall@0.1: 0.87 | recall@0.5: 0.53\n",
      "INFO:tensorflow:training step 61 | tagging_loss_video: 13.939|tagging_loss_audio: 16.942|tagging_loss_text: 17.203|tagging_loss_image: 17.256|tagging_loss_fusion: 20.677|total_loss: 86.017 | 71.87 Examples/sec\n",
      "INFO:tensorflow:training step 62 | tagging_loss_video: 14.315|tagging_loss_audio: 15.120|tagging_loss_text: 17.743|tagging_loss_image: 15.232|tagging_loss_fusion: 21.670|total_loss: 84.080 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 63 | tagging_loss_video: 16.056|tagging_loss_audio: 14.999|tagging_loss_text: 16.279|tagging_loss_image: 14.645|tagging_loss_fusion: 21.160|total_loss: 83.138 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 64 | tagging_loss_video: 18.372|tagging_loss_audio: 11.930|tagging_loss_text: 19.473|tagging_loss_image: 15.195|tagging_loss_fusion: 21.480|total_loss: 86.451 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 65 | tagging_loss_video: 15.125|tagging_loss_audio: 14.475|tagging_loss_text: 15.675|tagging_loss_image: 14.083|tagging_loss_fusion: 19.278|total_loss: 78.637 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 66 | tagging_loss_video: 15.989|tagging_loss_audio: 14.727|tagging_loss_text: 19.920|tagging_loss_image: 17.647|tagging_loss_fusion: 21.081|total_loss: 89.364 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 67 | tagging_loss_video: 15.168|tagging_loss_audio: 16.396|tagging_loss_text: 15.522|tagging_loss_image: 15.218|tagging_loss_fusion: 18.695|total_loss: 80.999 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 68 | tagging_loss_video: 15.304|tagging_loss_audio: 14.439|tagging_loss_text: 19.192|tagging_loss_image: 12.463|tagging_loss_fusion: 21.843|total_loss: 83.241 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 69 | tagging_loss_video: 12.052|tagging_loss_audio: 14.999|tagging_loss_text: 14.525|tagging_loss_image: 13.678|tagging_loss_fusion: 19.421|total_loss: 74.675 | 69.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 70 |tagging_loss_video: 12.438|tagging_loss_audio: 15.563|tagging_loss_text: 16.187|tagging_loss_image: 18.247|tagging_loss_fusion: 22.666|total_loss: 85.100 | Examples/sec: 71.50\n",
      "INFO:tensorflow:GAP: 0.62 | precision@0.1: 0.37 | precision@0.5: 0.83 |recall@0.1: 0.88 | recall@0.5: 0.43\n",
      "INFO:tensorflow:training step 71 | tagging_loss_video: 14.598|tagging_loss_audio: 14.785|tagging_loss_text: 17.907|tagging_loss_image: 16.459|tagging_loss_fusion: 20.425|total_loss: 84.175 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 72 | tagging_loss_video: 13.427|tagging_loss_audio: 16.069|tagging_loss_text: 14.560|tagging_loss_image: 17.531|tagging_loss_fusion: 21.817|total_loss: 83.405 | 72.24 Examples/sec\n",
      "INFO:tensorflow:training step 73 | tagging_loss_video: 14.818|tagging_loss_audio: 16.875|tagging_loss_text: 17.201|tagging_loss_image: 14.794|tagging_loss_fusion: 20.493|total_loss: 84.180 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 74 | tagging_loss_video: 16.479|tagging_loss_audio: 15.537|tagging_loss_text: 15.430|tagging_loss_image: 18.282|tagging_loss_fusion: 21.429|total_loss: 87.157 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 75 | tagging_loss_video: 22.445|tagging_loss_audio: 13.943|tagging_loss_text: 15.170|tagging_loss_image: 15.496|tagging_loss_fusion: 21.820|total_loss: 88.874 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 76 | tagging_loss_video: 13.347|tagging_loss_audio: 14.993|tagging_loss_text: 13.818|tagging_loss_image: 16.852|tagging_loss_fusion: 18.609|total_loss: 77.620 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 77 | tagging_loss_video: 16.099|tagging_loss_audio: 12.785|tagging_loss_text: 12.522|tagging_loss_image: 14.492|tagging_loss_fusion: 19.748|total_loss: 75.646 | 68.67 Examples/sec\n",
      "INFO:tensorflow:training step 78 | tagging_loss_video: 14.457|tagging_loss_audio: 15.983|tagging_loss_text: 18.464|tagging_loss_image: 15.326|tagging_loss_fusion: 20.631|total_loss: 84.861 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 79 | tagging_loss_video: 13.738|tagging_loss_audio: 12.563|tagging_loss_text: 14.037|tagging_loss_image: 15.066|tagging_loss_fusion: 19.951|total_loss: 75.355 | 70.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 80 |tagging_loss_video: 17.191|tagging_loss_audio: 13.033|tagging_loss_text: 13.990|tagging_loss_image: 16.587|tagging_loss_fusion: 20.585|total_loss: 81.386 | Examples/sec: 67.30\n",
      "INFO:tensorflow:GAP: 0.63 | precision@0.1: 0.41 | precision@0.5: 0.76 |recall@0.1: 0.85 | recall@0.5: 0.52\n",
      "INFO:tensorflow:training step 81 | tagging_loss_video: 15.549|tagging_loss_audio: 12.917|tagging_loss_text: 18.211|tagging_loss_image: 11.445|tagging_loss_fusion: 19.928|total_loss: 78.051 | 71.81 Examples/sec\n",
      "INFO:tensorflow:training step 82 | tagging_loss_video: 22.808|tagging_loss_audio: 13.814|tagging_loss_text: 17.743|tagging_loss_image: 17.202|tagging_loss_fusion: 20.835|total_loss: 92.402 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 83 | tagging_loss_video: 10.143|tagging_loss_audio: 13.105|tagging_loss_text: 12.436|tagging_loss_image: 14.576|tagging_loss_fusion: 21.685|total_loss: 71.947 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 84 | tagging_loss_video: 12.751|tagging_loss_audio: 13.515|tagging_loss_text: 15.070|tagging_loss_image: 14.033|tagging_loss_fusion: 20.884|total_loss: 76.253 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 85 | tagging_loss_video: 14.160|tagging_loss_audio: 12.554|tagging_loss_text: 13.592|tagging_loss_image: 14.276|tagging_loss_fusion: 18.983|total_loss: 73.564 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 86 | tagging_loss_video: 12.525|tagging_loss_audio: 15.042|tagging_loss_text: 13.678|tagging_loss_image: 14.350|tagging_loss_fusion: 18.919|total_loss: 74.514 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 87 | tagging_loss_video: 14.491|tagging_loss_audio: 12.341|tagging_loss_text: 17.000|tagging_loss_image: 15.008|tagging_loss_fusion: 18.133|total_loss: 76.973 | 72.70 Examples/sec\n",
      "INFO:tensorflow:training step 88 | tagging_loss_video: 13.981|tagging_loss_audio: 16.689|tagging_loss_text: 13.809|tagging_loss_image: 14.395|tagging_loss_fusion: 20.940|total_loss: 79.814 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 89 | tagging_loss_video: 13.878|tagging_loss_audio: 15.150|tagging_loss_text: 15.730|tagging_loss_image: 14.034|tagging_loss_fusion: 20.375|total_loss: 79.167 | 71.45 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 90 |tagging_loss_video: 16.724|tagging_loss_audio: 16.890|tagging_loss_text: 18.163|tagging_loss_image: 16.420|tagging_loss_fusion: 20.670|total_loss: 88.867 | Examples/sec: 69.07\n",
      "INFO:tensorflow:GAP: 0.66 | precision@0.1: 0.44 | precision@0.5: 0.84 |recall@0.1: 0.85 | recall@0.5: 0.51\n",
      "INFO:tensorflow:training step 91 | tagging_loss_video: 11.822|tagging_loss_audio: 14.589|tagging_loss_text: 13.407|tagging_loss_image: 15.078|tagging_loss_fusion: 19.796|total_loss: 74.693 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 92 | tagging_loss_video: 15.690|tagging_loss_audio: 12.265|tagging_loss_text: 14.691|tagging_loss_image: 18.484|tagging_loss_fusion: 21.559|total_loss: 82.689 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 93 | tagging_loss_video: 13.983|tagging_loss_audio: 14.305|tagging_loss_text: 15.173|tagging_loss_image: 14.868|tagging_loss_fusion: 20.922|total_loss: 79.251 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 94 | tagging_loss_video: 13.218|tagging_loss_audio: 12.832|tagging_loss_text: 12.368|tagging_loss_image: 13.247|tagging_loss_fusion: 18.157|total_loss: 69.823 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 95 | tagging_loss_video: 14.651|tagging_loss_audio: 13.137|tagging_loss_text: 12.187|tagging_loss_image: 13.938|tagging_loss_fusion: 18.635|total_loss: 72.548 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 96 | tagging_loss_video: 14.623|tagging_loss_audio: 13.599|tagging_loss_text: 13.492|tagging_loss_image: 15.813|tagging_loss_fusion: 19.481|total_loss: 77.008 | 62.61 Examples/sec\n",
      "INFO:tensorflow:training step 97 | tagging_loss_video: 14.163|tagging_loss_audio: 15.335|tagging_loss_text: 16.276|tagging_loss_image: 12.255|tagging_loss_fusion: 20.158|total_loss: 78.187 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 98 | tagging_loss_video: 17.404|tagging_loss_audio: 14.242|tagging_loss_text: 16.275|tagging_loss_image: 15.923|tagging_loss_fusion: 21.486|total_loss: 85.330 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 99 | tagging_loss_video: 12.893|tagging_loss_audio: 15.913|tagging_loss_text: 15.262|tagging_loss_image: 16.056|tagging_loss_fusion: 19.739|total_loss: 79.863 | 69.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 100 |tagging_loss_video: 18.194|tagging_loss_audio: 13.423|tagging_loss_text: 15.122|tagging_loss_image: 13.136|tagging_loss_fusion: 22.021|total_loss: 81.896 | Examples/sec: 69.27\n",
      "INFO:tensorflow:GAP: 0.59 | precision@0.1: 0.35 | precision@0.5: 0.70 |recall@0.1: 0.88 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 101 | tagging_loss_video: 15.130|tagging_loss_audio: 14.215|tagging_loss_text: 15.594|tagging_loss_image: 16.212|tagging_loss_fusion: 19.293|total_loss: 80.444 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 102 | tagging_loss_video: 13.027|tagging_loss_audio: 15.389|tagging_loss_text: 15.757|tagging_loss_image: 12.794|tagging_loss_fusion: 19.524|total_loss: 76.491 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 103 | tagging_loss_video: 14.793|tagging_loss_audio: 13.105|tagging_loss_text: 17.285|tagging_loss_image: 13.190|tagging_loss_fusion: 19.667|total_loss: 78.041 | 72.33 Examples/sec\n",
      "INFO:tensorflow:training step 104 | tagging_loss_video: 15.687|tagging_loss_audio: 11.813|tagging_loss_text: 13.809|tagging_loss_image: 15.123|tagging_loss_fusion: 19.163|total_loss: 75.595 | 59.39 Examples/sec\n",
      "INFO:tensorflow:training step 105 | tagging_loss_video: 13.815|tagging_loss_audio: 13.638|tagging_loss_text: 16.675|tagging_loss_image: 13.411|tagging_loss_fusion: 20.091|total_loss: 77.630 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 106 | tagging_loss_video: 14.789|tagging_loss_audio: 15.080|tagging_loss_text: 17.374|tagging_loss_image: 14.218|tagging_loss_fusion: 18.492|total_loss: 79.953 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 107 | tagging_loss_video: 12.887|tagging_loss_audio: 9.719|tagging_loss_text: 16.641|tagging_loss_image: 13.352|tagging_loss_fusion: 20.727|total_loss: 73.325 | 64.60 Examples/sec\n",
      "INFO:tensorflow:training step 108 | tagging_loss_video: 13.676|tagging_loss_audio: 12.985|tagging_loss_text: 15.083|tagging_loss_image: 14.358|tagging_loss_fusion: 19.779|total_loss: 75.881 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 109 | tagging_loss_video: 14.479|tagging_loss_audio: 15.925|tagging_loss_text: 17.795|tagging_loss_image: 13.625|tagging_loss_fusion: 20.099|total_loss: 81.924 | 70.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 110 |tagging_loss_video: 18.637|tagging_loss_audio: 17.297|tagging_loss_text: 19.782|tagging_loss_image: 17.578|tagging_loss_fusion: 21.800|total_loss: 95.094 | Examples/sec: 68.92\n",
      "INFO:tensorflow:GAP: 0.66 | precision@0.1: 0.42 | precision@0.5: 0.83 |recall@0.1: 0.85 | recall@0.5: 0.49\n",
      "INFO:tensorflow:training step 111 | tagging_loss_video: 15.310|tagging_loss_audio: 15.192|tagging_loss_text: 13.999|tagging_loss_image: 14.759|tagging_loss_fusion: 20.834|total_loss: 80.094 | 67.19 Examples/sec\n",
      "INFO:tensorflow:training step 112 | tagging_loss_video: 19.723|tagging_loss_audio: 15.482|tagging_loss_text: 18.756|tagging_loss_image: 11.874|tagging_loss_fusion: 19.380|total_loss: 85.216 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 113 | tagging_loss_video: 18.504|tagging_loss_audio: 14.703|tagging_loss_text: 17.719|tagging_loss_image: 14.862|tagging_loss_fusion: 21.667|total_loss: 87.455 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 114 | tagging_loss_video: 11.752|tagging_loss_audio: 14.155|tagging_loss_text: 13.891|tagging_loss_image: 14.588|tagging_loss_fusion: 19.208|total_loss: 73.594 | 69.56 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 115 | tagging_loss_video: 16.267|tagging_loss_audio: 13.723|tagging_loss_text: 14.338|tagging_loss_image: 16.956|tagging_loss_fusion: 19.036|total_loss: 80.320 | 61.75 Examples/sec\n",
      "INFO:tensorflow:training step 116 | tagging_loss_video: 12.742|tagging_loss_audio: 14.291|tagging_loss_text: 15.203|tagging_loss_image: 17.183|tagging_loss_fusion: 22.871|total_loss: 82.290 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 117 | tagging_loss_video: 14.983|tagging_loss_audio: 12.817|tagging_loss_text: 15.955|tagging_loss_image: 13.366|tagging_loss_fusion: 19.857|total_loss: 76.977 | 71.99 Examples/sec\n",
      "INFO:tensorflow:training step 118 | tagging_loss_video: 17.400|tagging_loss_audio: 18.371|tagging_loss_text: 17.172|tagging_loss_image: 14.278|tagging_loss_fusion: 21.776|total_loss: 88.997 | 60.30 Examples/sec\n",
      "INFO:tensorflow:training step 119 | tagging_loss_video: 13.671|tagging_loss_audio: 11.560|tagging_loss_text: 15.122|tagging_loss_image: 16.953|tagging_loss_fusion: 19.522|total_loss: 76.827 | 70.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 120 |tagging_loss_video: 14.908|tagging_loss_audio: 13.566|tagging_loss_text: 17.937|tagging_loss_image: 16.090|tagging_loss_fusion: 21.081|total_loss: 83.581 | Examples/sec: 69.89\n",
      "INFO:tensorflow:GAP: 0.65 | precision@0.1: 0.40 | precision@0.5: 0.80 |recall@0.1: 0.90 | recall@0.5: 0.49\n",
      "INFO:tensorflow:training step 121 | tagging_loss_video: 14.466|tagging_loss_audio: 16.352|tagging_loss_text: 15.070|tagging_loss_image: 16.474|tagging_loss_fusion: 18.981|total_loss: 81.343 | 67.48 Examples/sec\n",
      "INFO:tensorflow:training step 122 | tagging_loss_video: 12.471|tagging_loss_audio: 12.933|tagging_loss_text: 13.137|tagging_loss_image: 13.137|tagging_loss_fusion: 18.471|total_loss: 70.149 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 123 | tagging_loss_video: 14.521|tagging_loss_audio: 11.312|tagging_loss_text: 16.323|tagging_loss_image: 13.688|tagging_loss_fusion: 18.381|total_loss: 74.225 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 124 | tagging_loss_video: 15.034|tagging_loss_audio: 11.603|tagging_loss_text: 14.330|tagging_loss_image: 14.526|tagging_loss_fusion: 19.968|total_loss: 75.461 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 125 | tagging_loss_video: 13.218|tagging_loss_audio: 12.346|tagging_loss_text: 10.591|tagging_loss_image: 14.975|tagging_loss_fusion: 19.456|total_loss: 70.586 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 126 | tagging_loss_video: 16.244|tagging_loss_audio: 15.710|tagging_loss_text: 17.093|tagging_loss_image: 15.532|tagging_loss_fusion: 23.330|total_loss: 87.908 | 59.87 Examples/sec\n",
      "INFO:tensorflow:training step 127 | tagging_loss_video: 12.846|tagging_loss_audio: 13.600|tagging_loss_text: 15.681|tagging_loss_image: 12.729|tagging_loss_fusion: 20.865|total_loss: 75.721 | 71.74 Examples/sec\n",
      "INFO:tensorflow:training step 128 | tagging_loss_video: 12.592|tagging_loss_audio: 11.410|tagging_loss_text: 13.181|tagging_loss_image: 12.334|tagging_loss_fusion: 18.418|total_loss: 67.934 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 129 | tagging_loss_video: 14.557|tagging_loss_audio: 12.656|tagging_loss_text: 13.549|tagging_loss_image: 13.435|tagging_loss_fusion: 21.262|total_loss: 75.459 | 65.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 130 |tagging_loss_video: 14.000|tagging_loss_audio: 15.757|tagging_loss_text: 16.381|tagging_loss_image: 15.556|tagging_loss_fusion: 20.147|total_loss: 81.841 | Examples/sec: 68.63\n",
      "INFO:tensorflow:GAP: 0.66 | precision@0.1: 0.39 | precision@0.5: 0.82 |recall@0.1: 0.88 | recall@0.5: 0.53\n",
      "INFO:tensorflow:training step 131 | tagging_loss_video: 12.880|tagging_loss_audio: 14.281|tagging_loss_text: 16.769|tagging_loss_image: 13.005|tagging_loss_fusion: 17.568|total_loss: 74.503 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 132 | tagging_loss_video: 17.660|tagging_loss_audio: 13.406|tagging_loss_text: 17.663|tagging_loss_image: 14.355|tagging_loss_fusion: 20.801|total_loss: 83.885 | 65.38 Examples/sec\n",
      "INFO:tensorflow:training step 133 | tagging_loss_video: 13.626|tagging_loss_audio: 13.150|tagging_loss_text: 13.204|tagging_loss_image: 13.041|tagging_loss_fusion: 18.711|total_loss: 71.732 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 134 | tagging_loss_video: 14.020|tagging_loss_audio: 12.765|tagging_loss_text: 16.281|tagging_loss_image: 13.984|tagging_loss_fusion: 19.435|total_loss: 76.485 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 135 | tagging_loss_video: 12.757|tagging_loss_audio: 11.229|tagging_loss_text: 15.700|tagging_loss_image: 12.551|tagging_loss_fusion: 20.163|total_loss: 72.401 | 65.88 Examples/sec\n",
      "INFO:tensorflow:training step 136 | tagging_loss_video: 15.976|tagging_loss_audio: 14.785|tagging_loss_text: 18.475|tagging_loss_image: 15.620|tagging_loss_fusion: 20.159|total_loss: 85.016 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 137 | tagging_loss_video: 14.731|tagging_loss_audio: 16.984|tagging_loss_text: 13.993|tagging_loss_image: 14.718|tagging_loss_fusion: 20.778|total_loss: 81.205 | 72.40 Examples/sec\n",
      "INFO:tensorflow:training step 138 | tagging_loss_video: 14.748|tagging_loss_audio: 14.584|tagging_loss_text: 14.603|tagging_loss_image: 15.231|tagging_loss_fusion: 21.071|total_loss: 80.238 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 139 | tagging_loss_video: 10.794|tagging_loss_audio: 12.469|tagging_loss_text: 17.428|tagging_loss_image: 17.113|tagging_loss_fusion: 20.354|total_loss: 78.157 | 69.08 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 140 |tagging_loss_video: 15.555|tagging_loss_audio: 14.199|tagging_loss_text: 15.348|tagging_loss_image: 14.203|tagging_loss_fusion: 18.991|total_loss: 78.296 | Examples/sec: 66.20\n",
      "INFO:tensorflow:GAP: 0.67 | precision@0.1: 0.42 | precision@0.5: 0.82 |recall@0.1: 0.90 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 141 | tagging_loss_video: 13.771|tagging_loss_audio: 14.474|tagging_loss_text: 15.751|tagging_loss_image: 16.477|tagging_loss_fusion: 18.812|total_loss: 79.285 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 142 | tagging_loss_video: 12.369|tagging_loss_audio: 12.515|tagging_loss_text: 16.692|tagging_loss_image: 14.361|tagging_loss_fusion: 18.970|total_loss: 74.908 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 143 | tagging_loss_video: 14.733|tagging_loss_audio: 15.893|tagging_loss_text: 18.525|tagging_loss_image: 14.145|tagging_loss_fusion: 19.191|total_loss: 82.487 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 144 | tagging_loss_video: 14.076|tagging_loss_audio: 10.805|tagging_loss_text: 18.155|tagging_loss_image: 12.331|tagging_loss_fusion: 17.009|total_loss: 72.377 | 63.82 Examples/sec\n",
      "INFO:tensorflow:training step 145 | tagging_loss_video: 12.334|tagging_loss_audio: 10.091|tagging_loss_text: 13.428|tagging_loss_image: 10.707|tagging_loss_fusion: 16.478|total_loss: 63.038 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 146 | tagging_loss_video: 14.396|tagging_loss_audio: 12.462|tagging_loss_text: 11.787|tagging_loss_image: 14.646|tagging_loss_fusion: 17.156|total_loss: 70.446 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 147 | tagging_loss_video: 12.100|tagging_loss_audio: 16.390|tagging_loss_text: 15.118|tagging_loss_image: 10.289|tagging_loss_fusion: 17.119|total_loss: 71.016 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 148 | tagging_loss_video: 13.815|tagging_loss_audio: 16.257|tagging_loss_text: 16.858|tagging_loss_image: 14.760|tagging_loss_fusion: 18.694|total_loss: 80.384 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 149 | tagging_loss_video: 12.896|tagging_loss_audio: 14.500|tagging_loss_text: 18.563|tagging_loss_image: 14.170|tagging_loss_fusion: 18.356|total_loss: 78.485 | 71.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 150 |tagging_loss_video: 11.946|tagging_loss_audio: 13.891|tagging_loss_text: 11.556|tagging_loss_image: 13.071|tagging_loss_fusion: 17.885|total_loss: 68.350 | Examples/sec: 61.67\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.43 | precision@0.5: 0.79 |recall@0.1: 0.88 | recall@0.5: 0.52\n",
      "INFO:tensorflow:training step 151 | tagging_loss_video: 14.438|tagging_loss_audio: 15.140|tagging_loss_text: 16.417|tagging_loss_image: 14.241|tagging_loss_fusion: 20.012|total_loss: 80.249 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 152 | tagging_loss_video: 14.303|tagging_loss_audio: 14.004|tagging_loss_text: 16.574|tagging_loss_image: 15.403|tagging_loss_fusion: 17.973|total_loss: 78.257 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 153 | tagging_loss_video: 14.971|tagging_loss_audio: 13.415|tagging_loss_text: 16.445|tagging_loss_image: 13.283|tagging_loss_fusion: 20.081|total_loss: 78.195 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 154 | tagging_loss_video: 15.941|tagging_loss_audio: 15.505|tagging_loss_text: 17.182|tagging_loss_image: 15.210|tagging_loss_fusion: 20.220|total_loss: 84.059 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 155 | tagging_loss_video: 13.598|tagging_loss_audio: 13.502|tagging_loss_text: 16.965|tagging_loss_image: 14.578|tagging_loss_fusion: 18.958|total_loss: 77.601 | 60.29 Examples/sec\n",
      "INFO:tensorflow:training step 156 | tagging_loss_video: 15.547|tagging_loss_audio: 13.118|tagging_loss_text: 17.045|tagging_loss_image: 12.585|tagging_loss_fusion: 17.144|total_loss: 75.439 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 157 | tagging_loss_video: 13.226|tagging_loss_audio: 13.273|tagging_loss_text: 14.644|tagging_loss_image: 11.452|tagging_loss_fusion: 17.741|total_loss: 70.336 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 158 | tagging_loss_video: 12.578|tagging_loss_audio: 13.424|tagging_loss_text: 18.235|tagging_loss_image: 13.885|tagging_loss_fusion: 19.129|total_loss: 77.251 | 72.08 Examples/sec\n",
      "INFO:tensorflow:training step 159 | tagging_loss_video: 12.889|tagging_loss_audio: 13.669|tagging_loss_text: 18.288|tagging_loss_image: 15.251|tagging_loss_fusion: 18.721|total_loss: 78.818 | 66.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 160 |tagging_loss_video: 11.959|tagging_loss_audio: 13.693|tagging_loss_text: 15.259|tagging_loss_image: 12.806|tagging_loss_fusion: 19.206|total_loss: 72.924 | Examples/sec: 70.24\n",
      "INFO:tensorflow:GAP: 0.65 | precision@0.1: 0.40 | precision@0.5: 0.75 |recall@0.1: 0.87 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 161 | tagging_loss_video: 11.602|tagging_loss_audio: 13.073|tagging_loss_text: 12.714|tagging_loss_image: 13.632|tagging_loss_fusion: 19.910|total_loss: 70.931 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 162 | tagging_loss_video: 13.981|tagging_loss_audio: 14.822|tagging_loss_text: 13.512|tagging_loss_image: 14.452|tagging_loss_fusion: 17.815|total_loss: 74.583 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 163 | tagging_loss_video: 17.062|tagging_loss_audio: 13.966|tagging_loss_text: 16.337|tagging_loss_image: 16.091|tagging_loss_fusion: 20.534|total_loss: 83.990 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 164 | tagging_loss_video: 12.939|tagging_loss_audio: 14.501|tagging_loss_text: 13.220|tagging_loss_image: 14.836|tagging_loss_fusion: 20.102|total_loss: 75.598 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 165 | tagging_loss_video: 12.922|tagging_loss_audio: 17.704|tagging_loss_text: 17.351|tagging_loss_image: 13.647|tagging_loss_fusion: 19.137|total_loss: 80.761 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 166 | tagging_loss_video: 10.952|tagging_loss_audio: 14.049|tagging_loss_text: 14.394|tagging_loss_image: 15.365|tagging_loss_fusion: 18.343|total_loss: 73.103 | 59.42 Examples/sec\n",
      "INFO:tensorflow:training step 167 | tagging_loss_video: 12.605|tagging_loss_audio: 14.579|tagging_loss_text: 12.595|tagging_loss_image: 12.981|tagging_loss_fusion: 20.278|total_loss: 73.039 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 168 | tagging_loss_video: 14.670|tagging_loss_audio: 10.223|tagging_loss_text: 12.096|tagging_loss_image: 15.496|tagging_loss_fusion: 18.683|total_loss: 71.168 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 169 | tagging_loss_video: 12.808|tagging_loss_audio: 14.290|tagging_loss_text: 17.888|tagging_loss_image: 12.279|tagging_loss_fusion: 18.679|total_loss: 75.944 | 66.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 170 |tagging_loss_video: 17.280|tagging_loss_audio: 17.979|tagging_loss_text: 16.112|tagging_loss_image: 17.916|tagging_loss_fusion: 20.243|total_loss: 89.531 | Examples/sec: 71.69\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.49 | precision@0.5: 0.83 |recall@0.1: 0.87 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 171 | tagging_loss_video: 14.332|tagging_loss_audio: 14.465|tagging_loss_text: 16.260|tagging_loss_image: 14.040|tagging_loss_fusion: 18.082|total_loss: 77.178 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 172 | tagging_loss_video: 13.977|tagging_loss_audio: 13.519|tagging_loss_text: 16.640|tagging_loss_image: 14.054|tagging_loss_fusion: 19.212|total_loss: 77.403 | 61.29 Examples/sec\n",
      "INFO:tensorflow:training step 173 | tagging_loss_video: 14.355|tagging_loss_audio: 14.212|tagging_loss_text: 15.318|tagging_loss_image: 11.571|tagging_loss_fusion: 18.829|total_loss: 74.285 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 174 | tagging_loss_video: 15.038|tagging_loss_audio: 13.591|tagging_loss_text: 17.606|tagging_loss_image: 16.531|tagging_loss_fusion: 20.250|total_loss: 83.016 | 72.21 Examples/sec\n",
      "INFO:tensorflow:training step 175 | tagging_loss_video: 14.516|tagging_loss_audio: 12.205|tagging_loss_text: 18.835|tagging_loss_image: 12.455|tagging_loss_fusion: 19.053|total_loss: 77.063 | 65.17 Examples/sec\n",
      "INFO:tensorflow:training step 176 | tagging_loss_video: 14.964|tagging_loss_audio: 14.764|tagging_loss_text: 16.852|tagging_loss_image: 17.141|tagging_loss_fusion: 21.481|total_loss: 85.202 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 177 | tagging_loss_video: 11.976|tagging_loss_audio: 15.119|tagging_loss_text: 16.234|tagging_loss_image: 13.026|tagging_loss_fusion: 18.550|total_loss: 74.906 | 66.32 Examples/sec\n",
      "INFO:tensorflow:training step 178 | tagging_loss_video: 13.508|tagging_loss_audio: 14.753|tagging_loss_text: 18.189|tagging_loss_image: 16.515|tagging_loss_fusion: 18.564|total_loss: 81.530 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 179 | tagging_loss_video: 12.109|tagging_loss_audio: 12.775|tagging_loss_text: 13.038|tagging_loss_image: 13.159|tagging_loss_fusion: 16.779|total_loss: 67.861 | 70.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 180 |tagging_loss_video: 12.540|tagging_loss_audio: 18.766|tagging_loss_text: 15.372|tagging_loss_image: 12.813|tagging_loss_fusion: 20.204|total_loss: 79.695 | Examples/sec: 62.18\n",
      "INFO:tensorflow:GAP: 0.67 | precision@0.1: 0.40 | precision@0.5: 0.78 |recall@0.1: 0.88 | recall@0.5: 0.56\n",
      "INFO:tensorflow:training step 181 | tagging_loss_video: 14.899|tagging_loss_audio: 14.164|tagging_loss_text: 18.216|tagging_loss_image: 14.032|tagging_loss_fusion: 19.699|total_loss: 81.010 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 182 | tagging_loss_video: 14.229|tagging_loss_audio: 17.993|tagging_loss_text: 16.122|tagging_loss_image: 12.524|tagging_loss_fusion: 20.095|total_loss: 80.962 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 183 | tagging_loss_video: 13.956|tagging_loss_audio: 14.907|tagging_loss_text: 14.926|tagging_loss_image: 12.719|tagging_loss_fusion: 17.698|total_loss: 74.207 | 65.94 Examples/sec\n",
      "INFO:tensorflow:training step 184 | tagging_loss_video: 14.215|tagging_loss_audio: 18.673|tagging_loss_text: 15.306|tagging_loss_image: 12.923|tagging_loss_fusion: 21.117|total_loss: 82.234 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 185 | tagging_loss_video: 13.485|tagging_loss_audio: 14.426|tagging_loss_text: 17.600|tagging_loss_image: 15.886|tagging_loss_fusion: 17.577|total_loss: 78.974 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 186 | tagging_loss_video: 12.401|tagging_loss_audio: 12.928|tagging_loss_text: 12.438|tagging_loss_image: 11.637|tagging_loss_fusion: 17.995|total_loss: 67.399 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 187 | tagging_loss_video: 13.859|tagging_loss_audio: 11.919|tagging_loss_text: 15.024|tagging_loss_image: 15.946|tagging_loss_fusion: 19.737|total_loss: 76.485 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 188 | tagging_loss_video: 13.811|tagging_loss_audio: 17.850|tagging_loss_text: 13.798|tagging_loss_image: 17.824|tagging_loss_fusion: 19.285|total_loss: 82.568 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 189 | tagging_loss_video: 13.457|tagging_loss_audio: 15.089|tagging_loss_text: 17.608|tagging_loss_image: 12.436|tagging_loss_fusion: 18.397|total_loss: 76.986 | 67.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 190 |tagging_loss_video: 11.185|tagging_loss_audio: 17.596|tagging_loss_text: 14.222|tagging_loss_image: 15.096|tagging_loss_fusion: 21.755|total_loss: 79.855 | Examples/sec: 68.18\n",
      "INFO:tensorflow:GAP: 0.64 | precision@0.1: 0.43 | precision@0.5: 0.85 |recall@0.1: 0.88 | recall@0.5: 0.48\n",
      "INFO:tensorflow:training step 191 | tagging_loss_video: 14.539|tagging_loss_audio: 12.779|tagging_loss_text: 14.951|tagging_loss_image: 12.258|tagging_loss_fusion: 19.596|total_loss: 74.123 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 192 | tagging_loss_video: 14.139|tagging_loss_audio: 15.009|tagging_loss_text: 12.594|tagging_loss_image: 13.390|tagging_loss_fusion: 18.058|total_loss: 73.189 | 66.61 Examples/sec\n",
      "INFO:tensorflow:training step 193 | tagging_loss_video: 16.128|tagging_loss_audio: 15.398|tagging_loss_text: 16.187|tagging_loss_image: 14.638|tagging_loss_fusion: 19.495|total_loss: 81.846 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 194 | tagging_loss_video: 13.871|tagging_loss_audio: 14.598|tagging_loss_text: 13.753|tagging_loss_image: 12.056|tagging_loss_fusion: 16.427|total_loss: 70.705 | 63.83 Examples/sec\n",
      "INFO:tensorflow:training step 195 | tagging_loss_video: 16.042|tagging_loss_audio: 15.975|tagging_loss_text: 15.666|tagging_loss_image: 16.059|tagging_loss_fusion: 19.365|total_loss: 83.106 | 67.64 Examples/sec\n",
      "INFO:tensorflow:training step 196 | tagging_loss_video: 11.570|tagging_loss_audio: 14.295|tagging_loss_text: 19.519|tagging_loss_image: 15.528|tagging_loss_fusion: 19.547|total_loss: 80.458 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 197 | tagging_loss_video: 13.609|tagging_loss_audio: 13.874|tagging_loss_text: 13.380|tagging_loss_image: 11.770|tagging_loss_fusion: 16.505|total_loss: 69.139 | 61.83 Examples/sec\n",
      "INFO:tensorflow:training step 198 | tagging_loss_video: 12.289|tagging_loss_audio: 13.774|tagging_loss_text: 15.057|tagging_loss_image: 15.342|tagging_loss_fusion: 18.375|total_loss: 74.837 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 199 | tagging_loss_video: 12.894|tagging_loss_audio: 13.819|tagging_loss_text: 14.367|tagging_loss_image: 14.247|tagging_loss_fusion: 16.978|total_loss: 72.305 | 71.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 200 |tagging_loss_video: 12.485|tagging_loss_audio: 12.658|tagging_loss_text: 15.985|tagging_loss_image: 13.886|tagging_loss_fusion: 17.616|total_loss: 72.629 | Examples/sec: 66.16\n",
      "INFO:tensorflow:GAP: 0.71 | precision@0.1: 0.44 | precision@0.5: 0.85 |recall@0.1: 0.91 | recall@0.5: 0.56\n",
      "INFO:tensorflow:training step 201 | tagging_loss_video: 14.402|tagging_loss_audio: 11.053|tagging_loss_text: 14.577|tagging_loss_image: 14.766|tagging_loss_fusion: 16.334|total_loss: 71.133 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 202 | tagging_loss_video: 13.728|tagging_loss_audio: 15.092|tagging_loss_text: 14.809|tagging_loss_image: 14.194|tagging_loss_fusion: 17.654|total_loss: 75.477 | 69.56 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 203.\n",
      "INFO:tensorflow:training step 203 | tagging_loss_video: 11.885|tagging_loss_audio: 12.653|tagging_loss_text: 17.047|tagging_loss_image: 12.796|tagging_loss_fusion: 18.777|total_loss: 73.159 | 52.34 Examples/sec\n",
      "INFO:tensorflow:training step 204 | tagging_loss_video: 11.316|tagging_loss_audio: 11.568|tagging_loss_text: 15.766|tagging_loss_image: 12.632|tagging_loss_fusion: 18.210|total_loss: 69.491 | 57.33 Examples/sec\n",
      "INFO:tensorflow:training step 205 | tagging_loss_video: 12.559|tagging_loss_audio: 13.202|tagging_loss_text: 16.934|tagging_loss_image: 15.433|tagging_loss_fusion: 17.934|total_loss: 76.061 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 206 | tagging_loss_video: 12.198|tagging_loss_audio: 17.144|tagging_loss_text: 14.910|tagging_loss_image: 12.608|tagging_loss_fusion: 18.813|total_loss: 75.672 | 72.00 Examples/sec\n",
      "INFO:tensorflow:training step 207 | tagging_loss_video: 14.030|tagging_loss_audio: 16.278|tagging_loss_text: 17.054|tagging_loss_image: 14.661|tagging_loss_fusion: 18.973|total_loss: 80.997 | 62.05 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.11419\n",
      "INFO:tensorflow:training step 208 | tagging_loss_video: 14.821|tagging_loss_audio: 16.011|tagging_loss_text: 13.832|tagging_loss_image: 16.334|tagging_loss_fusion: 19.463|total_loss: 80.462 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 209 | tagging_loss_video: 16.877|tagging_loss_audio: 17.402|tagging_loss_text: 16.286|tagging_loss_image: 15.670|tagging_loss_fusion: 21.576|total_loss: 87.811 | 70.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 210 |tagging_loss_video: 15.113|tagging_loss_audio: 14.175|tagging_loss_text: 15.771|tagging_loss_image: 16.422|tagging_loss_fusion: 20.181|total_loss: 81.662 | Examples/sec: 64.75\n",
      "INFO:tensorflow:GAP: 0.66 | precision@0.1: 0.42 | precision@0.5: 0.78 |recall@0.1: 0.86 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 211 | tagging_loss_video: 13.440|tagging_loss_audio: 12.779|tagging_loss_text: 15.889|tagging_loss_image: 13.573|tagging_loss_fusion: 18.497|total_loss: 74.179 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 212 | tagging_loss_video: 10.883|tagging_loss_audio: 15.701|tagging_loss_text: 15.078|tagging_loss_image: 14.752|tagging_loss_fusion: 20.727|total_loss: 77.140 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 213 | tagging_loss_video: 12.643|tagging_loss_audio: 12.407|tagging_loss_text: 16.480|tagging_loss_image: 13.035|tagging_loss_fusion: 17.888|total_loss: 72.453 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 214 | tagging_loss_video: 10.730|tagging_loss_audio: 15.944|tagging_loss_text: 15.878|tagging_loss_image: 15.247|tagging_loss_fusion: 20.791|total_loss: 78.590 | 72.17 Examples/sec\n",
      "INFO:tensorflow:training step 215 | tagging_loss_video: 13.378|tagging_loss_audio: 13.761|tagging_loss_text: 15.509|tagging_loss_image: 12.786|tagging_loss_fusion: 17.616|total_loss: 73.050 | 56.46 Examples/sec\n",
      "INFO:tensorflow:training step 216 | tagging_loss_video: 12.566|tagging_loss_audio: 15.982|tagging_loss_text: 14.691|tagging_loss_image: 15.257|tagging_loss_fusion: 19.615|total_loss: 78.111 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 217 | tagging_loss_video: 13.716|tagging_loss_audio: 14.866|tagging_loss_text: 14.124|tagging_loss_image: 13.853|tagging_loss_fusion: 19.149|total_loss: 75.708 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 218 | tagging_loss_video: 12.454|tagging_loss_audio: 16.087|tagging_loss_text: 12.693|tagging_loss_image: 11.573|tagging_loss_fusion: 16.651|total_loss: 69.458 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 219 | tagging_loss_video: 9.716|tagging_loss_audio: 11.686|tagging_loss_text: 13.082|tagging_loss_image: 13.310|tagging_loss_fusion: 19.464|total_loss: 67.257 | 67.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 220 |tagging_loss_video: 17.077|tagging_loss_audio: 15.345|tagging_loss_text: 14.773|tagging_loss_image: 14.034|tagging_loss_fusion: 19.147|total_loss: 80.376 | Examples/sec: 70.76\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.46 | precision@0.5: 0.78 |recall@0.1: 0.88 | recall@0.5: 0.62\n",
      "INFO:tensorflow:training step 221 | tagging_loss_video: 13.092|tagging_loss_audio: 11.973|tagging_loss_text: 11.669|tagging_loss_image: 17.210|tagging_loss_fusion: 17.809|total_loss: 71.753 | 65.52 Examples/sec\n",
      "INFO:tensorflow:training step 222 | tagging_loss_video: 14.708|tagging_loss_audio: 16.514|tagging_loss_text: 17.323|tagging_loss_image: 14.139|tagging_loss_fusion: 19.706|total_loss: 82.390 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 223 | tagging_loss_video: 11.393|tagging_loss_audio: 15.432|tagging_loss_text: 17.366|tagging_loss_image: 13.730|tagging_loss_fusion: 18.790|total_loss: 76.710 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 224 | tagging_loss_video: 10.666|tagging_loss_audio: 13.635|tagging_loss_text: 13.772|tagging_loss_image: 14.675|tagging_loss_fusion: 19.420|total_loss: 72.167 | 65.18 Examples/sec\n",
      "INFO:tensorflow:training step 225 | tagging_loss_video: 13.725|tagging_loss_audio: 14.233|tagging_loss_text: 13.549|tagging_loss_image: 14.522|tagging_loss_fusion: 18.288|total_loss: 74.317 | 66.84 Examples/sec\n",
      "INFO:tensorflow:training step 226 | tagging_loss_video: 12.427|tagging_loss_audio: 14.797|tagging_loss_text: 16.596|tagging_loss_image: 12.912|tagging_loss_fusion: 18.131|total_loss: 74.865 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 227 | tagging_loss_video: 13.507|tagging_loss_audio: 13.894|tagging_loss_text: 17.792|tagging_loss_image: 14.318|tagging_loss_fusion: 18.762|total_loss: 78.274 | 68.11 Examples/sec\n",
      "INFO:tensorflow:training step 228 | tagging_loss_video: 12.634|tagging_loss_audio: 13.367|tagging_loss_text: 19.690|tagging_loss_image: 12.823|tagging_loss_fusion: 16.452|total_loss: 74.966 | 68.11 Examples/sec\n",
      "INFO:tensorflow:training step 229 | tagging_loss_video: 12.714|tagging_loss_audio: 10.739|tagging_loss_text: 11.326|tagging_loss_image: 15.353|tagging_loss_fusion: 18.241|total_loss: 68.374 | 68.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 230 |tagging_loss_video: 12.899|tagging_loss_audio: 9.972|tagging_loss_text: 13.577|tagging_loss_image: 13.654|tagging_loss_fusion: 17.483|total_loss: 67.586 | Examples/sec: 69.62\n",
      "INFO:tensorflow:GAP: 0.70 | precision@0.1: 0.40 | precision@0.5: 0.80 |recall@0.1: 0.90 | recall@0.5: 0.59\n",
      "INFO:tensorflow:training step 231 | tagging_loss_video: 16.939|tagging_loss_audio: 15.798|tagging_loss_text: 17.592|tagging_loss_image: 15.217|tagging_loss_fusion: 19.258|total_loss: 84.805 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 232 | tagging_loss_video: 11.952|tagging_loss_audio: 14.712|tagging_loss_text: 15.208|tagging_loss_image: 12.649|tagging_loss_fusion: 17.844|total_loss: 72.364 | 58.59 Examples/sec\n",
      "INFO:tensorflow:training step 233 | tagging_loss_video: 12.741|tagging_loss_audio: 16.959|tagging_loss_text: 16.206|tagging_loss_image: 16.332|tagging_loss_fusion: 19.680|total_loss: 81.918 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 234 | tagging_loss_video: 16.423|tagging_loss_audio: 14.063|tagging_loss_text: 19.094|tagging_loss_image: 14.535|tagging_loss_fusion: 17.623|total_loss: 81.738 | 72.30 Examples/sec\n",
      "INFO:tensorflow:training step 235 | tagging_loss_video: 13.294|tagging_loss_audio: 13.514|tagging_loss_text: 14.962|tagging_loss_image: 11.527|tagging_loss_fusion: 18.558|total_loss: 71.855 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 236 | tagging_loss_video: 14.771|tagging_loss_audio: 11.058|tagging_loss_text: 15.725|tagging_loss_image: 17.312|tagging_loss_fusion: 20.395|total_loss: 79.260 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 237 | tagging_loss_video: 12.871|tagging_loss_audio: 16.937|tagging_loss_text: 13.344|tagging_loss_image: 11.640|tagging_loss_fusion: 17.895|total_loss: 72.686 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 238 | tagging_loss_video: 14.054|tagging_loss_audio: 15.844|tagging_loss_text: 12.871|tagging_loss_image: 13.763|tagging_loss_fusion: 18.805|total_loss: 75.337 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 239 | tagging_loss_video: 13.595|tagging_loss_audio: 16.723|tagging_loss_text: 14.329|tagging_loss_image: 14.249|tagging_loss_fusion: 19.195|total_loss: 78.091 | 71.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 240 |tagging_loss_video: 10.711|tagging_loss_audio: 14.493|tagging_loss_text: 15.560|tagging_loss_image: 12.770|tagging_loss_fusion: 18.101|total_loss: 71.635 | Examples/sec: 61.23\n",
      "INFO:tensorflow:GAP: 0.66 | precision@0.1: 0.38 | precision@0.5: 0.78 |recall@0.1: 0.89 | recall@0.5: 0.53\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 241 | tagging_loss_video: 16.187|tagging_loss_audio: 15.580|tagging_loss_text: 17.266|tagging_loss_image: 12.827|tagging_loss_fusion: 18.281|total_loss: 80.142 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 242 | tagging_loss_video: 15.251|tagging_loss_audio: 15.465|tagging_loss_text: 17.024|tagging_loss_image: 16.539|tagging_loss_fusion: 19.478|total_loss: 83.757 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 243 | tagging_loss_video: 13.868|tagging_loss_audio: 14.571|tagging_loss_text: 14.695|tagging_loss_image: 15.173|tagging_loss_fusion: 18.932|total_loss: 77.239 | 66.54 Examples/sec\n",
      "INFO:tensorflow:training step 244 | tagging_loss_video: 15.115|tagging_loss_audio: 12.753|tagging_loss_text: 13.315|tagging_loss_image: 13.559|tagging_loss_fusion: 17.688|total_loss: 72.430 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 245 | tagging_loss_video: 14.282|tagging_loss_audio: 15.644|tagging_loss_text: 16.346|tagging_loss_image: 13.463|tagging_loss_fusion: 18.363|total_loss: 78.099 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 246 | tagging_loss_video: 13.549|tagging_loss_audio: 15.080|tagging_loss_text: 17.929|tagging_loss_image: 15.945|tagging_loss_fusion: 19.293|total_loss: 81.796 | 62.01 Examples/sec\n",
      "INFO:tensorflow:training step 247 | tagging_loss_video: 13.455|tagging_loss_audio: 14.621|tagging_loss_text: 13.015|tagging_loss_image: 13.201|tagging_loss_fusion: 17.511|total_loss: 71.802 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 248 | tagging_loss_video: 13.572|tagging_loss_audio: 16.296|tagging_loss_text: 15.259|tagging_loss_image: 15.054|tagging_loss_fusion: 20.201|total_loss: 80.382 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 249 | tagging_loss_video: 11.146|tagging_loss_audio: 13.620|tagging_loss_text: 18.089|tagging_loss_image: 14.133|tagging_loss_fusion: 19.504|total_loss: 76.491 | 68.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 250 |tagging_loss_video: 15.848|tagging_loss_audio: 15.347|tagging_loss_text: 15.679|tagging_loss_image: 13.189|tagging_loss_fusion: 18.054|total_loss: 78.117 | Examples/sec: 70.99\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.43 | precision@0.5: 0.78 |recall@0.1: 0.90 | recall@0.5: 0.59\n",
      "INFO:tensorflow:training step 251 | tagging_loss_video: 11.461|tagging_loss_audio: 10.243|tagging_loss_text: 12.128|tagging_loss_image: 16.362|tagging_loss_fusion: 18.051|total_loss: 68.244 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 252 | tagging_loss_video: 15.813|tagging_loss_audio: 13.057|tagging_loss_text: 12.647|tagging_loss_image: 13.666|tagging_loss_fusion: 18.395|total_loss: 73.579 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 253 | tagging_loss_video: 13.553|tagging_loss_audio: 16.475|tagging_loss_text: 15.895|tagging_loss_image: 18.292|tagging_loss_fusion: 20.042|total_loss: 84.256 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 254 | tagging_loss_video: 14.576|tagging_loss_audio: 14.540|tagging_loss_text: 16.250|tagging_loss_image: 16.014|tagging_loss_fusion: 20.735|total_loss: 82.114 | 63.22 Examples/sec\n",
      "INFO:tensorflow:training step 255 | tagging_loss_video: 12.150|tagging_loss_audio: 15.834|tagging_loss_text: 18.478|tagging_loss_image: 15.267|tagging_loss_fusion: 19.251|total_loss: 80.979 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 256 | tagging_loss_video: 13.492|tagging_loss_audio: 12.450|tagging_loss_text: 16.116|tagging_loss_image: 15.829|tagging_loss_fusion: 20.857|total_loss: 78.744 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 257 | tagging_loss_video: 11.074|tagging_loss_audio: 12.718|tagging_loss_text: 14.777|tagging_loss_image: 10.996|tagging_loss_fusion: 17.444|total_loss: 67.010 | 63.22 Examples/sec\n",
      "INFO:tensorflow:training step 258 | tagging_loss_video: 11.949|tagging_loss_audio: 13.381|tagging_loss_text: 16.620|tagging_loss_image: 14.073|tagging_loss_fusion: 18.170|total_loss: 74.193 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 259 | tagging_loss_video: 14.096|tagging_loss_audio: 13.446|tagging_loss_text: 15.589|tagging_loss_image: 15.519|tagging_loss_fusion: 18.492|total_loss: 77.143 | 71.41 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 260 |tagging_loss_video: 10.414|tagging_loss_audio: 9.924|tagging_loss_text: 11.426|tagging_loss_image: 11.943|tagging_loss_fusion: 17.277|total_loss: 60.984 | Examples/sec: 66.32\n",
      "INFO:tensorflow:GAP: 0.68 | precision@0.1: 0.34 | precision@0.5: 0.79 |recall@0.1: 0.90 | recall@0.5: 0.60\n",
      "INFO:tensorflow:training step 261 | tagging_loss_video: 12.986|tagging_loss_audio: 12.234|tagging_loss_text: 19.666|tagging_loss_image: 14.004|tagging_loss_fusion: 19.374|total_loss: 78.264 | 71.73 Examples/sec\n",
      "INFO:tensorflow:training step 262 | tagging_loss_video: 11.750|tagging_loss_audio: 13.640|tagging_loss_text: 14.703|tagging_loss_image: 13.463|tagging_loss_fusion: 18.442|total_loss: 71.997 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 263 | tagging_loss_video: 15.302|tagging_loss_audio: 15.775|tagging_loss_text: 16.255|tagging_loss_image: 14.373|tagging_loss_fusion: 17.624|total_loss: 79.329 | 72.15 Examples/sec\n",
      "INFO:tensorflow:training step 264 | tagging_loss_video: 12.072|tagging_loss_audio: 11.666|tagging_loss_text: 15.504|tagging_loss_image: 14.734|tagging_loss_fusion: 16.746|total_loss: 70.722 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 265 | tagging_loss_video: 14.209|tagging_loss_audio: 14.812|tagging_loss_text: 14.907|tagging_loss_image: 16.556|tagging_loss_fusion: 20.637|total_loss: 81.121 | 63.40 Examples/sec\n",
      "INFO:tensorflow:training step 266 | tagging_loss_video: 9.648|tagging_loss_audio: 12.534|tagging_loss_text: 13.196|tagging_loss_image: 13.385|tagging_loss_fusion: 19.341|total_loss: 68.103 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 267 | tagging_loss_video: 14.565|tagging_loss_audio: 14.669|tagging_loss_text: 15.521|tagging_loss_image: 12.226|tagging_loss_fusion: 16.542|total_loss: 73.523 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 268 | tagging_loss_video: 11.025|tagging_loss_audio: 12.824|tagging_loss_text: 15.034|tagging_loss_image: 13.846|tagging_loss_fusion: 15.527|total_loss: 68.255 | 62.49 Examples/sec\n",
      "INFO:tensorflow:training step 269 | tagging_loss_video: 12.183|tagging_loss_audio: 14.667|tagging_loss_text: 18.668|tagging_loss_image: 13.815|tagging_loss_fusion: 19.501|total_loss: 78.834 | 72.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 270 |tagging_loss_video: 15.020|tagging_loss_audio: 13.445|tagging_loss_text: 17.011|tagging_loss_image: 15.829|tagging_loss_fusion: 19.990|total_loss: 81.296 | Examples/sec: 70.69\n",
      "INFO:tensorflow:GAP: 0.67 | precision@0.1: 0.47 | precision@0.5: 0.81 |recall@0.1: 0.87 | recall@0.5: 0.54\n",
      "INFO:tensorflow:training step 271 | tagging_loss_video: 11.307|tagging_loss_audio: 13.687|tagging_loss_text: 15.106|tagging_loss_image: 13.489|tagging_loss_fusion: 17.717|total_loss: 71.305 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 272 | tagging_loss_video: 15.083|tagging_loss_audio: 13.147|tagging_loss_text: 15.062|tagging_loss_image: 14.427|tagging_loss_fusion: 19.102|total_loss: 76.821 | 65.71 Examples/sec\n",
      "INFO:tensorflow:training step 273 | tagging_loss_video: 11.842|tagging_loss_audio: 10.162|tagging_loss_text: 16.106|tagging_loss_image: 15.290|tagging_loss_fusion: 18.919|total_loss: 72.319 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 274 | tagging_loss_video: 12.583|tagging_loss_audio: 14.797|tagging_loss_text: 15.726|tagging_loss_image: 13.187|tagging_loss_fusion: 17.439|total_loss: 73.732 | 72.30 Examples/sec\n",
      "INFO:tensorflow:training step 275 | tagging_loss_video: 13.986|tagging_loss_audio: 11.745|tagging_loss_text: 13.923|tagging_loss_image: 11.270|tagging_loss_fusion: 17.101|total_loss: 68.025 | 71.34 Examples/sec\n",
      "INFO:tensorflow:training step 276 | tagging_loss_video: 14.688|tagging_loss_audio: 15.220|tagging_loss_text: 14.759|tagging_loss_image: 12.876|tagging_loss_fusion: 17.618|total_loss: 75.161 | 72.86 Examples/sec\n",
      "INFO:tensorflow:training step 277 | tagging_loss_video: 10.795|tagging_loss_audio: 14.492|tagging_loss_text: 15.495|tagging_loss_image: 15.409|tagging_loss_fusion: 18.114|total_loss: 74.306 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 278 | tagging_loss_video: 14.065|tagging_loss_audio: 11.105|tagging_loss_text: 18.500|tagging_loss_image: 15.194|tagging_loss_fusion: 17.625|total_loss: 76.489 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 279 | tagging_loss_video: 12.884|tagging_loss_audio: 14.132|tagging_loss_text: 16.215|tagging_loss_image: 11.526|tagging_loss_fusion: 17.204|total_loss: 71.961 | 62.08 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 280 |tagging_loss_video: 16.281|tagging_loss_audio: 16.408|tagging_loss_text: 18.736|tagging_loss_image: 13.125|tagging_loss_fusion: 18.612|total_loss: 83.162 | Examples/sec: 57.64\n",
      "INFO:tensorflow:GAP: 0.67 | precision@0.1: 0.44 | precision@0.5: 0.78 |recall@0.1: 0.88 | recall@0.5: 0.57\n",
      "INFO:tensorflow:training step 281 | tagging_loss_video: 13.068|tagging_loss_audio: 13.842|tagging_loss_text: 15.224|tagging_loss_image: 10.760|tagging_loss_fusion: 18.103|total_loss: 70.998 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 282 | tagging_loss_video: 13.804|tagging_loss_audio: 14.512|tagging_loss_text: 15.453|tagging_loss_image: 15.700|tagging_loss_fusion: 19.082|total_loss: 78.551 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 283 | tagging_loss_video: 11.103|tagging_loss_audio: 13.101|tagging_loss_text: 14.131|tagging_loss_image: 11.865|tagging_loss_fusion: 16.149|total_loss: 66.349 | 66.25 Examples/sec\n",
      "INFO:tensorflow:training step 284 | tagging_loss_video: 11.226|tagging_loss_audio: 12.849|tagging_loss_text: 13.213|tagging_loss_image: 12.752|tagging_loss_fusion: 14.657|total_loss: 64.698 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 285 | tagging_loss_video: 12.600|tagging_loss_audio: 13.034|tagging_loss_text: 16.912|tagging_loss_image: 12.930|tagging_loss_fusion: 15.906|total_loss: 71.382 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 286 | tagging_loss_video: 13.076|tagging_loss_audio: 11.495|tagging_loss_text: 16.242|tagging_loss_image: 14.016|tagging_loss_fusion: 16.004|total_loss: 70.834 | 62.22 Examples/sec\n",
      "INFO:tensorflow:training step 287 | tagging_loss_video: 12.391|tagging_loss_audio: 16.747|tagging_loss_text: 14.627|tagging_loss_image: 15.811|tagging_loss_fusion: 17.284|total_loss: 76.861 | 66.93 Examples/sec\n",
      "INFO:tensorflow:training step 288 | tagging_loss_video: 13.020|tagging_loss_audio: 13.906|tagging_loss_text: 16.784|tagging_loss_image: 13.631|tagging_loss_fusion: 16.625|total_loss: 73.965 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 289 | tagging_loss_video: 12.476|tagging_loss_audio: 13.239|tagging_loss_text: 14.962|tagging_loss_image: 13.287|tagging_loss_fusion: 16.403|total_loss: 70.367 | 69.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 290 |tagging_loss_video: 12.375|tagging_loss_audio: 15.834|tagging_loss_text: 18.475|tagging_loss_image: 15.267|tagging_loss_fusion: 20.052|total_loss: 82.003 | Examples/sec: 70.55\n",
      "INFO:tensorflow:GAP: 0.65 | precision@0.1: 0.45 | precision@0.5: 0.80 |recall@0.1: 0.87 | recall@0.5: 0.53\n",
      "INFO:tensorflow:training step 291 | tagging_loss_video: 14.916|tagging_loss_audio: 12.437|tagging_loss_text: 15.110|tagging_loss_image: 12.362|tagging_loss_fusion: 18.169|total_loss: 72.995 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 292 | tagging_loss_video: 12.849|tagging_loss_audio: 14.717|tagging_loss_text: 16.299|tagging_loss_image: 15.431|tagging_loss_fusion: 17.672|total_loss: 76.968 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 293 | tagging_loss_video: 12.366|tagging_loss_audio: 14.985|tagging_loss_text: 18.664|tagging_loss_image: 17.074|tagging_loss_fusion: 19.291|total_loss: 82.379 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 294 | tagging_loss_video: 13.179|tagging_loss_audio: 12.453|tagging_loss_text: 15.349|tagging_loss_image: 14.305|tagging_loss_fusion: 16.469|total_loss: 71.755 | 62.62 Examples/sec\n",
      "INFO:tensorflow:training step 295 | tagging_loss_video: 11.721|tagging_loss_audio: 11.930|tagging_loss_text: 14.212|tagging_loss_image: 11.340|tagging_loss_fusion: 16.354|total_loss: 65.558 | 71.75 Examples/sec\n",
      "INFO:tensorflow:training step 296 | tagging_loss_video: 11.891|tagging_loss_audio: 13.804|tagging_loss_text: 11.725|tagging_loss_image: 14.014|tagging_loss_fusion: 15.914|total_loss: 67.348 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 297 | tagging_loss_video: 11.641|tagging_loss_audio: 13.602|tagging_loss_text: 14.920|tagging_loss_image: 13.054|tagging_loss_fusion: 17.260|total_loss: 70.478 | 64.41 Examples/sec\n",
      "INFO:tensorflow:training step 298 | tagging_loss_video: 12.960|tagging_loss_audio: 14.128|tagging_loss_text: 16.665|tagging_loss_image: 13.915|tagging_loss_fusion: 16.573|total_loss: 74.240 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 299 | tagging_loss_video: 12.388|tagging_loss_audio: 11.485|tagging_loss_text: 12.997|tagging_loss_image: 12.773|tagging_loss_fusion: 15.218|total_loss: 64.860 | 72.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 300 |tagging_loss_video: 12.866|tagging_loss_audio: 12.033|tagging_loss_text: 14.828|tagging_loss_image: 14.464|tagging_loss_fusion: 19.068|total_loss: 73.259 | Examples/sec: 62.36\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.45 | precision@0.5: 0.82 |recall@0.1: 0.86 | recall@0.5: 0.59\n",
      "INFO:tensorflow:training step 301 | tagging_loss_video: 10.840|tagging_loss_audio: 11.956|tagging_loss_text: 13.143|tagging_loss_image: 12.291|tagging_loss_fusion: 16.785|total_loss: 65.015 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 302 | tagging_loss_video: 13.156|tagging_loss_audio: 16.015|tagging_loss_text: 15.981|tagging_loss_image: 13.619|tagging_loss_fusion: 17.769|total_loss: 76.540 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 303 | tagging_loss_video: 14.526|tagging_loss_audio: 18.008|tagging_loss_text: 15.165|tagging_loss_image: 13.402|tagging_loss_fusion: 19.262|total_loss: 80.363 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 304 | tagging_loss_video: 11.579|tagging_loss_audio: 15.515|tagging_loss_text: 14.796|tagging_loss_image: 14.544|tagging_loss_fusion: 18.210|total_loss: 74.643 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 305 | tagging_loss_video: 10.902|tagging_loss_audio: 12.749|tagging_loss_text: 13.176|tagging_loss_image: 15.057|tagging_loss_fusion: 16.890|total_loss: 68.774 | 62.77 Examples/sec\n",
      "INFO:tensorflow:training step 306 | tagging_loss_video: 11.405|tagging_loss_audio: 13.174|tagging_loss_text: 18.328|tagging_loss_image: 14.680|tagging_loss_fusion: 18.482|total_loss: 76.070 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 307 | tagging_loss_video: 12.307|tagging_loss_audio: 15.990|tagging_loss_text: 18.259|tagging_loss_image: 11.540|tagging_loss_fusion: 17.201|total_loss: 75.296 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 308 | tagging_loss_video: 13.536|tagging_loss_audio: 14.032|tagging_loss_text: 17.354|tagging_loss_image: 12.211|tagging_loss_fusion: 17.936|total_loss: 75.070 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 309 | tagging_loss_video: 15.628|tagging_loss_audio: 14.682|tagging_loss_text: 18.997|tagging_loss_image: 15.618|tagging_loss_fusion: 18.403|total_loss: 83.327 | 71.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 310 |tagging_loss_video: 9.512|tagging_loss_audio: 13.839|tagging_loss_text: 12.532|tagging_loss_image: 14.806|tagging_loss_fusion: 18.239|total_loss: 68.929 | Examples/sec: 70.77\n",
      "INFO:tensorflow:GAP: 0.70 | precision@0.1: 0.43 | precision@0.5: 0.80 |recall@0.1: 0.91 | recall@0.5: 0.56\n",
      "INFO:tensorflow:training step 311 | tagging_loss_video: 12.755|tagging_loss_audio: 14.801|tagging_loss_text: 12.945|tagging_loss_image: 13.087|tagging_loss_fusion: 18.335|total_loss: 71.924 | 67.74 Examples/sec\n",
      "INFO:tensorflow:training step 312 | tagging_loss_video: 12.785|tagging_loss_audio: 13.978|tagging_loss_text: 16.881|tagging_loss_image: 13.605|tagging_loss_fusion: 16.525|total_loss: 73.774 | 72.21 Examples/sec\n",
      "INFO:tensorflow:training step 313 | tagging_loss_video: 13.778|tagging_loss_audio: 11.449|tagging_loss_text: 15.968|tagging_loss_image: 14.313|tagging_loss_fusion: 18.449|total_loss: 73.957 | 67.08 Examples/sec\n",
      "INFO:tensorflow:training step 314 | tagging_loss_video: 13.917|tagging_loss_audio: 13.646|tagging_loss_text: 14.840|tagging_loss_image: 13.402|tagging_loss_fusion: 17.836|total_loss: 73.640 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 315 | tagging_loss_video: 16.183|tagging_loss_audio: 15.843|tagging_loss_text: 15.306|tagging_loss_image: 16.370|tagging_loss_fusion: 19.493|total_loss: 83.196 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 316 | tagging_loss_video: 12.716|tagging_loss_audio: 12.330|tagging_loss_text: 13.325|tagging_loss_image: 12.783|tagging_loss_fusion: 17.881|total_loss: 69.035 | 65.78 Examples/sec\n",
      "INFO:tensorflow:training step 317 | tagging_loss_video: 14.101|tagging_loss_audio: 15.918|tagging_loss_text: 16.402|tagging_loss_image: 13.774|tagging_loss_fusion: 18.273|total_loss: 78.468 | 67.74 Examples/sec\n",
      "INFO:tensorflow:training step 318 | tagging_loss_video: 9.987|tagging_loss_audio: 13.741|tagging_loss_text: 11.930|tagging_loss_image: 12.757|tagging_loss_fusion: 15.980|total_loss: 64.395 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 319 | tagging_loss_video: 11.811|tagging_loss_audio: 13.830|tagging_loss_text: 12.507|tagging_loss_image: 11.731|tagging_loss_fusion: 20.960|total_loss: 70.840 | 60.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 320 |tagging_loss_video: 13.849|tagging_loss_audio: 13.929|tagging_loss_text: 17.086|tagging_loss_image: 13.883|tagging_loss_fusion: 18.631|total_loss: 77.379 | Examples/sec: 71.81\n",
      "INFO:tensorflow:GAP: 0.71 | precision@0.1: 0.49 | precision@0.5: 0.84 |recall@0.1: 0.87 | recall@0.5: 0.60\n",
      "INFO:tensorflow:training step 321 | tagging_loss_video: 13.771|tagging_loss_audio: 14.497|tagging_loss_text: 19.513|tagging_loss_image: 13.687|tagging_loss_fusion: 18.065|total_loss: 79.533 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 322 | tagging_loss_video: 13.394|tagging_loss_audio: 13.639|tagging_loss_text: 17.741|tagging_loss_image: 13.468|tagging_loss_fusion: 17.074|total_loss: 75.316 | 64.78 Examples/sec\n",
      "INFO:tensorflow:training step 323 | tagging_loss_video: 13.856|tagging_loss_audio: 15.163|tagging_loss_text: 19.255|tagging_loss_image: 14.841|tagging_loss_fusion: 20.161|total_loss: 83.276 | 71.21 Examples/sec\n",
      "INFO:tensorflow:training step 324 | tagging_loss_video: 12.630|tagging_loss_audio: 14.813|tagging_loss_text: 15.389|tagging_loss_image: 10.562|tagging_loss_fusion: 16.128|total_loss: 69.523 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 325 | tagging_loss_video: 10.466|tagging_loss_audio: 15.184|tagging_loss_text: 16.957|tagging_loss_image: 12.016|tagging_loss_fusion: 17.873|total_loss: 72.497 | 65.73 Examples/sec\n",
      "INFO:tensorflow:training step 326 | tagging_loss_video: 16.095|tagging_loss_audio: 12.624|tagging_loss_text: 12.489|tagging_loss_image: 12.439|tagging_loss_fusion: 17.361|total_loss: 71.009 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 327 | tagging_loss_video: 12.205|tagging_loss_audio: 13.788|tagging_loss_text: 17.977|tagging_loss_image: 12.706|tagging_loss_fusion: 18.251|total_loss: 74.927 | 72.09 Examples/sec\n",
      "INFO:tensorflow:training step 328 | tagging_loss_video: 14.473|tagging_loss_audio: 13.261|tagging_loss_text: 14.067|tagging_loss_image: 13.171|tagging_loss_fusion: 17.832|total_loss: 72.805 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 329 | tagging_loss_video: 14.807|tagging_loss_audio: 14.450|tagging_loss_text: 19.632|tagging_loss_image: 15.720|tagging_loss_fusion: 20.296|total_loss: 84.905 | 68.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 330 |tagging_loss_video: 15.044|tagging_loss_audio: 16.089|tagging_loss_text: 13.696|tagging_loss_image: 13.210|tagging_loss_fusion: 18.572|total_loss: 76.611 | Examples/sec: 63.26\n",
      "INFO:tensorflow:GAP: 0.68 | precision@0.1: 0.43 | precision@0.5: 0.76 |recall@0.1: 0.90 | recall@0.5: 0.62\n",
      "INFO:tensorflow:training step 331 | tagging_loss_video: 12.198|tagging_loss_audio: 13.338|tagging_loss_text: 16.770|tagging_loss_image: 12.154|tagging_loss_fusion: 16.383|total_loss: 70.844 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 332 | tagging_loss_video: 13.277|tagging_loss_audio: 12.344|tagging_loss_text: 13.974|tagging_loss_image: 13.905|tagging_loss_fusion: 16.307|total_loss: 69.807 | 68.07 Examples/sec\n",
      "INFO:tensorflow:training step 333 | tagging_loss_video: 11.724|tagging_loss_audio: 12.020|tagging_loss_text: 16.323|tagging_loss_image: 13.443|tagging_loss_fusion: 15.421|total_loss: 68.932 | 65.75 Examples/sec\n",
      "INFO:tensorflow:training step 334 | tagging_loss_video: 13.128|tagging_loss_audio: 17.403|tagging_loss_text: 14.745|tagging_loss_image: 13.134|tagging_loss_fusion: 18.226|total_loss: 76.636 | 72.33 Examples/sec\n",
      "INFO:tensorflow:training step 335 | tagging_loss_video: 12.359|tagging_loss_audio: 13.434|tagging_loss_text: 11.644|tagging_loss_image: 11.881|tagging_loss_fusion: 18.035|total_loss: 67.353 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 336 | tagging_loss_video: 11.737|tagging_loss_audio: 14.863|tagging_loss_text: 15.505|tagging_loss_image: 14.342|tagging_loss_fusion: 16.143|total_loss: 72.589 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 337 | tagging_loss_video: 11.198|tagging_loss_audio: 12.609|tagging_loss_text: 16.860|tagging_loss_image: 13.236|tagging_loss_fusion: 16.451|total_loss: 70.354 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 338 | tagging_loss_video: 14.219|tagging_loss_audio: 13.592|tagging_loss_text: 14.344|tagging_loss_image: 14.164|tagging_loss_fusion: 15.416|total_loss: 71.736 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 339 | tagging_loss_video: 10.427|tagging_loss_audio: 12.837|tagging_loss_text: 15.985|tagging_loss_image: 12.187|tagging_loss_fusion: 17.043|total_loss: 68.478 | 63.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 340 |tagging_loss_video: 9.828|tagging_loss_audio: 14.643|tagging_loss_text: 14.583|tagging_loss_image: 10.234|tagging_loss_fusion: 15.356|total_loss: 64.643 | Examples/sec: 68.08\n",
      "INFO:tensorflow:GAP: 0.75 | precision@0.1: 0.43 | precision@0.5: 0.83 |recall@0.1: 0.92 | recall@0.5: 0.64\n",
      "INFO:tensorflow:training step 341 | tagging_loss_video: 11.814|tagging_loss_audio: 11.641|tagging_loss_text: 16.927|tagging_loss_image: 14.630|tagging_loss_fusion: 16.882|total_loss: 71.893 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 342 | tagging_loss_video: 11.826|tagging_loss_audio: 13.213|tagging_loss_text: 13.406|tagging_loss_image: 13.436|tagging_loss_fusion: 16.817|total_loss: 68.698 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 343 | tagging_loss_video: 12.888|tagging_loss_audio: 13.330|tagging_loss_text: 12.142|tagging_loss_image: 12.213|tagging_loss_fusion: 17.760|total_loss: 68.332 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 344 | tagging_loss_video: 12.815|tagging_loss_audio: 12.039|tagging_loss_text: 12.439|tagging_loss_image: 12.881|tagging_loss_fusion: 16.772|total_loss: 66.946 | 62.52 Examples/sec\n",
      "INFO:tensorflow:training step 345 | tagging_loss_video: 11.367|tagging_loss_audio: 11.223|tagging_loss_text: 17.685|tagging_loss_image: 11.815|tagging_loss_fusion: 15.454|total_loss: 67.544 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 346 | tagging_loss_video: 13.222|tagging_loss_audio: 10.554|tagging_loss_text: 16.215|tagging_loss_image: 13.513|tagging_loss_fusion: 16.652|total_loss: 70.156 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 347 | tagging_loss_video: 14.586|tagging_loss_audio: 11.765|tagging_loss_text: 13.689|tagging_loss_image: 10.027|tagging_loss_fusion: 16.495|total_loss: 66.562 | 61.50 Examples/sec\n",
      "INFO:tensorflow:training step 348 | tagging_loss_video: 13.329|tagging_loss_audio: 15.463|tagging_loss_text: 17.739|tagging_loss_image: 11.806|tagging_loss_fusion: 19.054|total_loss: 77.391 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 349 | tagging_loss_video: 15.464|tagging_loss_audio: 15.404|tagging_loss_text: 19.386|tagging_loss_image: 15.845|tagging_loss_fusion: 20.428|total_loss: 86.527 | 70.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 350 |tagging_loss_video: 11.917|tagging_loss_audio: 17.071|tagging_loss_text: 14.365|tagging_loss_image: 15.653|tagging_loss_fusion: 19.691|total_loss: 78.696 | Examples/sec: 66.10\n",
      "INFO:tensorflow:GAP: 0.68 | precision@0.1: 0.42 | precision@0.5: 0.80 |recall@0.1: 0.87 | recall@0.5: 0.54\n",
      "INFO:tensorflow:training step 351 | tagging_loss_video: 13.156|tagging_loss_audio: 14.549|tagging_loss_text: 17.257|tagging_loss_image: 12.940|tagging_loss_fusion: 16.780|total_loss: 74.684 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 352 | tagging_loss_video: 12.942|tagging_loss_audio: 13.531|tagging_loss_text: 17.343|tagging_loss_image: 14.295|tagging_loss_fusion: 19.195|total_loss: 77.305 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 353 | tagging_loss_video: 11.647|tagging_loss_audio: 16.501|tagging_loss_text: 14.943|tagging_loss_image: 12.213|tagging_loss_fusion: 16.759|total_loss: 72.063 | 71.90 Examples/sec\n",
      "INFO:tensorflow:training step 354 | tagging_loss_video: 12.858|tagging_loss_audio: 14.500|tagging_loss_text: 16.543|tagging_loss_image: 12.988|tagging_loss_fusion: 18.528|total_loss: 75.417 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 355 | tagging_loss_video: 12.020|tagging_loss_audio: 11.912|tagging_loss_text: 15.234|tagging_loss_image: 11.237|tagging_loss_fusion: 16.351|total_loss: 66.753 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 356 | tagging_loss_video: 12.516|tagging_loss_audio: 17.168|tagging_loss_text: 16.697|tagging_loss_image: 15.281|tagging_loss_fusion: 17.841|total_loss: 79.503 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 357 | tagging_loss_video: 12.892|tagging_loss_audio: 12.063|tagging_loss_text: 16.005|tagging_loss_image: 13.553|tagging_loss_fusion: 17.828|total_loss: 72.340 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 358 | tagging_loss_video: 11.990|tagging_loss_audio: 12.801|tagging_loss_text: 13.873|tagging_loss_image: 11.516|tagging_loss_fusion: 15.632|total_loss: 65.813 | 63.54 Examples/sec\n",
      "INFO:tensorflow:training step 359 | tagging_loss_video: 11.452|tagging_loss_audio: 12.021|tagging_loss_text: 15.148|tagging_loss_image: 12.423|tagging_loss_fusion: 18.158|total_loss: 69.202 | 69.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 360 |tagging_loss_video: 13.834|tagging_loss_audio: 14.184|tagging_loss_text: 14.210|tagging_loss_image: 13.421|tagging_loss_fusion: 17.441|total_loss: 73.090 | Examples/sec: 63.60\n",
      "INFO:tensorflow:GAP: 0.72 | precision@0.1: 0.46 | precision@0.5: 0.82 |recall@0.1: 0.89 | recall@0.5: 0.61\n",
      "INFO:tensorflow:training step 361 | tagging_loss_video: 13.116|tagging_loss_audio: 15.877|tagging_loss_text: 13.102|tagging_loss_image: 11.716|tagging_loss_fusion: 16.036|total_loss: 69.848 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 362 | tagging_loss_video: 14.679|tagging_loss_audio: 14.272|tagging_loss_text: 17.910|tagging_loss_image: 14.419|tagging_loss_fusion: 17.120|total_loss: 78.399 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 363 | tagging_loss_video: 15.037|tagging_loss_audio: 14.445|tagging_loss_text: 17.104|tagging_loss_image: 13.541|tagging_loss_fusion: 17.044|total_loss: 77.171 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 364 | tagging_loss_video: 13.156|tagging_loss_audio: 14.606|tagging_loss_text: 18.091|tagging_loss_image: 13.669|tagging_loss_fusion: 16.928|total_loss: 76.449 | 65.09 Examples/sec\n",
      "INFO:tensorflow:training step 365 | tagging_loss_video: 13.431|tagging_loss_audio: 12.685|tagging_loss_text: 16.332|tagging_loss_image: 15.062|tagging_loss_fusion: 17.840|total_loss: 75.349 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 366 | tagging_loss_video: 12.238|tagging_loss_audio: 11.261|tagging_loss_text: 19.477|tagging_loss_image: 12.186|tagging_loss_fusion: 16.637|total_loss: 71.800 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 367 | tagging_loss_video: 13.386|tagging_loss_audio: 14.934|tagging_loss_text: 10.689|tagging_loss_image: 13.525|tagging_loss_fusion: 17.310|total_loss: 69.844 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 368 | tagging_loss_video: 9.440|tagging_loss_audio: 13.498|tagging_loss_text: 12.230|tagging_loss_image: 13.244|tagging_loss_fusion: 16.607|total_loss: 65.019 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 369 | tagging_loss_video: 11.904|tagging_loss_audio: 16.521|tagging_loss_text: 14.511|tagging_loss_image: 14.576|tagging_loss_fusion: 16.869|total_loss: 74.382 | 72.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 370 |tagging_loss_video: 10.167|tagging_loss_audio: 11.776|tagging_loss_text: 15.662|tagging_loss_image: 10.913|tagging_loss_fusion: 16.743|total_loss: 65.260 | Examples/sec: 69.62\n",
      "INFO:tensorflow:GAP: 0.70 | precision@0.1: 0.43 | precision@0.5: 0.79 |recall@0.1: 0.92 | recall@0.5: 0.58\n",
      "INFO:tensorflow:training step 371 | tagging_loss_video: 13.634|tagging_loss_audio: 16.027|tagging_loss_text: 13.696|tagging_loss_image: 15.203|tagging_loss_fusion: 16.322|total_loss: 74.881 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 372 | tagging_loss_video: 11.695|tagging_loss_audio: 10.847|tagging_loss_text: 15.380|tagging_loss_image: 13.552|tagging_loss_fusion: 17.816|total_loss: 69.290 | 64.22 Examples/sec\n",
      "INFO:tensorflow:training step 373 | tagging_loss_video: 13.693|tagging_loss_audio: 12.577|tagging_loss_text: 18.714|tagging_loss_image: 13.573|tagging_loss_fusion: 18.242|total_loss: 76.801 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 374 | tagging_loss_video: 13.231|tagging_loss_audio: 11.682|tagging_loss_text: 13.955|tagging_loss_image: 11.446|tagging_loss_fusion: 16.427|total_loss: 66.741 | 72.08 Examples/sec\n",
      "INFO:tensorflow:training step 375 | tagging_loss_video: 11.474|tagging_loss_audio: 12.725|tagging_loss_text: 14.031|tagging_loss_image: 12.857|tagging_loss_fusion: 16.291|total_loss: 67.379 | 63.68 Examples/sec\n",
      "INFO:tensorflow:training step 376 | tagging_loss_video: 14.642|tagging_loss_audio: 14.602|tagging_loss_text: 15.767|tagging_loss_image: 15.138|tagging_loss_fusion: 17.833|total_loss: 77.981 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 377 | tagging_loss_video: 12.085|tagging_loss_audio: 14.979|tagging_loss_text: 16.014|tagging_loss_image: 11.896|tagging_loss_fusion: 16.381|total_loss: 71.355 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 378 | tagging_loss_video: 13.301|tagging_loss_audio: 14.011|tagging_loss_text: 16.189|tagging_loss_image: 13.341|tagging_loss_fusion: 16.662|total_loss: 73.504 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 379 | tagging_loss_video: 14.978|tagging_loss_audio: 15.079|tagging_loss_text: 15.684|tagging_loss_image: 12.497|tagging_loss_fusion: 17.851|total_loss: 76.090 | 68.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 380 |tagging_loss_video: 12.820|tagging_loss_audio: 14.004|tagging_loss_text: 16.858|tagging_loss_image: 14.728|tagging_loss_fusion: 17.200|total_loss: 75.611 | Examples/sec: 62.90\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.43 | precision@0.5: 0.78 |recall@0.1: 0.89 | recall@0.5: 0.59\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 381 | tagging_loss_video: 13.144|tagging_loss_audio: 13.380|tagging_loss_text: 17.791|tagging_loss_image: 14.495|tagging_loss_fusion: 17.505|total_loss: 76.315 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 382 | tagging_loss_video: 14.368|tagging_loss_audio: 13.415|tagging_loss_text: 14.895|tagging_loss_image: 13.430|tagging_loss_fusion: 17.696|total_loss: 73.803 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 383 | tagging_loss_video: 12.735|tagging_loss_audio: 14.414|tagging_loss_text: 17.461|tagging_loss_image: 13.531|tagging_loss_fusion: 17.587|total_loss: 75.728 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 384 | tagging_loss_video: 14.149|tagging_loss_audio: 15.702|tagging_loss_text: 16.030|tagging_loss_image: 11.908|tagging_loss_fusion: 15.740|total_loss: 73.529 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 385 | tagging_loss_video: 13.557|tagging_loss_audio: 14.999|tagging_loss_text: 17.523|tagging_loss_image: 13.276|tagging_loss_fusion: 16.509|total_loss: 75.863 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 386 | tagging_loss_video: 12.841|tagging_loss_audio: 15.159|tagging_loss_text: 19.216|tagging_loss_image: 14.780|tagging_loss_fusion: 18.730|total_loss: 80.725 | 63.30 Examples/sec\n",
      "INFO:tensorflow:training step 387 | tagging_loss_video: 11.019|tagging_loss_audio: 13.260|tagging_loss_text: 15.015|tagging_loss_image: 12.762|tagging_loss_fusion: 17.209|total_loss: 69.266 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 388 | tagging_loss_video: 11.834|tagging_loss_audio: 14.309|tagging_loss_text: 16.042|tagging_loss_image: 13.890|tagging_loss_fusion: 19.635|total_loss: 75.711 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 389 | tagging_loss_video: 15.083|tagging_loss_audio: 15.030|tagging_loss_text: 14.331|tagging_loss_image: 14.144|tagging_loss_fusion: 17.547|total_loss: 76.136 | 63.33 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 390 |tagging_loss_video: 11.710|tagging_loss_audio: 12.240|tagging_loss_text: 14.154|tagging_loss_image: 14.675|tagging_loss_fusion: 17.819|total_loss: 70.597 | Examples/sec: 66.91\n",
      "INFO:tensorflow:GAP: 0.70 | precision@0.1: 0.42 | precision@0.5: 0.80 |recall@0.1: 0.89 | recall@0.5: 0.60\n",
      "INFO:tensorflow:training step 391 | tagging_loss_video: 11.829|tagging_loss_audio: 13.364|tagging_loss_text: 14.430|tagging_loss_image: 11.718|tagging_loss_fusion: 16.407|total_loss: 67.748 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 392 | tagging_loss_video: 11.957|tagging_loss_audio: 16.212|tagging_loss_text: 12.419|tagging_loss_image: 13.473|tagging_loss_fusion: 17.556|total_loss: 71.616 | 70.56 Examples/sec\n",
      "INFO:tensorflow:training step 393 | tagging_loss_video: 15.266|tagging_loss_audio: 16.666|tagging_loss_text: 16.439|tagging_loss_image: 14.852|tagging_loss_fusion: 18.879|total_loss: 82.102 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 394 | tagging_loss_video: 15.026|tagging_loss_audio: 15.389|tagging_loss_text: 12.607|tagging_loss_image: 14.519|tagging_loss_fusion: 19.446|total_loss: 76.988 | 61.63 Examples/sec\n",
      "INFO:tensorflow:training step 395 | tagging_loss_video: 15.787|tagging_loss_audio: 14.883|tagging_loss_text: 16.393|tagging_loss_image: 12.009|tagging_loss_fusion: 17.360|total_loss: 76.431 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 396 | tagging_loss_video: 12.544|tagging_loss_audio: 13.206|tagging_loss_text: 19.073|tagging_loss_image: 15.362|tagging_loss_fusion: 18.412|total_loss: 78.596 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 397 | tagging_loss_video: 11.455|tagging_loss_audio: 13.359|tagging_loss_text: 12.645|tagging_loss_image: 12.452|tagging_loss_fusion: 15.876|total_loss: 65.787 | 63.68 Examples/sec\n",
      "INFO:tensorflow:training step 398 | tagging_loss_video: 11.080|tagging_loss_audio: 13.486|tagging_loss_text: 14.010|tagging_loss_image: 11.627|tagging_loss_fusion: 16.511|total_loss: 66.715 | 72.19 Examples/sec\n",
      "INFO:tensorflow:training step 399 | tagging_loss_video: 13.320|tagging_loss_audio: 12.652|tagging_loss_text: 15.704|tagging_loss_image: 15.288|tagging_loss_fusion: 16.166|total_loss: 73.130 | 72.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 400 |tagging_loss_video: 11.828|tagging_loss_audio: 12.911|tagging_loss_text: 12.932|tagging_loss_image: 13.044|tagging_loss_fusion: 15.539|total_loss: 66.253 | Examples/sec: 63.77\n",
      "INFO:tensorflow:GAP: 0.72 | precision@0.1: 0.42 | precision@0.5: 0.78 |recall@0.1: 0.94 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 401 | tagging_loss_video: 14.338|tagging_loss_audio: 15.006|tagging_loss_text: 16.951|tagging_loss_image: 14.427|tagging_loss_fusion: 18.030|total_loss: 78.752 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 402 | tagging_loss_video: 12.917|tagging_loss_audio: 15.249|tagging_loss_text: 11.980|tagging_loss_image: 10.360|tagging_loss_fusion: 17.531|total_loss: 68.037 | 72.29 Examples/sec\n",
      "INFO:tensorflow:training step 403 | tagging_loss_video: 12.268|tagging_loss_audio: 12.667|tagging_loss_text: 16.398|tagging_loss_image: 12.205|tagging_loss_fusion: 16.375|total_loss: 69.914 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 404 | tagging_loss_video: 14.012|tagging_loss_audio: 11.510|tagging_loss_text: 12.367|tagging_loss_image: 11.798|tagging_loss_fusion: 15.259|total_loss: 64.945 | 65.24 Examples/sec\n",
      "INFO:tensorflow:training step 405 | tagging_loss_video: 12.857|tagging_loss_audio: 14.119|tagging_loss_text: 15.658|tagging_loss_image: 15.227|tagging_loss_fusion: 19.541|total_loss: 77.402 | 61.45 Examples/sec\n",
      "INFO:tensorflow:training step 406 | tagging_loss_video: 13.121|tagging_loss_audio: 14.160|tagging_loss_text: 15.838|tagging_loss_image: 13.475|tagging_loss_fusion: 17.485|total_loss: 74.079 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 407 | tagging_loss_video: 11.405|tagging_loss_audio: 14.231|tagging_loss_text: 10.818|tagging_loss_image: 12.173|tagging_loss_fusion: 15.637|total_loss: 64.263 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 408 | tagging_loss_video: 11.559|tagging_loss_audio: 13.073|tagging_loss_text: 13.708|tagging_loss_image: 11.406|tagging_loss_fusion: 14.755|total_loss: 64.500 | 66.10 Examples/sec\n",
      "INFO:tensorflow:training step 409 | tagging_loss_video: 10.966|tagging_loss_audio: 14.775|tagging_loss_text: 19.814|tagging_loss_image: 13.693|tagging_loss_fusion: 18.622|total_loss: 77.869 | 68.53 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 410 |tagging_loss_video: 14.086|tagging_loss_audio: 14.914|tagging_loss_text: 19.085|tagging_loss_image: 16.096|tagging_loss_fusion: 18.690|total_loss: 82.872 | Examples/sec: 70.51\n",
      "INFO:tensorflow:GAP: 0.70 | precision@0.1: 0.49 | precision@0.5: 0.81 |recall@0.1: 0.87 | recall@0.5: 0.58\n",
      "INFO:tensorflow:training step 411 | tagging_loss_video: 12.022|tagging_loss_audio: 10.647|tagging_loss_text: 16.243|tagging_loss_image: 12.947|tagging_loss_fusion: 16.069|total_loss: 67.927 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 412 | tagging_loss_video: 10.451|tagging_loss_audio: 15.285|tagging_loss_text: 14.923|tagging_loss_image: 13.415|tagging_loss_fusion: 17.876|total_loss: 71.950 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 413 | tagging_loss_video: 10.480|tagging_loss_audio: 13.153|tagging_loss_text: 15.910|tagging_loss_image: 14.341|tagging_loss_fusion: 17.866|total_loss: 71.750 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 414 | tagging_loss_video: 11.435|tagging_loss_audio: 12.694|tagging_loss_text: 16.382|tagging_loss_image: 13.565|tagging_loss_fusion: 16.795|total_loss: 70.871 | 62.90 Examples/sec\n",
      "INFO:tensorflow:training step 415 | tagging_loss_video: 12.261|tagging_loss_audio: 13.375|tagging_loss_text: 17.184|tagging_loss_image: 11.294|tagging_loss_fusion: 15.473|total_loss: 69.587 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 416 | tagging_loss_video: 11.946|tagging_loss_audio: 13.983|tagging_loss_text: 13.346|tagging_loss_image: 12.243|tagging_loss_fusion: 17.341|total_loss: 68.859 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 417 | tagging_loss_video: 11.883|tagging_loss_audio: 15.347|tagging_loss_text: 14.841|tagging_loss_image: 14.016|tagging_loss_fusion: 18.613|total_loss: 74.700 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 418 | tagging_loss_video: 11.590|tagging_loss_audio: 12.523|tagging_loss_text: 12.802|tagging_loss_image: 15.202|tagging_loss_fusion: 16.648|total_loss: 68.766 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 419 | tagging_loss_video: 14.241|tagging_loss_audio: 13.909|tagging_loss_text: 16.020|tagging_loss_image: 13.108|tagging_loss_fusion: 16.334|total_loss: 73.612 | 68.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 420 |tagging_loss_video: 12.397|tagging_loss_audio: 14.852|tagging_loss_text: 10.976|tagging_loss_image: 16.176|tagging_loss_fusion: 16.784|total_loss: 71.186 | Examples/sec: 61.70\n",
      "INFO:tensorflow:GAP: 0.72 | precision@0.1: 0.47 | precision@0.5: 0.82 |recall@0.1: 0.93 | recall@0.5: 0.55\n",
      "INFO:tensorflow:training step 421 | tagging_loss_video: 11.042|tagging_loss_audio: 14.146|tagging_loss_text: 15.842|tagging_loss_image: 13.526|tagging_loss_fusion: 17.039|total_loss: 71.595 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 422 | tagging_loss_video: 14.080|tagging_loss_audio: 15.504|tagging_loss_text: 15.315|tagging_loss_image: 13.635|tagging_loss_fusion: 16.329|total_loss: 74.863 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 423 | tagging_loss_video: 10.662|tagging_loss_audio: 9.175|tagging_loss_text: 15.267|tagging_loss_image: 13.092|tagging_loss_fusion: 14.756|total_loss: 62.952 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 424 | tagging_loss_video: 9.132|tagging_loss_audio: 11.479|tagging_loss_text: 16.892|tagging_loss_image: 12.692|tagging_loss_fusion: 14.269|total_loss: 64.464 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 425 | tagging_loss_video: 11.608|tagging_loss_audio: 12.396|tagging_loss_text: 15.958|tagging_loss_image: 13.030|tagging_loss_fusion: 15.380|total_loss: 68.372 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 426 | tagging_loss_video: 8.612|tagging_loss_audio: 13.910|tagging_loss_text: 14.953|tagging_loss_image: 13.638|tagging_loss_fusion: 16.835|total_loss: 67.948 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 427 | tagging_loss_video: 11.911|tagging_loss_audio: 13.620|tagging_loss_text: 17.388|tagging_loss_image: 11.295|tagging_loss_fusion: 16.327|total_loss: 70.541 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 428 | tagging_loss_video: 13.153|tagging_loss_audio: 11.500|tagging_loss_text: 13.977|tagging_loss_image: 11.383|tagging_loss_fusion: 15.367|total_loss: 65.380 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 429 | tagging_loss_video: 11.857|tagging_loss_audio: 13.818|tagging_loss_text: 13.073|tagging_loss_image: 12.075|tagging_loss_fusion: 14.373|total_loss: 65.196 | 65.75 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 430 |tagging_loss_video: 12.492|tagging_loss_audio: 15.998|tagging_loss_text: 14.878|tagging_loss_image: 16.299|tagging_loss_fusion: 17.475|total_loss: 77.141 | Examples/sec: 69.92\n",
      "INFO:tensorflow:GAP: 0.73 | precision@0.1: 0.51 | precision@0.5: 0.84 |recall@0.1: 0.89 | recall@0.5: 0.57\n",
      "INFO:tensorflow:training step 431 | tagging_loss_video: 10.665|tagging_loss_audio: 12.543|tagging_loss_text: 12.997|tagging_loss_image: 12.137|tagging_loss_fusion: 16.610|total_loss: 64.952 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 432 | tagging_loss_video: 11.849|tagging_loss_audio: 15.372|tagging_loss_text: 11.424|tagging_loss_image: 14.838|tagging_loss_fusion: 17.083|total_loss: 70.566 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 433 | tagging_loss_video: 14.846|tagging_loss_audio: 17.740|tagging_loss_text: 17.215|tagging_loss_image: 12.769|tagging_loss_fusion: 18.061|total_loss: 80.631 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 434 | tagging_loss_video: 11.728|tagging_loss_audio: 15.152|tagging_loss_text: 16.803|tagging_loss_image: 13.990|tagging_loss_fusion: 15.806|total_loss: 73.479 | 59.58 Examples/sec\n",
      "INFO:tensorflow:training step 435 | tagging_loss_video: 12.542|tagging_loss_audio: 12.365|tagging_loss_text: 14.148|tagging_loss_image: 13.333|tagging_loss_fusion: 14.764|total_loss: 67.152 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 436 | tagging_loss_video: 10.635|tagging_loss_audio: 11.822|tagging_loss_text: 11.357|tagging_loss_image: 10.289|tagging_loss_fusion: 14.869|total_loss: 58.972 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 437 | tagging_loss_video: 11.125|tagging_loss_audio: 14.623|tagging_loss_text: 14.386|tagging_loss_image: 12.025|tagging_loss_fusion: 16.077|total_loss: 68.237 | 64.36 Examples/sec\n",
      "INFO:tensorflow:training step 438 | tagging_loss_video: 11.856|tagging_loss_audio: 13.531|tagging_loss_text: 14.937|tagging_loss_image: 11.361|tagging_loss_fusion: 16.034|total_loss: 67.719 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 439 | tagging_loss_video: 11.789|tagging_loss_audio: 14.032|tagging_loss_text: 17.077|tagging_loss_image: 11.931|tagging_loss_fusion: 15.167|total_loss: 69.995 | 69.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 440 |tagging_loss_video: 14.193|tagging_loss_audio: 13.547|tagging_loss_text: 16.102|tagging_loss_image: 13.355|tagging_loss_fusion: 16.502|total_loss: 73.698 | Examples/sec: 65.86\n",
      "INFO:tensorflow:GAP: 0.75 | precision@0.1: 0.48 | precision@0.5: 0.82 |recall@0.1: 0.90 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 441 | tagging_loss_video: 11.936|tagging_loss_audio: 15.698|tagging_loss_text: 19.033|tagging_loss_image: 13.802|tagging_loss_fusion: 15.656|total_loss: 76.126 | 68.74 Examples/sec\n",
      "INFO:tensorflow:training step 442 | tagging_loss_video: 14.158|tagging_loss_audio: 15.364|tagging_loss_text: 20.098|tagging_loss_image: 15.329|tagging_loss_fusion: 17.474|total_loss: 82.424 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 443 | tagging_loss_video: 14.741|tagging_loss_audio: 10.980|tagging_loss_text: 14.494|tagging_loss_image: 13.304|tagging_loss_fusion: 17.320|total_loss: 70.840 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 444 | tagging_loss_video: 10.666|tagging_loss_audio: 14.158|tagging_loss_text: 16.133|tagging_loss_image: 11.026|tagging_loss_fusion: 17.016|total_loss: 69.000 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 445 | tagging_loss_video: 8.822|tagging_loss_audio: 13.442|tagging_loss_text: 15.716|tagging_loss_image: 13.636|tagging_loss_fusion: 16.240|total_loss: 67.855 | 60.43 Examples/sec\n",
      "INFO:tensorflow:training step 446 | tagging_loss_video: 13.626|tagging_loss_audio: 14.787|tagging_loss_text: 18.091|tagging_loss_image: 11.752|tagging_loss_fusion: 17.868|total_loss: 76.123 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 447 | tagging_loss_video: 11.206|tagging_loss_audio: 11.126|tagging_loss_text: 12.800|tagging_loss_image: 14.593|tagging_loss_fusion: 16.092|total_loss: 65.815 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 448 | tagging_loss_video: 14.991|tagging_loss_audio: 12.810|tagging_loss_text: 15.160|tagging_loss_image: 12.839|tagging_loss_fusion: 16.111|total_loss: 71.911 | 62.94 Examples/sec\n",
      "INFO:tensorflow:training step 449 | tagging_loss_video: 13.211|tagging_loss_audio: 16.161|tagging_loss_text: 17.259|tagging_loss_image: 13.232|tagging_loss_fusion: 16.874|total_loss: 76.736 | 69.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 450 |tagging_loss_video: 11.820|tagging_loss_audio: 12.048|tagging_loss_text: 17.065|tagging_loss_image: 13.039|tagging_loss_fusion: 16.427|total_loss: 70.400 | Examples/sec: 70.56\n",
      "INFO:tensorflow:GAP: 0.74 | precision@0.1: 0.49 | precision@0.5: 0.83 |recall@0.1: 0.89 | recall@0.5: 0.61\n",
      "INFO:tensorflow:training step 451 | tagging_loss_video: 12.865|tagging_loss_audio: 12.506|tagging_loss_text: 16.054|tagging_loss_image: 13.635|tagging_loss_fusion: 16.527|total_loss: 71.588 | 65.36 Examples/sec\n",
      "INFO:tensorflow:training step 452 | tagging_loss_video: 12.252|tagging_loss_audio: 14.635|tagging_loss_text: 17.572|tagging_loss_image: 14.643|tagging_loss_fusion: 16.248|total_loss: 75.350 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 453 | tagging_loss_video: 12.230|tagging_loss_audio: 13.792|tagging_loss_text: 17.352|tagging_loss_image: 13.700|tagging_loss_fusion: 17.420|total_loss: 74.495 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 454 | tagging_loss_video: 11.427|tagging_loss_audio: 16.199|tagging_loss_text: 16.049|tagging_loss_image: 14.721|tagging_loss_fusion: 16.954|total_loss: 75.350 | 65.98 Examples/sec\n",
      "INFO:tensorflow:training step 455 | tagging_loss_video: 14.340|tagging_loss_audio: 16.580|tagging_loss_text: 17.207|tagging_loss_image: 16.966|tagging_loss_fusion: 17.066|total_loss: 82.159 | 71.49 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 456.\n",
      "INFO:tensorflow:training step 456 | tagging_loss_video: 12.135|tagging_loss_audio: 17.394|tagging_loss_text: 14.022|tagging_loss_image: 12.408|tagging_loss_fusion: 15.607|total_loss: 71.567 | 48.81 Examples/sec\n",
      "INFO:tensorflow:training step 457 | tagging_loss_video: 11.201|tagging_loss_audio: 8.827|tagging_loss_text: 14.288|tagging_loss_image: 10.591|tagging_loss_fusion: 13.618|total_loss: 58.525 | 62.03 Examples/sec\n",
      "INFO:tensorflow:training step 458 | tagging_loss_video: 11.982|tagging_loss_audio: 15.198|tagging_loss_text: 18.861|tagging_loss_image: 13.559|tagging_loss_fusion: 18.918|total_loss: 78.517 | 63.71 Examples/sec\n",
      "INFO:tensorflow:training step 459 | tagging_loss_video: 12.201|tagging_loss_audio: 13.581|tagging_loss_text: 13.797|tagging_loss_image: 12.812|tagging_loss_fusion: 18.781|total_loss: 71.172 | 68.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 460 |tagging_loss_video: 13.446|tagging_loss_audio: 14.845|tagging_loss_text: 13.757|tagging_loss_image: 12.140|tagging_loss_fusion: 17.171|total_loss: 71.360 | Examples/sec: 70.12\n",
      "INFO:tensorflow:GAP: 0.74 | precision@0.1: 0.52 | precision@0.5: 0.83 |recall@0.1: 0.90 | recall@0.5: 0.62\n",
      "INFO:tensorflow:global_step/sec: 2.11779\n",
      "INFO:tensorflow:training step 461 | tagging_loss_video: 12.106|tagging_loss_audio: 12.714|tagging_loss_text: 14.729|tagging_loss_image: 10.957|tagging_loss_fusion: 16.862|total_loss: 67.369 | 66.51 Examples/sec\n",
      "INFO:tensorflow:training step 462 | tagging_loss_video: 14.291|tagging_loss_audio: 14.048|tagging_loss_text: 16.110|tagging_loss_image: 13.719|tagging_loss_fusion: 19.918|total_loss: 78.087 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 463 | tagging_loss_video: 11.198|tagging_loss_audio: 14.733|tagging_loss_text: 15.277|tagging_loss_image: 12.826|tagging_loss_fusion: 15.481|total_loss: 69.516 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 464 | tagging_loss_video: 12.563|tagging_loss_audio: 9.912|tagging_loss_text: 16.118|tagging_loss_image: 11.496|tagging_loss_fusion: 15.896|total_loss: 65.985 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 465 | tagging_loss_video: 11.818|tagging_loss_audio: 16.082|tagging_loss_text: 11.642|tagging_loss_image: 14.055|tagging_loss_fusion: 17.370|total_loss: 70.966 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 466 | tagging_loss_video: 11.499|tagging_loss_audio: 13.736|tagging_loss_text: 17.910|tagging_loss_image: 13.413|tagging_loss_fusion: 17.400|total_loss: 73.957 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 467 | tagging_loss_video: 11.775|tagging_loss_audio: 10.854|tagging_loss_text: 14.100|tagging_loss_image: 13.575|tagging_loss_fusion: 15.738|total_loss: 66.042 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 468 | tagging_loss_video: 13.544|tagging_loss_audio: 13.005|tagging_loss_text: 14.965|tagging_loss_image: 14.959|tagging_loss_fusion: 19.173|total_loss: 75.645 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 469 | tagging_loss_video: 14.256|tagging_loss_audio: 12.171|tagging_loss_text: 17.189|tagging_loss_image: 14.557|tagging_loss_fusion: 16.258|total_loss: 74.431 | 65.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 470 |tagging_loss_video: 10.860|tagging_loss_audio: 12.939|tagging_loss_text: 13.988|tagging_loss_image: 8.929|tagging_loss_fusion: 17.186|total_loss: 63.902 | Examples/sec: 69.69\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.42 | precision@0.5: 0.74 |recall@0.1: 0.90 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 471 | tagging_loss_video: 11.249|tagging_loss_audio: 13.133|tagging_loss_text: 13.754|tagging_loss_image: 14.298|tagging_loss_fusion: 15.233|total_loss: 67.667 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 472 | tagging_loss_video: 11.751|tagging_loss_audio: 12.820|tagging_loss_text: 16.270|tagging_loss_image: 12.424|tagging_loss_fusion: 15.020|total_loss: 68.285 | 62.72 Examples/sec\n",
      "INFO:tensorflow:training step 473 | tagging_loss_video: 10.527|tagging_loss_audio: 11.956|tagging_loss_text: 16.148|tagging_loss_image: 14.301|tagging_loss_fusion: 19.322|total_loss: 72.254 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 474 | tagging_loss_video: 12.429|tagging_loss_audio: 13.683|tagging_loss_text: 14.476|tagging_loss_image: 11.828|tagging_loss_fusion: 17.826|total_loss: 70.243 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 475 | tagging_loss_video: 11.221|tagging_loss_audio: 12.926|tagging_loss_text: 18.692|tagging_loss_image: 13.562|tagging_loss_fusion: 15.430|total_loss: 71.832 | 67.68 Examples/sec\n",
      "INFO:tensorflow:training step 476 | tagging_loss_video: 11.340|tagging_loss_audio: 14.479|tagging_loss_text: 13.349|tagging_loss_image: 13.251|tagging_loss_fusion: 15.942|total_loss: 68.361 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 477 | tagging_loss_video: 11.911|tagging_loss_audio: 14.635|tagging_loss_text: 15.636|tagging_loss_image: 11.103|tagging_loss_fusion: 14.228|total_loss: 67.512 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 478 | tagging_loss_video: 11.540|tagging_loss_audio: 13.029|tagging_loss_text: 14.597|tagging_loss_image: 11.172|tagging_loss_fusion: 15.903|total_loss: 66.241 | 66.86 Examples/sec\n",
      "INFO:tensorflow:training step 479 | tagging_loss_video: 11.284|tagging_loss_audio: 12.710|tagging_loss_text: 15.797|tagging_loss_image: 14.530|tagging_loss_fusion: 14.781|total_loss: 69.101 | 71.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 480 |tagging_loss_video: 11.018|tagging_loss_audio: 12.393|tagging_loss_text: 13.975|tagging_loss_image: 12.190|tagging_loss_fusion: 15.889|total_loss: 65.464 | Examples/sec: 69.48\n",
      "INFO:tensorflow:GAP: 0.74 | precision@0.1: 0.46 | precision@0.5: 0.80 |recall@0.1: 0.90 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 481 | tagging_loss_video: 10.758|tagging_loss_audio: 13.649|tagging_loss_text: 16.190|tagging_loss_image: 12.959|tagging_loss_fusion: 16.374|total_loss: 69.929 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 482 | tagging_loss_video: 12.342|tagging_loss_audio: 14.208|tagging_loss_text: 13.241|tagging_loss_image: 13.230|tagging_loss_fusion: 16.832|total_loss: 69.853 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 483 | tagging_loss_video: 10.922|tagging_loss_audio: 14.785|tagging_loss_text: 14.971|tagging_loss_image: 12.816|tagging_loss_fusion: 15.043|total_loss: 68.537 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 484 | tagging_loss_video: 11.001|tagging_loss_audio: 10.867|tagging_loss_text: 16.104|tagging_loss_image: 11.147|tagging_loss_fusion: 14.181|total_loss: 63.300 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 485 | tagging_loss_video: 12.278|tagging_loss_audio: 13.832|tagging_loss_text: 12.705|tagging_loss_image: 10.195|tagging_loss_fusion: 15.944|total_loss: 64.953 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 486 | tagging_loss_video: 10.744|tagging_loss_audio: 12.646|tagging_loss_text: 18.624|tagging_loss_image: 10.242|tagging_loss_fusion: 16.744|total_loss: 69.002 | 63.27 Examples/sec\n",
      "INFO:tensorflow:training step 487 | tagging_loss_video: 13.120|tagging_loss_audio: 15.419|tagging_loss_text: 16.927|tagging_loss_image: 13.368|tagging_loss_fusion: 17.522|total_loss: 76.356 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 488 | tagging_loss_video: 12.683|tagging_loss_audio: 15.202|tagging_loss_text: 16.729|tagging_loss_image: 13.065|tagging_loss_fusion: 18.370|total_loss: 76.050 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 489 | tagging_loss_video: 14.876|tagging_loss_audio: 15.929|tagging_loss_text: 14.971|tagging_loss_image: 13.899|tagging_loss_fusion: 18.380|total_loss: 78.056 | 60.50 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 490 |tagging_loss_video: 12.026|tagging_loss_audio: 14.491|tagging_loss_text: 15.188|tagging_loss_image: 11.470|tagging_loss_fusion: 15.481|total_loss: 68.657 | Examples/sec: 70.84\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.46 | precision@0.5: 0.80 |recall@0.1: 0.92 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 491 | tagging_loss_video: 14.183|tagging_loss_audio: 15.741|tagging_loss_text: 16.683|tagging_loss_image: 14.613|tagging_loss_fusion: 17.908|total_loss: 79.127 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 492 | tagging_loss_video: 12.617|tagging_loss_audio: 13.502|tagging_loss_text: 16.386|tagging_loss_image: 12.427|tagging_loss_fusion: 15.169|total_loss: 70.102 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 493 | tagging_loss_video: 15.616|tagging_loss_audio: 15.630|tagging_loss_text: 17.400|tagging_loss_image: 11.007|tagging_loss_fusion: 18.403|total_loss: 78.057 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 494 | tagging_loss_video: 11.667|tagging_loss_audio: 14.947|tagging_loss_text: 13.413|tagging_loss_image: 11.281|tagging_loss_fusion: 15.606|total_loss: 66.914 | 59.86 Examples/sec\n",
      "INFO:tensorflow:training step 495 | tagging_loss_video: 13.427|tagging_loss_audio: 12.826|tagging_loss_text: 16.797|tagging_loss_image: 13.662|tagging_loss_fusion: 17.779|total_loss: 74.491 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 496 | tagging_loss_video: 10.932|tagging_loss_audio: 16.640|tagging_loss_text: 18.940|tagging_loss_image: 14.715|tagging_loss_fusion: 17.251|total_loss: 78.478 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 497 | tagging_loss_video: 11.782|tagging_loss_audio: 10.200|tagging_loss_text: 13.981|tagging_loss_image: 13.636|tagging_loss_fusion: 14.229|total_loss: 63.828 | 61.88 Examples/sec\n",
      "INFO:tensorflow:training step 498 | tagging_loss_video: 12.131|tagging_loss_audio: 13.846|tagging_loss_text: 12.518|tagging_loss_image: 13.498|tagging_loss_fusion: 15.575|total_loss: 67.568 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 499 | tagging_loss_video: 12.535|tagging_loss_audio: 13.548|tagging_loss_text: 14.694|tagging_loss_image: 15.841|tagging_loss_fusion: 17.023|total_loss: 73.642 | 70.34 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 500 |tagging_loss_video: 12.508|tagging_loss_audio: 14.261|tagging_loss_text: 15.666|tagging_loss_image: 12.441|tagging_loss_fusion: 14.710|total_loss: 69.585 | Examples/sec: 62.47\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.48 | precision@0.5: 0.80 |recall@0.1: 0.91 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 501 | tagging_loss_video: 10.920|tagging_loss_audio: 14.169|tagging_loss_text: 14.250|tagging_loss_image: 11.766|tagging_loss_fusion: 17.790|total_loss: 68.895 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 502 | tagging_loss_video: 12.368|tagging_loss_audio: 14.985|tagging_loss_text: 14.214|tagging_loss_image: 12.055|tagging_loss_fusion: 16.481|total_loss: 70.103 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 503 | tagging_loss_video: 13.007|tagging_loss_audio: 13.773|tagging_loss_text: 15.315|tagging_loss_image: 13.860|tagging_loss_fusion: 16.124|total_loss: 72.078 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 504 | tagging_loss_video: 14.516|tagging_loss_audio: 17.461|tagging_loss_text: 17.796|tagging_loss_image: 13.896|tagging_loss_fusion: 15.523|total_loss: 79.192 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 505 | tagging_loss_video: 11.386|tagging_loss_audio: 13.355|tagging_loss_text: 15.067|tagging_loss_image: 13.898|tagging_loss_fusion: 16.252|total_loss: 69.958 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 506 | tagging_loss_video: 11.540|tagging_loss_audio: 13.952|tagging_loss_text: 15.302|tagging_loss_image: 14.715|tagging_loss_fusion: 15.798|total_loss: 71.307 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 507 | tagging_loss_video: 12.084|tagging_loss_audio: 12.267|tagging_loss_text: 15.500|tagging_loss_image: 12.627|tagging_loss_fusion: 14.457|total_loss: 66.936 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 508 | tagging_loss_video: 10.896|tagging_loss_audio: 13.151|tagging_loss_text: 16.060|tagging_loss_image: 13.955|tagging_loss_fusion: 16.625|total_loss: 70.688 | 61.06 Examples/sec\n",
      "INFO:tensorflow:training step 509 | tagging_loss_video: 11.554|tagging_loss_audio: 11.949|tagging_loss_text: 15.667|tagging_loss_image: 12.428|tagging_loss_fusion: 14.895|total_loss: 66.493 | 70.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 510 |tagging_loss_video: 11.372|tagging_loss_audio: 13.578|tagging_loss_text: 17.122|tagging_loss_image: 13.710|tagging_loss_fusion: 17.111|total_loss: 72.892 | Examples/sec: 68.99\n",
      "INFO:tensorflow:GAP: 0.72 | precision@0.1: 0.44 | precision@0.5: 0.80 |recall@0.1: 0.90 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 511 | tagging_loss_video: 11.330|tagging_loss_audio: 12.601|tagging_loss_text: 10.620|tagging_loss_image: 12.848|tagging_loss_fusion: 17.022|total_loss: 64.421 | 66.44 Examples/sec\n",
      "INFO:tensorflow:training step 512 | tagging_loss_video: 12.895|tagging_loss_audio: 14.713|tagging_loss_text: 16.678|tagging_loss_image: 12.172|tagging_loss_fusion: 16.162|total_loss: 72.620 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 513 | tagging_loss_video: 11.839|tagging_loss_audio: 13.938|tagging_loss_text: 16.705|tagging_loss_image: 12.105|tagging_loss_fusion: 15.370|total_loss: 69.958 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 514 | tagging_loss_video: 11.967|tagging_loss_audio: 14.806|tagging_loss_text: 16.189|tagging_loss_image: 12.992|tagging_loss_fusion: 15.037|total_loss: 70.990 | 66.89 Examples/sec\n",
      "INFO:tensorflow:training step 515 | tagging_loss_video: 14.383|tagging_loss_audio: 14.689|tagging_loss_text: 16.470|tagging_loss_image: 16.640|tagging_loss_fusion: 16.987|total_loss: 79.169 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 516 | tagging_loss_video: 10.731|tagging_loss_audio: 12.576|tagging_loss_text: 16.643|tagging_loss_image: 13.479|tagging_loss_fusion: 16.101|total_loss: 69.530 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 517 | tagging_loss_video: 11.241|tagging_loss_audio: 11.429|tagging_loss_text: 19.751|tagging_loss_image: 12.890|tagging_loss_fusion: 17.163|total_loss: 72.474 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 518 | tagging_loss_video: 11.659|tagging_loss_audio: 12.347|tagging_loss_text: 15.570|tagging_loss_image: 13.234|tagging_loss_fusion: 16.221|total_loss: 69.031 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 519 | tagging_loss_video: 10.716|tagging_loss_audio: 12.015|tagging_loss_text: 14.274|tagging_loss_image: 12.119|tagging_loss_fusion: 16.316|total_loss: 65.440 | 63.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 520 |tagging_loss_video: 10.963|tagging_loss_audio: 15.430|tagging_loss_text: 17.097|tagging_loss_image: 11.554|tagging_loss_fusion: 18.166|total_loss: 73.210 | Examples/sec: 70.35\n",
      "INFO:tensorflow:GAP: 0.69 | precision@0.1: 0.48 | precision@0.5: 0.81 |recall@0.1: 0.86 | recall@0.5: 0.59\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 521 | tagging_loss_video: 12.791|tagging_loss_audio: 16.427|tagging_loss_text: 16.272|tagging_loss_image: 13.300|tagging_loss_fusion: 16.864|total_loss: 75.655 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 522 | tagging_loss_video: 14.411|tagging_loss_audio: 14.352|tagging_loss_text: 12.673|tagging_loss_image: 13.271|tagging_loss_fusion: 15.819|total_loss: 70.526 | 64.81 Examples/sec\n",
      "INFO:tensorflow:training step 523 | tagging_loss_video: 11.331|tagging_loss_audio: 14.312|tagging_loss_text: 16.035|tagging_loss_image: 12.904|tagging_loss_fusion: 15.812|total_loss: 70.394 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 524 | tagging_loss_video: 11.648|tagging_loss_audio: 14.425|tagging_loss_text: 13.545|tagging_loss_image: 13.261|tagging_loss_fusion: 16.015|total_loss: 68.893 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 525 | tagging_loss_video: 13.107|tagging_loss_audio: 13.490|tagging_loss_text: 16.962|tagging_loss_image: 11.536|tagging_loss_fusion: 18.836|total_loss: 73.929 | 59.34 Examples/sec\n",
      "INFO:tensorflow:training step 526 | tagging_loss_video: 11.411|tagging_loss_audio: 14.303|tagging_loss_text: 14.265|tagging_loss_image: 12.119|tagging_loss_fusion: 16.171|total_loss: 68.269 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 527 | tagging_loss_video: 12.664|tagging_loss_audio: 12.289|tagging_loss_text: 14.533|tagging_loss_image: 13.387|tagging_loss_fusion: 17.474|total_loss: 70.347 | 71.59 Examples/sec\n",
      "INFO:tensorflow:training step 528 | tagging_loss_video: 13.371|tagging_loss_audio: 13.853|tagging_loss_text: 14.144|tagging_loss_image: 13.420|tagging_loss_fusion: 15.971|total_loss: 70.759 | 66.12 Examples/sec\n",
      "INFO:tensorflow:training step 529 | tagging_loss_video: 12.225|tagging_loss_audio: 13.301|tagging_loss_text: 14.870|tagging_loss_image: 12.697|tagging_loss_fusion: 15.471|total_loss: 68.563 | 68.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 530 |tagging_loss_video: 11.066|tagging_loss_audio: 14.327|tagging_loss_text: 16.433|tagging_loss_image: 11.724|tagging_loss_fusion: 15.132|total_loss: 68.681 | Examples/sec: 72.81\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.45 | precision@0.5: 0.79 |recall@0.1: 0.91 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 531 | tagging_loss_video: 11.790|tagging_loss_audio: 12.969|tagging_loss_text: 14.323|tagging_loss_image: 11.980|tagging_loss_fusion: 17.004|total_loss: 68.065 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 532 | tagging_loss_video: 12.176|tagging_loss_audio: 18.692|tagging_loss_text: 14.241|tagging_loss_image: 13.999|tagging_loss_fusion: 17.593|total_loss: 76.701 | 62.87 Examples/sec\n",
      "INFO:tensorflow:training step 533 | tagging_loss_video: 13.703|tagging_loss_audio: 13.820|tagging_loss_text: 12.570|tagging_loss_image: 13.111|tagging_loss_fusion: 16.178|total_loss: 69.381 | 62.69 Examples/sec\n",
      "INFO:tensorflow:training step 534 | tagging_loss_video: 12.313|tagging_loss_audio: 12.852|tagging_loss_text: 16.152|tagging_loss_image: 14.734|tagging_loss_fusion: 15.803|total_loss: 71.855 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 535 | tagging_loss_video: 13.075|tagging_loss_audio: 16.872|tagging_loss_text: 16.832|tagging_loss_image: 15.163|tagging_loss_fusion: 17.303|total_loss: 79.245 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 536 | tagging_loss_video: 11.385|tagging_loss_audio: 9.918|tagging_loss_text: 13.939|tagging_loss_image: 11.528|tagging_loss_fusion: 13.883|total_loss: 60.653 | 63.78 Examples/sec\n",
      "INFO:tensorflow:training step 537 | tagging_loss_video: 10.950|tagging_loss_audio: 12.234|tagging_loss_text: 14.051|tagging_loss_image: 11.762|tagging_loss_fusion: 15.205|total_loss: 64.202 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 538 | tagging_loss_video: 12.684|tagging_loss_audio: 13.253|tagging_loss_text: 12.856|tagging_loss_image: 13.466|tagging_loss_fusion: 14.111|total_loss: 66.370 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 539 | tagging_loss_video: 11.649|tagging_loss_audio: 11.930|tagging_loss_text: 10.540|tagging_loss_image: 10.179|tagging_loss_fusion: 13.821|total_loss: 58.118 | 63.16 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 540 |tagging_loss_video: 13.547|tagging_loss_audio: 14.720|tagging_loss_text: 15.493|tagging_loss_image: 12.478|tagging_loss_fusion: 16.642|total_loss: 72.880 | Examples/sec: 67.90\n",
      "INFO:tensorflow:GAP: 0.75 | precision@0.1: 0.48 | precision@0.5: 0.81 |recall@0.1: 0.89 | recall@0.5: 0.65\n",
      "INFO:tensorflow:training step 541 | tagging_loss_video: 11.738|tagging_loss_audio: 13.995|tagging_loss_text: 16.527|tagging_loss_image: 13.908|tagging_loss_fusion: 16.675|total_loss: 72.843 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 542 | tagging_loss_video: 11.202|tagging_loss_audio: 12.909|tagging_loss_text: 16.079|tagging_loss_image: 13.492|tagging_loss_fusion: 15.176|total_loss: 68.858 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 543 | tagging_loss_video: 9.813|tagging_loss_audio: 12.577|tagging_loss_text: 18.315|tagging_loss_image: 11.435|tagging_loss_fusion: 14.995|total_loss: 67.136 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 544 | tagging_loss_video: 14.455|tagging_loss_audio: 14.918|tagging_loss_text: 17.084|tagging_loss_image: 15.358|tagging_loss_fusion: 17.342|total_loss: 79.157 | 65.39 Examples/sec\n",
      "INFO:tensorflow:training step 545 | tagging_loss_video: 12.770|tagging_loss_audio: 12.525|tagging_loss_text: 20.257|tagging_loss_image: 13.880|tagging_loss_fusion: 15.953|total_loss: 75.385 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 546 | tagging_loss_video: 9.747|tagging_loss_audio: 10.990|tagging_loss_text: 16.954|tagging_loss_image: 13.254|tagging_loss_fusion: 15.041|total_loss: 65.987 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 547 | tagging_loss_video: 11.077|tagging_loss_audio: 12.384|tagging_loss_text: 12.480|tagging_loss_image: 11.473|tagging_loss_fusion: 13.218|total_loss: 60.633 | 64.20 Examples/sec\n",
      "INFO:tensorflow:training step 548 | tagging_loss_video: 11.929|tagging_loss_audio: 13.829|tagging_loss_text: 13.214|tagging_loss_image: 12.624|tagging_loss_fusion: 16.121|total_loss: 67.716 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 549 | tagging_loss_video: 11.659|tagging_loss_audio: 10.167|tagging_loss_text: 17.604|tagging_loss_image: 13.215|tagging_loss_fusion: 17.081|total_loss: 69.725 | 70.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 550 |tagging_loss_video: 10.770|tagging_loss_audio: 12.297|tagging_loss_text: 14.322|tagging_loss_image: 12.784|tagging_loss_fusion: 15.731|total_loss: 65.903 | Examples/sec: 61.72\n",
      "INFO:tensorflow:GAP: 0.73 | precision@0.1: 0.48 | precision@0.5: 0.74 |recall@0.1: 0.94 | recall@0.5: 0.61\n",
      "INFO:tensorflow:training step 551 | tagging_loss_video: 11.725|tagging_loss_audio: 14.893|tagging_loss_text: 13.590|tagging_loss_image: 13.741|tagging_loss_fusion: 16.270|total_loss: 70.219 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 552 | tagging_loss_video: 11.838|tagging_loss_audio: 12.703|tagging_loss_text: 13.147|tagging_loss_image: 13.353|tagging_loss_fusion: 15.971|total_loss: 67.012 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 553 | tagging_loss_video: 9.810|tagging_loss_audio: 13.363|tagging_loss_text: 15.016|tagging_loss_image: 11.180|tagging_loss_fusion: 16.727|total_loss: 66.096 | 66.04 Examples/sec\n",
      "INFO:tensorflow:training step 554 | tagging_loss_video: 10.903|tagging_loss_audio: 13.320|tagging_loss_text: 15.124|tagging_loss_image: 12.118|tagging_loss_fusion: 14.679|total_loss: 66.145 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 555 | tagging_loss_video: 12.262|tagging_loss_audio: 14.905|tagging_loss_text: 16.017|tagging_loss_image: 13.513|tagging_loss_fusion: 15.597|total_loss: 72.294 | 65.43 Examples/sec\n",
      "INFO:tensorflow:training step 556 | tagging_loss_video: 12.413|tagging_loss_audio: 13.593|tagging_loss_text: 13.470|tagging_loss_image: 11.784|tagging_loss_fusion: 15.984|total_loss: 67.244 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 557 | tagging_loss_video: 11.441|tagging_loss_audio: 10.783|tagging_loss_text: 15.483|tagging_loss_image: 13.766|tagging_loss_fusion: 16.471|total_loss: 67.945 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 558 | tagging_loss_video: 10.691|tagging_loss_audio: 12.950|tagging_loss_text: 15.399|tagging_loss_image: 11.677|tagging_loss_fusion: 16.869|total_loss: 67.586 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 559 | tagging_loss_video: 11.632|tagging_loss_audio: 13.381|tagging_loss_text: 13.553|tagging_loss_image: 11.580|tagging_loss_fusion: 17.291|total_loss: 67.438 | 61.57 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 560 |tagging_loss_video: 11.996|tagging_loss_audio: 11.976|tagging_loss_text: 13.149|tagging_loss_image: 13.056|tagging_loss_fusion: 15.316|total_loss: 65.493 | Examples/sec: 71.24\n",
      "INFO:tensorflow:GAP: 0.74 | precision@0.1: 0.45 | precision@0.5: 0.82 |recall@0.1: 0.91 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 561 | tagging_loss_video: 12.262|tagging_loss_audio: 14.265|tagging_loss_text: 16.561|tagging_loss_image: 13.478|tagging_loss_fusion: 16.739|total_loss: 73.305 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 562 | tagging_loss_video: 10.925|tagging_loss_audio: 14.519|tagging_loss_text: 12.066|tagging_loss_image: 11.080|tagging_loss_fusion: 13.686|total_loss: 62.275 | 63.57 Examples/sec\n",
      "INFO:tensorflow:training step 563 | tagging_loss_video: 12.038|tagging_loss_audio: 10.648|tagging_loss_text: 13.042|tagging_loss_image: 11.030|tagging_loss_fusion: 12.735|total_loss: 59.493 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 564 | tagging_loss_video: 11.525|tagging_loss_audio: 13.768|tagging_loss_text: 16.083|tagging_loss_image: 11.463|tagging_loss_fusion: 13.973|total_loss: 66.812 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 565 | tagging_loss_video: 10.734|tagging_loss_audio: 13.643|tagging_loss_text: 13.572|tagging_loss_image: 10.983|tagging_loss_fusion: 14.442|total_loss: 63.374 | 65.00 Examples/sec\n",
      "INFO:tensorflow:training step 566 | tagging_loss_video: 12.964|tagging_loss_audio: 12.911|tagging_loss_text: 19.530|tagging_loss_image: 12.091|tagging_loss_fusion: 14.926|total_loss: 72.421 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 567 | tagging_loss_video: 12.083|tagging_loss_audio: 9.754|tagging_loss_text: 14.689|tagging_loss_image: 11.844|tagging_loss_fusion: 14.175|total_loss: 62.545 | 65.93 Examples/sec\n",
      "INFO:tensorflow:training step 568 | tagging_loss_video: 9.605|tagging_loss_audio: 13.642|tagging_loss_text: 14.846|tagging_loss_image: 10.346|tagging_loss_fusion: 14.781|total_loss: 63.221 | 65.19 Examples/sec\n",
      "INFO:tensorflow:training step 569 | tagging_loss_video: 12.306|tagging_loss_audio: 15.989|tagging_loss_text: 17.610|tagging_loss_image: 13.220|tagging_loss_fusion: 15.593|total_loss: 74.718 | 71.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 570 |tagging_loss_video: 12.385|tagging_loss_audio: 14.589|tagging_loss_text: 17.047|tagging_loss_image: 10.780|tagging_loss_fusion: 15.416|total_loss: 70.218 | Examples/sec: 61.69\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.52 | precision@0.5: 0.86 |recall@0.1: 0.91 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 571 | tagging_loss_video: 11.286|tagging_loss_audio: 14.972|tagging_loss_text: 14.113|tagging_loss_image: 13.167|tagging_loss_fusion: 16.623|total_loss: 70.162 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 572 | tagging_loss_video: 12.634|tagging_loss_audio: 16.246|tagging_loss_text: 18.665|tagging_loss_image: 13.875|tagging_loss_fusion: 15.583|total_loss: 77.003 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 573 | tagging_loss_video: 9.259|tagging_loss_audio: 12.965|tagging_loss_text: 16.621|tagging_loss_image: 14.044|tagging_loss_fusion: 15.274|total_loss: 68.163 | 66.07 Examples/sec\n",
      "INFO:tensorflow:training step 574 | tagging_loss_video: 11.448|tagging_loss_audio: 11.190|tagging_loss_text: 10.853|tagging_loss_image: 11.720|tagging_loss_fusion: 13.688|total_loss: 58.898 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 575 | tagging_loss_video: 10.519|tagging_loss_audio: 11.044|tagging_loss_text: 12.658|tagging_loss_image: 12.644|tagging_loss_fusion: 15.685|total_loss: 62.550 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 576 | tagging_loss_video: 10.586|tagging_loss_audio: 8.915|tagging_loss_text: 14.349|tagging_loss_image: 11.423|tagging_loss_fusion: 14.653|total_loss: 59.926 | 61.79 Examples/sec\n",
      "INFO:tensorflow:training step 577 | tagging_loss_video: 12.186|tagging_loss_audio: 11.292|tagging_loss_text: 17.070|tagging_loss_image: 13.101|tagging_loss_fusion: 14.753|total_loss: 68.402 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 578 | tagging_loss_video: 11.840|tagging_loss_audio: 14.033|tagging_loss_text: 15.641|tagging_loss_image: 10.496|tagging_loss_fusion: 12.649|total_loss: 64.660 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 579 | tagging_loss_video: 12.136|tagging_loss_audio: 14.914|tagging_loss_text: 15.506|tagging_loss_image: 12.081|tagging_loss_fusion: 15.042|total_loss: 69.678 | 63.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 580 |tagging_loss_video: 12.229|tagging_loss_audio: 12.613|tagging_loss_text: 14.969|tagging_loss_image: 9.808|tagging_loss_fusion: 15.071|total_loss: 64.690 | Examples/sec: 69.07\n",
      "INFO:tensorflow:GAP: 0.74 | precision@0.1: 0.48 | precision@0.5: 0.77 |recall@0.1: 0.93 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 581 | tagging_loss_video: 12.843|tagging_loss_audio: 15.721|tagging_loss_text: 13.512|tagging_loss_image: 14.184|tagging_loss_fusion: 15.382|total_loss: 71.641 | 66.15 Examples/sec\n",
      "INFO:tensorflow:training step 582 | tagging_loss_video: 12.939|tagging_loss_audio: 11.343|tagging_loss_text: 15.169|tagging_loss_image: 14.254|tagging_loss_fusion: 15.881|total_loss: 69.587 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 583 | tagging_loss_video: 12.979|tagging_loss_audio: 12.782|tagging_loss_text: 16.737|tagging_loss_image: 12.106|tagging_loss_fusion: 15.756|total_loss: 70.361 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 584 | tagging_loss_video: 12.966|tagging_loss_audio: 13.331|tagging_loss_text: 16.461|tagging_loss_image: 11.384|tagging_loss_fusion: 14.774|total_loss: 68.916 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 585 | tagging_loss_video: 11.216|tagging_loss_audio: 14.383|tagging_loss_text: 17.570|tagging_loss_image: 14.931|tagging_loss_fusion: 18.060|total_loss: 76.159 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 586 | tagging_loss_video: 12.306|tagging_loss_audio: 13.603|tagging_loss_text: 18.592|tagging_loss_image: 12.632|tagging_loss_fusion: 15.163|total_loss: 72.296 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 587 | tagging_loss_video: 11.186|tagging_loss_audio: 13.712|tagging_loss_text: 14.539|tagging_loss_image: 13.101|tagging_loss_fusion: 15.618|total_loss: 68.155 | 65.69 Examples/sec\n",
      "INFO:tensorflow:training step 588 | tagging_loss_video: 12.079|tagging_loss_audio: 15.813|tagging_loss_text: 16.505|tagging_loss_image: 11.541|tagging_loss_fusion: 15.996|total_loss: 71.933 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 589 | tagging_loss_video: 12.439|tagging_loss_audio: 13.502|tagging_loss_text: 13.812|tagging_loss_image: 15.049|tagging_loss_fusion: 15.186|total_loss: 69.988 | 71.33 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 590 |tagging_loss_video: 11.993|tagging_loss_audio: 12.358|tagging_loss_text: 15.916|tagging_loss_image: 14.093|tagging_loss_fusion: 15.067|total_loss: 69.427 | Examples/sec: 61.20\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.47 | precision@0.5: 0.79 |recall@0.1: 0.94 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 591 | tagging_loss_video: 11.294|tagging_loss_audio: 14.238|tagging_loss_text: 14.830|tagging_loss_image: 8.575|tagging_loss_fusion: 14.644|total_loss: 63.580 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 592 | tagging_loss_video: 12.878|tagging_loss_audio: 15.435|tagging_loss_text: 16.022|tagging_loss_image: 11.336|tagging_loss_fusion: 16.506|total_loss: 72.178 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 593 | tagging_loss_video: 11.606|tagging_loss_audio: 14.796|tagging_loss_text: 17.014|tagging_loss_image: 13.158|tagging_loss_fusion: 16.098|total_loss: 72.672 | 64.65 Examples/sec\n",
      "INFO:tensorflow:training step 594 | tagging_loss_video: 12.666|tagging_loss_audio: 15.951|tagging_loss_text: 14.332|tagging_loss_image: 12.926|tagging_loss_fusion: 17.429|total_loss: 73.304 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 595 | tagging_loss_video: 12.414|tagging_loss_audio: 11.474|tagging_loss_text: 16.645|tagging_loss_image: 14.010|tagging_loss_fusion: 14.399|total_loss: 68.942 | 68.39 Examples/sec\n",
      "INFO:tensorflow:training step 596 | tagging_loss_video: 14.513|tagging_loss_audio: 12.072|tagging_loss_text: 14.112|tagging_loss_image: 12.361|tagging_loss_fusion: 16.662|total_loss: 69.720 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 597 | tagging_loss_video: 10.046|tagging_loss_audio: 12.692|tagging_loss_text: 16.121|tagging_loss_image: 10.385|tagging_loss_fusion: 12.973|total_loss: 62.216 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 598 | tagging_loss_video: 12.260|tagging_loss_audio: 14.287|tagging_loss_text: 15.877|tagging_loss_image: 14.348|tagging_loss_fusion: 17.689|total_loss: 74.461 | 62.23 Examples/sec\n",
      "INFO:tensorflow:training step 599 | tagging_loss_video: 13.730|tagging_loss_audio: 16.358|tagging_loss_text: 18.975|tagging_loss_image: 13.197|tagging_loss_fusion: 15.832|total_loss: 78.092 | 69.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 600 |tagging_loss_video: 12.221|tagging_loss_audio: 12.830|tagging_loss_text: 16.149|tagging_loss_image: 12.519|tagging_loss_fusion: 15.697|total_loss: 69.418 | Examples/sec: 71.77\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.55 | precision@0.5: 0.88 |recall@0.1: 0.92 | recall@0.5: 0.65\n",
      "INFO:tensorflow:training step 601 | tagging_loss_video: 11.243|tagging_loss_audio: 14.721|tagging_loss_text: 14.765|tagging_loss_image: 10.801|tagging_loss_fusion: 16.177|total_loss: 67.708 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 602 | tagging_loss_video: 13.170|tagging_loss_audio: 12.084|tagging_loss_text: 16.001|tagging_loss_image: 16.810|tagging_loss_fusion: 18.455|total_loss: 76.520 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 603 | tagging_loss_video: 11.246|tagging_loss_audio: 12.525|tagging_loss_text: 16.090|tagging_loss_image: 12.453|tagging_loss_fusion: 14.053|total_loss: 66.368 | 65.03 Examples/sec\n",
      "INFO:tensorflow:training step 604 | tagging_loss_video: 9.667|tagging_loss_audio: 11.722|tagging_loss_text: 15.475|tagging_loss_image: 13.645|tagging_loss_fusion: 16.429|total_loss: 66.938 | 60.38 Examples/sec\n",
      "INFO:tensorflow:training step 605 | tagging_loss_video: 12.057|tagging_loss_audio: 14.615|tagging_loss_text: 16.283|tagging_loss_image: 13.231|tagging_loss_fusion: 15.789|total_loss: 71.975 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 606 | tagging_loss_video: 12.497|tagging_loss_audio: 13.040|tagging_loss_text: 15.161|tagging_loss_image: 11.977|tagging_loss_fusion: 15.948|total_loss: 68.624 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 607 | tagging_loss_video: 11.451|tagging_loss_audio: 12.749|tagging_loss_text: 16.802|tagging_loss_image: 10.899|tagging_loss_fusion: 16.079|total_loss: 67.979 | 71.90 Examples/sec\n",
      "INFO:tensorflow:training step 608 | tagging_loss_video: 13.757|tagging_loss_audio: 13.619|tagging_loss_text: 16.375|tagging_loss_image: 15.197|tagging_loss_fusion: 17.832|total_loss: 76.780 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 609 | tagging_loss_video: 13.403|tagging_loss_audio: 13.596|tagging_loss_text: 21.022|tagging_loss_image: 11.677|tagging_loss_fusion: 15.534|total_loss: 75.232 | 65.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 610 |tagging_loss_video: 11.364|tagging_loss_audio: 13.698|tagging_loss_text: 16.458|tagging_loss_image: 13.430|tagging_loss_fusion: 14.759|total_loss: 69.709 | Examples/sec: 70.34\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.48 | precision@0.5: 0.79 |recall@0.1: 0.92 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 611 | tagging_loss_video: 11.094|tagging_loss_audio: 13.984|tagging_loss_text: 15.027|tagging_loss_image: 13.298|tagging_loss_fusion: 14.681|total_loss: 68.084 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 612 | tagging_loss_video: 9.983|tagging_loss_audio: 12.461|tagging_loss_text: 18.105|tagging_loss_image: 11.630|tagging_loss_fusion: 13.474|total_loss: 65.652 | 64.46 Examples/sec\n",
      "INFO:tensorflow:training step 613 | tagging_loss_video: 11.963|tagging_loss_audio: 13.907|tagging_loss_text: 14.701|tagging_loss_image: 12.720|tagging_loss_fusion: 16.668|total_loss: 69.959 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 614 | tagging_loss_video: 11.797|tagging_loss_audio: 12.694|tagging_loss_text: 15.868|tagging_loss_image: 11.443|tagging_loss_fusion: 15.954|total_loss: 67.756 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 615 | tagging_loss_video: 10.633|tagging_loss_audio: 13.339|tagging_loss_text: 15.969|tagging_loss_image: 11.673|tagging_loss_fusion: 14.898|total_loss: 66.512 | 64.69 Examples/sec\n",
      "INFO:tensorflow:training step 616 | tagging_loss_video: 13.551|tagging_loss_audio: 12.443|tagging_loss_text: 12.934|tagging_loss_image: 13.991|tagging_loss_fusion: 14.512|total_loss: 67.430 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 617 | tagging_loss_video: 10.385|tagging_loss_audio: 12.887|tagging_loss_text: 13.767|tagging_loss_image: 11.534|tagging_loss_fusion: 13.438|total_loss: 62.010 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 618 | tagging_loss_video: 10.107|tagging_loss_audio: 13.679|tagging_loss_text: 16.517|tagging_loss_image: 11.572|tagging_loss_fusion: 14.818|total_loss: 66.693 | 64.80 Examples/sec\n",
      "INFO:tensorflow:training step 619 | tagging_loss_video: 11.013|tagging_loss_audio: 9.941|tagging_loss_text: 14.511|tagging_loss_image: 11.789|tagging_loss_fusion: 13.836|total_loss: 61.091 | 70.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 620 |tagging_loss_video: 11.513|tagging_loss_audio: 15.400|tagging_loss_text: 14.107|tagging_loss_image: 11.165|tagging_loss_fusion: 15.088|total_loss: 67.273 | Examples/sec: 66.64\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.49 | precision@0.5: 0.82 |recall@0.1: 0.90 | recall@0.5: 0.67\n",
      "INFO:tensorflow:training step 621 | tagging_loss_video: 9.313|tagging_loss_audio: 11.655|tagging_loss_text: 14.138|tagging_loss_image: 10.599|tagging_loss_fusion: 16.039|total_loss: 61.744 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 622 | tagging_loss_video: 11.297|tagging_loss_audio: 15.133|tagging_loss_text: 16.597|tagging_loss_image: 11.598|tagging_loss_fusion: 14.757|total_loss: 69.382 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 623 | tagging_loss_video: 10.598|tagging_loss_audio: 12.239|tagging_loss_text: 17.536|tagging_loss_image: 11.163|tagging_loss_fusion: 15.904|total_loss: 67.439 | 65.81 Examples/sec\n",
      "INFO:tensorflow:training step 624 | tagging_loss_video: 11.443|tagging_loss_audio: 11.236|tagging_loss_text: 13.404|tagging_loss_image: 12.603|tagging_loss_fusion: 13.796|total_loss: 62.481 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 625 | tagging_loss_video: 11.333|tagging_loss_audio: 14.565|tagging_loss_text: 18.046|tagging_loss_image: 12.368|tagging_loss_fusion: 13.964|total_loss: 70.276 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 626 | tagging_loss_video: 12.629|tagging_loss_audio: 15.135|tagging_loss_text: 17.019|tagging_loss_image: 14.524|tagging_loss_fusion: 13.151|total_loss: 72.458 | 62.56 Examples/sec\n",
      "INFO:tensorflow:training step 627 | tagging_loss_video: 12.414|tagging_loss_audio: 16.513|tagging_loss_text: 16.034|tagging_loss_image: 14.178|tagging_loss_fusion: 15.878|total_loss: 75.016 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 628 | tagging_loss_video: 11.895|tagging_loss_audio: 16.837|tagging_loss_text: 18.518|tagging_loss_image: 16.967|tagging_loss_fusion: 17.438|total_loss: 81.655 | 67.12 Examples/sec\n",
      "INFO:tensorflow:training step 629 | tagging_loss_video: 13.478|tagging_loss_audio: 14.930|tagging_loss_text: 13.473|tagging_loss_image: 14.045|tagging_loss_fusion: 17.108|total_loss: 73.033 | 66.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 630 |tagging_loss_video: 9.943|tagging_loss_audio: 13.059|tagging_loss_text: 13.709|tagging_loss_image: 12.446|tagging_loss_fusion: 14.531|total_loss: 63.688 | Examples/sec: 70.92\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.49 | precision@0.5: 0.82 |recall@0.1: 0.92 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 631 | tagging_loss_video: 13.920|tagging_loss_audio: 15.507|tagging_loss_text: 15.648|tagging_loss_image: 13.823|tagging_loss_fusion: 16.748|total_loss: 75.646 | 70.06 Examples/sec\n",
      "INFO:tensorflow:training step 632 | tagging_loss_video: 9.811|tagging_loss_audio: 11.685|tagging_loss_text: 16.869|tagging_loss_image: 10.817|tagging_loss_fusion: 15.516|total_loss: 64.697 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 633 | tagging_loss_video: 12.225|tagging_loss_audio: 13.675|tagging_loss_text: 12.859|tagging_loss_image: 14.932|tagging_loss_fusion: 15.801|total_loss: 69.492 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 634 | tagging_loss_video: 11.688|tagging_loss_audio: 13.610|tagging_loss_text: 10.703|tagging_loss_image: 10.265|tagging_loss_fusion: 13.759|total_loss: 60.025 | 65.69 Examples/sec\n",
      "INFO:tensorflow:training step 635 | tagging_loss_video: 13.272|tagging_loss_audio: 13.745|tagging_loss_text: 15.270|tagging_loss_image: 13.166|tagging_loss_fusion: 17.238|total_loss: 72.692 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 636 | tagging_loss_video: 11.076|tagging_loss_audio: 13.316|tagging_loss_text: 17.529|tagging_loss_image: 12.071|tagging_loss_fusion: 16.492|total_loss: 70.483 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 637 | tagging_loss_video: 10.839|tagging_loss_audio: 15.407|tagging_loss_text: 15.147|tagging_loss_image: 10.796|tagging_loss_fusion: 12.651|total_loss: 64.841 | 63.77 Examples/sec\n",
      "INFO:tensorflow:training step 638 | tagging_loss_video: 11.056|tagging_loss_audio: 13.309|tagging_loss_text: 15.724|tagging_loss_image: 12.891|tagging_loss_fusion: 15.272|total_loss: 68.252 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 639 | tagging_loss_video: 10.712|tagging_loss_audio: 13.761|tagging_loss_text: 15.262|tagging_loss_image: 13.313|tagging_loss_fusion: 15.933|total_loss: 68.981 | 70.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 640 |tagging_loss_video: 11.316|tagging_loss_audio: 12.621|tagging_loss_text: 16.951|tagging_loss_image: 12.060|tagging_loss_fusion: 14.515|total_loss: 67.463 | Examples/sec: 62.93\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.49 | precision@0.5: 0.79 |recall@0.1: 0.92 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 641 | tagging_loss_video: 12.968|tagging_loss_audio: 14.020|tagging_loss_text: 17.189|tagging_loss_image: 14.861|tagging_loss_fusion: 14.082|total_loss: 73.119 | 65.25 Examples/sec\n",
      "INFO:tensorflow:training step 642 | tagging_loss_video: 11.246|tagging_loss_audio: 11.678|tagging_loss_text: 14.894|tagging_loss_image: 12.880|tagging_loss_fusion: 15.195|total_loss: 65.892 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 643 | tagging_loss_video: 12.058|tagging_loss_audio: 14.420|tagging_loss_text: 15.040|tagging_loss_image: 12.070|tagging_loss_fusion: 14.404|total_loss: 67.992 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 644 | tagging_loss_video: 11.790|tagging_loss_audio: 14.771|tagging_loss_text: 14.998|tagging_loss_image: 11.179|tagging_loss_fusion: 16.261|total_loss: 68.999 | 68.74 Examples/sec\n",
      "INFO:tensorflow:training step 645 | tagging_loss_video: 12.182|tagging_loss_audio: 14.841|tagging_loss_text: 16.446|tagging_loss_image: 13.043|tagging_loss_fusion: 13.884|total_loss: 70.394 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 646 | tagging_loss_video: 10.635|tagging_loss_audio: 12.685|tagging_loss_text: 13.908|tagging_loss_image: 11.733|tagging_loss_fusion: 15.787|total_loss: 64.747 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 647 | tagging_loss_video: 11.314|tagging_loss_audio: 13.779|tagging_loss_text: 17.478|tagging_loss_image: 11.077|tagging_loss_fusion: 13.686|total_loss: 67.334 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 648 | tagging_loss_video: 12.118|tagging_loss_audio: 15.082|tagging_loss_text: 16.651|tagging_loss_image: 11.360|tagging_loss_fusion: 17.128|total_loss: 72.339 | 64.58 Examples/sec\n",
      "INFO:tensorflow:training step 649 | tagging_loss_video: 10.884|tagging_loss_audio: 10.674|tagging_loss_text: 11.496|tagging_loss_image: 12.451|tagging_loss_fusion: 14.466|total_loss: 59.971 | 70.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 650 |tagging_loss_video: 12.170|tagging_loss_audio: 12.313|tagging_loss_text: 15.748|tagging_loss_image: 11.962|tagging_loss_fusion: 14.592|total_loss: 66.784 | Examples/sec: 69.06\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.50 | precision@0.5: 0.81 |recall@0.1: 0.93 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 651 | tagging_loss_video: 11.014|tagging_loss_audio: 14.121|tagging_loss_text: 16.517|tagging_loss_image: 10.904|tagging_loss_fusion: 15.662|total_loss: 68.219 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 652 | tagging_loss_video: 11.375|tagging_loss_audio: 12.317|tagging_loss_text: 16.112|tagging_loss_image: 12.411|tagging_loss_fusion: 15.510|total_loss: 67.726 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 653 | tagging_loss_video: 11.565|tagging_loss_audio: 16.796|tagging_loss_text: 13.177|tagging_loss_image: 11.464|tagging_loss_fusion: 13.891|total_loss: 66.892 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 654 | tagging_loss_video: 10.889|tagging_loss_audio: 14.146|tagging_loss_text: 14.743|tagging_loss_image: 11.308|tagging_loss_fusion: 15.204|total_loss: 66.290 | 62.70 Examples/sec\n",
      "INFO:tensorflow:training step 655 | tagging_loss_video: 12.332|tagging_loss_audio: 17.509|tagging_loss_text: 14.198|tagging_loss_image: 14.371|tagging_loss_fusion: 15.004|total_loss: 73.413 | 72.12 Examples/sec\n",
      "INFO:tensorflow:training step 656 | tagging_loss_video: 12.454|tagging_loss_audio: 13.306|tagging_loss_text: 12.012|tagging_loss_image: 12.705|tagging_loss_fusion: 15.026|total_loss: 65.502 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 657 | tagging_loss_video: 10.952|tagging_loss_audio: 13.618|tagging_loss_text: 17.538|tagging_loss_image: 13.578|tagging_loss_fusion: 15.706|total_loss: 71.392 | 66.04 Examples/sec\n",
      "INFO:tensorflow:training step 658 | tagging_loss_video: 11.200|tagging_loss_audio: 15.364|tagging_loss_text: 15.598|tagging_loss_image: 13.429|tagging_loss_fusion: 16.210|total_loss: 71.801 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 659 | tagging_loss_video: 10.701|tagging_loss_audio: 11.603|tagging_loss_text: 13.559|tagging_loss_image: 11.965|tagging_loss_fusion: 15.811|total_loss: 63.639 | 66.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 660 |tagging_loss_video: 14.627|tagging_loss_audio: 16.382|tagging_loss_text: 15.612|tagging_loss_image: 12.755|tagging_loss_fusion: 15.202|total_loss: 74.578 | Examples/sec: 68.69\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.53 | precision@0.5: 0.79 |recall@0.1: 0.93 | recall@0.5: 0.70\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 661 | tagging_loss_video: 11.904|tagging_loss_audio: 14.492|tagging_loss_text: 19.874|tagging_loss_image: 13.129|tagging_loss_fusion: 15.414|total_loss: 74.813 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 662 | tagging_loss_video: 11.541|tagging_loss_audio: 15.304|tagging_loss_text: 15.998|tagging_loss_image: 11.829|tagging_loss_fusion: 14.909|total_loss: 69.582 | 64.41 Examples/sec\n",
      "INFO:tensorflow:training step 663 | tagging_loss_video: 11.193|tagging_loss_audio: 12.519|tagging_loss_text: 15.979|tagging_loss_image: 11.489|tagging_loss_fusion: 13.243|total_loss: 64.423 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 664 | tagging_loss_video: 11.298|tagging_loss_audio: 13.074|tagging_loss_text: 12.204|tagging_loss_image: 10.386|tagging_loss_fusion: 14.386|total_loss: 61.348 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 665 | tagging_loss_video: 12.400|tagging_loss_audio: 14.799|tagging_loss_text: 14.040|tagging_loss_image: 14.325|tagging_loss_fusion: 16.321|total_loss: 71.885 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 666 | tagging_loss_video: 11.429|tagging_loss_audio: 12.461|tagging_loss_text: 15.616|tagging_loss_image: 11.588|tagging_loss_fusion: 14.024|total_loss: 65.117 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 667 | tagging_loss_video: 12.065|tagging_loss_audio: 12.876|tagging_loss_text: 16.521|tagging_loss_image: 14.686|tagging_loss_fusion: 17.028|total_loss: 73.175 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 668 | tagging_loss_video: 10.962|tagging_loss_audio: 9.594|tagging_loss_text: 13.569|tagging_loss_image: 13.867|tagging_loss_fusion: 15.042|total_loss: 63.034 | 65.68 Examples/sec\n",
      "INFO:tensorflow:training step 669 | tagging_loss_video: 11.278|tagging_loss_audio: 15.186|tagging_loss_text: 19.721|tagging_loss_image: 13.470|tagging_loss_fusion: 15.235|total_loss: 74.890 | 69.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 670 |tagging_loss_video: 12.490|tagging_loss_audio: 11.763|tagging_loss_text: 13.791|tagging_loss_image: 11.241|tagging_loss_fusion: 13.899|total_loss: 63.184 | Examples/sec: 70.39\n",
      "INFO:tensorflow:GAP: 0.80 | precision@0.1: 0.49 | precision@0.5: 0.83 |recall@0.1: 0.93 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 671 | tagging_loss_video: 11.474|tagging_loss_audio: 14.163|tagging_loss_text: 16.424|tagging_loss_image: 11.687|tagging_loss_fusion: 16.455|total_loss: 70.203 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 672 | tagging_loss_video: 12.023|tagging_loss_audio: 16.145|tagging_loss_text: 16.423|tagging_loss_image: 15.407|tagging_loss_fusion: 18.249|total_loss: 78.246 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 673 | tagging_loss_video: 9.679|tagging_loss_audio: 14.214|tagging_loss_text: 16.876|tagging_loss_image: 13.756|tagging_loss_fusion: 17.618|total_loss: 72.142 | 62.31 Examples/sec\n",
      "INFO:tensorflow:training step 674 | tagging_loss_video: 11.095|tagging_loss_audio: 17.009|tagging_loss_text: 16.235|tagging_loss_image: 13.612|tagging_loss_fusion: 15.128|total_loss: 73.078 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 675 | tagging_loss_video: 11.587|tagging_loss_audio: 13.348|tagging_loss_text: 16.782|tagging_loss_image: 13.238|tagging_loss_fusion: 16.087|total_loss: 71.042 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 676 | tagging_loss_video: 8.953|tagging_loss_audio: 14.126|tagging_loss_text: 10.795|tagging_loss_image: 10.387|tagging_loss_fusion: 13.508|total_loss: 57.768 | 63.24 Examples/sec\n",
      "INFO:tensorflow:training step 677 | tagging_loss_video: 10.839|tagging_loss_audio: 12.512|tagging_loss_text: 17.832|tagging_loss_image: 11.566|tagging_loss_fusion: 13.298|total_loss: 66.047 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 678 | tagging_loss_video: 10.361|tagging_loss_audio: 13.492|tagging_loss_text: 16.774|tagging_loss_image: 12.239|tagging_loss_fusion: 14.952|total_loss: 67.817 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 679 | tagging_loss_video: 10.433|tagging_loss_audio: 7.923|tagging_loss_text: 14.228|tagging_loss_image: 10.272|tagging_loss_fusion: 13.800|total_loss: 56.656 | 63.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 680 |tagging_loss_video: 13.079|tagging_loss_audio: 17.085|tagging_loss_text: 13.384|tagging_loss_image: 13.921|tagging_loss_fusion: 14.609|total_loss: 72.078 | Examples/sec: 68.11\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.52 | precision@0.5: 0.81 |recall@0.1: 0.92 | recall@0.5: 0.65\n",
      "INFO:tensorflow:training step 681 | tagging_loss_video: 13.406|tagging_loss_audio: 12.431|tagging_loss_text: 13.247|tagging_loss_image: 12.278|tagging_loss_fusion: 15.379|total_loss: 66.742 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 682 | tagging_loss_video: 11.883|tagging_loss_audio: 14.029|tagging_loss_text: 15.311|tagging_loss_image: 12.891|tagging_loss_fusion: 14.234|total_loss: 68.348 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 683 | tagging_loss_video: 11.425|tagging_loss_audio: 10.369|tagging_loss_text: 14.124|tagging_loss_image: 10.177|tagging_loss_fusion: 12.746|total_loss: 58.840 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 684 | tagging_loss_video: 12.214|tagging_loss_audio: 16.234|tagging_loss_text: 15.770|tagging_loss_image: 14.203|tagging_loss_fusion: 17.072|total_loss: 75.493 | 61.76 Examples/sec\n",
      "INFO:tensorflow:training step 685 | tagging_loss_video: 12.077|tagging_loss_audio: 15.610|tagging_loss_text: 13.022|tagging_loss_image: 12.658|tagging_loss_fusion: 14.671|total_loss: 68.037 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 686 | tagging_loss_video: 10.049|tagging_loss_audio: 13.893|tagging_loss_text: 13.129|tagging_loss_image: 10.565|tagging_loss_fusion: 13.625|total_loss: 61.261 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 687 | tagging_loss_video: 9.088|tagging_loss_audio: 11.507|tagging_loss_text: 11.190|tagging_loss_image: 10.466|tagging_loss_fusion: 12.870|total_loss: 55.121 | 63.70 Examples/sec\n",
      "INFO:tensorflow:training step 688 | tagging_loss_video: 11.259|tagging_loss_audio: 13.931|tagging_loss_text: 18.175|tagging_loss_image: 12.921|tagging_loss_fusion: 15.478|total_loss: 71.764 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 689 | tagging_loss_video: 12.593|tagging_loss_audio: 14.843|tagging_loss_text: 12.472|tagging_loss_image: 12.797|tagging_loss_fusion: 16.263|total_loss: 68.968 | 71.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 690 |tagging_loss_video: 11.265|tagging_loss_audio: 13.217|tagging_loss_text: 15.466|tagging_loss_image: 12.576|tagging_loss_fusion: 14.025|total_loss: 66.549 | Examples/sec: 61.59\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.51 | precision@0.5: 0.81 |recall@0.1: 0.91 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 691 | tagging_loss_video: 11.910|tagging_loss_audio: 12.012|tagging_loss_text: 16.797|tagging_loss_image: 12.821|tagging_loss_fusion: 16.376|total_loss: 69.916 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 692 | tagging_loss_video: 10.946|tagging_loss_audio: 11.211|tagging_loss_text: 17.976|tagging_loss_image: 12.138|tagging_loss_fusion: 14.641|total_loss: 66.912 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 693 | tagging_loss_video: 9.555|tagging_loss_audio: 12.723|tagging_loss_text: 15.620|tagging_loss_image: 12.707|tagging_loss_fusion: 15.442|total_loss: 66.047 | 63.82 Examples/sec\n",
      "INFO:tensorflow:training step 694 | tagging_loss_video: 9.984|tagging_loss_audio: 14.476|tagging_loss_text: 14.353|tagging_loss_image: 11.993|tagging_loss_fusion: 13.297|total_loss: 64.103 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 695 | tagging_loss_video: 12.117|tagging_loss_audio: 13.179|tagging_loss_text: 14.712|tagging_loss_image: 12.993|tagging_loss_fusion: 14.795|total_loss: 67.795 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 696 | tagging_loss_video: 12.417|tagging_loss_audio: 12.240|tagging_loss_text: 17.512|tagging_loss_image: 13.614|tagging_loss_fusion: 14.317|total_loss: 70.100 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 697 | tagging_loss_video: 11.633|tagging_loss_audio: 13.989|tagging_loss_text: 15.460|tagging_loss_image: 12.933|tagging_loss_fusion: 14.343|total_loss: 68.358 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 698 | tagging_loss_video: 12.155|tagging_loss_audio: 14.756|tagging_loss_text: 12.597|tagging_loss_image: 11.687|tagging_loss_fusion: 14.429|total_loss: 65.624 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 699 | tagging_loss_video: 10.844|tagging_loss_audio: 15.362|tagging_loss_text: 15.602|tagging_loss_image: 12.744|tagging_loss_fusion: 15.370|total_loss: 69.922 | 64.15 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 700 |tagging_loss_video: 11.295|tagging_loss_audio: 12.202|tagging_loss_text: 17.248|tagging_loss_image: 12.137|tagging_loss_fusion: 13.570|total_loss: 66.453 | Examples/sec: 70.04\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.47 | precision@0.5: 0.85 |recall@0.1: 0.93 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 701 | tagging_loss_video: 12.482|tagging_loss_audio: 12.897|tagging_loss_text: 15.108|tagging_loss_image: 13.608|tagging_loss_fusion: 15.842|total_loss: 69.936 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 702 | tagging_loss_video: 9.476|tagging_loss_audio: 14.231|tagging_loss_text: 13.433|tagging_loss_image: 9.625|tagging_loss_fusion: 13.468|total_loss: 60.233 | 60.92 Examples/sec\n",
      "INFO:tensorflow:training step 703 | tagging_loss_video: 9.991|tagging_loss_audio: 11.551|tagging_loss_text: 12.523|tagging_loss_image: 9.639|tagging_loss_fusion: 11.902|total_loss: 55.607 | 71.71 Examples/sec\n",
      "INFO:tensorflow:training step 704 | tagging_loss_video: 9.947|tagging_loss_audio: 12.728|tagging_loss_text: 16.034|tagging_loss_image: 12.683|tagging_loss_fusion: 14.743|total_loss: 66.135 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 705 | tagging_loss_video: 10.206|tagging_loss_audio: 13.162|tagging_loss_text: 14.942|tagging_loss_image: 12.454|tagging_loss_fusion: 13.511|total_loss: 64.275 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 706 | tagging_loss_video: 10.706|tagging_loss_audio: 12.771|tagging_loss_text: 15.164|tagging_loss_image: 14.522|tagging_loss_fusion: 13.608|total_loss: 66.770 | 67.41 Examples/sec\n",
      "INFO:tensorflow:training step 707 | tagging_loss_video: 11.130|tagging_loss_audio: 14.500|tagging_loss_text: 14.692|tagging_loss_image: 12.979|tagging_loss_fusion: 13.853|total_loss: 67.154 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 708 | tagging_loss_video: 10.151|tagging_loss_audio: 12.471|tagging_loss_text: 10.992|tagging_loss_image: 11.173|tagging_loss_fusion: 13.441|total_loss: 58.229 | 66.37 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 708.\n",
      "INFO:tensorflow:training step 709 | tagging_loss_video: 11.727|tagging_loss_audio: 14.489|tagging_loss_text: 12.313|tagging_loss_image: 14.693|tagging_loss_fusion: 15.665|total_loss: 68.887 | 47.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 710 |tagging_loss_video: 12.422|tagging_loss_audio: 13.969|tagging_loss_text: 19.337|tagging_loss_image: 14.256|tagging_loss_fusion: 15.130|total_loss: 75.114 | Examples/sec: 67.83\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.52 | precision@0.5: 0.88 |recall@0.1: 0.91 | recall@0.5: 0.64\n",
      "INFO:tensorflow:training step 711 | tagging_loss_video: 11.178|tagging_loss_audio: 16.590|tagging_loss_text: 14.234|tagging_loss_image: 11.427|tagging_loss_fusion: 14.703|total_loss: 68.131 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 712 | tagging_loss_video: 10.555|tagging_loss_audio: 13.367|tagging_loss_text: 15.234|tagging_loss_image: 12.437|tagging_loss_fusion: 14.321|total_loss: 65.913 | 70.53 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.10009\n",
      "INFO:tensorflow:training step 713 | tagging_loss_video: 10.597|tagging_loss_audio: 12.611|tagging_loss_text: 12.053|tagging_loss_image: 10.527|tagging_loss_fusion: 13.314|total_loss: 59.102 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 714 | tagging_loss_video: 10.447|tagging_loss_audio: 11.631|tagging_loss_text: 15.830|tagging_loss_image: 11.908|tagging_loss_fusion: 13.710|total_loss: 63.527 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 715 | tagging_loss_video: 11.583|tagging_loss_audio: 12.329|tagging_loss_text: 16.273|tagging_loss_image: 10.191|tagging_loss_fusion: 13.219|total_loss: 63.596 | 65.59 Examples/sec\n",
      "INFO:tensorflow:training step 716 | tagging_loss_video: 10.658|tagging_loss_audio: 14.440|tagging_loss_text: 17.094|tagging_loss_image: 12.355|tagging_loss_fusion: 13.302|total_loss: 67.850 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 717 | tagging_loss_video: 9.867|tagging_loss_audio: 14.530|tagging_loss_text: 16.176|tagging_loss_image: 10.750|tagging_loss_fusion: 12.931|total_loss: 64.255 | 72.24 Examples/sec\n",
      "INFO:tensorflow:training step 718 | tagging_loss_video: 10.294|tagging_loss_audio: 14.096|tagging_loss_text: 15.449|tagging_loss_image: 13.548|tagging_loss_fusion: 15.406|total_loss: 68.794 | 63.01 Examples/sec\n",
      "INFO:tensorflow:training step 719 | tagging_loss_video: 10.324|tagging_loss_audio: 13.934|tagging_loss_text: 14.408|tagging_loss_image: 11.479|tagging_loss_fusion: 13.346|total_loss: 63.491 | 69.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 720 |tagging_loss_video: 11.480|tagging_loss_audio: 14.617|tagging_loss_text: 17.139|tagging_loss_image: 13.777|tagging_loss_fusion: 13.944|total_loss: 70.957 | Examples/sec: 71.83\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.50 | precision@0.5: 0.83 |recall@0.1: 0.93 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 721 | tagging_loss_video: 12.274|tagging_loss_audio: 16.094|tagging_loss_text: 17.274|tagging_loss_image: 12.990|tagging_loss_fusion: 14.766|total_loss: 73.397 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 722 | tagging_loss_video: 11.743|tagging_loss_audio: 13.981|tagging_loss_text: 16.865|tagging_loss_image: 13.247|tagging_loss_fusion: 14.518|total_loss: 70.354 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 723 | tagging_loss_video: 10.838|tagging_loss_audio: 14.041|tagging_loss_text: 16.442|tagging_loss_image: 11.079|tagging_loss_fusion: 13.965|total_loss: 66.366 | 64.09 Examples/sec\n",
      "INFO:tensorflow:training step 724 | tagging_loss_video: 12.473|tagging_loss_audio: 14.461|tagging_loss_text: 16.708|tagging_loss_image: 12.138|tagging_loss_fusion: 15.933|total_loss: 71.714 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 725 | tagging_loss_video: 12.159|tagging_loss_audio: 13.865|tagging_loss_text: 16.544|tagging_loss_image: 10.945|tagging_loss_fusion: 14.183|total_loss: 67.696 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 726 | tagging_loss_video: 8.823|tagging_loss_audio: 10.322|tagging_loss_text: 14.483|tagging_loss_image: 9.695|tagging_loss_fusion: 16.401|total_loss: 59.725 | 61.61 Examples/sec\n",
      "INFO:tensorflow:training step 727 | tagging_loss_video: 12.218|tagging_loss_audio: 13.342|tagging_loss_text: 13.486|tagging_loss_image: 13.128|tagging_loss_fusion: 17.007|total_loss: 69.181 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 728 | tagging_loss_video: 10.442|tagging_loss_audio: 13.049|tagging_loss_text: 11.733|tagging_loss_image: 12.278|tagging_loss_fusion: 14.243|total_loss: 61.745 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 729 | tagging_loss_video: 10.378|tagging_loss_audio: 13.712|tagging_loss_text: 15.834|tagging_loss_image: 12.329|tagging_loss_fusion: 15.392|total_loss: 67.645 | 64.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 730 |tagging_loss_video: 10.859|tagging_loss_audio: 14.880|tagging_loss_text: 14.034|tagging_loss_image: 13.713|tagging_loss_fusion: 14.320|total_loss: 67.805 | Examples/sec: 69.06\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.51 | precision@0.5: 0.83 |recall@0.1: 0.92 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 731 | tagging_loss_video: 10.047|tagging_loss_audio: 13.237|tagging_loss_text: 19.670|tagging_loss_image: 12.212|tagging_loss_fusion: 15.445|total_loss: 70.612 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 732 | tagging_loss_video: 10.968|tagging_loss_audio: 16.241|tagging_loss_text: 16.379|tagging_loss_image: 12.282|tagging_loss_fusion: 14.497|total_loss: 70.366 | 65.49 Examples/sec\n",
      "INFO:tensorflow:training step 733 | tagging_loss_video: 9.824|tagging_loss_audio: 13.859|tagging_loss_text: 17.296|tagging_loss_image: 16.225|tagging_loss_fusion: 18.736|total_loss: 75.940 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 734 | tagging_loss_video: 10.833|tagging_loss_audio: 13.310|tagging_loss_text: 13.913|tagging_loss_image: 12.089|tagging_loss_fusion: 13.872|total_loss: 64.018 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 735 | tagging_loss_video: 11.376|tagging_loss_audio: 16.395|tagging_loss_text: 16.320|tagging_loss_image: 14.483|tagging_loss_fusion: 15.268|total_loss: 73.841 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 736 | tagging_loss_video: 9.960|tagging_loss_audio: 13.266|tagging_loss_text: 12.464|tagging_loss_image: 9.909|tagging_loss_fusion: 11.903|total_loss: 57.501 | 71.99 Examples/sec\n",
      "INFO:tensorflow:training step 737 | tagging_loss_video: 14.120|tagging_loss_audio: 13.513|tagging_loss_text: 15.204|tagging_loss_image: 12.965|tagging_loss_fusion: 15.718|total_loss: 71.520 | 57.84 Examples/sec\n",
      "INFO:tensorflow:training step 738 | tagging_loss_video: 12.830|tagging_loss_audio: 11.716|tagging_loss_text: 15.208|tagging_loss_image: 13.391|tagging_loss_fusion: 14.508|total_loss: 67.653 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 739 | tagging_loss_video: 10.625|tagging_loss_audio: 13.871|tagging_loss_text: 13.833|tagging_loss_image: 14.829|tagging_loss_fusion: 15.555|total_loss: 68.712 | 66.30 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 740 |tagging_loss_video: 11.269|tagging_loss_audio: 12.334|tagging_loss_text: 13.461|tagging_loss_image: 13.561|tagging_loss_fusion: 13.625|total_loss: 64.251 | Examples/sec: 69.53\n",
      "INFO:tensorflow:GAP: 0.80 | precision@0.1: 0.53 | precision@0.5: 0.88 |recall@0.1: 0.93 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 741 | tagging_loss_video: 14.270|tagging_loss_audio: 11.734|tagging_loss_text: 14.482|tagging_loss_image: 14.650|tagging_loss_fusion: 16.933|total_loss: 72.070 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 742 | tagging_loss_video: 11.330|tagging_loss_audio: 14.741|tagging_loss_text: 15.229|tagging_loss_image: 13.036|tagging_loss_fusion: 11.842|total_loss: 66.179 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 743 | tagging_loss_video: 9.943|tagging_loss_audio: 12.171|tagging_loss_text: 15.407|tagging_loss_image: 13.295|tagging_loss_fusion: 13.600|total_loss: 64.416 | 61.07 Examples/sec\n",
      "INFO:tensorflow:training step 744 | tagging_loss_video: 12.196|tagging_loss_audio: 13.583|tagging_loss_text: 14.850|tagging_loss_image: 11.731|tagging_loss_fusion: 16.135|total_loss: 68.495 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 745 | tagging_loss_video: 11.648|tagging_loss_audio: 12.754|tagging_loss_text: 17.907|tagging_loss_image: 14.027|tagging_loss_fusion: 14.364|total_loss: 70.700 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 746 | tagging_loss_video: 9.659|tagging_loss_audio: 13.853|tagging_loss_text: 14.743|tagging_loss_image: 13.750|tagging_loss_fusion: 15.044|total_loss: 67.050 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 747 | tagging_loss_video: 14.084|tagging_loss_audio: 18.312|tagging_loss_text: 16.452|tagging_loss_image: 12.478|tagging_loss_fusion: 16.447|total_loss: 77.772 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 748 | tagging_loss_video: 11.165|tagging_loss_audio: 15.375|tagging_loss_text: 17.828|tagging_loss_image: 14.797|tagging_loss_fusion: 13.894|total_loss: 73.059 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 749 | tagging_loss_video: 12.043|tagging_loss_audio: 14.317|tagging_loss_text: 11.906|tagging_loss_image: 12.209|tagging_loss_fusion: 13.976|total_loss: 64.451 | 68.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 750 |tagging_loss_video: 10.084|tagging_loss_audio: 11.727|tagging_loss_text: 14.352|tagging_loss_image: 12.350|tagging_loss_fusion: 15.433|total_loss: 63.946 | Examples/sec: 69.90\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.50 | precision@0.5: 0.81 |recall@0.1: 0.94 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 751 | tagging_loss_video: 10.071|tagging_loss_audio: 11.094|tagging_loss_text: 12.989|tagging_loss_image: 10.721|tagging_loss_fusion: 12.859|total_loss: 57.734 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 752 | tagging_loss_video: 10.534|tagging_loss_audio: 15.159|tagging_loss_text: 13.982|tagging_loss_image: 12.999|tagging_loss_fusion: 17.366|total_loss: 70.040 | 67.15 Examples/sec\n",
      "INFO:tensorflow:training step 753 | tagging_loss_video: 12.856|tagging_loss_audio: 13.521|tagging_loss_text: 16.413|tagging_loss_image: 10.612|tagging_loss_fusion: 14.301|total_loss: 67.703 | 72.05 Examples/sec\n",
      "INFO:tensorflow:training step 754 | tagging_loss_video: 10.672|tagging_loss_audio: 10.485|tagging_loss_text: 15.292|tagging_loss_image: 10.734|tagging_loss_fusion: 12.800|total_loss: 59.983 | 61.42 Examples/sec\n",
      "INFO:tensorflow:training step 755 | tagging_loss_video: 10.718|tagging_loss_audio: 13.614|tagging_loss_text: 14.100|tagging_loss_image: 11.123|tagging_loss_fusion: 13.168|total_loss: 62.722 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 756 | tagging_loss_video: 11.186|tagging_loss_audio: 12.828|tagging_loss_text: 15.600|tagging_loss_image: 12.177|tagging_loss_fusion: 12.133|total_loss: 63.924 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 757 | tagging_loss_video: 11.520|tagging_loss_audio: 12.730|tagging_loss_text: 15.249|tagging_loss_image: 10.922|tagging_loss_fusion: 12.492|total_loss: 62.913 | 65.85 Examples/sec\n",
      "INFO:tensorflow:training step 758 | tagging_loss_video: 10.600|tagging_loss_audio: 13.086|tagging_loss_text: 14.507|tagging_loss_image: 11.101|tagging_loss_fusion: 11.613|total_loss: 60.907 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 759 | tagging_loss_video: 11.394|tagging_loss_audio: 12.438|tagging_loss_text: 14.054|tagging_loss_image: 11.969|tagging_loss_fusion: 13.708|total_loss: 63.564 | 66.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 760 |tagging_loss_video: 11.628|tagging_loss_audio: 12.447|tagging_loss_text: 14.727|tagging_loss_image: 11.334|tagging_loss_fusion: 14.436|total_loss: 64.572 | Examples/sec: 69.83\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.57 | precision@0.5: 0.88 |recall@0.1: 0.92 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 761 | tagging_loss_video: 10.726|tagging_loss_audio: 12.884|tagging_loss_text: 18.715|tagging_loss_image: 12.313|tagging_loss_fusion: 13.843|total_loss: 68.481 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 762 | tagging_loss_video: 11.348|tagging_loss_audio: 11.548|tagging_loss_text: 15.595|tagging_loss_image: 12.192|tagging_loss_fusion: 12.682|total_loss: 63.365 | 62.04 Examples/sec\n",
      "INFO:tensorflow:training step 763 | tagging_loss_video: 11.039|tagging_loss_audio: 13.860|tagging_loss_text: 14.728|tagging_loss_image: 12.186|tagging_loss_fusion: 12.391|total_loss: 64.204 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 764 | tagging_loss_video: 11.185|tagging_loss_audio: 11.898|tagging_loss_text: 16.602|tagging_loss_image: 11.864|tagging_loss_fusion: 13.030|total_loss: 64.580 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 765 | tagging_loss_video: 9.463|tagging_loss_audio: 15.808|tagging_loss_text: 16.286|tagging_loss_image: 12.576|tagging_loss_fusion: 14.604|total_loss: 68.736 | 61.12 Examples/sec\n",
      "INFO:tensorflow:training step 766 | tagging_loss_video: 11.521|tagging_loss_audio: 13.398|tagging_loss_text: 16.034|tagging_loss_image: 11.705|tagging_loss_fusion: 15.639|total_loss: 68.296 | 68.29 Examples/sec\n",
      "INFO:tensorflow:training step 767 | tagging_loss_video: 12.625|tagging_loss_audio: 14.948|tagging_loss_text: 16.745|tagging_loss_image: 14.335|tagging_loss_fusion: 16.318|total_loss: 74.971 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 768 | tagging_loss_video: 13.067|tagging_loss_audio: 15.016|tagging_loss_text: 16.237|tagging_loss_image: 13.205|tagging_loss_fusion: 16.217|total_loss: 73.743 | 64.08 Examples/sec\n",
      "INFO:tensorflow:training step 769 | tagging_loss_video: 10.548|tagging_loss_audio: 13.822|tagging_loss_text: 17.123|tagging_loss_image: 12.214|tagging_loss_fusion: 13.827|total_loss: 67.534 | 70.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 770 |tagging_loss_video: 10.815|tagging_loss_audio: 14.651|tagging_loss_text: 17.191|tagging_loss_image: 13.296|tagging_loss_fusion: 16.012|total_loss: 71.965 | Examples/sec: 66.84\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.53 | precision@0.5: 0.86 |recall@0.1: 0.91 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 771 | tagging_loss_video: 12.004|tagging_loss_audio: 12.033|tagging_loss_text: 14.768|tagging_loss_image: 12.034|tagging_loss_fusion: 13.216|total_loss: 64.056 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 772 | tagging_loss_video: 12.169|tagging_loss_audio: 12.358|tagging_loss_text: 16.564|tagging_loss_image: 12.618|tagging_loss_fusion: 13.834|total_loss: 67.544 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 773 | tagging_loss_video: 9.448|tagging_loss_audio: 12.670|tagging_loss_text: 14.559|tagging_loss_image: 11.469|tagging_loss_fusion: 13.580|total_loss: 61.726 | 62.64 Examples/sec\n",
      "INFO:tensorflow:training step 774 | tagging_loss_video: 11.356|tagging_loss_audio: 11.952|tagging_loss_text: 13.850|tagging_loss_image: 13.461|tagging_loss_fusion: 14.560|total_loss: 65.178 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 775 | tagging_loss_video: 12.693|tagging_loss_audio: 13.760|tagging_loss_text: 18.096|tagging_loss_image: 10.938|tagging_loss_fusion: 15.445|total_loss: 70.932 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 776 | tagging_loss_video: 10.005|tagging_loss_audio: 11.239|tagging_loss_text: 11.809|tagging_loss_image: 9.076|tagging_loss_fusion: 10.715|total_loss: 52.844 | 61.86 Examples/sec\n",
      "INFO:tensorflow:training step 777 | tagging_loss_video: 9.661|tagging_loss_audio: 12.383|tagging_loss_text: 14.273|tagging_loss_image: 12.870|tagging_loss_fusion: 15.583|total_loss: 64.770 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 778 | tagging_loss_video: 12.122|tagging_loss_audio: 13.580|tagging_loss_text: 16.802|tagging_loss_image: 11.350|tagging_loss_fusion: 15.071|total_loss: 68.925 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 779 | tagging_loss_video: 11.684|tagging_loss_audio: 12.800|tagging_loss_text: 10.815|tagging_loss_image: 11.501|tagging_loss_fusion: 12.266|total_loss: 59.067 | 63.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 780 |tagging_loss_video: 11.884|tagging_loss_audio: 13.808|tagging_loss_text: 19.382|tagging_loss_image: 12.782|tagging_loss_fusion: 16.592|total_loss: 74.447 | Examples/sec: 68.76\n",
      "INFO:tensorflow:GAP: 0.73 | precision@0.1: 0.56 | precision@0.5: 0.81 |recall@0.1: 0.91 | recall@0.5: 0.64\n",
      "INFO:tensorflow:training step 781 | tagging_loss_video: 11.513|tagging_loss_audio: 13.934|tagging_loss_text: 15.545|tagging_loss_image: 10.875|tagging_loss_fusion: 13.870|total_loss: 65.737 | 68.29 Examples/sec\n",
      "INFO:tensorflow:training step 782 | tagging_loss_video: 11.066|tagging_loss_audio: 13.387|tagging_loss_text: 14.473|tagging_loss_image: 12.898|tagging_loss_fusion: 12.001|total_loss: 63.824 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 783 | tagging_loss_video: 11.006|tagging_loss_audio: 14.689|tagging_loss_text: 13.472|tagging_loss_image: 13.128|tagging_loss_fusion: 14.031|total_loss: 66.326 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 784 | tagging_loss_video: 10.149|tagging_loss_audio: 13.913|tagging_loss_text: 15.084|tagging_loss_image: 12.658|tagging_loss_fusion: 12.928|total_loss: 64.732 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 785 | tagging_loss_video: 9.783|tagging_loss_audio: 12.501|tagging_loss_text: 16.034|tagging_loss_image: 12.889|tagging_loss_fusion: 16.121|total_loss: 67.328 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 786 | tagging_loss_video: 8.966|tagging_loss_audio: 15.069|tagging_loss_text: 14.050|tagging_loss_image: 10.422|tagging_loss_fusion: 12.557|total_loss: 61.065 | 67.04 Examples/sec\n",
      "INFO:tensorflow:training step 787 | tagging_loss_video: 11.719|tagging_loss_audio: 12.770|tagging_loss_text: 13.120|tagging_loss_image: 12.341|tagging_loss_fusion: 14.406|total_loss: 64.355 | 60.77 Examples/sec\n",
      "INFO:tensorflow:training step 788 | tagging_loss_video: 10.180|tagging_loss_audio: 12.853|tagging_loss_text: 16.962|tagging_loss_image: 10.432|tagging_loss_fusion: 12.902|total_loss: 63.329 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 789 | tagging_loss_video: 10.477|tagging_loss_audio: 15.683|tagging_loss_text: 13.652|tagging_loss_image: 10.815|tagging_loss_fusion: 13.260|total_loss: 63.886 | 67.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 790 |tagging_loss_video: 10.795|tagging_loss_audio: 14.407|tagging_loss_text: 15.076|tagging_loss_image: 12.468|tagging_loss_fusion: 14.265|total_loss: 67.012 | Examples/sec: 70.95\n",
      "INFO:tensorflow:GAP: 0.80 | precision@0.1: 0.55 | precision@0.5: 0.84 |recall@0.1: 0.93 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 791 | tagging_loss_video: 10.258|tagging_loss_audio: 12.969|tagging_loss_text: 16.788|tagging_loss_image: 11.994|tagging_loss_fusion: 14.787|total_loss: 66.795 | 67.76 Examples/sec\n",
      "INFO:tensorflow:training step 792 | tagging_loss_video: 10.490|tagging_loss_audio: 13.833|tagging_loss_text: 14.588|tagging_loss_image: 10.169|tagging_loss_fusion: 13.619|total_loss: 62.698 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 793 | tagging_loss_video: 11.595|tagging_loss_audio: 14.397|tagging_loss_text: 19.619|tagging_loss_image: 12.186|tagging_loss_fusion: 12.733|total_loss: 70.530 | 59.93 Examples/sec\n",
      "INFO:tensorflow:training step 794 | tagging_loss_video: 11.586|tagging_loss_audio: 12.605|tagging_loss_text: 17.979|tagging_loss_image: 13.360|tagging_loss_fusion: 12.943|total_loss: 68.473 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 795 | tagging_loss_video: 9.673|tagging_loss_audio: 13.937|tagging_loss_text: 13.989|tagging_loss_image: 9.771|tagging_loss_fusion: 14.501|total_loss: 61.871 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 796 | tagging_loss_video: 10.319|tagging_loss_audio: 16.919|tagging_loss_text: 13.303|tagging_loss_image: 11.171|tagging_loss_fusion: 13.942|total_loss: 65.654 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 797 | tagging_loss_video: 11.640|tagging_loss_audio: 13.027|tagging_loss_text: 14.979|tagging_loss_image: 13.460|tagging_loss_fusion: 13.963|total_loss: 67.069 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 798 | tagging_loss_video: 11.556|tagging_loss_audio: 15.064|tagging_loss_text: 14.163|tagging_loss_image: 12.951|tagging_loss_fusion: 13.214|total_loss: 66.947 | 64.38 Examples/sec\n",
      "INFO:tensorflow:training step 799 | tagging_loss_video: 11.134|tagging_loss_audio: 13.007|tagging_loss_text: 17.085|tagging_loss_image: 11.531|tagging_loss_fusion: 15.672|total_loss: 68.430 | 68.79 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 800 |tagging_loss_video: 11.907|tagging_loss_audio: 16.142|tagging_loss_text: 16.805|tagging_loss_image: 13.167|tagging_loss_fusion: 14.735|total_loss: 72.756 | Examples/sec: 70.79\n",
      "INFO:tensorflow:GAP: 0.75 | precision@0.1: 0.53 | precision@0.5: 0.84 |recall@0.1: 0.91 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 801 | tagging_loss_video: 10.070|tagging_loss_audio: 13.524|tagging_loss_text: 15.981|tagging_loss_image: 12.916|tagging_loss_fusion: 13.800|total_loss: 66.291 | 68.22 Examples/sec\n",
      "INFO:tensorflow:training step 802 | tagging_loss_video: 10.473|tagging_loss_audio: 13.518|tagging_loss_text: 15.952|tagging_loss_image: 12.929|tagging_loss_fusion: 12.057|total_loss: 64.929 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 803 | tagging_loss_video: 9.736|tagging_loss_audio: 12.045|tagging_loss_text: 14.131|tagging_loss_image: 11.122|tagging_loss_fusion: 13.325|total_loss: 60.358 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 804 | tagging_loss_video: 12.014|tagging_loss_audio: 16.129|tagging_loss_text: 11.854|tagging_loss_image: 14.751|tagging_loss_fusion: 14.756|total_loss: 69.505 | 63.13 Examples/sec\n",
      "INFO:tensorflow:training step 805 | tagging_loss_video: 11.151|tagging_loss_audio: 13.724|tagging_loss_text: 12.849|tagging_loss_image: 9.199|tagging_loss_fusion: 13.813|total_loss: 60.737 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 806 | tagging_loss_video: 11.747|tagging_loss_audio: 11.213|tagging_loss_text: 15.881|tagging_loss_image: 12.453|tagging_loss_fusion: 14.591|total_loss: 65.885 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 807 | tagging_loss_video: 11.307|tagging_loss_audio: 16.132|tagging_loss_text: 14.207|tagging_loss_image: 12.527|tagging_loss_fusion: 14.294|total_loss: 68.466 | 67.15 Examples/sec\n",
      "INFO:tensorflow:training step 808 | tagging_loss_video: 10.198|tagging_loss_audio: 13.119|tagging_loss_text: 16.096|tagging_loss_image: 12.181|tagging_loss_fusion: 14.700|total_loss: 66.294 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 809 | tagging_loss_video: 9.991|tagging_loss_audio: 13.274|tagging_loss_text: 14.410|tagging_loss_image: 10.839|tagging_loss_fusion: 13.647|total_loss: 62.161 | 72.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 810 |tagging_loss_video: 12.471|tagging_loss_audio: 15.808|tagging_loss_text: 15.676|tagging_loss_image: 11.401|tagging_loss_fusion: 13.386|total_loss: 68.743 | Examples/sec: 67.46\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.55 | precision@0.5: 0.86 |recall@0.1: 0.94 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 811 | tagging_loss_video: 12.302|tagging_loss_audio: 13.379|tagging_loss_text: 15.730|tagging_loss_image: 13.895|tagging_loss_fusion: 15.650|total_loss: 70.956 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 812 | tagging_loss_video: 10.640|tagging_loss_audio: 14.985|tagging_loss_text: 17.482|tagging_loss_image: 14.648|tagging_loss_fusion: 15.986|total_loss: 73.741 | 61.11 Examples/sec\n",
      "INFO:tensorflow:training step 813 | tagging_loss_video: 11.890|tagging_loss_audio: 14.241|tagging_loss_text: 16.247|tagging_loss_image: 13.085|tagging_loss_fusion: 13.364|total_loss: 68.828 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 814 | tagging_loss_video: 12.819|tagging_loss_audio: 11.961|tagging_loss_text: 19.051|tagging_loss_image: 13.198|tagging_loss_fusion: 16.293|total_loss: 73.322 | 67.88 Examples/sec\n",
      "INFO:tensorflow:training step 815 | tagging_loss_video: 10.201|tagging_loss_audio: 12.874|tagging_loss_text: 12.606|tagging_loss_image: 11.570|tagging_loss_fusion: 13.797|total_loss: 61.049 | 64.58 Examples/sec\n",
      "INFO:tensorflow:training step 816 | tagging_loss_video: 10.602|tagging_loss_audio: 13.065|tagging_loss_text: 15.245|tagging_loss_image: 12.306|tagging_loss_fusion: 12.330|total_loss: 63.547 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 817 | tagging_loss_video: 10.112|tagging_loss_audio: 16.318|tagging_loss_text: 13.454|tagging_loss_image: 8.748|tagging_loss_fusion: 14.497|total_loss: 63.129 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 818 | tagging_loss_video: 9.367|tagging_loss_audio: 12.607|tagging_loss_text: 15.410|tagging_loss_image: 9.814|tagging_loss_fusion: 13.302|total_loss: 60.500 | 66.53 Examples/sec\n",
      "INFO:tensorflow:training step 819 | tagging_loss_video: 8.897|tagging_loss_audio: 14.228|tagging_loss_text: 16.902|tagging_loss_image: 10.435|tagging_loss_fusion: 15.669|total_loss: 66.130 | 70.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 820 |tagging_loss_video: 11.196|tagging_loss_audio: 14.232|tagging_loss_text: 17.144|tagging_loss_image: 11.956|tagging_loss_fusion: 15.263|total_loss: 69.791 | Examples/sec: 67.10\n",
      "INFO:tensorflow:GAP: 0.73 | precision@0.1: 0.49 | precision@0.5: 0.80 |recall@0.1: 0.92 | recall@0.5: 0.65\n",
      "INFO:tensorflow:training step 821 | tagging_loss_video: 9.835|tagging_loss_audio: 14.897|tagging_loss_text: 15.323|tagging_loss_image: 11.893|tagging_loss_fusion: 13.807|total_loss: 65.754 | 65.10 Examples/sec\n",
      "INFO:tensorflow:training step 822 | tagging_loss_video: 9.518|tagging_loss_audio: 12.839|tagging_loss_text: 17.392|tagging_loss_image: 10.691|tagging_loss_fusion: 13.393|total_loss: 63.833 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 823 | tagging_loss_video: 11.592|tagging_loss_audio: 15.377|tagging_loss_text: 17.152|tagging_loss_image: 13.290|tagging_loss_fusion: 16.184|total_loss: 73.595 | 66.15 Examples/sec\n",
      "INFO:tensorflow:training step 824 | tagging_loss_video: 10.480|tagging_loss_audio: 15.388|tagging_loss_text: 12.449|tagging_loss_image: 11.943|tagging_loss_fusion: 12.709|total_loss: 62.970 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 825 | tagging_loss_video: 9.129|tagging_loss_audio: 11.366|tagging_loss_text: 13.673|tagging_loss_image: 10.115|tagging_loss_fusion: 12.864|total_loss: 57.146 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 826 | tagging_loss_video: 9.063|tagging_loss_audio: 10.733|tagging_loss_text: 16.092|tagging_loss_image: 9.979|tagging_loss_fusion: 12.711|total_loss: 58.577 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 827 | tagging_loss_video: 11.391|tagging_loss_audio: 15.258|tagging_loss_text: 17.457|tagging_loss_image: 11.557|tagging_loss_fusion: 14.491|total_loss: 70.154 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 828 | tagging_loss_video: 11.839|tagging_loss_audio: 15.264|tagging_loss_text: 16.940|tagging_loss_image: 12.036|tagging_loss_fusion: 15.962|total_loss: 72.041 | 65.84 Examples/sec\n",
      "INFO:tensorflow:training step 829 | tagging_loss_video: 10.760|tagging_loss_audio: 13.000|tagging_loss_text: 14.883|tagging_loss_image: 11.401|tagging_loss_fusion: 13.689|total_loss: 63.733 | 69.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 830 |tagging_loss_video: 10.522|tagging_loss_audio: 14.620|tagging_loss_text: 18.948|tagging_loss_image: 11.194|tagging_loss_fusion: 15.237|total_loss: 70.520 | Examples/sec: 68.84\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.51 | precision@0.5: 0.85 |recall@0.1: 0.88 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 831 | tagging_loss_video: 11.763|tagging_loss_audio: 13.314|tagging_loss_text: 13.899|tagging_loss_image: 10.557|tagging_loss_fusion: 13.407|total_loss: 62.939 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 832 | tagging_loss_video: 10.558|tagging_loss_audio: 14.228|tagging_loss_text: 11.104|tagging_loss_image: 9.306|tagging_loss_fusion: 14.755|total_loss: 59.951 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 833 | tagging_loss_video: 9.613|tagging_loss_audio: 13.844|tagging_loss_text: 15.704|tagging_loss_image: 10.775|tagging_loss_fusion: 13.134|total_loss: 63.070 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 834 | tagging_loss_video: 10.039|tagging_loss_audio: 11.837|tagging_loss_text: 17.261|tagging_loss_image: 11.186|tagging_loss_fusion: 13.574|total_loss: 63.897 | 63.53 Examples/sec\n",
      "INFO:tensorflow:training step 835 | tagging_loss_video: 9.909|tagging_loss_audio: 12.431|tagging_loss_text: 11.982|tagging_loss_image: 10.469|tagging_loss_fusion: 14.750|total_loss: 59.540 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 836 | tagging_loss_video: 9.242|tagging_loss_audio: 12.278|tagging_loss_text: 12.733|tagging_loss_image: 11.490|tagging_loss_fusion: 14.808|total_loss: 60.551 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 837 | tagging_loss_video: 10.575|tagging_loss_audio: 12.575|tagging_loss_text: 13.198|tagging_loss_image: 11.977|tagging_loss_fusion: 13.989|total_loss: 62.314 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 838 | tagging_loss_video: 11.578|tagging_loss_audio: 13.912|tagging_loss_text: 14.782|tagging_loss_image: 12.329|tagging_loss_fusion: 14.854|total_loss: 67.454 | 60.12 Examples/sec\n",
      "INFO:tensorflow:training step 839 | tagging_loss_video: 10.832|tagging_loss_audio: 13.888|tagging_loss_text: 17.045|tagging_loss_image: 11.665|tagging_loss_fusion: 13.863|total_loss: 67.293 | 71.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 840 |tagging_loss_video: 10.520|tagging_loss_audio: 15.569|tagging_loss_text: 18.613|tagging_loss_image: 11.957|tagging_loss_fusion: 15.075|total_loss: 71.734 | Examples/sec: 71.47\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.55 | precision@0.5: 0.86 |recall@0.1: 0.93 | recall@0.5: 0.63\n",
      "INFO:tensorflow:training step 841 | tagging_loss_video: 9.289|tagging_loss_audio: 12.712|tagging_loss_text: 14.080|tagging_loss_image: 11.493|tagging_loss_fusion: 13.388|total_loss: 60.962 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 842 | tagging_loss_video: 8.827|tagging_loss_audio: 11.794|tagging_loss_text: 13.077|tagging_loss_image: 10.279|tagging_loss_fusion: 10.407|total_loss: 54.383 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 843 | tagging_loss_video: 9.268|tagging_loss_audio: 12.327|tagging_loss_text: 14.701|tagging_loss_image: 10.180|tagging_loss_fusion: 14.979|total_loss: 61.456 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 844 | tagging_loss_video: 10.690|tagging_loss_audio: 9.414|tagging_loss_text: 12.300|tagging_loss_image: 10.523|tagging_loss_fusion: 13.088|total_loss: 56.015 | 65.32 Examples/sec\n",
      "INFO:tensorflow:training step 845 | tagging_loss_video: 11.192|tagging_loss_audio: 15.725|tagging_loss_text: 14.515|tagging_loss_image: 10.869|tagging_loss_fusion: 13.118|total_loss: 65.417 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 846 | tagging_loss_video: 9.629|tagging_loss_audio: 12.204|tagging_loss_text: 13.314|tagging_loss_image: 12.538|tagging_loss_fusion: 13.237|total_loss: 60.922 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 847 | tagging_loss_video: 8.755|tagging_loss_audio: 11.492|tagging_loss_text: 12.889|tagging_loss_image: 10.857|tagging_loss_fusion: 12.323|total_loss: 56.316 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 848 | tagging_loss_video: 12.082|tagging_loss_audio: 12.968|tagging_loss_text: 18.177|tagging_loss_image: 13.156|tagging_loss_fusion: 15.321|total_loss: 71.703 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 849 | tagging_loss_video: 11.239|tagging_loss_audio: 12.070|tagging_loss_text: 15.551|tagging_loss_image: 10.366|tagging_loss_fusion: 13.837|total_loss: 63.063 | 68.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 850 |tagging_loss_video: 9.731|tagging_loss_audio: 15.823|tagging_loss_text: 13.401|tagging_loss_image: 11.372|tagging_loss_fusion: 15.419|total_loss: 65.746 | Examples/sec: 71.06\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.54 | precision@0.5: 0.81 |recall@0.1: 0.90 | recall@0.5: 0.66\n",
      "INFO:tensorflow:training step 851 | tagging_loss_video: 10.595|tagging_loss_audio: 15.086|tagging_loss_text: 14.244|tagging_loss_image: 13.655|tagging_loss_fusion: 14.372|total_loss: 67.951 | 72.19 Examples/sec\n",
      "INFO:tensorflow:training step 852 | tagging_loss_video: 11.264|tagging_loss_audio: 11.886|tagging_loss_text: 17.329|tagging_loss_image: 11.187|tagging_loss_fusion: 12.749|total_loss: 64.414 | 63.17 Examples/sec\n",
      "INFO:tensorflow:training step 853 | tagging_loss_video: 8.900|tagging_loss_audio: 11.313|tagging_loss_text: 11.486|tagging_loss_image: 10.774|tagging_loss_fusion: 14.189|total_loss: 56.662 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 854 | tagging_loss_video: 9.141|tagging_loss_audio: 11.722|tagging_loss_text: 10.609|tagging_loss_image: 9.726|tagging_loss_fusion: 11.592|total_loss: 52.790 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 855 | tagging_loss_video: 9.953|tagging_loss_audio: 13.663|tagging_loss_text: 12.437|tagging_loss_image: 12.144|tagging_loss_fusion: 12.288|total_loss: 60.486 | 65.53 Examples/sec\n",
      "INFO:tensorflow:training step 856 | tagging_loss_video: 8.600|tagging_loss_audio: 13.282|tagging_loss_text: 15.722|tagging_loss_image: 12.302|tagging_loss_fusion: 13.588|total_loss: 63.494 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 857 | tagging_loss_video: 11.044|tagging_loss_audio: 12.383|tagging_loss_text: 16.346|tagging_loss_image: 10.898|tagging_loss_fusion: 10.949|total_loss: 61.621 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 858 | tagging_loss_video: 10.821|tagging_loss_audio: 12.964|tagging_loss_text: 16.136|tagging_loss_image: 10.137|tagging_loss_fusion: 15.361|total_loss: 65.419 | 63.15 Examples/sec\n",
      "INFO:tensorflow:training step 859 | tagging_loss_video: 10.478|tagging_loss_audio: 11.865|tagging_loss_text: 13.003|tagging_loss_image: 11.154|tagging_loss_fusion: 12.985|total_loss: 59.485 | 68.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 860 |tagging_loss_video: 10.285|tagging_loss_audio: 13.954|tagging_loss_text: 20.683|tagging_loss_image: 13.527|tagging_loss_fusion: 13.921|total_loss: 72.370 | Examples/sec: 71.72\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.52 | precision@0.5: 0.81 |recall@0.1: 0.92 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 861 | tagging_loss_video: 11.143|tagging_loss_audio: 15.155|tagging_loss_text: 14.450|tagging_loss_image: 10.850|tagging_loss_fusion: 14.183|total_loss: 65.781 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 862 | tagging_loss_video: 10.336|tagging_loss_audio: 11.390|tagging_loss_text: 14.034|tagging_loss_image: 11.668|tagging_loss_fusion: 15.492|total_loss: 62.919 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 863 | tagging_loss_video: 10.147|tagging_loss_audio: 12.937|tagging_loss_text: 11.546|tagging_loss_image: 9.950|tagging_loss_fusion: 13.600|total_loss: 58.180 | 60.02 Examples/sec\n",
      "INFO:tensorflow:training step 864 | tagging_loss_video: 13.042|tagging_loss_audio: 13.390|tagging_loss_text: 12.331|tagging_loss_image: 12.955|tagging_loss_fusion: 13.730|total_loss: 65.448 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 865 | tagging_loss_video: 10.077|tagging_loss_audio: 12.536|tagging_loss_text: 13.939|tagging_loss_image: 11.927|tagging_loss_fusion: 13.393|total_loss: 61.872 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 866 | tagging_loss_video: 12.815|tagging_loss_audio: 14.570|tagging_loss_text: 17.253|tagging_loss_image: 11.182|tagging_loss_fusion: 11.867|total_loss: 67.686 | 60.95 Examples/sec\n",
      "INFO:tensorflow:training step 867 | tagging_loss_video: 12.982|tagging_loss_audio: 15.647|tagging_loss_text: 15.645|tagging_loss_image: 14.773|tagging_loss_fusion: 15.394|total_loss: 74.441 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 868 | tagging_loss_video: 10.421|tagging_loss_audio: 12.025|tagging_loss_text: 12.474|tagging_loss_image: 11.090|tagging_loss_fusion: 13.275|total_loss: 59.286 | 71.21 Examples/sec\n",
      "INFO:tensorflow:training step 869 | tagging_loss_video: 12.682|tagging_loss_audio: 10.522|tagging_loss_text: 16.697|tagging_loss_image: 12.329|tagging_loss_fusion: 14.372|total_loss: 66.603 | 61.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 870 |tagging_loss_video: 10.373|tagging_loss_audio: 14.327|tagging_loss_text: 11.197|tagging_loss_image: 11.360|tagging_loss_fusion: 13.361|total_loss: 60.617 | Examples/sec: 71.33\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.52 | precision@0.5: 0.82 |recall@0.1: 0.95 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 871 | tagging_loss_video: 11.248|tagging_loss_audio: 15.631|tagging_loss_text: 18.853|tagging_loss_image: 11.645|tagging_loss_fusion: 15.038|total_loss: 72.414 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 872 | tagging_loss_video: 11.027|tagging_loss_audio: 12.985|tagging_loss_text: 14.787|tagging_loss_image: 10.857|tagging_loss_fusion: 15.528|total_loss: 65.183 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 873 | tagging_loss_video: 12.579|tagging_loss_audio: 15.380|tagging_loss_text: 19.658|tagging_loss_image: 12.604|tagging_loss_fusion: 15.492|total_loss: 75.713 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 874 | tagging_loss_video: 9.836|tagging_loss_audio: 12.862|tagging_loss_text: 13.152|tagging_loss_image: 11.695|tagging_loss_fusion: 13.018|total_loss: 60.564 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 875 | tagging_loss_video: 11.680|tagging_loss_audio: 14.274|tagging_loss_text: 17.744|tagging_loss_image: 11.747|tagging_loss_fusion: 14.726|total_loss: 70.171 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 876 | tagging_loss_video: 9.457|tagging_loss_audio: 12.161|tagging_loss_text: 11.757|tagging_loss_image: 9.159|tagging_loss_fusion: 12.342|total_loss: 54.876 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 877 | tagging_loss_video: 11.626|tagging_loss_audio: 13.615|tagging_loss_text: 16.689|tagging_loss_image: 12.612|tagging_loss_fusion: 14.194|total_loss: 68.736 | 62.57 Examples/sec\n",
      "INFO:tensorflow:training step 878 | tagging_loss_video: 11.680|tagging_loss_audio: 13.232|tagging_loss_text: 18.077|tagging_loss_image: 12.822|tagging_loss_fusion: 13.735|total_loss: 69.547 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 879 | tagging_loss_video: 11.670|tagging_loss_audio: 13.059|tagging_loss_text: 15.246|tagging_loss_image: 13.811|tagging_loss_fusion: 14.383|total_loss: 68.169 | 70.91 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 880 |tagging_loss_video: 11.515|tagging_loss_audio: 13.665|tagging_loss_text: 18.209|tagging_loss_image: 13.683|tagging_loss_fusion: 13.160|total_loss: 70.233 | Examples/sec: 64.07\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.56 | precision@0.5: 0.86 |recall@0.1: 0.92 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 881 | tagging_loss_video: 11.548|tagging_loss_audio: 13.266|tagging_loss_text: 20.684|tagging_loss_image: 15.508|tagging_loss_fusion: 15.945|total_loss: 76.951 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 882 | tagging_loss_video: 9.079|tagging_loss_audio: 12.367|tagging_loss_text: 15.254|tagging_loss_image: 9.567|tagging_loss_fusion: 13.779|total_loss: 60.046 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 883 | tagging_loss_video: 9.595|tagging_loss_audio: 11.785|tagging_loss_text: 14.088|tagging_loss_image: 9.310|tagging_loss_fusion: 13.919|total_loss: 58.697 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 884 | tagging_loss_video: 11.846|tagging_loss_audio: 13.509|tagging_loss_text: 12.768|tagging_loss_image: 11.092|tagging_loss_fusion: 14.236|total_loss: 63.451 | 67.34 Examples/sec\n",
      "INFO:tensorflow:training step 885 | tagging_loss_video: 9.979|tagging_loss_audio: 13.076|tagging_loss_text: 15.119|tagging_loss_image: 12.415|tagging_loss_fusion: 15.463|total_loss: 66.052 | 72.03 Examples/sec\n",
      "INFO:tensorflow:training step 886 | tagging_loss_video: 9.819|tagging_loss_audio: 12.232|tagging_loss_text: 16.177|tagging_loss_image: 12.251|tagging_loss_fusion: 13.223|total_loss: 63.702 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 887 | tagging_loss_video: 13.050|tagging_loss_audio: 12.251|tagging_loss_text: 17.126|tagging_loss_image: 15.741|tagging_loss_fusion: 15.120|total_loss: 73.288 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 888 | tagging_loss_video: 11.318|tagging_loss_audio: 16.156|tagging_loss_text: 18.562|tagging_loss_image: 13.612|tagging_loss_fusion: 15.057|total_loss: 74.705 | 63.81 Examples/sec\n",
      "INFO:tensorflow:training step 889 | tagging_loss_video: 10.925|tagging_loss_audio: 12.702|tagging_loss_text: 15.160|tagging_loss_image: 10.018|tagging_loss_fusion: 11.291|total_loss: 60.096 | 69.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 890 |tagging_loss_video: 9.800|tagging_loss_audio: 14.776|tagging_loss_text: 13.104|tagging_loss_image: 11.478|tagging_loss_fusion: 13.651|total_loss: 62.810 | Examples/sec: 70.91\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.55 | precision@0.5: 0.83 |recall@0.1: 0.94 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 891 | tagging_loss_video: 8.568|tagging_loss_audio: 12.397|tagging_loss_text: 14.317|tagging_loss_image: 12.842|tagging_loss_fusion: 12.339|total_loss: 60.462 | 62.61 Examples/sec\n",
      "INFO:tensorflow:training step 892 | tagging_loss_video: 11.540|tagging_loss_audio: 16.469|tagging_loss_text: 15.351|tagging_loss_image: 11.478|tagging_loss_fusion: 14.222|total_loss: 69.061 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 893 | tagging_loss_video: 10.399|tagging_loss_audio: 13.151|tagging_loss_text: 14.402|tagging_loss_image: 10.540|tagging_loss_fusion: 14.421|total_loss: 62.912 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 894 | tagging_loss_video: 10.143|tagging_loss_audio: 13.402|tagging_loss_text: 14.660|tagging_loss_image: 12.232|tagging_loss_fusion: 12.262|total_loss: 62.699 | 64.44 Examples/sec\n",
      "INFO:tensorflow:training step 895 | tagging_loss_video: 9.156|tagging_loss_audio: 15.318|tagging_loss_text: 14.017|tagging_loss_image: 10.876|tagging_loss_fusion: 11.937|total_loss: 61.303 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 896 | tagging_loss_video: 9.495|tagging_loss_audio: 13.177|tagging_loss_text: 11.537|tagging_loss_image: 10.354|tagging_loss_fusion: 12.072|total_loss: 56.635 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 897 | tagging_loss_video: 9.902|tagging_loss_audio: 13.353|tagging_loss_text: 11.313|tagging_loss_image: 10.294|tagging_loss_fusion: 13.034|total_loss: 57.896 | 62.77 Examples/sec\n",
      "INFO:tensorflow:training step 898 | tagging_loss_video: 9.137|tagging_loss_audio: 14.854|tagging_loss_text: 14.674|tagging_loss_image: 10.615|tagging_loss_fusion: 10.913|total_loss: 60.193 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 899 | tagging_loss_video: 9.555|tagging_loss_audio: 12.356|tagging_loss_text: 16.081|tagging_loss_image: 12.315|tagging_loss_fusion: 11.861|total_loss: 62.168 | 69.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 900 |tagging_loss_video: 11.300|tagging_loss_audio: 12.993|tagging_loss_text: 14.771|tagging_loss_image: 11.619|tagging_loss_fusion: 13.539|total_loss: 64.220 | Examples/sec: 67.95\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.59 | precision@0.5: 0.89 |recall@0.1: 0.93 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 901 | tagging_loss_video: 11.057|tagging_loss_audio: 15.004|tagging_loss_text: 15.249|tagging_loss_image: 11.839|tagging_loss_fusion: 11.944|total_loss: 65.093 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 902 | tagging_loss_video: 10.582|tagging_loss_audio: 13.709|tagging_loss_text: 16.217|tagging_loss_image: 8.333|tagging_loss_fusion: 13.228|total_loss: 62.068 | 66.32 Examples/sec\n",
      "INFO:tensorflow:training step 903 | tagging_loss_video: 9.392|tagging_loss_audio: 13.087|tagging_loss_text: 14.073|tagging_loss_image: 10.346|tagging_loss_fusion: 11.410|total_loss: 58.307 | 68.49 Examples/sec\n",
      "INFO:tensorflow:training step 904 | tagging_loss_video: 9.835|tagging_loss_audio: 11.867|tagging_loss_text: 15.282|tagging_loss_image: 11.745|tagging_loss_fusion: 13.272|total_loss: 62.001 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 905 | tagging_loss_video: 9.914|tagging_loss_audio: 14.846|tagging_loss_text: 10.918|tagging_loss_image: 12.034|tagging_loss_fusion: 11.400|total_loss: 59.112 | 66.45 Examples/sec\n",
      "INFO:tensorflow:training step 906 | tagging_loss_video: 10.854|tagging_loss_audio: 13.599|tagging_loss_text: 17.487|tagging_loss_image: 12.843|tagging_loss_fusion: 14.209|total_loss: 68.992 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 907 | tagging_loss_video: 11.325|tagging_loss_audio: 14.492|tagging_loss_text: 18.562|tagging_loss_image: 12.329|tagging_loss_fusion: 15.870|total_loss: 72.578 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 908 | tagging_loss_video: 10.935|tagging_loss_audio: 13.146|tagging_loss_text: 11.494|tagging_loss_image: 11.891|tagging_loss_fusion: 13.849|total_loss: 61.315 | 64.19 Examples/sec\n",
      "INFO:tensorflow:training step 909 | tagging_loss_video: 10.557|tagging_loss_audio: 10.511|tagging_loss_text: 15.744|tagging_loss_image: 11.147|tagging_loss_fusion: 12.270|total_loss: 60.228 | 64.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 910 |tagging_loss_video: 9.900|tagging_loss_audio: 11.917|tagging_loss_text: 14.330|tagging_loss_image: 14.106|tagging_loss_fusion: 16.399|total_loss: 66.653 | Examples/sec: 71.97\n",
      "INFO:tensorflow:GAP: 0.77 | precision@0.1: 0.52 | precision@0.5: 0.83 |recall@0.1: 0.90 | recall@0.5: 0.64\n",
      "INFO:tensorflow:training step 911 | tagging_loss_video: 9.706|tagging_loss_audio: 10.882|tagging_loss_text: 16.183|tagging_loss_image: 11.391|tagging_loss_fusion: 12.702|total_loss: 60.863 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 912 | tagging_loss_video: 10.106|tagging_loss_audio: 11.840|tagging_loss_text: 18.055|tagging_loss_image: 11.146|tagging_loss_fusion: 13.943|total_loss: 65.090 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 913 | tagging_loss_video: 8.887|tagging_loss_audio: 12.693|tagging_loss_text: 14.598|tagging_loss_image: 11.809|tagging_loss_fusion: 13.443|total_loss: 61.430 | 62.84 Examples/sec\n",
      "INFO:tensorflow:training step 914 | tagging_loss_video: 10.444|tagging_loss_audio: 14.770|tagging_loss_text: 17.329|tagging_loss_image: 11.034|tagging_loss_fusion: 13.616|total_loss: 67.192 | 72.23 Examples/sec\n",
      "INFO:tensorflow:training step 915 | tagging_loss_video: 10.602|tagging_loss_audio: 12.332|tagging_loss_text: 16.787|tagging_loss_image: 10.445|tagging_loss_fusion: 13.904|total_loss: 64.070 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 916 | tagging_loss_video: 7.797|tagging_loss_audio: 13.737|tagging_loss_text: 15.581|tagging_loss_image: 11.039|tagging_loss_fusion: 12.093|total_loss: 60.246 | 65.78 Examples/sec\n",
      "INFO:tensorflow:training step 917 | tagging_loss_video: 11.076|tagging_loss_audio: 13.052|tagging_loss_text: 16.314|tagging_loss_image: 10.919|tagging_loss_fusion: 12.845|total_loss: 64.205 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 918 | tagging_loss_video: 10.312|tagging_loss_audio: 14.606|tagging_loss_text: 13.906|tagging_loss_image: 10.927|tagging_loss_fusion: 11.087|total_loss: 60.839 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 919 | tagging_loss_video: 9.335|tagging_loss_audio: 12.001|tagging_loss_text: 17.607|tagging_loss_image: 12.514|tagging_loss_fusion: 11.259|total_loss: 62.716 | 60.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 920 |tagging_loss_video: 10.372|tagging_loss_audio: 13.019|tagging_loss_text: 18.076|tagging_loss_image: 12.788|tagging_loss_fusion: 13.277|total_loss: 67.531 | Examples/sec: 70.79\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.59 | precision@0.5: 0.85 |recall@0.1: 0.94 | recall@0.5: 0.72\n",
      "INFO:tensorflow:training step 921 | tagging_loss_video: 10.531|tagging_loss_audio: 12.552|tagging_loss_text: 15.614|tagging_loss_image: 12.604|tagging_loss_fusion: 12.190|total_loss: 63.491 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 922 | tagging_loss_video: 9.625|tagging_loss_audio: 12.862|tagging_loss_text: 13.883|tagging_loss_image: 11.241|tagging_loss_fusion: 13.280|total_loss: 60.890 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 923 | tagging_loss_video: 10.232|tagging_loss_audio: 13.175|tagging_loss_text: 14.996|tagging_loss_image: 13.648|tagging_loss_fusion: 12.532|total_loss: 64.583 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 924 | tagging_loss_video: 9.917|tagging_loss_audio: 12.466|tagging_loss_text: 16.223|tagging_loss_image: 10.775|tagging_loss_fusion: 11.415|total_loss: 60.794 | 65.43 Examples/sec\n",
      "INFO:tensorflow:training step 925 | tagging_loss_video: 10.144|tagging_loss_audio: 13.982|tagging_loss_text: 16.000|tagging_loss_image: 12.356|tagging_loss_fusion: 13.317|total_loss: 65.798 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 926 | tagging_loss_video: 9.561|tagging_loss_audio: 12.887|tagging_loss_text: 16.867|tagging_loss_image: 11.550|tagging_loss_fusion: 11.397|total_loss: 62.261 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 927 | tagging_loss_video: 9.925|tagging_loss_audio: 14.054|tagging_loss_text: 17.333|tagging_loss_image: 11.394|tagging_loss_fusion: 14.784|total_loss: 67.490 | 65.56 Examples/sec\n",
      "INFO:tensorflow:training step 928 | tagging_loss_video: 8.683|tagging_loss_audio: 12.570|tagging_loss_text: 17.058|tagging_loss_image: 11.766|tagging_loss_fusion: 13.680|total_loss: 63.758 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 929 | tagging_loss_video: 10.432|tagging_loss_audio: 13.603|tagging_loss_text: 17.223|tagging_loss_image: 12.675|tagging_loss_fusion: 13.938|total_loss: 67.870 | 71.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 930 |tagging_loss_video: 8.953|tagging_loss_audio: 11.539|tagging_loss_text: 17.005|tagging_loss_image: 10.797|tagging_loss_fusion: 14.213|total_loss: 62.507 | Examples/sec: 62.33\n",
      "INFO:tensorflow:GAP: 0.78 | precision@0.1: 0.54 | precision@0.5: 0.86 |recall@0.1: 0.94 | recall@0.5: 0.67\n",
      "INFO:tensorflow:training step 931 | tagging_loss_video: 13.067|tagging_loss_audio: 11.811|tagging_loss_text: 16.042|tagging_loss_image: 10.335|tagging_loss_fusion: 13.771|total_loss: 65.026 | 67.31 Examples/sec\n",
      "INFO:tensorflow:training step 932 | tagging_loss_video: 10.449|tagging_loss_audio: 12.037|tagging_loss_text: 15.227|tagging_loss_image: 12.559|tagging_loss_fusion: 13.341|total_loss: 63.614 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 933 | tagging_loss_video: 9.642|tagging_loss_audio: 11.873|tagging_loss_text: 15.382|tagging_loss_image: 11.426|tagging_loss_fusion: 12.843|total_loss: 61.166 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 934 | tagging_loss_video: 10.499|tagging_loss_audio: 14.622|tagging_loss_text: 17.327|tagging_loss_image: 12.155|tagging_loss_fusion: 14.050|total_loss: 68.652 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 935 | tagging_loss_video: 10.743|tagging_loss_audio: 13.435|tagging_loss_text: 13.252|tagging_loss_image: 11.317|tagging_loss_fusion: 11.972|total_loss: 60.719 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 936 | tagging_loss_video: 10.578|tagging_loss_audio: 13.307|tagging_loss_text: 16.797|tagging_loss_image: 12.561|tagging_loss_fusion: 13.115|total_loss: 66.357 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 937 | tagging_loss_video: 10.783|tagging_loss_audio: 13.249|tagging_loss_text: 14.114|tagging_loss_image: 12.809|tagging_loss_fusion: 12.621|total_loss: 63.576 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 938 | tagging_loss_video: 10.622|tagging_loss_audio: 12.921|tagging_loss_text: 13.485|tagging_loss_image: 11.557|tagging_loss_fusion: 11.977|total_loss: 60.562 | 59.96 Examples/sec\n",
      "INFO:tensorflow:training step 939 | tagging_loss_video: 9.414|tagging_loss_audio: 11.910|tagging_loss_text: 16.162|tagging_loss_image: 13.111|tagging_loss_fusion: 14.527|total_loss: 65.124 | 69.13 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 940 |tagging_loss_video: 11.696|tagging_loss_audio: 12.275|tagging_loss_text: 15.644|tagging_loss_image: 12.407|tagging_loss_fusion: 12.580|total_loss: 64.602 | Examples/sec: 70.95\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.59 | precision@0.5: 0.84 |recall@0.1: 0.93 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 941 | tagging_loss_video: 9.899|tagging_loss_audio: 13.273|tagging_loss_text: 18.843|tagging_loss_image: 12.044|tagging_loss_fusion: 13.157|total_loss: 67.217 | 65.49 Examples/sec\n",
      "INFO:tensorflow:training step 942 | tagging_loss_video: 9.474|tagging_loss_audio: 11.581|tagging_loss_text: 13.204|tagging_loss_image: 12.013|tagging_loss_fusion: 12.451|total_loss: 58.723 | 67.74 Examples/sec\n",
      "INFO:tensorflow:training step 943 | tagging_loss_video: 10.368|tagging_loss_audio: 11.938|tagging_loss_text: 14.912|tagging_loss_image: 10.599|tagging_loss_fusion: 11.707|total_loss: 59.524 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 944 | tagging_loss_video: 11.146|tagging_loss_audio: 13.076|tagging_loss_text: 18.432|tagging_loss_image: 11.217|tagging_loss_fusion: 14.923|total_loss: 68.795 | 65.54 Examples/sec\n",
      "INFO:tensorflow:training step 945 | tagging_loss_video: 10.323|tagging_loss_audio: 13.446|tagging_loss_text: 12.927|tagging_loss_image: 10.732|tagging_loss_fusion: 11.805|total_loss: 59.233 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 946 | tagging_loss_video: 11.464|tagging_loss_audio: 15.003|tagging_loss_text: 15.166|tagging_loss_image: 11.140|tagging_loss_fusion: 14.870|total_loss: 67.644 | 70.68 Examples/sec\n",
      "INFO:tensorflow:training step 947 | tagging_loss_video: 11.150|tagging_loss_audio: 14.050|tagging_loss_text: 12.772|tagging_loss_image: 11.065|tagging_loss_fusion: 15.204|total_loss: 64.241 | 63.72 Examples/sec\n",
      "INFO:tensorflow:training step 948 | tagging_loss_video: 11.276|tagging_loss_audio: 13.762|tagging_loss_text: 13.336|tagging_loss_image: 11.388|tagging_loss_fusion: 12.949|total_loss: 62.711 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 949 | tagging_loss_video: 10.179|tagging_loss_audio: 11.235|tagging_loss_text: 15.783|tagging_loss_image: 10.926|tagging_loss_fusion: 12.193|total_loss: 60.316 | 68.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 950 |tagging_loss_video: 10.856|tagging_loss_audio: 13.450|tagging_loss_text: 15.773|tagging_loss_image: 12.590|tagging_loss_fusion: 14.508|total_loss: 67.176 | Examples/sec: 68.75\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.55 | precision@0.5: 0.80 |recall@0.1: 0.94 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 951 | tagging_loss_video: 10.699|tagging_loss_audio: 15.414|tagging_loss_text: 16.402|tagging_loss_image: 13.598|tagging_loss_fusion: 14.120|total_loss: 70.233 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 952 | tagging_loss_video: 11.067|tagging_loss_audio: 12.666|tagging_loss_text: 16.698|tagging_loss_image: 12.069|tagging_loss_fusion: 16.060|total_loss: 68.560 | 61.19 Examples/sec\n",
      "INFO:tensorflow:training step 953 | tagging_loss_video: 10.633|tagging_loss_audio: 12.806|tagging_loss_text: 14.826|tagging_loss_image: 11.667|tagging_loss_fusion: 14.373|total_loss: 64.305 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 954 | tagging_loss_video: 11.467|tagging_loss_audio: 11.420|tagging_loss_text: 14.613|tagging_loss_image: 12.451|tagging_loss_fusion: 14.586|total_loss: 64.537 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 955 | tagging_loss_video: 9.545|tagging_loss_audio: 11.867|tagging_loss_text: 12.617|tagging_loss_image: 10.155|tagging_loss_fusion: 10.914|total_loss: 55.098 | 66.93 Examples/sec\n",
      "INFO:tensorflow:training step 956 | tagging_loss_video: 9.754|tagging_loss_audio: 12.152|tagging_loss_text: 11.340|tagging_loss_image: 11.811|tagging_loss_fusion: 11.299|total_loss: 56.355 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 957 | tagging_loss_video: 10.496|tagging_loss_audio: 12.762|tagging_loss_text: 12.781|tagging_loss_image: 12.431|tagging_loss_fusion: 12.687|total_loss: 61.157 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 958 | tagging_loss_video: 9.733|tagging_loss_audio: 13.341|tagging_loss_text: 13.549|tagging_loss_image: 10.305|tagging_loss_fusion: 12.290|total_loss: 59.218 | 61.61 Examples/sec\n",
      "INFO:tensorflow:training step 959 | tagging_loss_video: 10.232|tagging_loss_audio: 13.893|tagging_loss_text: 11.967|tagging_loss_image: 12.796|tagging_loss_fusion: 14.102|total_loss: 62.990 | 70.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 960 |tagging_loss_video: 10.941|tagging_loss_audio: 12.356|tagging_loss_text: 14.535|tagging_loss_image: 12.871|tagging_loss_fusion: 12.198|total_loss: 62.901 | Examples/sec: 68.46\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.51 | precision@0.5: 0.83 |recall@0.1: 0.94 | recall@0.5: 0.73\n",
      "INFO:tensorflow:Recording summary at step 960.\n",
      "INFO:tensorflow:training step 961 | tagging_loss_video: 10.010|tagging_loss_audio: 10.431|tagging_loss_text: 15.442|tagging_loss_image: 9.945|tagging_loss_fusion: 11.392|total_loss: 57.221 | 53.30 Examples/sec\n",
      "INFO:tensorflow:training step 962 | tagging_loss_video: 12.459|tagging_loss_audio: 13.222|tagging_loss_text: 14.168|tagging_loss_image: 12.643|tagging_loss_fusion: 14.376|total_loss: 66.867 | 58.36 Examples/sec\n",
      "INFO:tensorflow:training step 963 | tagging_loss_video: 9.953|tagging_loss_audio: 10.715|tagging_loss_text: 16.554|tagging_loss_image: 12.153|tagging_loss_fusion: 11.672|total_loss: 61.047 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 964 | tagging_loss_video: 8.710|tagging_loss_audio: 13.119|tagging_loss_text: 14.435|tagging_loss_image: 11.589|tagging_loss_fusion: 13.474|total_loss: 61.327 | 70.48 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.09984\n",
      "INFO:tensorflow:training step 965 | tagging_loss_video: 8.997|tagging_loss_audio: 11.714|tagging_loss_text: 13.627|tagging_loss_image: 9.194|tagging_loss_fusion: 11.008|total_loss: 54.540 | 59.63 Examples/sec\n",
      "INFO:tensorflow:training step 966 | tagging_loss_video: 11.380|tagging_loss_audio: 11.491|tagging_loss_text: 14.658|tagging_loss_image: 12.319|tagging_loss_fusion: 13.040|total_loss: 62.888 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 967 | tagging_loss_video: 10.689|tagging_loss_audio: 15.203|tagging_loss_text: 16.930|tagging_loss_image: 12.350|tagging_loss_fusion: 13.937|total_loss: 69.110 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 968 | tagging_loss_video: 10.535|tagging_loss_audio: 13.333|tagging_loss_text: 13.415|tagging_loss_image: 10.724|tagging_loss_fusion: 12.167|total_loss: 60.175 | 61.30 Examples/sec\n",
      "INFO:tensorflow:training step 969 | tagging_loss_video: 11.259|tagging_loss_audio: 12.632|tagging_loss_text: 17.464|tagging_loss_image: 11.935|tagging_loss_fusion: 12.960|total_loss: 66.250 | 68.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 970 |tagging_loss_video: 9.231|tagging_loss_audio: 12.730|tagging_loss_text: 15.344|tagging_loss_image: 12.777|tagging_loss_fusion: 13.012|total_loss: 63.094 | Examples/sec: 71.54\n",
      "INFO:tensorflow:GAP: 0.79 | precision@0.1: 0.48 | precision@0.5: 0.84 |recall@0.1: 0.92 | recall@0.5: 0.72\n",
      "INFO:tensorflow:training step 971 | tagging_loss_video: 10.498|tagging_loss_audio: 12.852|tagging_loss_text: 14.420|tagging_loss_image: 11.525|tagging_loss_fusion: 12.911|total_loss: 62.205 | 67.37 Examples/sec\n",
      "INFO:tensorflow:training step 972 | tagging_loss_video: 8.951|tagging_loss_audio: 11.814|tagging_loss_text: 15.068|tagging_loss_image: 10.029|tagging_loss_fusion: 11.335|total_loss: 57.197 | 65.89 Examples/sec\n",
      "INFO:tensorflow:training step 973 | tagging_loss_video: 10.083|tagging_loss_audio: 10.995|tagging_loss_text: 14.590|tagging_loss_image: 11.905|tagging_loss_fusion: 11.966|total_loss: 59.539 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 974 | tagging_loss_video: 10.064|tagging_loss_audio: 13.258|tagging_loss_text: 14.686|tagging_loss_image: 11.570|tagging_loss_fusion: 14.118|total_loss: 63.696 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 975 | tagging_loss_video: 10.299|tagging_loss_audio: 14.291|tagging_loss_text: 16.133|tagging_loss_image: 10.533|tagging_loss_fusion: 12.522|total_loss: 63.776 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 976 | tagging_loss_video: 9.991|tagging_loss_audio: 14.997|tagging_loss_text: 15.340|tagging_loss_image: 11.998|tagging_loss_fusion: 12.551|total_loss: 64.879 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 977 | tagging_loss_video: 12.196|tagging_loss_audio: 14.001|tagging_loss_text: 19.836|tagging_loss_image: 12.591|tagging_loss_fusion: 12.308|total_loss: 70.932 | 62.32 Examples/sec\n",
      "INFO:tensorflow:training step 978 | tagging_loss_video: 9.258|tagging_loss_audio: 11.189|tagging_loss_text: 16.532|tagging_loss_image: 10.205|tagging_loss_fusion: 13.630|total_loss: 60.814 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 979 | tagging_loss_video: 10.757|tagging_loss_audio: 14.217|tagging_loss_text: 15.822|tagging_loss_image: 11.656|tagging_loss_fusion: 14.326|total_loss: 66.778 | 69.95 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 980 |tagging_loss_video: 9.907|tagging_loss_audio: 11.231|tagging_loss_text: 15.311|tagging_loss_image: 10.531|tagging_loss_fusion: 11.900|total_loss: 58.880 | Examples/sec: 65.06\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.54 | precision@0.5: 0.83 |recall@0.1: 0.91 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 981 | tagging_loss_video: 8.718|tagging_loss_audio: 9.852|tagging_loss_text: 11.975|tagging_loss_image: 10.437|tagging_loss_fusion: 10.461|total_loss: 51.443 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 982 | tagging_loss_video: 9.245|tagging_loss_audio: 12.674|tagging_loss_text: 17.329|tagging_loss_image: 10.179|tagging_loss_fusion: 13.089|total_loss: 62.517 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 983 | tagging_loss_video: 9.742|tagging_loss_audio: 11.072|tagging_loss_text: 17.423|tagging_loss_image: 11.609|tagging_loss_fusion: 12.174|total_loss: 62.020 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 984 | tagging_loss_video: 10.995|tagging_loss_audio: 13.691|tagging_loss_text: 15.226|tagging_loss_image: 11.844|tagging_loss_fusion: 12.348|total_loss: 64.103 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 985 | tagging_loss_video: 9.400|tagging_loss_audio: 10.817|tagging_loss_text: 14.612|tagging_loss_image: 10.533|tagging_loss_fusion: 12.790|total_loss: 58.152 | 71.80 Examples/sec\n",
      "INFO:tensorflow:training step 986 | tagging_loss_video: 9.937|tagging_loss_audio: 11.495|tagging_loss_text: 13.551|tagging_loss_image: 10.574|tagging_loss_fusion: 11.338|total_loss: 56.895 | 66.99 Examples/sec\n",
      "INFO:tensorflow:training step 987 | tagging_loss_video: 13.880|tagging_loss_audio: 15.654|tagging_loss_text: 16.807|tagging_loss_image: 13.578|tagging_loss_fusion: 14.307|total_loss: 74.226 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 988 | tagging_loss_video: 10.758|tagging_loss_audio: 11.023|tagging_loss_text: 15.640|tagging_loss_image: 12.668|tagging_loss_fusion: 12.601|total_loss: 62.691 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 989 | tagging_loss_video: 11.155|tagging_loss_audio: 14.183|tagging_loss_text: 14.809|tagging_loss_image: 11.375|tagging_loss_fusion: 15.289|total_loss: 66.812 | 71.11 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 990 |tagging_loss_video: 10.022|tagging_loss_audio: 11.916|tagging_loss_text: 13.395|tagging_loss_image: 11.979|tagging_loss_fusion: 16.509|total_loss: 63.822 | Examples/sec: 68.31\n",
      "INFO:tensorflow:GAP: 0.76 | precision@0.1: 0.58 | precision@0.5: 0.83 |recall@0.1: 0.89 | recall@0.5: 0.61\n",
      "INFO:tensorflow:training step 991 | tagging_loss_video: 10.911|tagging_loss_audio: 14.370|tagging_loss_text: 14.490|tagging_loss_image: 11.459|tagging_loss_fusion: 12.086|total_loss: 63.316 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 992 | tagging_loss_video: 10.250|tagging_loss_audio: 11.746|tagging_loss_text: 15.348|tagging_loss_image: 11.042|tagging_loss_fusion: 11.937|total_loss: 60.322 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 993 | tagging_loss_video: 8.569|tagging_loss_audio: 11.824|tagging_loss_text: 13.192|tagging_loss_image: 9.447|tagging_loss_fusion: 10.810|total_loss: 53.842 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 994 | tagging_loss_video: 8.787|tagging_loss_audio: 10.723|tagging_loss_text: 14.290|tagging_loss_image: 9.084|tagging_loss_fusion: 14.233|total_loss: 57.117 | 63.98 Examples/sec\n",
      "INFO:tensorflow:training step 995 | tagging_loss_video: 10.997|tagging_loss_audio: 11.307|tagging_loss_text: 16.462|tagging_loss_image: 11.036|tagging_loss_fusion: 11.605|total_loss: 61.406 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 996 | tagging_loss_video: 8.566|tagging_loss_audio: 13.432|tagging_loss_text: 15.577|tagging_loss_image: 9.260|tagging_loss_fusion: 11.134|total_loss: 57.970 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 997 | tagging_loss_video: 10.471|tagging_loss_audio: 13.462|tagging_loss_text: 16.842|tagging_loss_image: 11.738|tagging_loss_fusion: 14.360|total_loss: 66.873 | 63.94 Examples/sec\n",
      "INFO:tensorflow:training step 998 | tagging_loss_video: 10.220|tagging_loss_audio: 11.814|tagging_loss_text: 14.306|tagging_loss_image: 11.030|tagging_loss_fusion: 11.919|total_loss: 59.288 | 66.75 Examples/sec\n",
      "INFO:tensorflow:training step 999 | tagging_loss_video: 11.072|tagging_loss_audio: 12.982|tagging_loss_text: 14.921|tagging_loss_image: 12.267|tagging_loss_fusion: 13.170|total_loss: 64.413 | 69.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1000 |tagging_loss_video: 10.380|tagging_loss_audio: 13.597|tagging_loss_text: 18.063|tagging_loss_image: 12.365|tagging_loss_fusion: 12.822|total_loss: 67.228 | Examples/sec: 70.88\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.54 | precision@0.5: 0.80 |recall@0.1: 0.93 | recall@0.5: 0.73\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.670|loss: 21.347|GAP: 0.643|examples_per_second: 7.017\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.684|loss: 20.291|GAP: 0.704|examples_per_second: 95.741\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 1.000|perr: 0.639|loss: 26.650|GAP: 0.619|examples_per_second: 95.693\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.746|loss: 18.893|GAP: 0.721|examples_per_second: 98.649\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.725|loss: 18.690|GAP: 0.717|examples_per_second: 95.938\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.678|loss: 20.904|GAP: 0.668|examples_per_second: 98.765\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.675|loss: 23.948|GAP: 0.648|examples_per_second: 93.696\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 0.969|perr: 0.732|loss: 19.962|GAP: 0.726|examples_per_second: 100.471\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 0.938|perr: 0.682|loss: 20.208|GAP: 0.698|examples_per_second: 90.501\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 0.938|perr: 0.643|loss: 21.992|GAP: 0.655|examples_per_second: 98.400\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.712|loss: 19.091|GAP: 0.717|examples_per_second: 100.464\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 1.000|perr: 0.746|loss: 19.847|GAP: 0.718|examples_per_second: 98.192\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.692|loss: 20.099|GAP: 0.701|examples_per_second: 94.572\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 0.969|perr: 0.704|loss: 20.688|GAP: 0.688|examples_per_second: 102.071\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.704|loss: 19.771|GAP: 0.692|examples_per_second: 95.506\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 1000 | MAP: 0.196 | GAP: 0.642 | p@0.1: 0.439 | p@0.5:0.770 | r@0.1:0.841 | r@0.5: 0.558 | Avg_Loss: 20.515223\n",
      "INFO:tensorflow:epoch/eval number 1000 | MAP: 0.202 | GAP: 0.647 | p@0.1: 0.374 | p@0.5:0.794 | r@0.1:0.901 | r@0.5: 0.497 | Avg_Loss: 19.731115\n",
      "INFO:tensorflow:epoch/eval number 1000 | MAP: 0.110 | GAP: 0.572 | p@0.1: 0.397 | p@0.5:0.784 | r@0.1:0.807 | r@0.5: 0.406 | Avg_Loss: 22.915301\n",
      "INFO:tensorflow:epoch/eval number 1000 | MAP: 0.255 | GAP: 0.654 | p@0.1: 0.626 | p@0.5:0.793 | r@0.1:0.721 | r@0.5: 0.552 | Avg_Loss: 27.817586\n",
      "INFO:tensorflow:epoch/eval number 1000 | MAP: 0.267 | GAP: 0.687 | p@0.1: 0.582 | p@0.5:0.811 | r@0.1:0.790 | r@0.5: 0.578 | Avg_Loss: 20.825410\n",
      "INFO:tensorflow:validation score on val799 is : 0.6873\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-1000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_1000_0.6873/saved_model.pb\n",
      "INFO:tensorflow:training step 1001 | tagging_loss_video: 11.218|tagging_loss_audio: 14.380|tagging_loss_text: 13.189|tagging_loss_image: 12.265|tagging_loss_fusion: 15.120|total_loss: 66.173 | 71.64 Examples/sec\n",
      "INFO:tensorflow:training step 1002 | tagging_loss_video: 10.877|tagging_loss_audio: 15.211|tagging_loss_text: 14.616|tagging_loss_image: 14.542|tagging_loss_fusion: 14.371|total_loss: 69.615 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 1003 | tagging_loss_video: 11.290|tagging_loss_audio: 16.322|tagging_loss_text: 16.724|tagging_loss_image: 11.279|tagging_loss_fusion: 12.299|total_loss: 67.913 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 1004 | tagging_loss_video: 11.690|tagging_loss_audio: 12.684|tagging_loss_text: 17.540|tagging_loss_image: 10.996|tagging_loss_fusion: 12.742|total_loss: 65.654 | 67.67 Examples/sec\n",
      "INFO:tensorflow:training step 1005 | tagging_loss_video: 11.570|tagging_loss_audio: 16.678|tagging_loss_text: 18.467|tagging_loss_image: 15.009|tagging_loss_fusion: 15.867|total_loss: 77.592 | 72.44 Examples/sec\n",
      "INFO:tensorflow:training step 1006 | tagging_loss_video: 9.037|tagging_loss_audio: 13.588|tagging_loss_text: 15.266|tagging_loss_image: 10.098|tagging_loss_fusion: 14.237|total_loss: 62.225 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 1007 | tagging_loss_video: 10.530|tagging_loss_audio: 10.529|tagging_loss_text: 14.183|tagging_loss_image: 11.455|tagging_loss_fusion: 11.968|total_loss: 58.665 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 1008 | tagging_loss_video: 10.010|tagging_loss_audio: 14.687|tagging_loss_text: 16.907|tagging_loss_image: 12.092|tagging_loss_fusion: 12.429|total_loss: 66.124 | 67.16 Examples/sec\n",
      "INFO:tensorflow:training step 1009 | tagging_loss_video: 10.575|tagging_loss_audio: 11.908|tagging_loss_text: 13.726|tagging_loss_image: 10.771|tagging_loss_fusion: 12.943|total_loss: 59.922 | 72.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1010 |tagging_loss_video: 8.653|tagging_loss_audio: 13.760|tagging_loss_text: 16.760|tagging_loss_image: 13.074|tagging_loss_fusion: 14.003|total_loss: 66.250 | Examples/sec: 67.61\n",
      "INFO:tensorflow:GAP: 0.80 | precision@0.1: 0.52 | precision@0.5: 0.80 |recall@0.1: 0.92 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 1011 | tagging_loss_video: 10.547|tagging_loss_audio: 14.892|tagging_loss_text: 16.350|tagging_loss_image: 12.206|tagging_loss_fusion: 14.420|total_loss: 68.416 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 1012 | tagging_loss_video: 11.844|tagging_loss_audio: 13.232|tagging_loss_text: 12.537|tagging_loss_image: 12.365|tagging_loss_fusion: 13.498|total_loss: 63.476 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 1013 | tagging_loss_video: 9.462|tagging_loss_audio: 11.204|tagging_loss_text: 15.148|tagging_loss_image: 12.729|tagging_loss_fusion: 10.205|total_loss: 58.747 | 71.44 Examples/sec\n",
      "INFO:tensorflow:training step 1014 | tagging_loss_video: 11.020|tagging_loss_audio: 15.737|tagging_loss_text: 14.365|tagging_loss_image: 12.106|tagging_loss_fusion: 11.362|total_loss: 64.590 | 67.16 Examples/sec\n",
      "INFO:tensorflow:training step 1015 | tagging_loss_video: 9.558|tagging_loss_audio: 11.482|tagging_loss_text: 16.191|tagging_loss_image: 10.781|tagging_loss_fusion: 10.111|total_loss: 58.123 | 68.00 Examples/sec\n",
      "INFO:tensorflow:training step 1016 | tagging_loss_video: 10.777|tagging_loss_audio: 13.336|tagging_loss_text: 15.451|tagging_loss_image: 11.590|tagging_loss_fusion: 13.889|total_loss: 65.042 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 1017 | tagging_loss_video: 11.512|tagging_loss_audio: 11.924|tagging_loss_text: 16.973|tagging_loss_image: 11.989|tagging_loss_fusion: 12.593|total_loss: 64.992 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 1018 | tagging_loss_video: 9.888|tagging_loss_audio: 13.322|tagging_loss_text: 14.570|tagging_loss_image: 10.630|tagging_loss_fusion: 11.762|total_loss: 60.172 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 1019 | tagging_loss_video: 9.739|tagging_loss_audio: 13.254|tagging_loss_text: 14.720|tagging_loss_image: 9.350|tagging_loss_fusion: 11.526|total_loss: 58.588 | 65.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1020 |tagging_loss_video: 8.711|tagging_loss_audio: 13.748|tagging_loss_text: 14.157|tagging_loss_image: 9.791|tagging_loss_fusion: 10.611|total_loss: 57.018 | Examples/sec: 69.69\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.56 | precision@0.5: 0.86 |recall@0.1: 0.97 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 1021 | tagging_loss_video: 9.710|tagging_loss_audio: 12.320|tagging_loss_text: 12.542|tagging_loss_image: 9.339|tagging_loss_fusion: 10.709|total_loss: 54.620 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 1022 | tagging_loss_video: 8.296|tagging_loss_audio: 11.430|tagging_loss_text: 11.837|tagging_loss_image: 9.951|tagging_loss_fusion: 10.454|total_loss: 51.967 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 1023 | tagging_loss_video: 9.744|tagging_loss_audio: 15.351|tagging_loss_text: 16.244|tagging_loss_image: 10.729|tagging_loss_fusion: 10.518|total_loss: 62.586 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 1024 | tagging_loss_video: 10.831|tagging_loss_audio: 12.112|tagging_loss_text: 12.826|tagging_loss_image: 11.458|tagging_loss_fusion: 12.187|total_loss: 59.414 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 1025 | tagging_loss_video: 9.187|tagging_loss_audio: 11.641|tagging_loss_text: 12.054|tagging_loss_image: 11.162|tagging_loss_fusion: 12.106|total_loss: 56.150 | 72.36 Examples/sec\n",
      "INFO:tensorflow:training step 1026 | tagging_loss_video: 8.034|tagging_loss_audio: 12.570|tagging_loss_text: 12.950|tagging_loss_image: 10.936|tagging_loss_fusion: 13.364|total_loss: 57.854 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 1027 | tagging_loss_video: 9.332|tagging_loss_audio: 13.116|tagging_loss_text: 12.800|tagging_loss_image: 9.351|tagging_loss_fusion: 11.272|total_loss: 55.870 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 1028 | tagging_loss_video: 9.402|tagging_loss_audio: 13.992|tagging_loss_text: 11.328|tagging_loss_image: 10.887|tagging_loss_fusion: 10.770|total_loss: 56.378 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 1029 | tagging_loss_video: 8.926|tagging_loss_audio: 14.090|tagging_loss_text: 14.890|tagging_loss_image: 10.353|tagging_loss_fusion: 10.180|total_loss: 58.439 | 70.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1030 |tagging_loss_video: 9.957|tagging_loss_audio: 13.572|tagging_loss_text: 16.012|tagging_loss_image: 10.876|tagging_loss_fusion: 14.013|total_loss: 64.429 | Examples/sec: 70.61\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.60 | precision@0.5: 0.89 |recall@0.1: 0.92 | recall@0.5: 0.67\n",
      "INFO:tensorflow:training step 1031 | tagging_loss_video: 12.109|tagging_loss_audio: 13.772|tagging_loss_text: 17.509|tagging_loss_image: 14.451|tagging_loss_fusion: 13.191|total_loss: 71.032 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 1032 | tagging_loss_video: 10.971|tagging_loss_audio: 13.618|tagging_loss_text: 14.192|tagging_loss_image: 14.291|tagging_loss_fusion: 13.933|total_loss: 67.006 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 1033 | tagging_loss_video: 10.460|tagging_loss_audio: 11.919|tagging_loss_text: 15.214|tagging_loss_image: 10.495|tagging_loss_fusion: 11.145|total_loss: 59.233 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 1034 | tagging_loss_video: 11.186|tagging_loss_audio: 17.532|tagging_loss_text: 17.788|tagging_loss_image: 11.962|tagging_loss_fusion: 12.753|total_loss: 71.220 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 1035 | tagging_loss_video: 9.194|tagging_loss_audio: 12.937|tagging_loss_text: 12.202|tagging_loss_image: 10.547|tagging_loss_fusion: 12.260|total_loss: 57.140 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 1036 | tagging_loss_video: 10.586|tagging_loss_audio: 13.743|tagging_loss_text: 16.493|tagging_loss_image: 11.156|tagging_loss_fusion: 13.482|total_loss: 65.460 | 71.58 Examples/sec\n",
      "INFO:tensorflow:training step 1037 | tagging_loss_video: 8.683|tagging_loss_audio: 13.359|tagging_loss_text: 12.596|tagging_loss_image: 10.905|tagging_loss_fusion: 12.704|total_loss: 58.246 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 1038 | tagging_loss_video: 10.452|tagging_loss_audio: 15.371|tagging_loss_text: 14.600|tagging_loss_image: 11.116|tagging_loss_fusion: 13.160|total_loss: 64.698 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 1039 | tagging_loss_video: 10.008|tagging_loss_audio: 14.449|tagging_loss_text: 18.108|tagging_loss_image: 11.160|tagging_loss_fusion: 12.717|total_loss: 66.442 | 70.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1040 |tagging_loss_video: 9.500|tagging_loss_audio: 11.664|tagging_loss_text: 13.769|tagging_loss_image: 10.402|tagging_loss_fusion: 11.383|total_loss: 56.718 | Examples/sec: 64.76\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.55 | precision@0.5: 0.80 |recall@0.1: 0.95 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 1041 | tagging_loss_video: 11.061|tagging_loss_audio: 12.316|tagging_loss_text: 16.250|tagging_loss_image: 10.930|tagging_loss_fusion: 10.889|total_loss: 61.446 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 1042 | tagging_loss_video: 10.359|tagging_loss_audio: 14.028|tagging_loss_text: 17.316|tagging_loss_image: 12.252|tagging_loss_fusion: 11.760|total_loss: 65.715 | 71.71 Examples/sec\n",
      "INFO:tensorflow:training step 1043 | tagging_loss_video: 8.566|tagging_loss_audio: 12.765|tagging_loss_text: 16.869|tagging_loss_image: 10.174|tagging_loss_fusion: 10.813|total_loss: 59.187 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 1044 | tagging_loss_video: 11.320|tagging_loss_audio: 14.518|tagging_loss_text: 18.673|tagging_loss_image: 10.404|tagging_loss_fusion: 12.382|total_loss: 67.297 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 1045 | tagging_loss_video: 9.564|tagging_loss_audio: 13.736|tagging_loss_text: 16.140|tagging_loss_image: 11.467|tagging_loss_fusion: 10.530|total_loss: 61.437 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 1046 | tagging_loss_video: 10.146|tagging_loss_audio: 14.277|tagging_loss_text: 15.114|tagging_loss_image: 9.803|tagging_loss_fusion: 12.257|total_loss: 61.598 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 1047 | tagging_loss_video: 10.671|tagging_loss_audio: 12.400|tagging_loss_text: 18.455|tagging_loss_image: 13.446|tagging_loss_fusion: 13.240|total_loss: 68.211 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 1048 | tagging_loss_video: 8.960|tagging_loss_audio: 13.332|tagging_loss_text: 17.812|tagging_loss_image: 9.934|tagging_loss_fusion: 12.845|total_loss: 62.884 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 1049 | tagging_loss_video: 10.475|tagging_loss_audio: 11.774|tagging_loss_text: 15.965|tagging_loss_image: 11.025|tagging_loss_fusion: 13.619|total_loss: 62.859 | 71.45 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1050 |tagging_loss_video: 9.238|tagging_loss_audio: 13.679|tagging_loss_text: 13.976|tagging_loss_image: 9.791|tagging_loss_fusion: 10.580|total_loss: 57.265 | Examples/sec: 68.69\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.62 | precision@0.5: 0.84 |recall@0.1: 0.95 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1051 | tagging_loss_video: 10.204|tagging_loss_audio: 11.053|tagging_loss_text: 13.791|tagging_loss_image: 12.192|tagging_loss_fusion: 13.486|total_loss: 60.726 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 1052 | tagging_loss_video: 9.077|tagging_loss_audio: 12.542|tagging_loss_text: 13.512|tagging_loss_image: 11.262|tagging_loss_fusion: 11.746|total_loss: 58.138 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 1053 | tagging_loss_video: 8.797|tagging_loss_audio: 13.128|tagging_loss_text: 15.765|tagging_loss_image: 11.229|tagging_loss_fusion: 12.838|total_loss: 61.756 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 1054 | tagging_loss_video: 10.221|tagging_loss_audio: 10.431|tagging_loss_text: 17.755|tagging_loss_image: 11.770|tagging_loss_fusion: 13.366|total_loss: 63.543 | 68.34 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1055 | tagging_loss_video: 9.494|tagging_loss_audio: 14.530|tagging_loss_text: 18.866|tagging_loss_image: 12.399|tagging_loss_fusion: 12.662|total_loss: 67.951 | 72.25 Examples/sec\n",
      "INFO:tensorflow:training step 1056 | tagging_loss_video: 9.831|tagging_loss_audio: 15.014|tagging_loss_text: 15.237|tagging_loss_image: 11.186|tagging_loss_fusion: 11.527|total_loss: 62.795 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 1057 | tagging_loss_video: 10.242|tagging_loss_audio: 13.465|tagging_loss_text: 14.730|tagging_loss_image: 10.641|tagging_loss_fusion: 10.298|total_loss: 59.377 | 72.68 Examples/sec\n",
      "INFO:tensorflow:training step 1058 | tagging_loss_video: 10.417|tagging_loss_audio: 13.195|tagging_loss_text: 20.972|tagging_loss_image: 12.039|tagging_loss_fusion: 12.130|total_loss: 68.753 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 1059 | tagging_loss_video: 8.899|tagging_loss_audio: 13.942|tagging_loss_text: 16.011|tagging_loss_image: 11.241|tagging_loss_fusion: 11.220|total_loss: 61.313 | 68.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1060 |tagging_loss_video: 9.802|tagging_loss_audio: 13.195|tagging_loss_text: 17.522|tagging_loss_image: 10.966|tagging_loss_fusion: 12.631|total_loss: 64.117 | Examples/sec: 71.11\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.57 | precision@0.5: 0.86 |recall@0.1: 0.92 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 1061 | tagging_loss_video: 9.540|tagging_loss_audio: 14.905|tagging_loss_text: 16.282|tagging_loss_image: 12.790|tagging_loss_fusion: 13.291|total_loss: 66.808 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 1062 | tagging_loss_video: 10.259|tagging_loss_audio: 10.859|tagging_loss_text: 14.715|tagging_loss_image: 11.043|tagging_loss_fusion: 11.961|total_loss: 58.836 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 1063 | tagging_loss_video: 9.825|tagging_loss_audio: 14.798|tagging_loss_text: 16.956|tagging_loss_image: 11.660|tagging_loss_fusion: 13.733|total_loss: 66.972 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 1064 | tagging_loss_video: 10.362|tagging_loss_audio: 14.887|tagging_loss_text: 17.483|tagging_loss_image: 11.195|tagging_loss_fusion: 13.974|total_loss: 67.902 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 1065 | tagging_loss_video: 9.602|tagging_loss_audio: 13.026|tagging_loss_text: 15.925|tagging_loss_image: 10.913|tagging_loss_fusion: 12.312|total_loss: 61.777 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 1066 | tagging_loss_video: 8.640|tagging_loss_audio: 14.037|tagging_loss_text: 18.000|tagging_loss_image: 10.935|tagging_loss_fusion: 12.249|total_loss: 63.861 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 1067 | tagging_loss_video: 9.767|tagging_loss_audio: 13.987|tagging_loss_text: 16.192|tagging_loss_image: 9.386|tagging_loss_fusion: 10.840|total_loss: 60.172 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 1068 | tagging_loss_video: 10.907|tagging_loss_audio: 12.724|tagging_loss_text: 16.971|tagging_loss_image: 11.766|tagging_loss_fusion: 13.220|total_loss: 65.588 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 1069 | tagging_loss_video: 7.601|tagging_loss_audio: 11.244|tagging_loss_text: 16.207|tagging_loss_image: 11.739|tagging_loss_fusion: 13.921|total_loss: 60.712 | 67.95 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1070 |tagging_loss_video: 9.962|tagging_loss_audio: 15.193|tagging_loss_text: 14.594|tagging_loss_image: 9.906|tagging_loss_fusion: 12.498|total_loss: 62.152 | Examples/sec: 72.20\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.59 | precision@0.5: 0.87 |recall@0.1: 0.92 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 1071 | tagging_loss_video: 10.410|tagging_loss_audio: 11.779|tagging_loss_text: 16.187|tagging_loss_image: 10.888|tagging_loss_fusion: 12.052|total_loss: 61.317 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 1072 | tagging_loss_video: 10.313|tagging_loss_audio: 14.065|tagging_loss_text: 12.659|tagging_loss_image: 12.095|tagging_loss_fusion: 11.698|total_loss: 60.830 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 1073 | tagging_loss_video: 8.771|tagging_loss_audio: 14.104|tagging_loss_text: 12.456|tagging_loss_image: 10.577|tagging_loss_fusion: 12.303|total_loss: 58.211 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 1074 | tagging_loss_video: 10.626|tagging_loss_audio: 12.563|tagging_loss_text: 13.692|tagging_loss_image: 10.276|tagging_loss_fusion: 13.084|total_loss: 60.242 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 1075 | tagging_loss_video: 11.719|tagging_loss_audio: 16.159|tagging_loss_text: 17.172|tagging_loss_image: 13.856|tagging_loss_fusion: 13.135|total_loss: 72.041 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 1076 | tagging_loss_video: 9.956|tagging_loss_audio: 15.077|tagging_loss_text: 16.728|tagging_loss_image: 12.382|tagging_loss_fusion: 16.721|total_loss: 70.864 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 1077 | tagging_loss_video: 9.750|tagging_loss_audio: 8.131|tagging_loss_text: 14.831|tagging_loss_image: 12.345|tagging_loss_fusion: 11.556|total_loss: 56.613 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 1078 | tagging_loss_video: 10.459|tagging_loss_audio: 15.610|tagging_loss_text: 12.263|tagging_loss_image: 11.536|tagging_loss_fusion: 13.360|total_loss: 63.229 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 1079 | tagging_loss_video: 9.911|tagging_loss_audio: 12.025|tagging_loss_text: 13.952|tagging_loss_image: 10.711|tagging_loss_fusion: 10.963|total_loss: 57.561 | 68.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1080 |tagging_loss_video: 8.601|tagging_loss_audio: 11.421|tagging_loss_text: 13.895|tagging_loss_image: 9.751|tagging_loss_fusion: 12.160|total_loss: 55.829 | Examples/sec: 72.27\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.55 | precision@0.5: 0.83 |recall@0.1: 0.94 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 1081 | tagging_loss_video: 8.962|tagging_loss_audio: 11.150|tagging_loss_text: 15.587|tagging_loss_image: 10.378|tagging_loss_fusion: 12.333|total_loss: 58.409 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 1082 | tagging_loss_video: 8.868|tagging_loss_audio: 12.686|tagging_loss_text: 15.475|tagging_loss_image: 7.693|tagging_loss_fusion: 12.180|total_loss: 56.902 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 1083 | tagging_loss_video: 10.215|tagging_loss_audio: 12.863|tagging_loss_text: 14.131|tagging_loss_image: 11.093|tagging_loss_fusion: 13.431|total_loss: 61.733 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 1084 | tagging_loss_video: 10.575|tagging_loss_audio: 12.905|tagging_loss_text: 15.193|tagging_loss_image: 10.340|tagging_loss_fusion: 12.210|total_loss: 61.224 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 1085 | tagging_loss_video: 10.237|tagging_loss_audio: 13.401|tagging_loss_text: 14.650|tagging_loss_image: 10.974|tagging_loss_fusion: 13.093|total_loss: 62.355 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 1086 | tagging_loss_video: 9.187|tagging_loss_audio: 12.298|tagging_loss_text: 14.734|tagging_loss_image: 10.340|tagging_loss_fusion: 10.559|total_loss: 57.118 | 72.39 Examples/sec\n",
      "INFO:tensorflow:training step 1087 | tagging_loss_video: 10.006|tagging_loss_audio: 15.153|tagging_loss_text: 17.671|tagging_loss_image: 12.241|tagging_loss_fusion: 14.952|total_loss: 70.022 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 1088 | tagging_loss_video: 8.554|tagging_loss_audio: 12.822|tagging_loss_text: 13.197|tagging_loss_image: 10.535|tagging_loss_fusion: 9.904|total_loss: 55.012 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 1089 | tagging_loss_video: 9.621|tagging_loss_audio: 11.608|tagging_loss_text: 14.958|tagging_loss_image: 9.806|tagging_loss_fusion: 10.606|total_loss: 56.599 | 69.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1090 |tagging_loss_video: 8.249|tagging_loss_audio: 14.029|tagging_loss_text: 12.348|tagging_loss_image: 10.106|tagging_loss_fusion: 11.455|total_loss: 56.186 | Examples/sec: 70.21\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.58 | precision@0.5: 0.86 |recall@0.1: 0.94 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1091 | tagging_loss_video: 10.742|tagging_loss_audio: 13.037|tagging_loss_text: 16.781|tagging_loss_image: 10.101|tagging_loss_fusion: 13.111|total_loss: 63.772 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 1092 | tagging_loss_video: 10.910|tagging_loss_audio: 15.411|tagging_loss_text: 19.185|tagging_loss_image: 12.242|tagging_loss_fusion: 11.319|total_loss: 69.066 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 1093 | tagging_loss_video: 9.201|tagging_loss_audio: 12.748|tagging_loss_text: 17.509|tagging_loss_image: 10.331|tagging_loss_fusion: 10.418|total_loss: 60.207 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 1094 | tagging_loss_video: 9.539|tagging_loss_audio: 13.682|tagging_loss_text: 16.879|tagging_loss_image: 10.661|tagging_loss_fusion: 13.340|total_loss: 64.102 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 1095 | tagging_loss_video: 9.623|tagging_loss_audio: 12.559|tagging_loss_text: 15.838|tagging_loss_image: 12.121|tagging_loss_fusion: 10.912|total_loss: 61.053 | 67.32 Examples/sec\n",
      "INFO:tensorflow:training step 1096 | tagging_loss_video: 10.561|tagging_loss_audio: 11.519|tagging_loss_text: 14.970|tagging_loss_image: 9.369|tagging_loss_fusion: 11.769|total_loss: 58.188 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 1097 | tagging_loss_video: 8.871|tagging_loss_audio: 11.752|tagging_loss_text: 15.733|tagging_loss_image: 9.718|tagging_loss_fusion: 11.010|total_loss: 57.084 | 68.17 Examples/sec\n",
      "INFO:tensorflow:training step 1098 | tagging_loss_video: 9.768|tagging_loss_audio: 11.937|tagging_loss_text: 14.574|tagging_loss_image: 11.522|tagging_loss_fusion: 10.776|total_loss: 58.577 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 1099 | tagging_loss_video: 9.869|tagging_loss_audio: 12.614|tagging_loss_text: 15.377|tagging_loss_image: 10.034|tagging_loss_fusion: 13.378|total_loss: 61.271 | 69.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1100 |tagging_loss_video: 10.541|tagging_loss_audio: 11.844|tagging_loss_text: 14.795|tagging_loss_image: 11.762|tagging_loss_fusion: 10.735|total_loss: 59.677 | Examples/sec: 67.98\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.58 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1101 | tagging_loss_video: 10.047|tagging_loss_audio: 13.792|tagging_loss_text: 14.686|tagging_loss_image: 11.815|tagging_loss_fusion: 13.442|total_loss: 63.782 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 1102 | tagging_loss_video: 9.769|tagging_loss_audio: 11.521|tagging_loss_text: 18.370|tagging_loss_image: 11.915|tagging_loss_fusion: 13.050|total_loss: 64.626 | 60.82 Examples/sec\n",
      "INFO:tensorflow:training step 1103 | tagging_loss_video: 9.258|tagging_loss_audio: 12.879|tagging_loss_text: 17.813|tagging_loss_image: 10.985|tagging_loss_fusion: 12.792|total_loss: 63.728 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 1104 | tagging_loss_video: 11.396|tagging_loss_audio: 13.843|tagging_loss_text: 14.404|tagging_loss_image: 13.558|tagging_loss_fusion: 14.055|total_loss: 67.257 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 1105 | tagging_loss_video: 9.193|tagging_loss_audio: 11.248|tagging_loss_text: 16.003|tagging_loss_image: 10.022|tagging_loss_fusion: 11.543|total_loss: 58.010 | 65.39 Examples/sec\n",
      "INFO:tensorflow:training step 1106 | tagging_loss_video: 8.505|tagging_loss_audio: 10.024|tagging_loss_text: 10.700|tagging_loss_image: 8.463|tagging_loss_fusion: 10.595|total_loss: 48.287 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 1107 | tagging_loss_video: 10.422|tagging_loss_audio: 13.771|tagging_loss_text: 14.020|tagging_loss_image: 10.277|tagging_loss_fusion: 10.933|total_loss: 59.423 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 1108 | tagging_loss_video: 10.165|tagging_loss_audio: 12.506|tagging_loss_text: 12.923|tagging_loss_image: 10.480|tagging_loss_fusion: 10.850|total_loss: 56.924 | 61.05 Examples/sec\n",
      "INFO:tensorflow:training step 1109 | tagging_loss_video: 10.883|tagging_loss_audio: 12.410|tagging_loss_text: 14.529|tagging_loss_image: 10.570|tagging_loss_fusion: 10.772|total_loss: 59.164 | 69.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1110 |tagging_loss_video: 9.436|tagging_loss_audio: 13.645|tagging_loss_text: 15.914|tagging_loss_image: 10.612|tagging_loss_fusion: 11.675|total_loss: 61.282 | Examples/sec: 67.62\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.57 | precision@0.5: 0.85 |recall@0.1: 0.92 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1111 | tagging_loss_video: 9.490|tagging_loss_audio: 10.443|tagging_loss_text: 17.453|tagging_loss_image: 9.387|tagging_loss_fusion: 10.555|total_loss: 57.328 | 66.26 Examples/sec\n",
      "INFO:tensorflow:training step 1112 | tagging_loss_video: 10.249|tagging_loss_audio: 13.435|tagging_loss_text: 16.098|tagging_loss_image: 11.516|tagging_loss_fusion: 13.233|total_loss: 64.531 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 1113 | tagging_loss_video: 9.604|tagging_loss_audio: 11.281|tagging_loss_text: 12.932|tagging_loss_image: 11.690|tagging_loss_fusion: 12.219|total_loss: 57.725 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 1114 | tagging_loss_video: 10.890|tagging_loss_audio: 10.863|tagging_loss_text: 16.209|tagging_loss_image: 12.114|tagging_loss_fusion: 12.903|total_loss: 62.978 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 1115 | tagging_loss_video: 10.227|tagging_loss_audio: 14.146|tagging_loss_text: 16.449|tagging_loss_image: 11.700|tagging_loss_fusion: 12.429|total_loss: 64.951 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 1116 | tagging_loss_video: 9.050|tagging_loss_audio: 11.003|tagging_loss_text: 11.809|tagging_loss_image: 10.195|tagging_loss_fusion: 11.885|total_loss: 53.942 | 61.17 Examples/sec\n",
      "INFO:tensorflow:training step 1117 | tagging_loss_video: 8.158|tagging_loss_audio: 14.592|tagging_loss_text: 15.889|tagging_loss_image: 11.372|tagging_loss_fusion: 12.329|total_loss: 62.340 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 1118 | tagging_loss_video: 8.035|tagging_loss_audio: 12.346|tagging_loss_text: 14.424|tagging_loss_image: 9.909|tagging_loss_fusion: 10.557|total_loss: 55.271 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 1119 | tagging_loss_video: 12.110|tagging_loss_audio: 13.134|tagging_loss_text: 16.947|tagging_loss_image: 11.085|tagging_loss_fusion: 10.113|total_loss: 63.389 | 67.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1120 |tagging_loss_video: 9.414|tagging_loss_audio: 12.498|tagging_loss_text: 20.862|tagging_loss_image: 10.414|tagging_loss_fusion: 12.462|total_loss: 65.649 | Examples/sec: 68.24\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.57 | precision@0.5: 0.83 |recall@0.1: 0.94 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1121 | tagging_loss_video: 8.521|tagging_loss_audio: 13.259|tagging_loss_text: 16.911|tagging_loss_image: 9.063|tagging_loss_fusion: 11.520|total_loss: 59.275 | 71.58 Examples/sec\n",
      "INFO:tensorflow:training step 1122 | tagging_loss_video: 11.548|tagging_loss_audio: 12.769|tagging_loss_text: 18.205|tagging_loss_image: 9.563|tagging_loss_fusion: 13.232|total_loss: 65.318 | 63.74 Examples/sec\n",
      "INFO:tensorflow:training step 1123 | tagging_loss_video: 8.787|tagging_loss_audio: 11.639|tagging_loss_text: 16.812|tagging_loss_image: 10.332|tagging_loss_fusion: 12.071|total_loss: 59.641 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 1124 | tagging_loss_video: 10.110|tagging_loss_audio: 15.104|tagging_loss_text: 14.281|tagging_loss_image: 10.928|tagging_loss_fusion: 13.580|total_loss: 64.003 | 64.71 Examples/sec\n",
      "INFO:tensorflow:training step 1125 | tagging_loss_video: 9.044|tagging_loss_audio: 12.009|tagging_loss_text: 18.777|tagging_loss_image: 12.627|tagging_loss_fusion: 10.657|total_loss: 63.114 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 1126 | tagging_loss_video: 10.698|tagging_loss_audio: 13.755|tagging_loss_text: 14.720|tagging_loss_image: 10.582|tagging_loss_fusion: 14.074|total_loss: 63.828 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 1127 | tagging_loss_video: 11.243|tagging_loss_audio: 12.780|tagging_loss_text: 10.259|tagging_loss_image: 10.677|tagging_loss_fusion: 13.121|total_loss: 58.079 | 66.18 Examples/sec\n",
      "INFO:tensorflow:training step 1128 | tagging_loss_video: 10.084|tagging_loss_audio: 14.384|tagging_loss_text: 16.587|tagging_loss_image: 12.189|tagging_loss_fusion: 15.148|total_loss: 68.392 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 1129 | tagging_loss_video: 8.641|tagging_loss_audio: 14.194|tagging_loss_text: 13.865|tagging_loss_image: 11.867|tagging_loss_fusion: 14.309|total_loss: 62.877 | 71.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1130 |tagging_loss_video: 10.526|tagging_loss_audio: 15.602|tagging_loss_text: 17.270|tagging_loss_image: 11.974|tagging_loss_fusion: 12.563|total_loss: 67.935 | Examples/sec: 61.12\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.54 | precision@0.5: 0.83 |recall@0.1: 0.92 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1131 | tagging_loss_video: 10.276|tagging_loss_audio: 14.049|tagging_loss_text: 19.335|tagging_loss_image: 12.834|tagging_loss_fusion: 15.223|total_loss: 71.717 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 1132 | tagging_loss_video: 9.137|tagging_loss_audio: 11.372|tagging_loss_text: 15.084|tagging_loss_image: 11.957|tagging_loss_fusion: 13.513|total_loss: 61.063 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 1133 | tagging_loss_video: 10.317|tagging_loss_audio: 12.371|tagging_loss_text: 17.180|tagging_loss_image: 11.491|tagging_loss_fusion: 12.419|total_loss: 63.779 | 65.28 Examples/sec\n",
      "INFO:tensorflow:training step 1134 | tagging_loss_video: 10.749|tagging_loss_audio: 14.199|tagging_loss_text: 16.051|tagging_loss_image: 10.377|tagging_loss_fusion: 11.730|total_loss: 63.106 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 1135 | tagging_loss_video: 13.637|tagging_loss_audio: 14.695|tagging_loss_text: 16.022|tagging_loss_image: 10.792|tagging_loss_fusion: 13.671|total_loss: 68.817 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 1136 | tagging_loss_video: 11.570|tagging_loss_audio: 13.727|tagging_loss_text: 19.664|tagging_loss_image: 10.978|tagging_loss_fusion: 13.286|total_loss: 69.225 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 1137 | tagging_loss_video: 11.697|tagging_loss_audio: 13.676|tagging_loss_text: 18.080|tagging_loss_image: 13.834|tagging_loss_fusion: 15.107|total_loss: 72.394 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 1138 | tagging_loss_video: 10.194|tagging_loss_audio: 12.232|tagging_loss_text: 16.557|tagging_loss_image: 10.945|tagging_loss_fusion: 13.703|total_loss: 63.631 | 65.86 Examples/sec\n",
      "INFO:tensorflow:training step 1139 | tagging_loss_video: 10.650|tagging_loss_audio: 13.537|tagging_loss_text: 16.997|tagging_loss_image: 11.495|tagging_loss_fusion: 15.196|total_loss: 67.875 | 70.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1140 |tagging_loss_video: 9.595|tagging_loss_audio: 12.312|tagging_loss_text: 11.114|tagging_loss_image: 9.778|tagging_loss_fusion: 10.457|total_loss: 53.255 | Examples/sec: 69.63\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.55 | precision@0.5: 0.83 |recall@0.1: 0.94 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1141 | tagging_loss_video: 11.116|tagging_loss_audio: 11.846|tagging_loss_text: 14.516|tagging_loss_image: 11.308|tagging_loss_fusion: 12.107|total_loss: 60.892 | 64.53 Examples/sec\n",
      "INFO:tensorflow:training step 1142 | tagging_loss_video: 9.105|tagging_loss_audio: 14.088|tagging_loss_text: 16.715|tagging_loss_image: 12.248|tagging_loss_fusion: 14.569|total_loss: 66.725 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 1143 | tagging_loss_video: 10.336|tagging_loss_audio: 14.882|tagging_loss_text: 17.507|tagging_loss_image: 11.535|tagging_loss_fusion: 11.004|total_loss: 65.263 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 1144 | tagging_loss_video: 9.008|tagging_loss_audio: 13.807|tagging_loss_text: 17.502|tagging_loss_image: 11.516|tagging_loss_fusion: 12.091|total_loss: 63.924 | 60.73 Examples/sec\n",
      "INFO:tensorflow:training step 1145 | tagging_loss_video: 10.585|tagging_loss_audio: 17.941|tagging_loss_text: 13.813|tagging_loss_image: 14.855|tagging_loss_fusion: 13.602|total_loss: 70.796 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 1146 | tagging_loss_video: 9.962|tagging_loss_audio: 12.149|tagging_loss_text: 16.011|tagging_loss_image: 9.891|tagging_loss_fusion: 12.117|total_loss: 60.130 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 1147 | tagging_loss_video: 9.338|tagging_loss_audio: 12.378|tagging_loss_text: 16.132|tagging_loss_image: 9.723|tagging_loss_fusion: 11.108|total_loss: 58.680 | 65.39 Examples/sec\n",
      "INFO:tensorflow:training step 1148 | tagging_loss_video: 10.228|tagging_loss_audio: 11.872|tagging_loss_text: 10.133|tagging_loss_image: 11.835|tagging_loss_fusion: 12.596|total_loss: 56.664 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 1149 | tagging_loss_video: 9.593|tagging_loss_audio: 13.934|tagging_loss_text: 17.212|tagging_loss_image: 10.611|tagging_loss_fusion: 11.184|total_loss: 62.535 | 64.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1150 |tagging_loss_video: 9.417|tagging_loss_audio: 12.289|tagging_loss_text: 18.033|tagging_loss_image: 10.656|tagging_loss_fusion: 11.922|total_loss: 62.318 | Examples/sec: 66.99\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.56 | precision@0.5: 0.85 |recall@0.1: 0.94 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1151 | tagging_loss_video: 10.422|tagging_loss_audio: 14.583|tagging_loss_text: 17.862|tagging_loss_image: 12.172|tagging_loss_fusion: 13.617|total_loss: 68.657 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 1152 | tagging_loss_video: 9.871|tagging_loss_audio: 13.213|tagging_loss_text: 19.213|tagging_loss_image: 10.272|tagging_loss_fusion: 12.305|total_loss: 64.874 | 63.41 Examples/sec\n",
      "INFO:tensorflow:training step 1153 | tagging_loss_video: 8.836|tagging_loss_audio: 12.552|tagging_loss_text: 13.851|tagging_loss_image: 11.234|tagging_loss_fusion: 11.055|total_loss: 57.528 | 71.58 Examples/sec\n",
      "INFO:tensorflow:training step 1154 | tagging_loss_video: 9.462|tagging_loss_audio: 13.721|tagging_loss_text: 16.462|tagging_loss_image: 11.136|tagging_loss_fusion: 11.801|total_loss: 62.583 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 1155 | tagging_loss_video: 8.351|tagging_loss_audio: 10.532|tagging_loss_text: 11.660|tagging_loss_image: 10.466|tagging_loss_fusion: 11.080|total_loss: 52.089 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 1156 | tagging_loss_video: 11.014|tagging_loss_audio: 14.179|tagging_loss_text: 19.118|tagging_loss_image: 10.622|tagging_loss_fusion: 12.896|total_loss: 67.829 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 1157 | tagging_loss_video: 10.246|tagging_loss_audio: 12.175|tagging_loss_text: 14.351|tagging_loss_image: 9.807|tagging_loss_fusion: 12.522|total_loss: 59.101 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 1158 | tagging_loss_video: 9.620|tagging_loss_audio: 10.197|tagging_loss_text: 13.263|tagging_loss_image: 10.168|tagging_loss_fusion: 10.379|total_loss: 53.628 | 61.95 Examples/sec\n",
      "INFO:tensorflow:training step 1159 | tagging_loss_video: 9.082|tagging_loss_audio: 12.988|tagging_loss_text: 18.122|tagging_loss_image: 10.347|tagging_loss_fusion: 10.454|total_loss: 60.993 | 69.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1160 |tagging_loss_video: 9.006|tagging_loss_audio: 13.724|tagging_loss_text: 17.640|tagging_loss_image: 9.723|tagging_loss_fusion: 11.198|total_loss: 61.293 | Examples/sec: 71.53\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.58 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.75\n",
      "INFO:tensorflow:Recording summary at step 1160.\n",
      "INFO:tensorflow:training step 1161 | tagging_loss_video: 7.500|tagging_loss_audio: 11.523|tagging_loss_text: 13.296|tagging_loss_image: 9.578|tagging_loss_fusion: 10.242|total_loss: 52.140 | 51.28 Examples/sec\n",
      "INFO:tensorflow:training step 1162 | tagging_loss_video: 8.799|tagging_loss_audio: 10.247|tagging_loss_text: 17.633|tagging_loss_image: 10.648|tagging_loss_fusion: 10.294|total_loss: 57.620 | 59.49 Examples/sec\n",
      "INFO:tensorflow:training step 1163 | tagging_loss_video: 9.390|tagging_loss_audio: 12.931|tagging_loss_text: 15.378|tagging_loss_image: 9.493|tagging_loss_fusion: 10.467|total_loss: 57.659 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 1164 | tagging_loss_video: 8.910|tagging_loss_audio: 13.240|tagging_loss_text: 11.954|tagging_loss_image: 13.016|tagging_loss_fusion: 13.462|total_loss: 60.581 | 67.03 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 1.66622\n",
      "INFO:tensorflow:training step 1165 | tagging_loss_video: 9.134|tagging_loss_audio: 13.401|tagging_loss_text: 11.657|tagging_loss_image: 10.273|tagging_loss_fusion: 9.781|total_loss: 54.246 | 62.28 Examples/sec\n",
      "INFO:tensorflow:training step 1166 | tagging_loss_video: 9.297|tagging_loss_audio: 12.788|tagging_loss_text: 10.786|tagging_loss_image: 10.496|tagging_loss_fusion: 10.431|total_loss: 53.799 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 1167 | tagging_loss_video: 9.089|tagging_loss_audio: 12.896|tagging_loss_text: 14.577|tagging_loss_image: 10.792|tagging_loss_fusion: 9.646|total_loss: 57.001 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 1168 | tagging_loss_video: 9.044|tagging_loss_audio: 13.439|tagging_loss_text: 13.520|tagging_loss_image: 11.145|tagging_loss_fusion: 10.834|total_loss: 57.982 | 63.37 Examples/sec\n",
      "INFO:tensorflow:training step 1169 | tagging_loss_video: 10.411|tagging_loss_audio: 14.027|tagging_loss_text: 16.106|tagging_loss_image: 11.350|tagging_loss_fusion: 12.267|total_loss: 64.161 | 70.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1170 |tagging_loss_video: 10.387|tagging_loss_audio: 17.445|tagging_loss_text: 15.347|tagging_loss_image: 12.865|tagging_loss_fusion: 12.478|total_loss: 68.521 | Examples/sec: 71.54\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.65 | precision@0.5: 0.88 |recall@0.1: 0.94 | recall@0.5: 0.70\n",
      "INFO:tensorflow:training step 1171 | tagging_loss_video: 9.488|tagging_loss_audio: 13.483|tagging_loss_text: 16.800|tagging_loss_image: 13.384|tagging_loss_fusion: 11.917|total_loss: 65.073 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 1172 | tagging_loss_video: 7.423|tagging_loss_audio: 10.067|tagging_loss_text: 15.767|tagging_loss_image: 10.939|tagging_loss_fusion: 13.627|total_loss: 57.824 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 1173 | tagging_loss_video: 11.118|tagging_loss_audio: 14.254|tagging_loss_text: 14.946|tagging_loss_image: 11.344|tagging_loss_fusion: 11.934|total_loss: 63.597 | 66.04 Examples/sec\n",
      "INFO:tensorflow:training step 1174 | tagging_loss_video: 9.275|tagging_loss_audio: 12.711|tagging_loss_text: 12.718|tagging_loss_image: 10.057|tagging_loss_fusion: 10.289|total_loss: 55.050 | 67.24 Examples/sec\n",
      "INFO:tensorflow:training step 1175 | tagging_loss_video: 10.420|tagging_loss_audio: 12.535|tagging_loss_text: 17.244|tagging_loss_image: 11.786|tagging_loss_fusion: 11.604|total_loss: 63.590 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 1176 | tagging_loss_video: 8.773|tagging_loss_audio: 12.498|tagging_loss_text: 15.340|tagging_loss_image: 10.130|tagging_loss_fusion: 11.466|total_loss: 58.206 | 59.88 Examples/sec\n",
      "INFO:tensorflow:training step 1177 | tagging_loss_video: 10.895|tagging_loss_audio: 14.656|tagging_loss_text: 15.239|tagging_loss_image: 12.072|tagging_loss_fusion: 11.269|total_loss: 64.130 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 1178 | tagging_loss_video: 10.158|tagging_loss_audio: 14.577|tagging_loss_text: 14.693|tagging_loss_image: 11.746|tagging_loss_fusion: 12.130|total_loss: 63.304 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 1179 | tagging_loss_video: 7.958|tagging_loss_audio: 11.454|tagging_loss_text: 14.345|tagging_loss_image: 9.671|tagging_loss_fusion: 9.536|total_loss: 52.964 | 59.50 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1180 |tagging_loss_video: 8.864|tagging_loss_audio: 9.878|tagging_loss_text: 16.896|tagging_loss_image: 10.217|tagging_loss_fusion: 12.215|total_loss: 58.070 | Examples/sec: 70.99\n",
      "INFO:tensorflow:GAP: 0.82 | precision@0.1: 0.58 | precision@0.5: 0.82 |recall@0.1: 0.95 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 1181 | tagging_loss_video: 8.993|tagging_loss_audio: 13.386|tagging_loss_text: 13.856|tagging_loss_image: 11.776|tagging_loss_fusion: 11.037|total_loss: 59.049 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 1182 | tagging_loss_video: 8.697|tagging_loss_audio: 11.268|tagging_loss_text: 14.887|tagging_loss_image: 11.769|tagging_loss_fusion: 11.100|total_loss: 57.721 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 1183 | tagging_loss_video: 9.446|tagging_loss_audio: 14.653|tagging_loss_text: 15.685|tagging_loss_image: 11.002|tagging_loss_fusion: 13.195|total_loss: 63.981 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 1184 | tagging_loss_video: 8.756|tagging_loss_audio: 12.412|tagging_loss_text: 15.534|tagging_loss_image: 10.672|tagging_loss_fusion: 10.672|total_loss: 58.046 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 1185 | tagging_loss_video: 9.832|tagging_loss_audio: 12.988|tagging_loss_text: 16.492|tagging_loss_image: 10.892|tagging_loss_fusion: 10.013|total_loss: 60.216 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 1186 | tagging_loss_video: 9.831|tagging_loss_audio: 12.333|tagging_loss_text: 14.928|tagging_loss_image: 10.634|tagging_loss_fusion: 10.995|total_loss: 58.721 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 1187 | tagging_loss_video: 8.821|tagging_loss_audio: 12.796|tagging_loss_text: 17.158|tagging_loss_image: 11.260|tagging_loss_fusion: 10.928|total_loss: 60.963 | 64.09 Examples/sec\n",
      "INFO:tensorflow:training step 1188 | tagging_loss_video: 10.283|tagging_loss_audio: 12.866|tagging_loss_text: 16.651|tagging_loss_image: 11.482|tagging_loss_fusion: 12.324|total_loss: 63.607 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 1189 | tagging_loss_video: 8.474|tagging_loss_audio: 14.212|tagging_loss_text: 10.669|tagging_loss_image: 7.755|tagging_loss_fusion: 11.565|total_loss: 52.676 | 71.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1190 |tagging_loss_video: 9.505|tagging_loss_audio: 14.739|tagging_loss_text: 15.129|tagging_loss_image: 11.028|tagging_loss_fusion: 12.101|total_loss: 62.503 | Examples/sec: 62.19\n",
      "INFO:tensorflow:GAP: 0.81 | precision@0.1: 0.60 | precision@0.5: 0.86 |recall@0.1: 0.94 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 1191 | tagging_loss_video: 9.908|tagging_loss_audio: 13.763|tagging_loss_text: 12.866|tagging_loss_image: 10.384|tagging_loss_fusion: 9.836|total_loss: 56.758 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 1192 | tagging_loss_video: 10.652|tagging_loss_audio: 10.073|tagging_loss_text: 16.560|tagging_loss_image: 10.627|tagging_loss_fusion: 11.818|total_loss: 59.730 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 1193 | tagging_loss_video: 11.019|tagging_loss_audio: 11.921|tagging_loss_text: 15.114|tagging_loss_image: 10.268|tagging_loss_fusion: 10.075|total_loss: 58.397 | 64.26 Examples/sec\n",
      "INFO:tensorflow:training step 1194 | tagging_loss_video: 8.719|tagging_loss_audio: 11.977|tagging_loss_text: 13.280|tagging_loss_image: 13.102|tagging_loss_fusion: 10.929|total_loss: 58.008 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 1195 | tagging_loss_video: 9.737|tagging_loss_audio: 10.457|tagging_loss_text: 15.800|tagging_loss_image: 10.680|tagging_loss_fusion: 8.976|total_loss: 55.650 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 1196 | tagging_loss_video: 8.760|tagging_loss_audio: 11.659|tagging_loss_text: 15.329|tagging_loss_image: 11.413|tagging_loss_fusion: 11.055|total_loss: 58.216 | 64.59 Examples/sec\n",
      "INFO:tensorflow:training step 1197 | tagging_loss_video: 10.502|tagging_loss_audio: 13.737|tagging_loss_text: 15.819|tagging_loss_image: 12.160|tagging_loss_fusion: 12.942|total_loss: 65.160 | 67.72 Examples/sec\n",
      "INFO:tensorflow:training step 1198 | tagging_loss_video: 9.391|tagging_loss_audio: 12.475|tagging_loss_text: 16.806|tagging_loss_image: 10.841|tagging_loss_fusion: 11.421|total_loss: 60.933 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 1199 | tagging_loss_video: 10.050|tagging_loss_audio: 14.499|tagging_loss_text: 17.646|tagging_loss_image: 11.375|tagging_loss_fusion: 12.443|total_loss: 66.013 | 67.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1200 |tagging_loss_video: 8.744|tagging_loss_audio: 14.514|tagging_loss_text: 17.756|tagging_loss_image: 12.088|tagging_loss_fusion: 12.025|total_loss: 65.127 | Examples/sec: 68.54\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.61 | precision@0.5: 0.90 |recall@0.1: 0.93 | recall@0.5: 0.72\n",
      "INFO:tensorflow:training step 1201 | tagging_loss_video: 9.019|tagging_loss_audio: 12.505|tagging_loss_text: 12.151|tagging_loss_image: 9.379|tagging_loss_fusion: 12.571|total_loss: 55.625 | 62.62 Examples/sec\n",
      "INFO:tensorflow:training step 1202 | tagging_loss_video: 9.893|tagging_loss_audio: 12.692|tagging_loss_text: 17.676|tagging_loss_image: 11.681|tagging_loss_fusion: 10.980|total_loss: 62.923 | 71.59 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1203 | tagging_loss_video: 9.796|tagging_loss_audio: 12.409|tagging_loss_text: 14.656|tagging_loss_image: 10.449|tagging_loss_fusion: 13.126|total_loss: 60.436 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 1204 | tagging_loss_video: 9.257|tagging_loss_audio: 11.441|tagging_loss_text: 17.350|tagging_loss_image: 10.606|tagging_loss_fusion: 13.357|total_loss: 62.010 | 65.45 Examples/sec\n",
      "INFO:tensorflow:training step 1205 | tagging_loss_video: 9.687|tagging_loss_audio: 12.923|tagging_loss_text: 17.298|tagging_loss_image: 10.525|tagging_loss_fusion: 10.426|total_loss: 60.858 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 1206 | tagging_loss_video: 8.905|tagging_loss_audio: 13.592|tagging_loss_text: 17.566|tagging_loss_image: 10.750|tagging_loss_fusion: 11.413|total_loss: 62.225 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 1207 | tagging_loss_video: 10.000|tagging_loss_audio: 13.488|tagging_loss_text: 19.097|tagging_loss_image: 11.155|tagging_loss_fusion: 12.226|total_loss: 65.965 | 65.78 Examples/sec\n",
      "INFO:tensorflow:training step 1208 | tagging_loss_video: 9.925|tagging_loss_audio: 14.546|tagging_loss_text: 15.525|tagging_loss_image: 9.605|tagging_loss_fusion: 10.956|total_loss: 60.557 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 1209 | tagging_loss_video: 10.525|tagging_loss_audio: 13.126|tagging_loss_text: 17.215|tagging_loss_image: 11.286|tagging_loss_fusion: 13.639|total_loss: 65.791 | 70.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1210 |tagging_loss_video: 9.708|tagging_loss_audio: 13.545|tagging_loss_text: 13.396|tagging_loss_image: 11.241|tagging_loss_fusion: 11.132|total_loss: 59.021 | Examples/sec: 65.86\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.60 | precision@0.5: 0.85 |recall@0.1: 0.95 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 1211 | tagging_loss_video: 9.270|tagging_loss_audio: 17.049|tagging_loss_text: 16.862|tagging_loss_image: 12.657|tagging_loss_fusion: 12.518|total_loss: 68.356 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 1212 | tagging_loss_video: 9.339|tagging_loss_audio: 14.109|tagging_loss_text: 16.873|tagging_loss_image: 9.787|tagging_loss_fusion: 10.167|total_loss: 60.275 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 1213 | tagging_loss_video: 9.111|tagging_loss_audio: 12.002|tagging_loss_text: 16.408|tagging_loss_image: 12.354|tagging_loss_fusion: 11.710|total_loss: 61.585 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 1214 | tagging_loss_video: 9.785|tagging_loss_audio: 14.425|tagging_loss_text: 14.936|tagging_loss_image: 11.309|tagging_loss_fusion: 13.654|total_loss: 64.110 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 1215 | tagging_loss_video: 12.042|tagging_loss_audio: 14.709|tagging_loss_text: 16.799|tagging_loss_image: 10.843|tagging_loss_fusion: 13.157|total_loss: 67.551 | 62.96 Examples/sec\n",
      "INFO:tensorflow:training step 1216 | tagging_loss_video: 8.741|tagging_loss_audio: 12.713|tagging_loss_text: 15.532|tagging_loss_image: 11.169|tagging_loss_fusion: 12.588|total_loss: 60.742 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 1217 | tagging_loss_video: 10.079|tagging_loss_audio: 14.091|tagging_loss_text: 20.558|tagging_loss_image: 11.955|tagging_loss_fusion: 12.416|total_loss: 69.099 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 1218 | tagging_loss_video: 8.049|tagging_loss_audio: 10.872|tagging_loss_text: 14.499|tagging_loss_image: 9.071|tagging_loss_fusion: 10.171|total_loss: 52.661 | 59.57 Examples/sec\n",
      "INFO:tensorflow:training step 1219 | tagging_loss_video: 8.235|tagging_loss_audio: 11.415|tagging_loss_text: 14.575|tagging_loss_image: 10.499|tagging_loss_fusion: 10.505|total_loss: 55.228 | 69.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1220 |tagging_loss_video: 9.479|tagging_loss_audio: 12.506|tagging_loss_text: 14.181|tagging_loss_image: 11.025|tagging_loss_fusion: 10.914|total_loss: 58.106 | Examples/sec: 67.04\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.59 | precision@0.5: 0.85 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1221 | tagging_loss_video: 8.018|tagging_loss_audio: 12.211|tagging_loss_text: 11.109|tagging_loss_image: 9.678|tagging_loss_fusion: 12.234|total_loss: 53.249 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 1222 | tagging_loss_video: 9.101|tagging_loss_audio: 12.620|tagging_loss_text: 18.248|tagging_loss_image: 11.371|tagging_loss_fusion: 12.384|total_loss: 63.725 | 68.93 Examples/sec\n",
      "INFO:tensorflow:training step 1223 | tagging_loss_video: 9.349|tagging_loss_audio: 12.631|tagging_loss_text: 15.188|tagging_loss_image: 11.796|tagging_loss_fusion: 11.676|total_loss: 60.641 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 1224 | tagging_loss_video: 8.646|tagging_loss_audio: 13.148|tagging_loss_text: 17.395|tagging_loss_image: 9.518|tagging_loss_fusion: 13.973|total_loss: 62.680 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 1225 | tagging_loss_video: 8.498|tagging_loss_audio: 13.884|tagging_loss_text: 12.871|tagging_loss_image: 9.450|tagging_loss_fusion: 10.411|total_loss: 55.113 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 1226 | tagging_loss_video: 10.338|tagging_loss_audio: 13.032|tagging_loss_text: 14.800|tagging_loss_image: 11.045|tagging_loss_fusion: 11.738|total_loss: 60.952 | 62.90 Examples/sec\n",
      "INFO:tensorflow:training step 1227 | tagging_loss_video: 8.342|tagging_loss_audio: 12.709|tagging_loss_text: 16.484|tagging_loss_image: 10.474|tagging_loss_fusion: 9.936|total_loss: 57.944 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 1228 | tagging_loss_video: 8.287|tagging_loss_audio: 12.218|tagging_loss_text: 11.820|tagging_loss_image: 10.471|tagging_loss_fusion: 10.686|total_loss: 53.482 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 1229 | tagging_loss_video: 8.447|tagging_loss_audio: 11.209|tagging_loss_text: 12.397|tagging_loss_image: 10.142|tagging_loss_fusion: 8.875|total_loss: 51.071 | 61.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1230 |tagging_loss_video: 8.951|tagging_loss_audio: 11.707|tagging_loss_text: 15.926|tagging_loss_image: 10.400|tagging_loss_fusion: 12.777|total_loss: 59.760 | Examples/sec: 71.07\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.59 | precision@0.5: 0.88 |recall@0.1: 0.92 | recall@0.5: 0.72\n",
      "INFO:tensorflow:training step 1231 | tagging_loss_video: 9.117|tagging_loss_audio: 12.545|tagging_loss_text: 16.999|tagging_loss_image: 11.719|tagging_loss_fusion: 11.856|total_loss: 62.236 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 1232 | tagging_loss_video: 8.927|tagging_loss_audio: 12.968|tagging_loss_text: 14.241|tagging_loss_image: 11.111|tagging_loss_fusion: 11.412|total_loss: 58.659 | 62.50 Examples/sec\n",
      "INFO:tensorflow:training step 1233 | tagging_loss_video: 10.490|tagging_loss_audio: 14.494|tagging_loss_text: 16.190|tagging_loss_image: 11.098|tagging_loss_fusion: 13.127|total_loss: 65.399 | 68.94 Examples/sec\n",
      "INFO:tensorflow:training step 1234 | tagging_loss_video: 9.208|tagging_loss_audio: 14.147|tagging_loss_text: 15.242|tagging_loss_image: 10.783|tagging_loss_fusion: 10.903|total_loss: 60.283 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 1235 | tagging_loss_video: 9.344|tagging_loss_audio: 14.077|tagging_loss_text: 16.353|tagging_loss_image: 11.263|tagging_loss_fusion: 11.324|total_loss: 62.361 | 64.29 Examples/sec\n",
      "INFO:tensorflow:training step 1236 | tagging_loss_video: 8.300|tagging_loss_audio: 11.521|tagging_loss_text: 15.655|tagging_loss_image: 9.201|tagging_loss_fusion: 10.350|total_loss: 55.027 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 1237 | tagging_loss_video: 8.540|tagging_loss_audio: 13.289|tagging_loss_text: 15.284|tagging_loss_image: 11.529|tagging_loss_fusion: 11.828|total_loss: 60.472 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 1238 | tagging_loss_video: 8.666|tagging_loss_audio: 14.429|tagging_loss_text: 16.798|tagging_loss_image: 10.236|tagging_loss_fusion: 12.788|total_loss: 62.918 | 71.80 Examples/sec\n",
      "INFO:tensorflow:training step 1239 | tagging_loss_video: 9.556|tagging_loss_audio: 11.453|tagging_loss_text: 17.433|tagging_loss_image: 9.886|tagging_loss_fusion: 11.315|total_loss: 59.643 | 69.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1240 |tagging_loss_video: 9.835|tagging_loss_audio: 13.515|tagging_loss_text: 15.432|tagging_loss_image: 11.288|tagging_loss_fusion: 10.674|total_loss: 60.744 | Examples/sec: 69.45\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.64 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 1241 | tagging_loss_video: 9.297|tagging_loss_audio: 12.729|tagging_loss_text: 13.342|tagging_loss_image: 10.151|tagging_loss_fusion: 11.622|total_loss: 57.140 | 66.86 Examples/sec\n",
      "INFO:tensorflow:training step 1242 | tagging_loss_video: 9.565|tagging_loss_audio: 13.552|tagging_loss_text: 16.289|tagging_loss_image: 9.657|tagging_loss_fusion: 10.485|total_loss: 59.548 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 1243 | tagging_loss_video: 11.120|tagging_loss_audio: 14.510|tagging_loss_text: 16.411|tagging_loss_image: 12.286|tagging_loss_fusion: 11.447|total_loss: 65.773 | 72.01 Examples/sec\n",
      "INFO:tensorflow:training step 1244 | tagging_loss_video: 7.819|tagging_loss_audio: 11.391|tagging_loss_text: 15.290|tagging_loss_image: 10.148|tagging_loss_fusion: 11.305|total_loss: 55.954 | 62.43 Examples/sec\n",
      "INFO:tensorflow:training step 1245 | tagging_loss_video: 7.682|tagging_loss_audio: 11.351|tagging_loss_text: 13.709|tagging_loss_image: 8.377|tagging_loss_fusion: 10.318|total_loss: 51.436 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 1246 | tagging_loss_video: 8.409|tagging_loss_audio: 10.835|tagging_loss_text: 14.613|tagging_loss_image: 9.899|tagging_loss_fusion: 10.430|total_loss: 54.187 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 1247 | tagging_loss_video: 8.738|tagging_loss_audio: 12.990|tagging_loss_text: 16.730|tagging_loss_image: 10.511|tagging_loss_fusion: 10.427|total_loss: 59.396 | 59.18 Examples/sec\n",
      "INFO:tensorflow:training step 1248 | tagging_loss_video: 9.206|tagging_loss_audio: 12.636|tagging_loss_text: 17.868|tagging_loss_image: 10.818|tagging_loss_fusion: 8.133|total_loss: 58.660 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 1249 | tagging_loss_video: 9.125|tagging_loss_audio: 11.829|tagging_loss_text: 15.220|tagging_loss_image: 10.925|tagging_loss_fusion: 10.581|total_loss: 57.681 | 70.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1250 |tagging_loss_video: 8.124|tagging_loss_audio: 12.623|tagging_loss_text: 14.781|tagging_loss_image: 9.333|tagging_loss_fusion: 11.939|total_loss: 56.799 | Examples/sec: 64.31\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.60 | precision@0.5: 0.84 |recall@0.1: 0.92 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 1251 | tagging_loss_video: 9.983|tagging_loss_audio: 13.535|tagging_loss_text: 17.503|tagging_loss_image: 10.652|tagging_loss_fusion: 11.482|total_loss: 63.156 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 1252 | tagging_loss_video: 9.455|tagging_loss_audio: 14.738|tagging_loss_text: 16.459|tagging_loss_image: 12.855|tagging_loss_fusion: 10.749|total_loss: 64.256 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 1253 | tagging_loss_video: 9.385|tagging_loss_audio: 12.805|tagging_loss_text: 19.100|tagging_loss_image: 11.546|tagging_loss_fusion: 12.413|total_loss: 65.249 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 1254 | tagging_loss_video: 10.598|tagging_loss_audio: 13.639|tagging_loss_text: 16.369|tagging_loss_image: 12.533|tagging_loss_fusion: 11.431|total_loss: 64.570 | 65.65 Examples/sec\n",
      "INFO:tensorflow:training step 1255 | tagging_loss_video: 9.002|tagging_loss_audio: 13.434|tagging_loss_text: 19.342|tagging_loss_image: 11.060|tagging_loss_fusion: 10.170|total_loss: 63.007 | 61.33 Examples/sec\n",
      "INFO:tensorflow:training step 1256 | tagging_loss_video: 8.214|tagging_loss_audio: 11.878|tagging_loss_text: 15.979|tagging_loss_image: 10.465|tagging_loss_fusion: 9.735|total_loss: 56.271 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 1257 | tagging_loss_video: 9.019|tagging_loss_audio: 11.413|tagging_loss_text: 15.140|tagging_loss_image: 8.581|tagging_loss_fusion: 10.788|total_loss: 54.941 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 1258 | tagging_loss_video: 8.758|tagging_loss_audio: 12.089|tagging_loss_text: 16.272|tagging_loss_image: 9.880|tagging_loss_fusion: 11.210|total_loss: 58.208 | 65.61 Examples/sec\n",
      "INFO:tensorflow:training step 1259 | tagging_loss_video: 9.072|tagging_loss_audio: 12.326|tagging_loss_text: 16.483|tagging_loss_image: 11.392|tagging_loss_fusion: 11.429|total_loss: 60.702 | 70.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1260 |tagging_loss_video: 8.324|tagging_loss_audio: 11.509|tagging_loss_text: 14.217|tagging_loss_image: 11.232|tagging_loss_fusion: 9.863|total_loss: 55.145 | Examples/sec: 69.87\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.66 | precision@0.5: 0.90 |recall@0.1: 0.95 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1261 | tagging_loss_video: 10.050|tagging_loss_audio: 13.940|tagging_loss_text: 14.764|tagging_loss_image: 9.880|tagging_loss_fusion: 11.862|total_loss: 60.496 | 65.86 Examples/sec\n",
      "INFO:tensorflow:training step 1262 | tagging_loss_video: 9.484|tagging_loss_audio: 12.222|tagging_loss_text: 13.572|tagging_loss_image: 12.231|tagging_loss_fusion: 10.977|total_loss: 58.487 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 1263 | tagging_loss_video: 9.397|tagging_loss_audio: 15.605|tagging_loss_text: 16.493|tagging_loss_image: 9.392|tagging_loss_fusion: 12.557|total_loss: 63.443 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 1264 | tagging_loss_video: 9.565|tagging_loss_audio: 13.688|tagging_loss_text: 15.144|tagging_loss_image: 11.988|tagging_loss_fusion: 12.958|total_loss: 63.343 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 1265 | tagging_loss_video: 10.755|tagging_loss_audio: 10.794|tagging_loss_text: 13.275|tagging_loss_image: 11.252|tagging_loss_fusion: 13.633|total_loss: 59.710 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 1266 | tagging_loss_video: 9.454|tagging_loss_audio: 13.022|tagging_loss_text: 15.696|tagging_loss_image: 10.722|tagging_loss_fusion: 11.630|total_loss: 60.524 | 63.23 Examples/sec\n",
      "INFO:tensorflow:training step 1267 | tagging_loss_video: 12.284|tagging_loss_audio: 12.724|tagging_loss_text: 18.101|tagging_loss_image: 11.701|tagging_loss_fusion: 14.110|total_loss: 68.919 | 68.49 Examples/sec\n",
      "INFO:tensorflow:training step 1268 | tagging_loss_video: 10.729|tagging_loss_audio: 12.095|tagging_loss_text: 15.263|tagging_loss_image: 9.729|tagging_loss_fusion: 11.699|total_loss: 59.514 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 1269 | tagging_loss_video: 10.311|tagging_loss_audio: 12.122|tagging_loss_text: 16.555|tagging_loss_image: 11.757|tagging_loss_fusion: 12.066|total_loss: 62.810 | 62.36 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1270 |tagging_loss_video: 10.441|tagging_loss_audio: 16.339|tagging_loss_text: 13.432|tagging_loss_image: 12.234|tagging_loss_fusion: 12.140|total_loss: 64.586 | Examples/sec: 71.14\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.63 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 1271 | tagging_loss_video: 10.016|tagging_loss_audio: 10.095|tagging_loss_text: 13.137|tagging_loss_image: 10.962|tagging_loss_fusion: 12.133|total_loss: 56.343 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 1272 | tagging_loss_video: 9.953|tagging_loss_audio: 13.995|tagging_loss_text: 13.182|tagging_loss_image: 10.898|tagging_loss_fusion: 12.169|total_loss: 60.198 | 71.90 Examples/sec\n",
      "INFO:tensorflow:training step 1273 | tagging_loss_video: 9.099|tagging_loss_audio: 12.479|tagging_loss_text: 17.352|tagging_loss_image: 9.422|tagging_loss_fusion: 12.028|total_loss: 60.380 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 1274 | tagging_loss_video: 11.171|tagging_loss_audio: 14.195|tagging_loss_text: 18.209|tagging_loss_image: 10.979|tagging_loss_fusion: 13.229|total_loss: 67.783 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 1275 | tagging_loss_video: 10.379|tagging_loss_audio: 14.498|tagging_loss_text: 15.477|tagging_loss_image: 12.797|tagging_loss_fusion: 12.466|total_loss: 65.617 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 1276 | tagging_loss_video: 9.861|tagging_loss_audio: 12.922|tagging_loss_text: 15.774|tagging_loss_image: 12.055|tagging_loss_fusion: 15.839|total_loss: 66.451 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 1277 | tagging_loss_video: 10.394|tagging_loss_audio: 11.083|tagging_loss_text: 14.593|tagging_loss_image: 10.543|tagging_loss_fusion: 11.285|total_loss: 57.898 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 1278 | tagging_loss_video: 10.074|tagging_loss_audio: 12.366|tagging_loss_text: 14.782|tagging_loss_image: 11.737|tagging_loss_fusion: 12.913|total_loss: 61.871 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 1279 | tagging_loss_video: 10.054|tagging_loss_audio: 12.638|tagging_loss_text: 13.683|tagging_loss_image: 9.764|tagging_loss_fusion: 12.775|total_loss: 58.914 | 71.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1280 |tagging_loss_video: 9.845|tagging_loss_audio: 12.374|tagging_loss_text: 15.225|tagging_loss_image: 12.517|tagging_loss_fusion: 12.611|total_loss: 62.573 | Examples/sec: 59.11\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.59 | precision@0.5: 0.83 |recall@0.1: 0.95 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1281 | tagging_loss_video: 9.851|tagging_loss_audio: 13.198|tagging_loss_text: 15.976|tagging_loss_image: 12.209|tagging_loss_fusion: 14.970|total_loss: 66.204 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 1282 | tagging_loss_video: 9.802|tagging_loss_audio: 12.124|tagging_loss_text: 13.796|tagging_loss_image: 12.375|tagging_loss_fusion: 10.923|total_loss: 59.019 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 1283 | tagging_loss_video: 9.281|tagging_loss_audio: 14.571|tagging_loss_text: 16.822|tagging_loss_image: 11.589|tagging_loss_fusion: 11.022|total_loss: 63.285 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 1284 | tagging_loss_video: 11.629|tagging_loss_audio: 13.393|tagging_loss_text: 18.264|tagging_loss_image: 10.858|tagging_loss_fusion: 13.166|total_loss: 67.310 | 69.30 Examples/sec\n",
      "INFO:tensorflow:training step 1285 | tagging_loss_video: 10.480|tagging_loss_audio: 12.460|tagging_loss_text: 16.653|tagging_loss_image: 10.576|tagging_loss_fusion: 10.693|total_loss: 60.862 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 1286 | tagging_loss_video: 9.998|tagging_loss_audio: 11.832|tagging_loss_text: 16.031|tagging_loss_image: 10.341|tagging_loss_fusion: 10.520|total_loss: 58.722 | 63.13 Examples/sec\n",
      "INFO:tensorflow:training step 1287 | tagging_loss_video: 9.227|tagging_loss_audio: 13.789|tagging_loss_text: 18.946|tagging_loss_image: 10.831|tagging_loss_fusion: 10.473|total_loss: 63.267 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 1288 | tagging_loss_video: 9.072|tagging_loss_audio: 12.945|tagging_loss_text: 17.227|tagging_loss_image: 10.198|tagging_loss_fusion: 11.789|total_loss: 61.231 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 1289 | tagging_loss_video: 9.400|tagging_loss_audio: 11.999|tagging_loss_text: 13.377|tagging_loss_image: 10.922|tagging_loss_fusion: 10.476|total_loss: 56.174 | 68.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1290 |tagging_loss_video: 10.720|tagging_loss_audio: 14.075|tagging_loss_text: 18.535|tagging_loss_image: 10.770|tagging_loss_fusion: 11.635|total_loss: 65.735 | Examples/sec: 70.46\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.64 | precision@0.5: 0.86 |recall@0.1: 0.92 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1291 | tagging_loss_video: 9.610|tagging_loss_audio: 11.703|tagging_loss_text: 17.103|tagging_loss_image: 12.274|tagging_loss_fusion: 11.978|total_loss: 62.669 | 60.07 Examples/sec\n",
      "INFO:tensorflow:training step 1292 | tagging_loss_video: 8.532|tagging_loss_audio: 12.252|tagging_loss_text: 14.526|tagging_loss_image: 11.053|tagging_loss_fusion: 11.048|total_loss: 57.411 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 1293 | tagging_loss_video: 9.570|tagging_loss_audio: 13.300|tagging_loss_text: 15.055|tagging_loss_image: 12.054|tagging_loss_fusion: 10.413|total_loss: 60.392 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 1294 | tagging_loss_video: 8.705|tagging_loss_audio: 11.599|tagging_loss_text: 13.671|tagging_loss_image: 9.163|tagging_loss_fusion: 10.573|total_loss: 53.709 | 60.36 Examples/sec\n",
      "INFO:tensorflow:training step 1295 | tagging_loss_video: 10.003|tagging_loss_audio: 10.925|tagging_loss_text: 15.335|tagging_loss_image: 11.512|tagging_loss_fusion: 12.165|total_loss: 59.940 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 1296 | tagging_loss_video: 9.364|tagging_loss_audio: 14.507|tagging_loss_text: 16.501|tagging_loss_image: 12.101|tagging_loss_fusion: 9.076|total_loss: 61.549 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 1297 | tagging_loss_video: 8.178|tagging_loss_audio: 11.721|tagging_loss_text: 13.235|tagging_loss_image: 10.120|tagging_loss_fusion: 10.735|total_loss: 53.989 | 62.10 Examples/sec\n",
      "INFO:tensorflow:training step 1298 | tagging_loss_video: 8.173|tagging_loss_audio: 11.939|tagging_loss_text: 16.713|tagging_loss_image: 10.048|tagging_loss_fusion: 10.180|total_loss: 57.052 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 1299 | tagging_loss_video: 8.502|tagging_loss_audio: 12.676|tagging_loss_text: 13.695|tagging_loss_image: 9.178|tagging_loss_fusion: 9.516|total_loss: 53.566 | 70.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1300 |tagging_loss_video: 9.311|tagging_loss_audio: 12.631|tagging_loss_text: 15.308|tagging_loss_image: 10.358|tagging_loss_fusion: 12.232|total_loss: 59.841 | Examples/sec: 64.58\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.59 | precision@0.5: 0.85 |recall@0.1: 0.94 | recall@0.5: 0.72\n",
      "INFO:tensorflow:training step 1301 | tagging_loss_video: 7.128|tagging_loss_audio: 13.472|tagging_loss_text: 18.155|tagging_loss_image: 8.679|tagging_loss_fusion: 10.784|total_loss: 58.217 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 1302 | tagging_loss_video: 9.109|tagging_loss_audio: 11.075|tagging_loss_text: 15.370|tagging_loss_image: 10.161|tagging_loss_fusion: 10.347|total_loss: 56.063 | 72.26 Examples/sec\n",
      "INFO:tensorflow:training step 1303 | tagging_loss_video: 9.068|tagging_loss_audio: 13.124|tagging_loss_text: 18.714|tagging_loss_image: 9.602|tagging_loss_fusion: 9.800|total_loss: 60.308 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 1304 | tagging_loss_video: 9.318|tagging_loss_audio: 12.856|tagging_loss_text: 13.308|tagging_loss_image: 10.093|tagging_loss_fusion: 10.031|total_loss: 55.606 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 1305 | tagging_loss_video: 9.382|tagging_loss_audio: 14.806|tagging_loss_text: 13.679|tagging_loss_image: 10.337|tagging_loss_fusion: 9.589|total_loss: 57.793 | 58.94 Examples/sec\n",
      "INFO:tensorflow:training step 1306 | tagging_loss_video: 8.917|tagging_loss_audio: 10.815|tagging_loss_text: 14.698|tagging_loss_image: 10.838|tagging_loss_fusion: 10.008|total_loss: 55.277 | 71.34 Examples/sec\n",
      "INFO:tensorflow:training step 1307 | tagging_loss_video: 8.140|tagging_loss_audio: 13.737|tagging_loss_text: 15.919|tagging_loss_image: 9.849|tagging_loss_fusion: 9.395|total_loss: 57.040 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 1308 | tagging_loss_video: 8.846|tagging_loss_audio: 12.641|tagging_loss_text: 15.661|tagging_loss_image: 9.955|tagging_loss_fusion: 9.692|total_loss: 56.795 | 65.09 Examples/sec\n",
      "INFO:tensorflow:training step 1309 | tagging_loss_video: 9.401|tagging_loss_audio: 12.980|tagging_loss_text: 18.824|tagging_loss_image: 11.257|tagging_loss_fusion: 13.113|total_loss: 65.575 | 70.16 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1310 |tagging_loss_video: 10.094|tagging_loss_audio: 13.147|tagging_loss_text: 13.000|tagging_loss_image: 11.513|tagging_loss_fusion: 13.000|total_loss: 60.753 | Examples/sec: 71.05\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.66 | precision@0.5: 0.91 |recall@0.1: 0.91 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1311 | tagging_loss_video: 9.322|tagging_loss_audio: 14.237|tagging_loss_text: 14.812|tagging_loss_image: 12.905|tagging_loss_fusion: 11.432|total_loss: 62.708 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 1312 | tagging_loss_video: 8.530|tagging_loss_audio: 11.516|tagging_loss_text: 18.526|tagging_loss_image: 11.768|tagging_loss_fusion: 10.443|total_loss: 60.783 | 67.40 Examples/sec\n",
      "INFO:tensorflow:training step 1313 | tagging_loss_video: 10.370|tagging_loss_audio: 13.027|tagging_loss_text: 16.431|tagging_loss_image: 11.136|tagging_loss_fusion: 12.219|total_loss: 63.183 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 1314 | tagging_loss_video: 8.829|tagging_loss_audio: 12.267|tagging_loss_text: 14.062|tagging_loss_image: 10.409|tagging_loss_fusion: 11.587|total_loss: 57.154 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 1315 | tagging_loss_video: 9.466|tagging_loss_audio: 13.233|tagging_loss_text: 17.265|tagging_loss_image: 11.435|tagging_loss_fusion: 11.767|total_loss: 63.166 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 1316 | tagging_loss_video: 9.188|tagging_loss_audio: 10.430|tagging_loss_text: 16.585|tagging_loss_image: 9.291|tagging_loss_fusion: 11.341|total_loss: 56.835 | 63.03 Examples/sec\n",
      "INFO:tensorflow:training step 1317 | tagging_loss_video: 8.535|tagging_loss_audio: 11.795|tagging_loss_text: 15.942|tagging_loss_image: 11.526|tagging_loss_fusion: 12.396|total_loss: 60.195 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 1318 | tagging_loss_video: 9.351|tagging_loss_audio: 11.854|tagging_loss_text: 15.325|tagging_loss_image: 11.707|tagging_loss_fusion: 9.800|total_loss: 58.039 | 67.39 Examples/sec\n",
      "INFO:tensorflow:training step 1319 | tagging_loss_video: 8.062|tagging_loss_audio: 11.256|tagging_loss_text: 15.645|tagging_loss_image: 8.781|tagging_loss_fusion: 11.252|total_loss: 54.996 | 71.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1320 |tagging_loss_video: 8.857|tagging_loss_audio: 10.484|tagging_loss_text: 15.706|tagging_loss_image: 9.838|tagging_loss_fusion: 9.811|total_loss: 54.695 | Examples/sec: 67.40\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.64 | precision@0.5: 0.87 |recall@0.1: 0.93 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1321 | tagging_loss_video: 9.161|tagging_loss_audio: 14.887|tagging_loss_text: 15.295|tagging_loss_image: 11.444|tagging_loss_fusion: 9.367|total_loss: 60.153 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 1322 | tagging_loss_video: 7.259|tagging_loss_audio: 12.878|tagging_loss_text: 10.122|tagging_loss_image: 10.885|tagging_loss_fusion: 9.941|total_loss: 51.083 | 64.59 Examples/sec\n",
      "INFO:tensorflow:training step 1323 | tagging_loss_video: 9.551|tagging_loss_audio: 16.824|tagging_loss_text: 15.704|tagging_loss_image: 12.232|tagging_loss_fusion: 9.194|total_loss: 63.505 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 1324 | tagging_loss_video: 8.773|tagging_loss_audio: 13.353|tagging_loss_text: 14.117|tagging_loss_image: 10.381|tagging_loss_fusion: 9.032|total_loss: 55.656 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 1325 | tagging_loss_video: 8.334|tagging_loss_audio: 12.979|tagging_loss_text: 12.965|tagging_loss_image: 11.719|tagging_loss_fusion: 9.275|total_loss: 55.271 | 62.61 Examples/sec\n",
      "INFO:tensorflow:training step 1326 | tagging_loss_video: 9.388|tagging_loss_audio: 12.934|tagging_loss_text: 15.644|tagging_loss_image: 10.879|tagging_loss_fusion: 12.920|total_loss: 61.765 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 1327 | tagging_loss_video: 9.617|tagging_loss_audio: 10.761|tagging_loss_text: 16.362|tagging_loss_image: 10.169|tagging_loss_fusion: 9.886|total_loss: 56.794 | 67.66 Examples/sec\n",
      "INFO:tensorflow:training step 1328 | tagging_loss_video: 9.903|tagging_loss_audio: 10.069|tagging_loss_text: 13.970|tagging_loss_image: 10.344|tagging_loss_fusion: 12.157|total_loss: 56.442 | 72.17 Examples/sec\n",
      "INFO:tensorflow:training step 1329 | tagging_loss_video: 8.262|tagging_loss_audio: 13.184|tagging_loss_text: 16.068|tagging_loss_image: 10.620|tagging_loss_fusion: 10.842|total_loss: 58.975 | 69.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1330 |tagging_loss_video: 9.155|tagging_loss_audio: 13.969|tagging_loss_text: 16.605|tagging_loss_image: 10.843|tagging_loss_fusion: 10.806|total_loss: 61.378 | Examples/sec: 65.27\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.67 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1331 | tagging_loss_video: 8.104|tagging_loss_audio: 11.929|tagging_loss_text: 12.813|tagging_loss_image: 9.319|tagging_loss_fusion: 10.779|total_loss: 52.944 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 1332 | tagging_loss_video: 10.053|tagging_loss_audio: 14.238|tagging_loss_text: 16.380|tagging_loss_image: 11.706|tagging_loss_fusion: 9.810|total_loss: 62.187 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 1333 | tagging_loss_video: 10.695|tagging_loss_audio: 13.421|tagging_loss_text: 13.772|tagging_loss_image: 8.838|tagging_loss_fusion: 11.610|total_loss: 58.336 | 63.26 Examples/sec\n",
      "INFO:tensorflow:training step 1334 | tagging_loss_video: 9.759|tagging_loss_audio: 14.802|tagging_loss_text: 16.005|tagging_loss_image: 10.641|tagging_loss_fusion: 11.090|total_loss: 62.297 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 1335 | tagging_loss_video: 8.468|tagging_loss_audio: 12.849|tagging_loss_text: 12.402|tagging_loss_image: 9.140|tagging_loss_fusion: 10.411|total_loss: 53.270 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 1336 | tagging_loss_video: 8.262|tagging_loss_audio: 13.295|tagging_loss_text: 13.321|tagging_loss_image: 9.928|tagging_loss_fusion: 9.240|total_loss: 54.044 | 60.13 Examples/sec\n",
      "INFO:tensorflow:training step 1337 | tagging_loss_video: 9.504|tagging_loss_audio: 15.117|tagging_loss_text: 14.143|tagging_loss_image: 11.781|tagging_loss_fusion: 10.098|total_loss: 60.643 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 1338 | tagging_loss_video: 8.667|tagging_loss_audio: 13.180|tagging_loss_text: 12.664|tagging_loss_image: 8.760|tagging_loss_fusion: 12.597|total_loss: 55.869 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 1339 | tagging_loss_video: 9.920|tagging_loss_audio: 13.023|tagging_loss_text: 15.435|tagging_loss_image: 9.954|tagging_loss_fusion: 10.444|total_loss: 58.775 | 71.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1340 |tagging_loss_video: 10.608|tagging_loss_audio: 10.673|tagging_loss_text: 16.286|tagging_loss_image: 10.856|tagging_loss_fusion: 10.722|total_loss: 59.146 | Examples/sec: 69.77\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.66 | precision@0.5: 0.85 |recall@0.1: 0.94 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1341 | tagging_loss_video: 8.874|tagging_loss_audio: 11.852|tagging_loss_text: 15.405|tagging_loss_image: 11.313|tagging_loss_fusion: 11.188|total_loss: 58.633 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 1342 | tagging_loss_video: 10.390|tagging_loss_audio: 13.875|tagging_loss_text: 12.716|tagging_loss_image: 9.459|tagging_loss_fusion: 11.009|total_loss: 57.449 | 68.40 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1343 | tagging_loss_video: 9.407|tagging_loss_audio: 12.275|tagging_loss_text: 17.543|tagging_loss_image: 10.679|tagging_loss_fusion: 11.769|total_loss: 61.673 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 1344 | tagging_loss_video: 9.378|tagging_loss_audio: 14.236|tagging_loss_text: 12.500|tagging_loss_image: 11.494|tagging_loss_fusion: 11.924|total_loss: 59.532 | 65.01 Examples/sec\n",
      "INFO:tensorflow:training step 1345 | tagging_loss_video: 7.875|tagging_loss_audio: 12.533|tagging_loss_text: 14.608|tagging_loss_image: 9.646|tagging_loss_fusion: 12.724|total_loss: 57.385 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 1346 | tagging_loss_video: 8.813|tagging_loss_audio: 13.390|tagging_loss_text: 16.839|tagging_loss_image: 10.208|tagging_loss_fusion: 11.330|total_loss: 60.580 | 66.59 Examples/sec\n",
      "INFO:tensorflow:training step 1347 | tagging_loss_video: 9.421|tagging_loss_audio: 12.376|tagging_loss_text: 18.444|tagging_loss_image: 10.959|tagging_loss_fusion: 12.379|total_loss: 63.579 | 61.92 Examples/sec\n",
      "INFO:tensorflow:training step 1348 | tagging_loss_video: 8.169|tagging_loss_audio: 12.377|tagging_loss_text: 15.523|tagging_loss_image: 10.761|tagging_loss_fusion: 10.173|total_loss: 57.004 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 1349 | tagging_loss_video: 8.981|tagging_loss_audio: 13.887|tagging_loss_text: 16.619|tagging_loss_image: 11.859|tagging_loss_fusion: 12.004|total_loss: 63.350 | 67.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1350 |tagging_loss_video: 9.498|tagging_loss_audio: 13.701|tagging_loss_text: 13.520|tagging_loss_image: 10.749|tagging_loss_fusion: 10.121|total_loss: 57.589 | Examples/sec: 69.34\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.59 | precision@0.5: 0.84 |recall@0.1: 0.94 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1351 | tagging_loss_video: 8.810|tagging_loss_audio: 14.038|tagging_loss_text: 16.150|tagging_loss_image: 11.555|tagging_loss_fusion: 9.613|total_loss: 60.167 | 68.00 Examples/sec\n",
      "INFO:tensorflow:training step 1352 | tagging_loss_video: 8.745|tagging_loss_audio: 11.453|tagging_loss_text: 13.103|tagging_loss_image: 9.542|tagging_loss_fusion: 11.382|total_loss: 54.225 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 1353 | tagging_loss_video: 9.189|tagging_loss_audio: 12.507|tagging_loss_text: 16.406|tagging_loss_image: 11.122|tagging_loss_fusion: 11.650|total_loss: 60.873 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 1354 | tagging_loss_video: 9.315|tagging_loss_audio: 15.292|tagging_loss_text: 17.133|tagging_loss_image: 10.887|tagging_loss_fusion: 12.288|total_loss: 64.915 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 1355 | tagging_loss_video: 9.504|tagging_loss_audio: 13.801|tagging_loss_text: 19.647|tagging_loss_image: 12.606|tagging_loss_fusion: 11.673|total_loss: 67.232 | 57.73 Examples/sec\n",
      "INFO:tensorflow:training step 1356 | tagging_loss_video: 9.610|tagging_loss_audio: 13.556|tagging_loss_text: 14.094|tagging_loss_image: 10.691|tagging_loss_fusion: 10.665|total_loss: 58.615 | 68.40 Examples/sec\n",
      "INFO:tensorflow:training step 1357 | tagging_loss_video: 9.165|tagging_loss_audio: 12.005|tagging_loss_text: 14.518|tagging_loss_image: 9.551|tagging_loss_fusion: 12.645|total_loss: 57.884 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 1358 | tagging_loss_video: 8.548|tagging_loss_audio: 13.610|tagging_loss_text: 13.282|tagging_loss_image: 8.827|tagging_loss_fusion: 10.079|total_loss: 54.346 | 61.30 Examples/sec\n",
      "INFO:tensorflow:training step 1359 | tagging_loss_video: 8.523|tagging_loss_audio: 14.557|tagging_loss_text: 15.843|tagging_loss_image: 8.920|tagging_loss_fusion: 10.435|total_loss: 58.278 | 66.32 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1360 |tagging_loss_video: 7.846|tagging_loss_audio: 14.377|tagging_loss_text: 13.552|tagging_loss_image: 11.380|tagging_loss_fusion: 11.498|total_loss: 58.654 | Examples/sec: 69.93\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.61 | precision@0.5: 0.86 |recall@0.1: 0.96 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1361 | tagging_loss_video: 7.878|tagging_loss_audio: 10.731|tagging_loss_text: 11.072|tagging_loss_image: 9.365|tagging_loss_fusion: 10.381|total_loss: 49.428 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 1362 | tagging_loss_video: 9.516|tagging_loss_audio: 14.085|tagging_loss_text: 14.039|tagging_loss_image: 10.986|tagging_loss_fusion: 10.191|total_loss: 58.818 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 1363 | tagging_loss_video: 8.702|tagging_loss_audio: 11.748|tagging_loss_text: 17.085|tagging_loss_image: 10.096|tagging_loss_fusion: 10.387|total_loss: 58.018 | 72.25 Examples/sec\n",
      "INFO:tensorflow:training step 1364 | tagging_loss_video: 10.168|tagging_loss_audio: 12.384|tagging_loss_text: 15.311|tagging_loss_image: 10.412|tagging_loss_fusion: 11.839|total_loss: 60.114 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 1365 | tagging_loss_video: 8.527|tagging_loss_audio: 11.895|tagging_loss_text: 12.201|tagging_loss_image: 10.389|tagging_loss_fusion: 7.933|total_loss: 50.945 | 72.05 Examples/sec\n",
      "INFO:tensorflow:training step 1366 | tagging_loss_video: 10.433|tagging_loss_audio: 11.387|tagging_loss_text: 20.198|tagging_loss_image: 12.144|tagging_loss_fusion: 12.005|total_loss: 66.168 | 58.11 Examples/sec\n",
      "INFO:tensorflow:training step 1367 | tagging_loss_video: 9.059|tagging_loss_audio: 11.381|tagging_loss_text: 10.981|tagging_loss_image: 11.141|tagging_loss_fusion: 9.235|total_loss: 51.797 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 1368 | tagging_loss_video: 8.227|tagging_loss_audio: 12.628|tagging_loss_text: 15.451|tagging_loss_image: 9.043|tagging_loss_fusion: 9.635|total_loss: 54.984 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 1369 | tagging_loss_video: 7.582|tagging_loss_audio: 13.193|tagging_loss_text: 14.936|tagging_loss_image: 9.122|tagging_loss_fusion: 7.390|total_loss: 52.222 | 59.71 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1370 |tagging_loss_video: 8.973|tagging_loss_audio: 10.803|tagging_loss_text: 16.003|tagging_loss_image: 10.601|tagging_loss_fusion: 10.283|total_loss: 56.662 | Examples/sec: 68.43\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.65 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 1371 | tagging_loss_video: 8.733|tagging_loss_audio: 13.261|tagging_loss_text: 18.231|tagging_loss_image: 12.178|tagging_loss_fusion: 12.723|total_loss: 65.126 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 1372 | tagging_loss_video: 8.142|tagging_loss_audio: 13.099|tagging_loss_text: 10.370|tagging_loss_image: 9.443|tagging_loss_fusion: 9.846|total_loss: 50.899 | 62.67 Examples/sec\n",
      "INFO:tensorflow:training step 1373 | tagging_loss_video: 9.512|tagging_loss_audio: 11.081|tagging_loss_text: 12.843|tagging_loss_image: 9.780|tagging_loss_fusion: 13.133|total_loss: 56.349 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 1374 | tagging_loss_video: 9.005|tagging_loss_audio: 12.456|tagging_loss_text: 16.573|tagging_loss_image: 10.441|tagging_loss_fusion: 8.143|total_loss: 56.617 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 1375 | tagging_loss_video: 8.519|tagging_loss_audio: 12.207|tagging_loss_text: 13.790|tagging_loss_image: 10.840|tagging_loss_fusion: 10.456|total_loss: 55.812 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 1376 | tagging_loss_video: 8.492|tagging_loss_audio: 13.183|tagging_loss_text: 16.972|tagging_loss_image: 9.495|tagging_loss_fusion: 9.185|total_loss: 57.327 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 1377 | tagging_loss_video: 9.149|tagging_loss_audio: 13.043|tagging_loss_text: 15.367|tagging_loss_image: 10.882|tagging_loss_fusion: 9.797|total_loss: 58.238 | 64.78 Examples/sec\n",
      "INFO:tensorflow:training step 1378 | tagging_loss_video: 9.462|tagging_loss_audio: 13.230|tagging_loss_text: 17.462|tagging_loss_image: 11.412|tagging_loss_fusion: 10.539|total_loss: 62.105 | 72.37 Examples/sec\n",
      "INFO:tensorflow:training step 1379 | tagging_loss_video: 9.510|tagging_loss_audio: 13.836|tagging_loss_text: 18.100|tagging_loss_image: 10.651|tagging_loss_fusion: 11.123|total_loss: 63.219 | 67.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1380 |tagging_loss_video: 8.291|tagging_loss_audio: 12.916|tagging_loss_text: 16.664|tagging_loss_image: 11.855|tagging_loss_fusion: 12.798|total_loss: 62.524 | Examples/sec: 68.87\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.63 | precision@0.5: 0.89 |recall@0.1: 0.92 | recall@0.5: 0.68\n",
      "INFO:tensorflow:training step 1381 | tagging_loss_video: 8.765|tagging_loss_audio: 14.386|tagging_loss_text: 16.933|tagging_loss_image: 11.609|tagging_loss_fusion: 11.835|total_loss: 63.528 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 1382 | tagging_loss_video: 9.222|tagging_loss_audio: 12.898|tagging_loss_text: 14.238|tagging_loss_image: 10.408|tagging_loss_fusion: 11.440|total_loss: 58.206 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 1383 | tagging_loss_video: 10.092|tagging_loss_audio: 13.310|tagging_loss_text: 12.155|tagging_loss_image: 11.071|tagging_loss_fusion: 12.377|total_loss: 59.005 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 1384 | tagging_loss_video: 7.757|tagging_loss_audio: 11.673|tagging_loss_text: 15.318|tagging_loss_image: 8.223|tagging_loss_fusion: 10.533|total_loss: 53.504 | 61.07 Examples/sec\n",
      "INFO:tensorflow:training step 1385 | tagging_loss_video: 8.619|tagging_loss_audio: 11.898|tagging_loss_text: 16.092|tagging_loss_image: 8.618|tagging_loss_fusion: 9.211|total_loss: 54.438 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 1386 | tagging_loss_video: 8.082|tagging_loss_audio: 13.458|tagging_loss_text: 15.909|tagging_loss_image: 10.234|tagging_loss_fusion: 11.063|total_loss: 58.745 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 1387 | tagging_loss_video: 8.758|tagging_loss_audio: 12.351|tagging_loss_text: 12.873|tagging_loss_image: 9.572|tagging_loss_fusion: 10.413|total_loss: 53.968 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 1388 | tagging_loss_video: 8.223|tagging_loss_audio: 11.411|tagging_loss_text: 15.999|tagging_loss_image: 10.416|tagging_loss_fusion: 8.938|total_loss: 54.987 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 1389 | tagging_loss_video: 8.510|tagging_loss_audio: 13.239|tagging_loss_text: 19.164|tagging_loss_image: 10.064|tagging_loss_fusion: 13.046|total_loss: 64.022 | 68.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1390 |tagging_loss_video: 9.135|tagging_loss_audio: 11.511|tagging_loss_text: 16.093|tagging_loss_image: 9.149|tagging_loss_fusion: 9.516|total_loss: 55.404 | Examples/sec: 62.52\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.65 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1391 | tagging_loss_video: 8.683|tagging_loss_audio: 13.295|tagging_loss_text: 16.086|tagging_loss_image: 11.956|tagging_loss_fusion: 10.685|total_loss: 60.705 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 1392 | tagging_loss_video: 8.900|tagging_loss_audio: 13.325|tagging_loss_text: 14.388|tagging_loss_image: 10.813|tagging_loss_fusion: 9.983|total_loss: 57.408 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 1393 | tagging_loss_video: 9.748|tagging_loss_audio: 13.514|tagging_loss_text: 19.070|tagging_loss_image: 11.843|tagging_loss_fusion: 10.984|total_loss: 65.159 | 72.35 Examples/sec\n",
      "INFO:tensorflow:training step 1394 | tagging_loss_video: 8.920|tagging_loss_audio: 14.806|tagging_loss_text: 13.413|tagging_loss_image: 11.504|tagging_loss_fusion: 10.779|total_loss: 59.421 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 1395 | tagging_loss_video: 9.134|tagging_loss_audio: 12.975|tagging_loss_text: 15.749|tagging_loss_image: 11.054|tagging_loss_fusion: 11.797|total_loss: 60.710 | 65.43 Examples/sec\n",
      "INFO:tensorflow:training step 1396 | tagging_loss_video: 8.621|tagging_loss_audio: 11.262|tagging_loss_text: 13.941|tagging_loss_image: 9.976|tagging_loss_fusion: 9.840|total_loss: 53.640 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 1397 | tagging_loss_video: 7.985|tagging_loss_audio: 12.809|tagging_loss_text: 13.226|tagging_loss_image: 9.349|tagging_loss_fusion: 9.492|total_loss: 52.860 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 1398 | tagging_loss_video: 7.967|tagging_loss_audio: 12.960|tagging_loss_text: 16.296|tagging_loss_image: 10.064|tagging_loss_fusion: 10.452|total_loss: 57.740 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 1399 | tagging_loss_video: 8.608|tagging_loss_audio: 13.216|tagging_loss_text: 16.329|tagging_loss_image: 10.551|tagging_loss_fusion: 8.816|total_loss: 57.521 | 69.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1400 |tagging_loss_video: 8.546|tagging_loss_audio: 11.247|tagging_loss_text: 13.625|tagging_loss_image: 9.989|tagging_loss_fusion: 8.758|total_loss: 52.166 | Examples/sec: 69.66\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.69 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 1401 | tagging_loss_video: 8.443|tagging_loss_audio: 12.293|tagging_loss_text: 17.551|tagging_loss_image: 11.418|tagging_loss_fusion: 12.193|total_loss: 61.898 | 67.27 Examples/sec\n",
      "INFO:tensorflow:training step 1402 | tagging_loss_video: 8.059|tagging_loss_audio: 11.899|tagging_loss_text: 15.557|tagging_loss_image: 10.334|tagging_loss_fusion: 9.929|total_loss: 55.778 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 1403 | tagging_loss_video: 8.750|tagging_loss_audio: 13.829|tagging_loss_text: 15.755|tagging_loss_image: 10.010|tagging_loss_fusion: 10.241|total_loss: 58.585 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 1404 | tagging_loss_video: 9.042|tagging_loss_audio: 13.680|tagging_loss_text: 17.344|tagging_loss_image: 11.188|tagging_loss_fusion: 13.544|total_loss: 64.798 | 71.10 Examples/sec\n",
      "INFO:tensorflow:training step 1405 | tagging_loss_video: 8.762|tagging_loss_audio: 12.962|tagging_loss_text: 17.513|tagging_loss_image: 11.156|tagging_loss_fusion: 12.891|total_loss: 63.284 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 1406 | tagging_loss_video: 9.091|tagging_loss_audio: 11.966|tagging_loss_text: 17.009|tagging_loss_image: 8.827|tagging_loss_fusion: 11.579|total_loss: 58.472 | 61.74 Examples/sec\n",
      "INFO:tensorflow:training step 1407 | tagging_loss_video: 10.408|tagging_loss_audio: 13.214|tagging_loss_text: 17.386|tagging_loss_image: 11.105|tagging_loss_fusion: 11.279|total_loss: 63.392 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 1408 | tagging_loss_video: 9.023|tagging_loss_audio: 14.414|tagging_loss_text: 16.526|tagging_loss_image: 9.592|tagging_loss_fusion: 10.652|total_loss: 60.207 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 1409 | tagging_loss_video: 8.992|tagging_loss_audio: 14.105|tagging_loss_text: 11.745|tagging_loss_image: 10.728|tagging_loss_fusion: 13.395|total_loss: 58.965 | 66.32 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1410 |tagging_loss_video: 11.369|tagging_loss_audio: 14.274|tagging_loss_text: 13.457|tagging_loss_image: 11.079|tagging_loss_fusion: 11.953|total_loss: 62.132 | Examples/sec: 67.53\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.67 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1411 | tagging_loss_video: 9.069|tagging_loss_audio: 13.100|tagging_loss_text: 15.113|tagging_loss_image: 10.303|tagging_loss_fusion: 11.441|total_loss: 59.027 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 1412 | tagging_loss_video: 9.334|tagging_loss_audio: 13.719|tagging_loss_text: 18.678|tagging_loss_image: 10.097|tagging_loss_fusion: 11.170|total_loss: 63.000 | 63.42 Examples/sec\n",
      "INFO:tensorflow:training step 1413 | tagging_loss_video: 9.359|tagging_loss_audio: 11.609|tagging_loss_text: 14.679|tagging_loss_image: 10.857|tagging_loss_fusion: 10.049|total_loss: 56.555 | 67.39 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 1413.\n",
      "INFO:tensorflow:training step 1414 | tagging_loss_video: 10.771|tagging_loss_audio: 13.527|tagging_loss_text: 16.262|tagging_loss_image: 11.427|tagging_loss_fusion: 10.974|total_loss: 62.960 | 44.46 Examples/sec\n",
      "INFO:tensorflow:training step 1415 | tagging_loss_video: 10.640|tagging_loss_audio: 14.370|tagging_loss_text: 17.374|tagging_loss_image: 11.817|tagging_loss_fusion: 13.685|total_loss: 67.886 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 1416 | tagging_loss_video: 9.536|tagging_loss_audio: 13.766|tagging_loss_text: 14.621|tagging_loss_image: 10.268|tagging_loss_fusion: 12.900|total_loss: 61.091 | 66.66 Examples/sec\n",
      "INFO:tensorflow:training step 1417 | tagging_loss_video: 9.937|tagging_loss_audio: 14.965|tagging_loss_text: 18.377|tagging_loss_image: 11.191|tagging_loss_fusion: 10.311|total_loss: 64.781 | 71.85 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.10071\n",
      "INFO:tensorflow:training step 1418 | tagging_loss_video: 7.865|tagging_loss_audio: 12.362|tagging_loss_text: 15.392|tagging_loss_image: 11.309|tagging_loss_fusion: 10.213|total_loss: 57.142 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 1419 | tagging_loss_video: 9.379|tagging_loss_audio: 13.498|tagging_loss_text: 16.610|tagging_loss_image: 10.769|tagging_loss_fusion: 11.203|total_loss: 61.459 | 65.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1420 |tagging_loss_video: 9.032|tagging_loss_audio: 13.747|tagging_loss_text: 14.487|tagging_loss_image: 12.557|tagging_loss_fusion: 12.658|total_loss: 62.482 | Examples/sec: 69.93\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.63 | precision@0.5: 0.86 |recall@0.1: 0.93 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 1421 | tagging_loss_video: 8.673|tagging_loss_audio: 14.005|tagging_loss_text: 15.359|tagging_loss_image: 11.461|tagging_loss_fusion: 10.281|total_loss: 59.779 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 1422 | tagging_loss_video: 8.553|tagging_loss_audio: 13.858|tagging_loss_text: 16.846|tagging_loss_image: 10.430|tagging_loss_fusion: 12.447|total_loss: 62.133 | 64.40 Examples/sec\n",
      "INFO:tensorflow:training step 1423 | tagging_loss_video: 9.934|tagging_loss_audio: 13.154|tagging_loss_text: 17.576|tagging_loss_image: 13.940|tagging_loss_fusion: 13.068|total_loss: 67.672 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 1424 | tagging_loss_video: 8.147|tagging_loss_audio: 11.275|tagging_loss_text: 16.695|tagging_loss_image: 10.869|tagging_loss_fusion: 9.296|total_loss: 56.281 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 1425 | tagging_loss_video: 8.312|tagging_loss_audio: 12.328|tagging_loss_text: 17.297|tagging_loss_image: 9.686|tagging_loss_fusion: 9.343|total_loss: 56.966 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 1426 | tagging_loss_video: 8.189|tagging_loss_audio: 12.786|tagging_loss_text: 17.551|tagging_loss_image: 10.116|tagging_loss_fusion: 11.146|total_loss: 59.787 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 1427 | tagging_loss_video: 9.176|tagging_loss_audio: 14.636|tagging_loss_text: 13.751|tagging_loss_image: 11.242|tagging_loss_fusion: 10.103|total_loss: 58.908 | 67.58 Examples/sec\n",
      "INFO:tensorflow:training step 1428 | tagging_loss_video: 8.658|tagging_loss_audio: 13.064|tagging_loss_text: 16.006|tagging_loss_image: 9.850|tagging_loss_fusion: 9.919|total_loss: 57.497 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 1429 | tagging_loss_video: 9.577|tagging_loss_audio: 13.349|tagging_loss_text: 16.318|tagging_loss_image: 11.768|tagging_loss_fusion: 12.733|total_loss: 63.745 | 71.41 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1430 |tagging_loss_video: 9.139|tagging_loss_audio: 12.193|tagging_loss_text: 16.283|tagging_loss_image: 10.394|tagging_loss_fusion: 12.002|total_loss: 60.011 | Examples/sec: 63.97\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.61 | precision@0.5: 0.84 |recall@0.1: 0.95 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1431 | tagging_loss_video: 7.959|tagging_loss_audio: 12.868|tagging_loss_text: 13.870|tagging_loss_image: 9.210|tagging_loss_fusion: 10.990|total_loss: 54.897 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 1432 | tagging_loss_video: 8.702|tagging_loss_audio: 12.338|tagging_loss_text: 17.079|tagging_loss_image: 9.831|tagging_loss_fusion: 11.429|total_loss: 59.380 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 1433 | tagging_loss_video: 8.219|tagging_loss_audio: 12.645|tagging_loss_text: 14.924|tagging_loss_image: 9.391|tagging_loss_fusion: 9.654|total_loss: 54.833 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 1434 | tagging_loss_video: 9.744|tagging_loss_audio: 12.741|tagging_loss_text: 14.767|tagging_loss_image: 12.205|tagging_loss_fusion: 10.756|total_loss: 60.213 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 1435 | tagging_loss_video: 8.259|tagging_loss_audio: 13.320|tagging_loss_text: 17.154|tagging_loss_image: 10.390|tagging_loss_fusion: 11.760|total_loss: 60.884 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 1436 | tagging_loss_video: 8.379|tagging_loss_audio: 11.945|tagging_loss_text: 17.979|tagging_loss_image: 9.598|tagging_loss_fusion: 8.543|total_loss: 56.444 | 64.64 Examples/sec\n",
      "INFO:tensorflow:training step 1437 | tagging_loss_video: 8.236|tagging_loss_audio: 12.078|tagging_loss_text: 15.348|tagging_loss_image: 9.458|tagging_loss_fusion: 11.365|total_loss: 56.486 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 1438 | tagging_loss_video: 8.427|tagging_loss_audio: 11.326|tagging_loss_text: 15.572|tagging_loss_image: 10.173|tagging_loss_fusion: 10.698|total_loss: 56.196 | 65.37 Examples/sec\n",
      "INFO:tensorflow:training step 1439 | tagging_loss_video: 9.204|tagging_loss_audio: 11.727|tagging_loss_text: 18.633|tagging_loss_image: 10.015|tagging_loss_fusion: 10.292|total_loss: 59.871 | 66.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1440 |tagging_loss_video: 8.299|tagging_loss_audio: 10.317|tagging_loss_text: 15.110|tagging_loss_image: 8.773|tagging_loss_fusion: 9.213|total_loss: 51.713 | Examples/sec: 70.31\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.60 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1441 | tagging_loss_video: 7.668|tagging_loss_audio: 12.108|tagging_loss_text: 15.435|tagging_loss_image: 9.623|tagging_loss_fusion: 9.378|total_loss: 54.212 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 1442 | tagging_loss_video: 8.238|tagging_loss_audio: 12.748|tagging_loss_text: 16.736|tagging_loss_image: 10.228|tagging_loss_fusion: 9.529|total_loss: 57.480 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 1443 | tagging_loss_video: 7.842|tagging_loss_audio: 12.131|tagging_loss_text: 14.596|tagging_loss_image: 9.128|tagging_loss_fusion: 7.498|total_loss: 51.194 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 1444 | tagging_loss_video: 8.755|tagging_loss_audio: 11.023|tagging_loss_text: 16.209|tagging_loss_image: 9.372|tagging_loss_fusion: 11.285|total_loss: 56.644 | 60.02 Examples/sec\n",
      "INFO:tensorflow:training step 1445 | tagging_loss_video: 7.810|tagging_loss_audio: 12.525|tagging_loss_text: 16.724|tagging_loss_image: 10.688|tagging_loss_fusion: 7.872|total_loss: 55.619 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 1446 | tagging_loss_video: 7.449|tagging_loss_audio: 13.028|tagging_loss_text: 15.327|tagging_loss_image: 9.471|tagging_loss_fusion: 8.119|total_loss: 53.394 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 1447 | tagging_loss_video: 8.277|tagging_loss_audio: 9.455|tagging_loss_text: 16.952|tagging_loss_image: 9.951|tagging_loss_fusion: 10.326|total_loss: 54.961 | 63.67 Examples/sec\n",
      "INFO:tensorflow:training step 1448 | tagging_loss_video: 9.747|tagging_loss_audio: 12.788|tagging_loss_text: 17.484|tagging_loss_image: 10.562|tagging_loss_fusion: 10.642|total_loss: 61.222 | 66.65 Examples/sec\n",
      "INFO:tensorflow:training step 1449 | tagging_loss_video: 11.076|tagging_loss_audio: 14.140|tagging_loss_text: 17.618|tagging_loss_image: 11.287|tagging_loss_fusion: 11.397|total_loss: 65.518 | 70.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1450 |tagging_loss_video: 8.956|tagging_loss_audio: 14.048|tagging_loss_text: 14.900|tagging_loss_image: 10.781|tagging_loss_fusion: 12.542|total_loss: 61.228 | Examples/sec: 64.25\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.61 | precision@0.5: 0.87 |recall@0.1: 0.93 | recall@0.5: 0.71\n",
      "INFO:tensorflow:training step 1451 | tagging_loss_video: 9.050|tagging_loss_audio: 13.324|tagging_loss_text: 15.773|tagging_loss_image: 10.586|tagging_loss_fusion: 10.818|total_loss: 59.552 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 1452 | tagging_loss_video: 10.225|tagging_loss_audio: 12.062|tagging_loss_text: 16.438|tagging_loss_image: 11.779|tagging_loss_fusion: 12.819|total_loss: 63.323 | 66.32 Examples/sec\n",
      "INFO:tensorflow:training step 1453 | tagging_loss_video: 8.776|tagging_loss_audio: 11.311|tagging_loss_text: 14.752|tagging_loss_image: 9.347|tagging_loss_fusion: 8.067|total_loss: 52.253 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 1454 | tagging_loss_video: 8.877|tagging_loss_audio: 12.535|tagging_loss_text: 17.347|tagging_loss_image: 11.897|tagging_loss_fusion: 13.730|total_loss: 64.386 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 1455 | tagging_loss_video: 8.840|tagging_loss_audio: 13.691|tagging_loss_text: 9.953|tagging_loss_image: 10.550|tagging_loss_fusion: 9.265|total_loss: 52.299 | 60.46 Examples/sec\n",
      "INFO:tensorflow:training step 1456 | tagging_loss_video: 9.102|tagging_loss_audio: 14.676|tagging_loss_text: 17.303|tagging_loss_image: 11.646|tagging_loss_fusion: 10.929|total_loss: 63.655 | 67.32 Examples/sec\n",
      "INFO:tensorflow:training step 1457 | tagging_loss_video: 8.916|tagging_loss_audio: 10.665|tagging_loss_text: 16.655|tagging_loss_image: 12.359|tagging_loss_fusion: 10.128|total_loss: 58.721 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 1458 | tagging_loss_video: 7.730|tagging_loss_audio: 11.797|tagging_loss_text: 13.122|tagging_loss_image: 9.650|tagging_loss_fusion: 9.800|total_loss: 52.099 | 64.52 Examples/sec\n",
      "INFO:tensorflow:training step 1459 | tagging_loss_video: 8.693|tagging_loss_audio: 11.748|tagging_loss_text: 16.356|tagging_loss_image: 10.692|tagging_loss_fusion: 9.433|total_loss: 56.923 | 68.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1460 |tagging_loss_video: 8.735|tagging_loss_audio: 12.879|tagging_loss_text: 16.667|tagging_loss_image: 12.041|tagging_loss_fusion: 8.164|total_loss: 58.487 | Examples/sec: 67.65\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.70 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 1461 | tagging_loss_video: 7.959|tagging_loss_audio: 13.183|tagging_loss_text: 14.138|tagging_loss_image: 9.585|tagging_loss_fusion: 9.949|total_loss: 54.816 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 1462 | tagging_loss_video: 8.607|tagging_loss_audio: 13.148|tagging_loss_text: 14.885|tagging_loss_image: 11.017|tagging_loss_fusion: 8.276|total_loss: 55.932 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 1463 | tagging_loss_video: 8.804|tagging_loss_audio: 11.145|tagging_loss_text: 18.206|tagging_loss_image: 9.870|tagging_loss_fusion: 8.240|total_loss: 56.265 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 1464 | tagging_loss_video: 7.758|tagging_loss_audio: 11.834|tagging_loss_text: 12.392|tagging_loss_image: 7.776|tagging_loss_fusion: 11.634|total_loss: 51.394 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 1465 | tagging_loss_video: 9.621|tagging_loss_audio: 14.453|tagging_loss_text: 15.589|tagging_loss_image: 10.458|tagging_loss_fusion: 11.201|total_loss: 61.323 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 1466 | tagging_loss_video: 8.150|tagging_loss_audio: 12.685|tagging_loss_text: 15.032|tagging_loss_image: 9.783|tagging_loss_fusion: 8.116|total_loss: 53.765 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 1467 | tagging_loss_video: 8.506|tagging_loss_audio: 12.619|tagging_loss_text: 15.241|tagging_loss_image: 10.607|tagging_loss_fusion: 11.430|total_loss: 58.402 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 1468 | tagging_loss_video: 7.903|tagging_loss_audio: 10.626|tagging_loss_text: 14.714|tagging_loss_image: 9.208|tagging_loss_fusion: 9.324|total_loss: 51.774 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 1469 | tagging_loss_video: 9.594|tagging_loss_audio: 14.278|tagging_loss_text: 15.201|tagging_loss_image: 10.718|tagging_loss_fusion: 8.927|total_loss: 58.718 | 62.54 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1470 |tagging_loss_video: 8.523|tagging_loss_audio: 9.745|tagging_loss_text: 11.424|tagging_loss_image: 9.744|tagging_loss_fusion: 9.489|total_loss: 48.925 | Examples/sec: 68.59\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.65 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1471 | tagging_loss_video: 8.755|tagging_loss_audio: 13.901|tagging_loss_text: 14.323|tagging_loss_image: 10.200|tagging_loss_fusion: 9.899|total_loss: 57.077 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 1472 | tagging_loss_video: 7.940|tagging_loss_audio: 11.721|tagging_loss_text: 12.421|tagging_loss_image: 10.618|tagging_loss_fusion: 10.641|total_loss: 53.341 | 63.78 Examples/sec\n",
      "INFO:tensorflow:training step 1473 | tagging_loss_video: 9.324|tagging_loss_audio: 13.083|tagging_loss_text: 15.298|tagging_loss_image: 10.439|tagging_loss_fusion: 9.426|total_loss: 57.570 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 1474 | tagging_loss_video: 8.115|tagging_loss_audio: 13.429|tagging_loss_text: 12.483|tagging_loss_image: 10.609|tagging_loss_fusion: 7.587|total_loss: 52.222 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 1475 | tagging_loss_video: 8.189|tagging_loss_audio: 12.298|tagging_loss_text: 14.726|tagging_loss_image: 10.174|tagging_loss_fusion: 9.425|total_loss: 54.812 | 62.97 Examples/sec\n",
      "INFO:tensorflow:training step 1476 | tagging_loss_video: 9.316|tagging_loss_audio: 12.559|tagging_loss_text: 16.264|tagging_loss_image: 11.308|tagging_loss_fusion: 10.901|total_loss: 60.349 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 1477 | tagging_loss_video: 7.572|tagging_loss_audio: 12.998|tagging_loss_text: 15.331|tagging_loss_image: 10.476|tagging_loss_fusion: 10.222|total_loss: 56.600 | 64.60 Examples/sec\n",
      "INFO:tensorflow:training step 1478 | tagging_loss_video: 8.885|tagging_loss_audio: 13.872|tagging_loss_text: 13.917|tagging_loss_image: 11.318|tagging_loss_fusion: 11.742|total_loss: 59.734 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 1479 | tagging_loss_video: 8.241|tagging_loss_audio: 13.090|tagging_loss_text: 18.393|tagging_loss_image: 11.117|tagging_loss_fusion: 10.205|total_loss: 61.047 | 69.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1480 |tagging_loss_video: 9.257|tagging_loss_audio: 12.696|tagging_loss_text: 16.832|tagging_loss_image: 9.905|tagging_loss_fusion: 10.357|total_loss: 59.046 | Examples/sec: 62.73\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.61 | precision@0.5: 0.85 |recall@0.1: 0.96 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 1481 | tagging_loss_video: 8.230|tagging_loss_audio: 13.433|tagging_loss_text: 15.558|tagging_loss_image: 11.055|tagging_loss_fusion: 10.288|total_loss: 58.564 | 70.06 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1482 | tagging_loss_video: 8.855|tagging_loss_audio: 12.747|tagging_loss_text: 14.026|tagging_loss_image: 11.305|tagging_loss_fusion: 11.396|total_loss: 58.329 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 1483 | tagging_loss_video: 8.646|tagging_loss_audio: 13.850|tagging_loss_text: 16.660|tagging_loss_image: 10.136|tagging_loss_fusion: 9.862|total_loss: 59.154 | 63.48 Examples/sec\n",
      "INFO:tensorflow:training step 1484 | tagging_loss_video: 8.894|tagging_loss_audio: 13.653|tagging_loss_text: 16.702|tagging_loss_image: 10.001|tagging_loss_fusion: 11.100|total_loss: 60.350 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 1485 | tagging_loss_video: 9.580|tagging_loss_audio: 11.902|tagging_loss_text: 14.864|tagging_loss_image: 10.928|tagging_loss_fusion: 9.645|total_loss: 56.920 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 1486 | tagging_loss_video: 10.474|tagging_loss_audio: 13.649|tagging_loss_text: 16.917|tagging_loss_image: 11.792|tagging_loss_fusion: 10.890|total_loss: 63.721 | 63.58 Examples/sec\n",
      "INFO:tensorflow:training step 1487 | tagging_loss_video: 8.776|tagging_loss_audio: 12.341|tagging_loss_text: 18.028|tagging_loss_image: 9.755|tagging_loss_fusion: 9.922|total_loss: 58.821 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 1488 | tagging_loss_video: 9.351|tagging_loss_audio: 14.070|tagging_loss_text: 15.225|tagging_loss_image: 10.369|tagging_loss_fusion: 10.115|total_loss: 59.129 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 1489 | tagging_loss_video: 7.819|tagging_loss_audio: 11.165|tagging_loss_text: 14.202|tagging_loss_image: 9.874|tagging_loss_fusion: 12.082|total_loss: 55.142 | 64.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1490 |tagging_loss_video: 8.090|tagging_loss_audio: 11.189|tagging_loss_text: 14.711|tagging_loss_image: 11.199|tagging_loss_fusion: 9.736|total_loss: 54.926 | Examples/sec: 68.35\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.64 | precision@0.5: 0.86 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1491 | tagging_loss_video: 7.797|tagging_loss_audio: 11.098|tagging_loss_text: 14.391|tagging_loss_image: 10.123|tagging_loss_fusion: 8.843|total_loss: 52.250 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 1492 | tagging_loss_video: 8.851|tagging_loss_audio: 13.180|tagging_loss_text: 15.769|tagging_loss_image: 11.200|tagging_loss_fusion: 10.259|total_loss: 59.259 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 1493 | tagging_loss_video: 8.529|tagging_loss_audio: 14.566|tagging_loss_text: 19.478|tagging_loss_image: 12.028|tagging_loss_fusion: 9.230|total_loss: 63.830 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 1494 | tagging_loss_video: 9.679|tagging_loss_audio: 12.287|tagging_loss_text: 16.762|tagging_loss_image: 12.138|tagging_loss_fusion: 10.980|total_loss: 61.847 | 63.97 Examples/sec\n",
      "INFO:tensorflow:training step 1495 | tagging_loss_video: 8.528|tagging_loss_audio: 11.974|tagging_loss_text: 13.411|tagging_loss_image: 9.701|tagging_loss_fusion: 10.917|total_loss: 54.531 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 1496 | tagging_loss_video: 9.251|tagging_loss_audio: 13.811|tagging_loss_text: 18.995|tagging_loss_image: 12.467|tagging_loss_fusion: 9.869|total_loss: 64.393 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 1497 | tagging_loss_video: 8.570|tagging_loss_audio: 11.418|tagging_loss_text: 15.171|tagging_loss_image: 10.181|tagging_loss_fusion: 9.446|total_loss: 54.786 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 1498 | tagging_loss_video: 7.575|tagging_loss_audio: 12.459|tagging_loss_text: 14.527|tagging_loss_image: 8.657|tagging_loss_fusion: 9.368|total_loss: 52.586 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 1499 | tagging_loss_video: 8.709|tagging_loss_audio: 11.362|tagging_loss_text: 16.319|tagging_loss_image: 9.888|tagging_loss_fusion: 10.147|total_loss: 56.425 | 70.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1500 |tagging_loss_video: 7.552|tagging_loss_audio: 7.933|tagging_loss_text: 12.351|tagging_loss_image: 9.063|tagging_loss_fusion: 9.388|total_loss: 46.287 | Examples/sec: 63.57\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.59 | precision@0.5: 0.85 |recall@0.1: 0.97 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1501 | tagging_loss_video: 8.959|tagging_loss_audio: 12.756|tagging_loss_text: 15.524|tagging_loss_image: 10.641|tagging_loss_fusion: 10.862|total_loss: 58.742 | 67.29 Examples/sec\n",
      "INFO:tensorflow:training step 1502 | tagging_loss_video: 8.110|tagging_loss_audio: 12.649|tagging_loss_text: 15.846|tagging_loss_image: 10.568|tagging_loss_fusion: 10.663|total_loss: 57.837 | 67.67 Examples/sec\n",
      "INFO:tensorflow:training step 1503 | tagging_loss_video: 10.344|tagging_loss_audio: 11.053|tagging_loss_text: 17.276|tagging_loss_image: 10.197|tagging_loss_fusion: 10.333|total_loss: 59.203 | 72.21 Examples/sec\n",
      "INFO:tensorflow:training step 1504 | tagging_loss_video: 7.504|tagging_loss_audio: 11.883|tagging_loss_text: 16.695|tagging_loss_image: 8.448|tagging_loss_fusion: 9.751|total_loss: 54.280 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 1505 | tagging_loss_video: 10.032|tagging_loss_audio: 14.028|tagging_loss_text: 14.951|tagging_loss_image: 11.307|tagging_loss_fusion: 11.545|total_loss: 61.863 | 72.20 Examples/sec\n",
      "INFO:tensorflow:training step 1506 | tagging_loss_video: 6.900|tagging_loss_audio: 10.544|tagging_loss_text: 17.286|tagging_loss_image: 10.736|tagging_loss_fusion: 7.900|total_loss: 53.365 | 66.76 Examples/sec\n",
      "INFO:tensorflow:training step 1507 | tagging_loss_video: 7.653|tagging_loss_audio: 11.566|tagging_loss_text: 14.282|tagging_loss_image: 10.011|tagging_loss_fusion: 8.230|total_loss: 51.742 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 1508 | tagging_loss_video: 7.553|tagging_loss_audio: 12.224|tagging_loss_text: 14.832|tagging_loss_image: 8.096|tagging_loss_fusion: 8.629|total_loss: 51.334 | 62.04 Examples/sec\n",
      "INFO:tensorflow:training step 1509 | tagging_loss_video: 9.013|tagging_loss_audio: 11.097|tagging_loss_text: 15.989|tagging_loss_image: 10.204|tagging_loss_fusion: 9.852|total_loss: 56.155 | 69.15 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1510 |tagging_loss_video: 9.456|tagging_loss_audio: 13.076|tagging_loss_text: 13.181|tagging_loss_image: 10.830|tagging_loss_fusion: 10.926|total_loss: 57.469 | Examples/sec: 70.04\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.69 | precision@0.5: 0.90 |recall@0.1: 0.93 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1511 | tagging_loss_video: 8.339|tagging_loss_audio: 12.049|tagging_loss_text: 15.465|tagging_loss_image: 8.657|tagging_loss_fusion: 11.306|total_loss: 55.816 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 1512 | tagging_loss_video: 9.738|tagging_loss_audio: 11.907|tagging_loss_text: 13.469|tagging_loss_image: 9.421|tagging_loss_fusion: 11.862|total_loss: 56.396 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 1513 | tagging_loss_video: 8.169|tagging_loss_audio: 13.333|tagging_loss_text: 15.240|tagging_loss_image: 10.372|tagging_loss_fusion: 10.318|total_loss: 57.431 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 1514 | tagging_loss_video: 9.691|tagging_loss_audio: 11.401|tagging_loss_text: 13.690|tagging_loss_image: 9.656|tagging_loss_fusion: 11.064|total_loss: 55.502 | 65.05 Examples/sec\n",
      "INFO:tensorflow:training step 1515 | tagging_loss_video: 8.316|tagging_loss_audio: 10.974|tagging_loss_text: 12.432|tagging_loss_image: 8.810|tagging_loss_fusion: 10.241|total_loss: 50.773 | 68.75 Examples/sec\n",
      "INFO:tensorflow:training step 1516 | tagging_loss_video: 9.419|tagging_loss_audio: 13.088|tagging_loss_text: 15.907|tagging_loss_image: 9.482|tagging_loss_fusion: 9.173|total_loss: 57.068 | 72.34 Examples/sec\n",
      "INFO:tensorflow:training step 1517 | tagging_loss_video: 8.686|tagging_loss_audio: 11.461|tagging_loss_text: 14.024|tagging_loss_image: 10.064|tagging_loss_fusion: 11.197|total_loss: 55.430 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 1518 | tagging_loss_video: 8.333|tagging_loss_audio: 13.396|tagging_loss_text: 16.794|tagging_loss_image: 10.532|tagging_loss_fusion: 11.224|total_loss: 60.279 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 1519 | tagging_loss_video: 9.165|tagging_loss_audio: 12.591|tagging_loss_text: 15.301|tagging_loss_image: 10.852|tagging_loss_fusion: 8.799|total_loss: 56.709 | 65.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1520 |tagging_loss_video: 9.153|tagging_loss_audio: 12.689|tagging_loss_text: 16.235|tagging_loss_image: 9.570|tagging_loss_fusion: 9.234|total_loss: 56.881 | Examples/sec: 65.19\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.65 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1521 | tagging_loss_video: 8.124|tagging_loss_audio: 12.895|tagging_loss_text: 15.028|tagging_loss_image: 9.222|tagging_loss_fusion: 12.833|total_loss: 58.102 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 1522 | tagging_loss_video: 9.871|tagging_loss_audio: 13.125|tagging_loss_text: 15.096|tagging_loss_image: 11.100|tagging_loss_fusion: 12.328|total_loss: 61.519 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 1523 | tagging_loss_video: 8.892|tagging_loss_audio: 12.614|tagging_loss_text: 14.644|tagging_loss_image: 9.534|tagging_loss_fusion: 10.496|total_loss: 56.180 | 71.99 Examples/sec\n",
      "INFO:tensorflow:training step 1524 | tagging_loss_video: 6.639|tagging_loss_audio: 10.819|tagging_loss_text: 14.822|tagging_loss_image: 8.269|tagging_loss_fusion: 9.749|total_loss: 50.299 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 1525 | tagging_loss_video: 8.695|tagging_loss_audio: 11.607|tagging_loss_text: 14.006|tagging_loss_image: 9.526|tagging_loss_fusion: 8.817|total_loss: 52.652 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 1526 | tagging_loss_video: 8.300|tagging_loss_audio: 12.047|tagging_loss_text: 12.913|tagging_loss_image: 9.424|tagging_loss_fusion: 9.247|total_loss: 51.932 | 61.13 Examples/sec\n",
      "INFO:tensorflow:training step 1527 | tagging_loss_video: 8.360|tagging_loss_audio: 12.991|tagging_loss_text: 16.571|tagging_loss_image: 9.738|tagging_loss_fusion: 10.470|total_loss: 58.130 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 1528 | tagging_loss_video: 7.647|tagging_loss_audio: 13.146|tagging_loss_text: 12.574|tagging_loss_image: 9.941|tagging_loss_fusion: 9.564|total_loss: 52.872 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 1529 | tagging_loss_video: 7.639|tagging_loss_audio: 10.761|tagging_loss_text: 15.412|tagging_loss_image: 8.284|tagging_loss_fusion: 11.859|total_loss: 53.956 | 65.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1530 |tagging_loss_video: 9.530|tagging_loss_audio: 12.082|tagging_loss_text: 13.315|tagging_loss_image: 11.574|tagging_loss_fusion: 13.432|total_loss: 59.934 | Examples/sec: 68.95\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.63 | precision@0.5: 0.88 |recall@0.1: 0.92 | recall@0.5: 0.69\n",
      "INFO:tensorflow:training step 1531 | tagging_loss_video: 8.392|tagging_loss_audio: 8.771|tagging_loss_text: 15.653|tagging_loss_image: 8.647|tagging_loss_fusion: 10.102|total_loss: 51.565 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 1532 | tagging_loss_video: 9.268|tagging_loss_audio: 15.059|tagging_loss_text: 14.130|tagging_loss_image: 10.199|tagging_loss_fusion: 11.780|total_loss: 60.436 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 1533 | tagging_loss_video: 9.405|tagging_loss_audio: 12.340|tagging_loss_text: 17.898|tagging_loss_image: 10.909|tagging_loss_fusion: 12.075|total_loss: 62.627 | 66.53 Examples/sec\n",
      "INFO:tensorflow:training step 1534 | tagging_loss_video: 9.072|tagging_loss_audio: 11.561|tagging_loss_text: 13.070|tagging_loss_image: 10.153|tagging_loss_fusion: 9.546|total_loss: 53.402 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 1535 | tagging_loss_video: 7.351|tagging_loss_audio: 10.982|tagging_loss_text: 10.821|tagging_loss_image: 9.465|tagging_loss_fusion: 9.040|total_loss: 47.659 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 1536 | tagging_loss_video: 8.386|tagging_loss_audio: 12.196|tagging_loss_text: 11.902|tagging_loss_image: 9.467|tagging_loss_fusion: 7.347|total_loss: 49.299 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 1537 | tagging_loss_video: 7.753|tagging_loss_audio: 10.960|tagging_loss_text: 13.679|tagging_loss_image: 8.893|tagging_loss_fusion: 8.267|total_loss: 49.554 | 66.61 Examples/sec\n",
      "INFO:tensorflow:training step 1538 | tagging_loss_video: 7.286|tagging_loss_audio: 11.103|tagging_loss_text: 15.705|tagging_loss_image: 10.274|tagging_loss_fusion: 8.522|total_loss: 52.890 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 1539 | tagging_loss_video: 7.823|tagging_loss_audio: 10.892|tagging_loss_text: 16.379|tagging_loss_image: 9.684|tagging_loss_fusion: 8.908|total_loss: 53.686 | 71.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1540 |tagging_loss_video: 8.986|tagging_loss_audio: 13.759|tagging_loss_text: 13.316|tagging_loss_image: 10.355|tagging_loss_fusion: 10.259|total_loss: 56.675 | Examples/sec: 63.28\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.64 | precision@0.5: 0.85 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1541 | tagging_loss_video: 8.014|tagging_loss_audio: 12.609|tagging_loss_text: 17.534|tagging_loss_image: 9.253|tagging_loss_fusion: 10.798|total_loss: 58.209 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 1542 | tagging_loss_video: 10.016|tagging_loss_audio: 11.550|tagging_loss_text: 14.234|tagging_loss_image: 11.108|tagging_loss_fusion: 11.029|total_loss: 57.938 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 1543 | tagging_loss_video: 8.616|tagging_loss_audio: 13.009|tagging_loss_text: 18.689|tagging_loss_image: 10.629|tagging_loss_fusion: 9.709|total_loss: 60.652 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 1544 | tagging_loss_video: 10.464|tagging_loss_audio: 13.865|tagging_loss_text: 13.337|tagging_loss_image: 10.639|tagging_loss_fusion: 9.870|total_loss: 58.175 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 1545 | tagging_loss_video: 8.913|tagging_loss_audio: 10.820|tagging_loss_text: 19.032|tagging_loss_image: 10.077|tagging_loss_fusion: 11.002|total_loss: 59.843 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 1546 | tagging_loss_video: 9.050|tagging_loss_audio: 13.618|tagging_loss_text: 20.338|tagging_loss_image: 10.956|tagging_loss_fusion: 12.689|total_loss: 66.650 | 68.94 Examples/sec\n",
      "INFO:tensorflow:training step 1547 | tagging_loss_video: 9.004|tagging_loss_audio: 12.024|tagging_loss_text: 15.269|tagging_loss_image: 10.403|tagging_loss_fusion: 11.006|total_loss: 57.705 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 1548 | tagging_loss_video: 9.226|tagging_loss_audio: 12.781|tagging_loss_text: 15.818|tagging_loss_image: 10.250|tagging_loss_fusion: 9.702|total_loss: 57.777 | 65.69 Examples/sec\n",
      "INFO:tensorflow:training step 1549 | tagging_loss_video: 10.095|tagging_loss_audio: 14.898|tagging_loss_text: 19.419|tagging_loss_image: 11.930|tagging_loss_fusion: 10.579|total_loss: 66.920 | 69.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1550 |tagging_loss_video: 8.769|tagging_loss_audio: 13.225|tagging_loss_text: 15.731|tagging_loss_image: 9.330|tagging_loss_fusion: 11.166|total_loss: 58.221 | Examples/sec: 65.24\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.64 | precision@0.5: 0.84 |recall@0.1: 0.96 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1551 | tagging_loss_video: 9.631|tagging_loss_audio: 11.812|tagging_loss_text: 17.959|tagging_loss_image: 9.463|tagging_loss_fusion: 11.575|total_loss: 60.440 | 66.78 Examples/sec\n",
      "INFO:tensorflow:training step 1552 | tagging_loss_video: 9.118|tagging_loss_audio: 12.171|tagging_loss_text: 15.292|tagging_loss_image: 9.672|tagging_loss_fusion: 10.902|total_loss: 57.155 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 1553 | tagging_loss_video: 10.232|tagging_loss_audio: 14.633|tagging_loss_text: 12.357|tagging_loss_image: 12.616|tagging_loss_fusion: 11.512|total_loss: 61.351 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 1554 | tagging_loss_video: 8.817|tagging_loss_audio: 15.198|tagging_loss_text: 14.788|tagging_loss_image: 11.012|tagging_loss_fusion: 10.714|total_loss: 60.529 | 65.19 Examples/sec\n",
      "INFO:tensorflow:training step 1555 | tagging_loss_video: 10.901|tagging_loss_audio: 16.589|tagging_loss_text: 16.554|tagging_loss_image: 11.939|tagging_loss_fusion: 12.107|total_loss: 68.090 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 1556 | tagging_loss_video: 8.567|tagging_loss_audio: 9.248|tagging_loss_text: 15.852|tagging_loss_image: 10.157|tagging_loss_fusion: 10.297|total_loss: 54.121 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 1557 | tagging_loss_video: 9.229|tagging_loss_audio: 14.967|tagging_loss_text: 16.221|tagging_loss_image: 10.860|tagging_loss_fusion: 11.442|total_loss: 62.720 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 1558 | tagging_loss_video: 7.457|tagging_loss_audio: 10.290|tagging_loss_text: 11.763|tagging_loss_image: 9.238|tagging_loss_fusion: 11.423|total_loss: 50.171 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 1559 | tagging_loss_video: 9.629|tagging_loss_audio: 16.500|tagging_loss_text: 12.287|tagging_loss_image: 11.814|tagging_loss_fusion: 11.962|total_loss: 62.191 | 61.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1560 |tagging_loss_video: 10.190|tagging_loss_audio: 12.103|tagging_loss_text: 16.605|tagging_loss_image: 12.052|tagging_loss_fusion: 12.723|total_loss: 63.673 | Examples/sec: 71.12\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.62 | precision@0.5: 0.85 |recall@0.1: 0.93 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1561 | tagging_loss_video: 8.548|tagging_loss_audio: 11.204|tagging_loss_text: 16.698|tagging_loss_image: 10.974|tagging_loss_fusion: 10.268|total_loss: 57.693 | 67.61 Examples/sec\n",
      "INFO:tensorflow:training step 1562 | tagging_loss_video: 8.588|tagging_loss_audio: 12.329|tagging_loss_text: 15.413|tagging_loss_image: 10.731|tagging_loss_fusion: 10.754|total_loss: 57.815 | 65.76 Examples/sec\n",
      "INFO:tensorflow:training step 1563 | tagging_loss_video: 9.969|tagging_loss_audio: 16.541|tagging_loss_text: 18.309|tagging_loss_image: 11.128|tagging_loss_fusion: 15.768|total_loss: 71.715 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 1564 | tagging_loss_video: 8.765|tagging_loss_audio: 12.231|tagging_loss_text: 17.313|tagging_loss_image: 9.129|tagging_loss_fusion: 10.182|total_loss: 57.621 | 68.87 Examples/sec\n",
      "INFO:tensorflow:training step 1565 | tagging_loss_video: 8.562|tagging_loss_audio: 13.180|tagging_loss_text: 12.071|tagging_loss_image: 9.502|tagging_loss_fusion: 10.000|total_loss: 53.314 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 1566 | tagging_loss_video: 7.754|tagging_loss_audio: 10.558|tagging_loss_text: 14.858|tagging_loss_image: 9.744|tagging_loss_fusion: 13.649|total_loss: 56.563 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 1567 | tagging_loss_video: 8.950|tagging_loss_audio: 10.726|tagging_loss_text: 15.089|tagging_loss_image: 11.048|tagging_loss_fusion: 10.977|total_loss: 56.789 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 1568 | tagging_loss_video: 8.291|tagging_loss_audio: 13.375|tagging_loss_text: 14.076|tagging_loss_image: 10.370|tagging_loss_fusion: 9.857|total_loss: 55.970 | 67.31 Examples/sec\n",
      "INFO:tensorflow:training step 1569 | tagging_loss_video: 8.895|tagging_loss_audio: 13.484|tagging_loss_text: 18.595|tagging_loss_image: 11.496|tagging_loss_fusion: 13.002|total_loss: 65.471 | 70.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1570 |tagging_loss_video: 10.820|tagging_loss_audio: 12.186|tagging_loss_text: 18.600|tagging_loss_image: 11.475|tagging_loss_fusion: 10.830|total_loss: 63.912 | Examples/sec: 62.15\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.65 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1571 | tagging_loss_video: 8.475|tagging_loss_audio: 10.568|tagging_loss_text: 14.580|tagging_loss_image: 10.121|tagging_loss_fusion: 9.414|total_loss: 53.158 | 68.10 Examples/sec\n",
      "INFO:tensorflow:training step 1572 | tagging_loss_video: 9.193|tagging_loss_audio: 12.639|tagging_loss_text: 16.488|tagging_loss_image: 10.270|tagging_loss_fusion: 10.345|total_loss: 58.936 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 1573 | tagging_loss_video: 7.975|tagging_loss_audio: 11.643|tagging_loss_text: 12.309|tagging_loss_image: 8.933|tagging_loss_fusion: 9.358|total_loss: 50.218 | 65.02 Examples/sec\n",
      "INFO:tensorflow:training step 1574 | tagging_loss_video: 8.919|tagging_loss_audio: 15.519|tagging_loss_text: 16.080|tagging_loss_image: 11.331|tagging_loss_fusion: 10.973|total_loss: 62.822 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 1575 | tagging_loss_video: 9.232|tagging_loss_audio: 11.992|tagging_loss_text: 18.482|tagging_loss_image: 10.634|tagging_loss_fusion: 9.963|total_loss: 60.303 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 1576 | tagging_loss_video: 8.618|tagging_loss_audio: 11.568|tagging_loss_text: 13.885|tagging_loss_image: 9.781|tagging_loss_fusion: 9.446|total_loss: 53.297 | 64.81 Examples/sec\n",
      "INFO:tensorflow:training step 1577 | tagging_loss_video: 7.512|tagging_loss_audio: 13.628|tagging_loss_text: 14.710|tagging_loss_image: 10.121|tagging_loss_fusion: 9.200|total_loss: 55.171 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 1578 | tagging_loss_video: 8.519|tagging_loss_audio: 12.356|tagging_loss_text: 19.623|tagging_loss_image: 8.807|tagging_loss_fusion: 7.682|total_loss: 56.987 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 1579 | tagging_loss_video: 8.663|tagging_loss_audio: 13.852|tagging_loss_text: 15.869|tagging_loss_image: 9.097|tagging_loss_fusion: 9.212|total_loss: 56.694 | 67.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1580 |tagging_loss_video: 8.126|tagging_loss_audio: 11.269|tagging_loss_text: 15.091|tagging_loss_image: 9.139|tagging_loss_fusion: 9.294|total_loss: 52.919 | Examples/sec: 70.16\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.62 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 1581 | tagging_loss_video: 7.995|tagging_loss_audio: 12.283|tagging_loss_text: 14.734|tagging_loss_image: 9.552|tagging_loss_fusion: 7.197|total_loss: 51.761 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 1582 | tagging_loss_video: 8.415|tagging_loss_audio: 9.827|tagging_loss_text: 14.028|tagging_loss_image: 10.214|tagging_loss_fusion: 8.466|total_loss: 50.950 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 1583 | tagging_loss_video: 8.026|tagging_loss_audio: 12.475|tagging_loss_text: 13.930|tagging_loss_image: 9.679|tagging_loss_fusion: 9.587|total_loss: 53.697 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 1584 | tagging_loss_video: 8.068|tagging_loss_audio: 11.337|tagging_loss_text: 15.529|tagging_loss_image: 9.796|tagging_loss_fusion: 8.068|total_loss: 52.798 | 60.56 Examples/sec\n",
      "INFO:tensorflow:training step 1585 | tagging_loss_video: 8.140|tagging_loss_audio: 11.502|tagging_loss_text: 14.710|tagging_loss_image: 9.176|tagging_loss_fusion: 11.535|total_loss: 55.062 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 1586 | tagging_loss_video: 8.073|tagging_loss_audio: 12.243|tagging_loss_text: 16.572|tagging_loss_image: 9.776|tagging_loss_fusion: 8.420|total_loss: 55.084 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 1587 | tagging_loss_video: 7.738|tagging_loss_audio: 12.975|tagging_loss_text: 16.231|tagging_loss_image: 10.815|tagging_loss_fusion: 9.760|total_loss: 57.519 | 65.05 Examples/sec\n",
      "INFO:tensorflow:training step 1588 | tagging_loss_video: 8.150|tagging_loss_audio: 13.309|tagging_loss_text: 13.258|tagging_loss_image: 10.423|tagging_loss_fusion: 9.785|total_loss: 54.924 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 1589 | tagging_loss_video: 9.936|tagging_loss_audio: 15.117|tagging_loss_text: 16.087|tagging_loss_image: 11.542|tagging_loss_fusion: 11.581|total_loss: 64.262 | 71.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1590 |tagging_loss_video: 9.564|tagging_loss_audio: 14.958|tagging_loss_text: 16.901|tagging_loss_image: 9.753|tagging_loss_fusion: 10.020|total_loss: 61.196 | Examples/sec: 65.66\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.67 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 1591 | tagging_loss_video: 8.100|tagging_loss_audio: 12.163|tagging_loss_text: 13.710|tagging_loss_image: 9.232|tagging_loss_fusion: 10.198|total_loss: 53.404 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 1592 | tagging_loss_video: 10.211|tagging_loss_audio: 12.726|tagging_loss_text: 14.315|tagging_loss_image: 10.970|tagging_loss_fusion: 10.981|total_loss: 59.203 | 72.36 Examples/sec\n",
      "INFO:tensorflow:training step 1593 | tagging_loss_video: 7.811|tagging_loss_audio: 12.392|tagging_loss_text: 16.787|tagging_loss_image: 9.079|tagging_loss_fusion: 8.746|total_loss: 54.815 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 1594 | tagging_loss_video: 9.426|tagging_loss_audio: 13.935|tagging_loss_text: 17.295|tagging_loss_image: 10.244|tagging_loss_fusion: 9.326|total_loss: 60.227 | 62.68 Examples/sec\n",
      "INFO:tensorflow:training step 1595 | tagging_loss_video: 8.391|tagging_loss_audio: 12.191|tagging_loss_text: 12.595|tagging_loss_image: 10.140|tagging_loss_fusion: 10.783|total_loss: 54.101 | 66.11 Examples/sec\n",
      "INFO:tensorflow:training step 1596 | tagging_loss_video: 8.575|tagging_loss_audio: 11.160|tagging_loss_text: 14.563|tagging_loss_image: 10.189|tagging_loss_fusion: 9.343|total_loss: 53.830 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 1597 | tagging_loss_video: 8.792|tagging_loss_audio: 13.726|tagging_loss_text: 15.316|tagging_loss_image: 10.614|tagging_loss_fusion: 8.159|total_loss: 56.606 | 67.88 Examples/sec\n",
      "INFO:tensorflow:training step 1598 | tagging_loss_video: 6.860|tagging_loss_audio: 9.244|tagging_loss_text: 16.163|tagging_loss_image: 8.955|tagging_loss_fusion: 9.250|total_loss: 50.472 | 64.77 Examples/sec\n",
      "INFO:tensorflow:training step 1599 | tagging_loss_video: 7.607|tagging_loss_audio: 12.616|tagging_loss_text: 16.960|tagging_loss_image: 9.482|tagging_loss_fusion: 10.373|total_loss: 57.038 | 67.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1600 |tagging_loss_video: 9.027|tagging_loss_audio: 12.382|tagging_loss_text: 14.556|tagging_loss_image: 10.529|tagging_loss_fusion: 9.288|total_loss: 55.783 | Examples/sec: 72.12\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.65 | precision@0.5: 0.87 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1601 | tagging_loss_video: 8.020|tagging_loss_audio: 11.212|tagging_loss_text: 14.170|tagging_loss_image: 9.683|tagging_loss_fusion: 9.262|total_loss: 52.347 | 64.45 Examples/sec\n",
      "INFO:tensorflow:training step 1602 | tagging_loss_video: 8.700|tagging_loss_audio: 13.860|tagging_loss_text: 16.292|tagging_loss_image: 10.127|tagging_loss_fusion: 9.581|total_loss: 58.560 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 1603 | tagging_loss_video: 8.181|tagging_loss_audio: 13.718|tagging_loss_text: 17.534|tagging_loss_image: 10.484|tagging_loss_fusion: 10.042|total_loss: 59.959 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 1604 | tagging_loss_video: 8.389|tagging_loss_audio: 13.052|tagging_loss_text: 17.132|tagging_loss_image: 10.027|tagging_loss_fusion: 9.923|total_loss: 58.523 | 64.50 Examples/sec\n",
      "INFO:tensorflow:training step 1605 | tagging_loss_video: 8.432|tagging_loss_audio: 13.288|tagging_loss_text: 14.148|tagging_loss_image: 10.533|tagging_loss_fusion: 11.596|total_loss: 57.997 | 70.68 Examples/sec\n",
      "INFO:tensorflow:training step 1606 | tagging_loss_video: 7.857|tagging_loss_audio: 12.379|tagging_loss_text: 16.265|tagging_loss_image: 10.394|tagging_loss_fusion: 9.012|total_loss: 55.907 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 1607 | tagging_loss_video: 9.195|tagging_loss_audio: 10.586|tagging_loss_text: 17.428|tagging_loss_image: 9.211|tagging_loss_fusion: 11.845|total_loss: 58.266 | 64.99 Examples/sec\n",
      "INFO:tensorflow:training step 1608 | tagging_loss_video: 7.507|tagging_loss_audio: 13.497|tagging_loss_text: 13.432|tagging_loss_image: 9.055|tagging_loss_fusion: 7.890|total_loss: 51.381 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 1609 | tagging_loss_video: 8.413|tagging_loss_audio: 10.707|tagging_loss_text: 17.513|tagging_loss_image: 9.767|tagging_loss_fusion: 9.782|total_loss: 56.182 | 61.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1610 |tagging_loss_video: 8.210|tagging_loss_audio: 11.790|tagging_loss_text: 13.550|tagging_loss_image: 8.708|tagging_loss_fusion: 9.518|total_loss: 51.775 | Examples/sec: 71.09\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.64 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1611 | tagging_loss_video: 8.515|tagging_loss_audio: 14.096|tagging_loss_text: 15.725|tagging_loss_image: 10.976|tagging_loss_fusion: 9.528|total_loss: 58.840 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 1612 | tagging_loss_video: 8.781|tagging_loss_audio: 12.354|tagging_loss_text: 12.395|tagging_loss_image: 9.664|tagging_loss_fusion: 10.541|total_loss: 53.734 | 64.50 Examples/sec\n",
      "INFO:tensorflow:training step 1613 | tagging_loss_video: 8.380|tagging_loss_audio: 11.426|tagging_loss_text: 16.700|tagging_loss_image: 10.253|tagging_loss_fusion: 8.911|total_loss: 55.670 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 1614 | tagging_loss_video: 7.449|tagging_loss_audio: 12.397|tagging_loss_text: 14.491|tagging_loss_image: 10.475|tagging_loss_fusion: 8.349|total_loss: 53.161 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 1615 | tagging_loss_video: 9.455|tagging_loss_audio: 11.615|tagging_loss_text: 13.292|tagging_loss_image: 9.538|tagging_loss_fusion: 10.711|total_loss: 54.611 | 66.45 Examples/sec\n",
      "INFO:tensorflow:training step 1616 | tagging_loss_video: 8.833|tagging_loss_audio: 11.316|tagging_loss_text: 15.727|tagging_loss_image: 10.586|tagging_loss_fusion: 10.620|total_loss: 57.081 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 1617 | tagging_loss_video: 8.050|tagging_loss_audio: 11.659|tagging_loss_text: 14.684|tagging_loss_image: 9.658|tagging_loss_fusion: 10.694|total_loss: 54.746 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 1618 | tagging_loss_video: 9.028|tagging_loss_audio: 12.893|tagging_loss_text: 12.625|tagging_loss_image: 10.832|tagging_loss_fusion: 9.478|total_loss: 54.855 | 71.84 Examples/sec\n",
      "INFO:tensorflow:training step 1619 | tagging_loss_video: 8.991|tagging_loss_audio: 13.361|tagging_loss_text: 12.747|tagging_loss_image: 9.969|tagging_loss_fusion: 8.594|total_loss: 53.663 | 70.91 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1620 |tagging_loss_video: 8.604|tagging_loss_audio: 12.132|tagging_loss_text: 14.154|tagging_loss_image: 10.673|tagging_loss_fusion: 10.504|total_loss: 56.066 | Examples/sec: 61.79\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.60 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1621 | tagging_loss_video: 9.053|tagging_loss_audio: 13.051|tagging_loss_text: 14.246|tagging_loss_image: 10.536|tagging_loss_fusion: 8.848|total_loss: 55.733 | 70.19 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1622 | tagging_loss_video: 9.391|tagging_loss_audio: 11.909|tagging_loss_text: 14.681|tagging_loss_image: 10.095|tagging_loss_fusion: 11.495|total_loss: 57.571 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 1623 | tagging_loss_video: 7.543|tagging_loss_audio: 13.012|tagging_loss_text: 16.689|tagging_loss_image: 10.228|tagging_loss_fusion: 10.213|total_loss: 57.684 | 65.63 Examples/sec\n",
      "INFO:tensorflow:training step 1624 | tagging_loss_video: 8.134|tagging_loss_audio: 11.900|tagging_loss_text: 15.279|tagging_loss_image: 9.926|tagging_loss_fusion: 8.687|total_loss: 53.926 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 1625 | tagging_loss_video: 8.384|tagging_loss_audio: 12.810|tagging_loss_text: 14.803|tagging_loss_image: 9.991|tagging_loss_fusion: 9.888|total_loss: 55.876 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 1626 | tagging_loss_video: 8.893|tagging_loss_audio: 12.830|tagging_loss_text: 14.015|tagging_loss_image: 11.319|tagging_loss_fusion: 10.713|total_loss: 57.770 | 60.48 Examples/sec\n",
      "INFO:tensorflow:training step 1627 | tagging_loss_video: 7.835|tagging_loss_audio: 13.021|tagging_loss_text: 16.067|tagging_loss_image: 9.747|tagging_loss_fusion: 8.653|total_loss: 55.322 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 1628 | tagging_loss_video: 8.513|tagging_loss_audio: 11.872|tagging_loss_text: 15.106|tagging_loss_image: 11.049|tagging_loss_fusion: 8.074|total_loss: 54.615 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 1629 | tagging_loss_video: 8.517|tagging_loss_audio: 13.595|tagging_loss_text: 16.775|tagging_loss_image: 10.403|tagging_loss_fusion: 9.392|total_loss: 58.683 | 64.30 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1630 |tagging_loss_video: 8.161|tagging_loss_audio: 14.086|tagging_loss_text: 13.947|tagging_loss_image: 9.690|tagging_loss_fusion: 11.633|total_loss: 57.517 | Examples/sec: 70.60\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.62 | precision@0.5: 0.84 |recall@0.1: 0.92 | recall@0.5: 0.74\n",
      "INFO:tensorflow:training step 1631 | tagging_loss_video: 7.446|tagging_loss_audio: 9.920|tagging_loss_text: 14.389|tagging_loss_image: 9.406|tagging_loss_fusion: 9.906|total_loss: 51.068 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 1632 | tagging_loss_video: 8.187|tagging_loss_audio: 12.344|tagging_loss_text: 17.108|tagging_loss_image: 10.239|tagging_loss_fusion: 9.727|total_loss: 57.605 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 1633 | tagging_loss_video: 9.331|tagging_loss_audio: 10.292|tagging_loss_text: 18.727|tagging_loss_image: 10.992|tagging_loss_fusion: 12.799|total_loss: 62.141 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 1634 | tagging_loss_video: 9.033|tagging_loss_audio: 15.228|tagging_loss_text: 16.792|tagging_loss_image: 12.224|tagging_loss_fusion: 11.366|total_loss: 64.643 | 62.43 Examples/sec\n",
      "INFO:tensorflow:training step 1635 | tagging_loss_video: 9.394|tagging_loss_audio: 12.154|tagging_loss_text: 13.387|tagging_loss_image: 10.456|tagging_loss_fusion: 10.062|total_loss: 55.453 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 1636 | tagging_loss_video: 8.525|tagging_loss_audio: 14.181|tagging_loss_text: 13.692|tagging_loss_image: 10.994|tagging_loss_fusion: 8.929|total_loss: 56.321 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 1637 | tagging_loss_video: 7.377|tagging_loss_audio: 10.942|tagging_loss_text: 13.966|tagging_loss_image: 8.740|tagging_loss_fusion: 7.983|total_loss: 49.009 | 61.87 Examples/sec\n",
      "INFO:tensorflow:training step 1638 | tagging_loss_video: 7.075|tagging_loss_audio: 9.887|tagging_loss_text: 12.711|tagging_loss_image: 8.674|tagging_loss_fusion: 9.270|total_loss: 47.617 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 1639 | tagging_loss_video: 7.761|tagging_loss_audio: 11.551|tagging_loss_text: 17.563|tagging_loss_image: 9.823|tagging_loss_fusion: 8.115|total_loss: 54.813 | 70.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1640 |tagging_loss_video: 7.118|tagging_loss_audio: 11.779|tagging_loss_text: 16.610|tagging_loss_image: 8.598|tagging_loss_fusion: 8.110|total_loss: 52.215 | Examples/sec: 61.93\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.58 | precision@0.5: 0.87 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1641 | tagging_loss_video: 8.798|tagging_loss_audio: 12.985|tagging_loss_text: 16.194|tagging_loss_image: 9.683|tagging_loss_fusion: 10.036|total_loss: 57.695 | 66.81 Examples/sec\n",
      "INFO:tensorflow:training step 1642 | tagging_loss_video: 8.445|tagging_loss_audio: 11.504|tagging_loss_text: 12.505|tagging_loss_image: 8.934|tagging_loss_fusion: 10.501|total_loss: 51.889 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 1643 | tagging_loss_video: 8.160|tagging_loss_audio: 13.344|tagging_loss_text: 12.655|tagging_loss_image: 10.434|tagging_loss_fusion: 9.420|total_loss: 54.012 | 72.46 Examples/sec\n",
      "INFO:tensorflow:training step 1644 | tagging_loss_video: 7.062|tagging_loss_audio: 9.362|tagging_loss_text: 14.072|tagging_loss_image: 9.254|tagging_loss_fusion: 6.681|total_loss: 46.431 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 1645 | tagging_loss_video: 9.210|tagging_loss_audio: 14.751|tagging_loss_text: 16.416|tagging_loss_image: 11.648|tagging_loss_fusion: 10.938|total_loss: 62.964 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 1646 | tagging_loss_video: 6.793|tagging_loss_audio: 12.037|tagging_loss_text: 15.192|tagging_loss_image: 9.817|tagging_loss_fusion: 7.429|total_loss: 51.269 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 1647 | tagging_loss_video: 7.393|tagging_loss_audio: 11.463|tagging_loss_text: 12.965|tagging_loss_image: 8.676|tagging_loss_fusion: 8.909|total_loss: 49.406 | 71.92 Examples/sec\n",
      "INFO:tensorflow:training step 1648 | tagging_loss_video: 7.389|tagging_loss_audio: 10.797|tagging_loss_text: 11.124|tagging_loss_image: 8.416|tagging_loss_fusion: 9.809|total_loss: 47.535 | 59.99 Examples/sec\n",
      "INFO:tensorflow:training step 1649 | tagging_loss_video: 8.010|tagging_loss_audio: 11.171|tagging_loss_text: 16.690|tagging_loss_image: 10.045|tagging_loss_fusion: 9.358|total_loss: 55.275 | 67.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1650 |tagging_loss_video: 9.358|tagging_loss_audio: 12.584|tagging_loss_text: 18.336|tagging_loss_image: 10.676|tagging_loss_fusion: 10.694|total_loss: 61.649 | Examples/sec: 72.01\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.69 | precision@0.5: 0.90 |recall@0.1: 0.94 | recall@0.5: 0.76\n",
      "INFO:tensorflow:training step 1651 | tagging_loss_video: 7.521|tagging_loss_audio: 10.747|tagging_loss_text: 14.133|tagging_loss_image: 8.951|tagging_loss_fusion: 9.678|total_loss: 51.030 | 63.30 Examples/sec\n",
      "INFO:tensorflow:training step 1652 | tagging_loss_video: 8.720|tagging_loss_audio: 12.304|tagging_loss_text: 13.551|tagging_loss_image: 11.300|tagging_loss_fusion: 9.239|total_loss: 55.115 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 1653 | tagging_loss_video: 7.643|tagging_loss_audio: 11.425|tagging_loss_text: 16.492|tagging_loss_image: 9.440|tagging_loss_fusion: 7.759|total_loss: 52.758 | 72.28 Examples/sec\n",
      "INFO:tensorflow:training step 1654 | tagging_loss_video: 8.152|tagging_loss_audio: 12.677|tagging_loss_text: 13.063|tagging_loss_image: 8.539|tagging_loss_fusion: 8.366|total_loss: 50.797 | 65.32 Examples/sec\n",
      "INFO:tensorflow:training step 1655 | tagging_loss_video: 8.926|tagging_loss_audio: 9.434|tagging_loss_text: 17.015|tagging_loss_image: 9.874|tagging_loss_fusion: 9.442|total_loss: 54.691 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 1656 | tagging_loss_video: 7.894|tagging_loss_audio: 11.745|tagging_loss_text: 15.278|tagging_loss_image: 9.412|tagging_loss_fusion: 11.889|total_loss: 56.218 | 66.40 Examples/sec\n",
      "INFO:tensorflow:training step 1657 | tagging_loss_video: 9.469|tagging_loss_audio: 13.026|tagging_loss_text: 17.500|tagging_loss_image: 10.621|tagging_loss_fusion: 9.631|total_loss: 60.247 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 1658 | tagging_loss_video: 8.252|tagging_loss_audio: 11.596|tagging_loss_text: 17.517|tagging_loss_image: 9.990|tagging_loss_fusion: 9.379|total_loss: 56.734 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 1659 | tagging_loss_video: 8.797|tagging_loss_audio: 13.207|tagging_loss_text: 16.008|tagging_loss_image: 9.856|tagging_loss_fusion: 8.329|total_loss: 56.197 | 71.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1660 |tagging_loss_video: 8.932|tagging_loss_audio: 13.511|tagging_loss_text: 17.545|tagging_loss_image: 10.137|tagging_loss_fusion: 11.038|total_loss: 61.163 | Examples/sec: 58.98\n",
      "INFO:tensorflow:GAP: 0.85 | precision@0.1: 0.62 | precision@0.5: 0.85 |recall@0.1: 0.95 | recall@0.5: 0.77\n",
      "INFO:tensorflow:training step 1661 | tagging_loss_video: 8.845|tagging_loss_audio: 12.680|tagging_loss_text: 15.775|tagging_loss_image: 9.637|tagging_loss_fusion: 9.960|total_loss: 56.897 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 1662 | tagging_loss_video: 9.335|tagging_loss_audio: 12.294|tagging_loss_text: 14.998|tagging_loss_image: 11.494|tagging_loss_fusion: 8.329|total_loss: 56.450 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 1663 | tagging_loss_video: 8.738|tagging_loss_audio: 13.079|tagging_loss_text: 15.227|tagging_loss_image: 8.805|tagging_loss_fusion: 8.737|total_loss: 54.586 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 1664 | tagging_loss_video: 7.485|tagging_loss_audio: 10.142|tagging_loss_text: 16.111|tagging_loss_image: 8.202|tagging_loss_fusion: 7.853|total_loss: 49.794 | 63.31 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 1664.\n",
      "INFO:tensorflow:training step 1665 | tagging_loss_video: 8.452|tagging_loss_audio: 9.868|tagging_loss_text: 14.975|tagging_loss_image: 9.561|tagging_loss_fusion: 7.395|total_loss: 50.251 | 53.40 Examples/sec\n",
      "INFO:tensorflow:training step 1666 | tagging_loss_video: 7.763|tagging_loss_audio: 12.250|tagging_loss_text: 16.543|tagging_loss_image: 10.226|tagging_loss_fusion: 9.831|total_loss: 56.613 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 1667 | tagging_loss_video: 7.801|tagging_loss_audio: 10.161|tagging_loss_text: 15.202|tagging_loss_image: 10.451|tagging_loss_fusion: 9.536|total_loss: 53.151 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 1668 | tagging_loss_video: 8.140|tagging_loss_audio: 13.211|tagging_loss_text: 14.774|tagging_loss_image: 9.771|tagging_loss_fusion: 9.170|total_loss: 55.065 | 62.66 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.10001\n",
      "INFO:tensorflow:training step 1669 | tagging_loss_video: 8.825|tagging_loss_audio: 14.293|tagging_loss_text: 18.183|tagging_loss_image: 10.493|tagging_loss_fusion: 10.600|total_loss: 62.395 | 69.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1670 |tagging_loss_video: 9.396|tagging_loss_audio: 13.217|tagging_loss_text: 17.063|tagging_loss_image: 10.002|tagging_loss_fusion: 9.757|total_loss: 59.435 | Examples/sec: 72.20\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.67 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 1671 | tagging_loss_video: 8.363|tagging_loss_audio: 11.619|tagging_loss_text: 18.416|tagging_loss_image: 11.687|tagging_loss_fusion: 9.867|total_loss: 59.951 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 1672 | tagging_loss_video: 8.654|tagging_loss_audio: 14.098|tagging_loss_text: 15.661|tagging_loss_image: 10.572|tagging_loss_fusion: 9.194|total_loss: 58.179 | 66.83 Examples/sec\n",
      "INFO:tensorflow:training step 1673 | tagging_loss_video: 8.722|tagging_loss_audio: 11.312|tagging_loss_text: 15.251|tagging_loss_image: 10.427|tagging_loss_fusion: 9.831|total_loss: 55.543 | 58.54 Examples/sec\n",
      "INFO:tensorflow:training step 1674 | tagging_loss_video: 8.053|tagging_loss_audio: 12.216|tagging_loss_text: 10.179|tagging_loss_image: 8.347|tagging_loss_fusion: 9.854|total_loss: 48.649 | 71.10 Examples/sec\n",
      "INFO:tensorflow:training step 1675 | tagging_loss_video: 7.896|tagging_loss_audio: 11.613|tagging_loss_text: 13.768|tagging_loss_image: 8.514|tagging_loss_fusion: 8.026|total_loss: 49.816 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 1676 | tagging_loss_video: 7.274|tagging_loss_audio: 10.490|tagging_loss_text: 14.305|tagging_loss_image: 9.244|tagging_loss_fusion: 7.496|total_loss: 48.809 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 1677 | tagging_loss_video: 8.216|tagging_loss_audio: 11.816|tagging_loss_text: 16.469|tagging_loss_image: 10.017|tagging_loss_fusion: 9.143|total_loss: 55.661 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 1678 | tagging_loss_video: 7.522|tagging_loss_audio: 12.664|tagging_loss_text: 16.279|tagging_loss_image: 9.629|tagging_loss_fusion: 7.569|total_loss: 53.662 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 1679 | tagging_loss_video: 8.276|tagging_loss_audio: 13.559|tagging_loss_text: 15.418|tagging_loss_image: 9.078|tagging_loss_fusion: 10.772|total_loss: 57.102 | 68.57 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1680 |tagging_loss_video: 8.176|tagging_loss_audio: 11.271|tagging_loss_text: 14.245|tagging_loss_image: 9.123|tagging_loss_fusion: 9.002|total_loss: 51.817 | Examples/sec: 69.17\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.63 | precision@0.5: 0.85 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 1681 | tagging_loss_video: 9.620|tagging_loss_audio: 14.189|tagging_loss_text: 17.055|tagging_loss_image: 10.282|tagging_loss_fusion: 11.999|total_loss: 63.144 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 1682 | tagging_loss_video: 8.634|tagging_loss_audio: 13.819|tagging_loss_text: 17.302|tagging_loss_image: 11.130|tagging_loss_fusion: 9.289|total_loss: 60.174 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 1683 | tagging_loss_video: 9.405|tagging_loss_audio: 11.069|tagging_loss_text: 16.057|tagging_loss_image: 11.677|tagging_loss_fusion: 11.160|total_loss: 59.368 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 1684 | tagging_loss_video: 8.791|tagging_loss_audio: 11.418|tagging_loss_text: 13.688|tagging_loss_image: 9.535|tagging_loss_fusion: 8.152|total_loss: 51.584 | 60.11 Examples/sec\n",
      "INFO:tensorflow:training step 1685 | tagging_loss_video: 9.632|tagging_loss_audio: 15.321|tagging_loss_text: 18.108|tagging_loss_image: 10.385|tagging_loss_fusion: 11.245|total_loss: 64.691 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 1686 | tagging_loss_video: 8.816|tagging_loss_audio: 10.886|tagging_loss_text: 11.980|tagging_loss_image: 9.876|tagging_loss_fusion: 10.233|total_loss: 51.791 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 1687 | tagging_loss_video: 7.937|tagging_loss_audio: 12.107|tagging_loss_text: 13.058|tagging_loss_image: 10.135|tagging_loss_fusion: 13.283|total_loss: 56.520 | 61.77 Examples/sec\n",
      "INFO:tensorflow:training step 1688 | tagging_loss_video: 9.741|tagging_loss_audio: 14.497|tagging_loss_text: 14.940|tagging_loss_image: 12.264|tagging_loss_fusion: 9.381|total_loss: 60.822 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 1689 | tagging_loss_video: 7.835|tagging_loss_audio: 11.651|tagging_loss_text: 15.071|tagging_loss_image: 10.482|tagging_loss_fusion: 10.617|total_loss: 55.656 | 69.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1690 |tagging_loss_video: 8.029|tagging_loss_audio: 11.471|tagging_loss_text: 18.631|tagging_loss_image: 10.964|tagging_loss_fusion: 8.467|total_loss: 57.562 | Examples/sec: 65.53\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.68 | precision@0.5: 0.88 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1691 | tagging_loss_video: 8.842|tagging_loss_audio: 11.611|tagging_loss_text: 16.700|tagging_loss_image: 10.578|tagging_loss_fusion: 11.509|total_loss: 59.240 | 68.87 Examples/sec\n",
      "INFO:tensorflow:training step 1692 | tagging_loss_video: 8.756|tagging_loss_audio: 10.154|tagging_loss_text: 16.025|tagging_loss_image: 11.668|tagging_loss_fusion: 12.354|total_loss: 58.958 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 1693 | tagging_loss_video: 9.530|tagging_loss_audio: 12.773|tagging_loss_text: 10.549|tagging_loss_image: 11.876|tagging_loss_fusion: 10.882|total_loss: 55.610 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 1694 | tagging_loss_video: 9.255|tagging_loss_audio: 12.210|tagging_loss_text: 17.258|tagging_loss_image: 13.654|tagging_loss_fusion: 11.552|total_loss: 63.929 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 1695 | tagging_loss_video: 9.123|tagging_loss_audio: 12.535|tagging_loss_text: 13.841|tagging_loss_image: 9.958|tagging_loss_fusion: 10.129|total_loss: 55.586 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 1696 | tagging_loss_video: 9.535|tagging_loss_audio: 13.146|tagging_loss_text: 19.094|tagging_loss_image: 10.798|tagging_loss_fusion: 8.988|total_loss: 61.562 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 1697 | tagging_loss_video: 8.793|tagging_loss_audio: 10.537|tagging_loss_text: 14.874|tagging_loss_image: 9.886|tagging_loss_fusion: 9.187|total_loss: 53.277 | 71.74 Examples/sec\n",
      "INFO:tensorflow:training step 1698 | tagging_loss_video: 9.159|tagging_loss_audio: 14.767|tagging_loss_text: 15.218|tagging_loss_image: 11.122|tagging_loss_fusion: 9.668|total_loss: 59.933 | 57.74 Examples/sec\n",
      "INFO:tensorflow:training step 1699 | tagging_loss_video: 9.642|tagging_loss_audio: 15.154|tagging_loss_text: 16.630|tagging_loss_image: 10.702|tagging_loss_fusion: 11.448|total_loss: 63.575 | 72.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1700 |tagging_loss_video: 7.323|tagging_loss_audio: 13.996|tagging_loss_text: 15.367|tagging_loss_image: 11.463|tagging_loss_fusion: 9.037|total_loss: 57.187 | Examples/sec: 69.37\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.67 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1701 | tagging_loss_video: 8.413|tagging_loss_audio: 12.424|tagging_loss_text: 17.537|tagging_loss_image: 9.776|tagging_loss_fusion: 11.285|total_loss: 59.434 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 1702 | tagging_loss_video: 9.678|tagging_loss_audio: 14.434|tagging_loss_text: 14.493|tagging_loss_image: 12.885|tagging_loss_fusion: 11.503|total_loss: 62.993 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 1703 | tagging_loss_video: 8.778|tagging_loss_audio: 11.435|tagging_loss_text: 17.264|tagging_loss_image: 10.364|tagging_loss_fusion: 10.305|total_loss: 58.146 | 71.74 Examples/sec\n",
      "INFO:tensorflow:training step 1704 | tagging_loss_video: 7.863|tagging_loss_audio: 11.820|tagging_loss_text: 18.051|tagging_loss_image: 9.156|tagging_loss_fusion: 8.594|total_loss: 55.484 | 62.71 Examples/sec\n",
      "INFO:tensorflow:training step 1705 | tagging_loss_video: 8.885|tagging_loss_audio: 12.973|tagging_loss_text: 14.831|tagging_loss_image: 10.583|tagging_loss_fusion: 9.160|total_loss: 56.433 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 1706 | tagging_loss_video: 7.628|tagging_loss_audio: 12.098|tagging_loss_text: 16.445|tagging_loss_image: 10.243|tagging_loss_fusion: 12.404|total_loss: 58.818 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 1707 | tagging_loss_video: 8.433|tagging_loss_audio: 9.931|tagging_loss_text: 14.709|tagging_loss_image: 9.370|tagging_loss_fusion: 11.121|total_loss: 53.563 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 1708 | tagging_loss_video: 8.721|tagging_loss_audio: 11.281|tagging_loss_text: 16.288|tagging_loss_image: 11.884|tagging_loss_fusion: 13.133|total_loss: 61.308 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 1709 | tagging_loss_video: 8.614|tagging_loss_audio: 13.395|tagging_loss_text: 17.067|tagging_loss_image: 10.225|tagging_loss_fusion: 9.425|total_loss: 58.727 | 63.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1710 |tagging_loss_video: 7.726|tagging_loss_audio: 10.726|tagging_loss_text: 17.790|tagging_loss_image: 8.808|tagging_loss_fusion: 8.294|total_loss: 53.344 | Examples/sec: 68.95\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.65 | precision@0.5: 0.85 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 1711 | tagging_loss_video: 8.652|tagging_loss_audio: 11.783|tagging_loss_text: 17.108|tagging_loss_image: 9.942|tagging_loss_fusion: 8.609|total_loss: 56.094 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 1712 | tagging_loss_video: 7.707|tagging_loss_audio: 10.956|tagging_loss_text: 13.002|tagging_loss_image: 8.569|tagging_loss_fusion: 7.942|total_loss: 48.176 | 62.94 Examples/sec\n",
      "INFO:tensorflow:training step 1713 | tagging_loss_video: 8.432|tagging_loss_audio: 13.446|tagging_loss_text: 14.610|tagging_loss_image: 10.333|tagging_loss_fusion: 9.410|total_loss: 56.230 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 1714 | tagging_loss_video: 8.471|tagging_loss_audio: 11.293|tagging_loss_text: 15.781|tagging_loss_image: 10.303|tagging_loss_fusion: 10.159|total_loss: 56.006 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 1715 | tagging_loss_video: 8.223|tagging_loss_audio: 12.558|tagging_loss_text: 15.179|tagging_loss_image: 9.559|tagging_loss_fusion: 8.879|total_loss: 54.398 | 64.23 Examples/sec\n",
      "INFO:tensorflow:training step 1716 | tagging_loss_video: 8.242|tagging_loss_audio: 13.605|tagging_loss_text: 12.032|tagging_loss_image: 9.929|tagging_loss_fusion: 8.083|total_loss: 51.891 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 1717 | tagging_loss_video: 7.233|tagging_loss_audio: 12.328|tagging_loss_text: 12.883|tagging_loss_image: 8.934|tagging_loss_fusion: 6.832|total_loss: 48.210 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 1718 | tagging_loss_video: 8.852|tagging_loss_audio: 11.751|tagging_loss_text: 14.608|tagging_loss_image: 8.927|tagging_loss_fusion: 7.886|total_loss: 52.023 | 65.38 Examples/sec\n",
      "INFO:tensorflow:training step 1719 | tagging_loss_video: 7.778|tagging_loss_audio: 10.481|tagging_loss_text: 15.085|tagging_loss_image: 7.811|tagging_loss_fusion: 8.185|total_loss: 49.341 | 70.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1720 |tagging_loss_video: 7.340|tagging_loss_audio: 12.939|tagging_loss_text: 14.686|tagging_loss_image: 9.103|tagging_loss_fusion: 7.262|total_loss: 51.330 | Examples/sec: 65.15\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.71 | precision@0.5: 0.86 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 1721 | tagging_loss_video: 8.235|tagging_loss_audio: 9.552|tagging_loss_text: 13.407|tagging_loss_image: 9.849|tagging_loss_fusion: 9.203|total_loss: 50.246 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 1722 | tagging_loss_video: 7.829|tagging_loss_audio: 10.679|tagging_loss_text: 15.879|tagging_loss_image: 9.331|tagging_loss_fusion: 8.994|total_loss: 52.713 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 1723 | tagging_loss_video: 7.689|tagging_loss_audio: 9.604|tagging_loss_text: 13.587|tagging_loss_image: 9.009|tagging_loss_fusion: 8.964|total_loss: 48.852 | 65.97 Examples/sec\n",
      "INFO:tensorflow:training step 1724 | tagging_loss_video: 8.071|tagging_loss_audio: 10.722|tagging_loss_text: 15.416|tagging_loss_image: 8.528|tagging_loss_fusion: 9.092|total_loss: 51.828 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 1725 | tagging_loss_video: 8.195|tagging_loss_audio: 10.697|tagging_loss_text: 17.181|tagging_loss_image: 9.032|tagging_loss_fusion: 9.268|total_loss: 54.373 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 1726 | tagging_loss_video: 7.257|tagging_loss_audio: 12.872|tagging_loss_text: 14.911|tagging_loss_image: 9.541|tagging_loss_fusion: 7.904|total_loss: 52.484 | 62.30 Examples/sec\n",
      "INFO:tensorflow:training step 1727 | tagging_loss_video: 8.890|tagging_loss_audio: 14.068|tagging_loss_text: 13.915|tagging_loss_image: 10.361|tagging_loss_fusion: 10.463|total_loss: 57.697 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 1728 | tagging_loss_video: 8.654|tagging_loss_audio: 13.841|tagging_loss_text: 20.069|tagging_loss_image: 10.977|tagging_loss_fusion: 8.457|total_loss: 61.997 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 1729 | tagging_loss_video: 8.762|tagging_loss_audio: 14.200|tagging_loss_text: 17.516|tagging_loss_image: 10.505|tagging_loss_fusion: 10.752|total_loss: 61.735 | 61.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1730 |tagging_loss_video: 8.777|tagging_loss_audio: 10.638|tagging_loss_text: 19.238|tagging_loss_image: 10.481|tagging_loss_fusion: 10.014|total_loss: 59.148 | Examples/sec: 71.89\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.67 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1731 | tagging_loss_video: 9.081|tagging_loss_audio: 14.860|tagging_loss_text: 14.303|tagging_loss_image: 10.195|tagging_loss_fusion: 11.644|total_loss: 60.082 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 1732 | tagging_loss_video: 8.074|tagging_loss_audio: 12.462|tagging_loss_text: 15.352|tagging_loss_image: 8.408|tagging_loss_fusion: 10.341|total_loss: 54.636 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 1733 | tagging_loss_video: 8.512|tagging_loss_audio: 13.116|tagging_loss_text: 18.633|tagging_loss_image: 10.360|tagging_loss_fusion: 8.654|total_loss: 59.275 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 1734 | tagging_loss_video: 7.748|tagging_loss_audio: 12.071|tagging_loss_text: 16.526|tagging_loss_image: 8.876|tagging_loss_fusion: 7.932|total_loss: 53.153 | 60.67 Examples/sec\n",
      "INFO:tensorflow:training step 1735 | tagging_loss_video: 8.545|tagging_loss_audio: 12.324|tagging_loss_text: 18.018|tagging_loss_image: 10.065|tagging_loss_fusion: 9.165|total_loss: 58.117 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 1736 | tagging_loss_video: 7.561|tagging_loss_audio: 12.270|tagging_loss_text: 16.665|tagging_loss_image: 10.623|tagging_loss_fusion: 11.161|total_loss: 58.281 | 68.25 Examples/sec\n",
      "INFO:tensorflow:training step 1737 | tagging_loss_video: 7.621|tagging_loss_audio: 11.776|tagging_loss_text: 14.309|tagging_loss_image: 9.192|tagging_loss_fusion: 8.293|total_loss: 51.191 | 67.21 Examples/sec\n",
      "INFO:tensorflow:training step 1738 | tagging_loss_video: 8.215|tagging_loss_audio: 11.959|tagging_loss_text: 16.234|tagging_loss_image: 9.059|tagging_loss_fusion: 10.863|total_loss: 56.330 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 1739 | tagging_loss_video: 8.215|tagging_loss_audio: 10.269|tagging_loss_text: 15.987|tagging_loss_image: 10.024|tagging_loss_fusion: 9.681|total_loss: 54.176 | 71.03 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1740 |tagging_loss_video: 7.445|tagging_loss_audio: 11.188|tagging_loss_text: 14.172|tagging_loss_image: 9.105|tagging_loss_fusion: 7.646|total_loss: 49.555 | Examples/sec: 61.59\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.66 | precision@0.5: 0.87 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 1741 | tagging_loss_video: 8.374|tagging_loss_audio: 14.734|tagging_loss_text: 14.158|tagging_loss_image: 10.279|tagging_loss_fusion: 9.009|total_loss: 56.553 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 1742 | tagging_loss_video: 7.793|tagging_loss_audio: 11.348|tagging_loss_text: 16.853|tagging_loss_image: 9.607|tagging_loss_fusion: 8.511|total_loss: 54.112 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 1743 | tagging_loss_video: 8.715|tagging_loss_audio: 11.294|tagging_loss_text: 13.032|tagging_loss_image: 9.424|tagging_loss_fusion: 9.510|total_loss: 51.976 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 1744 | tagging_loss_video: 8.783|tagging_loss_audio: 12.821|tagging_loss_text: 15.582|tagging_loss_image: 10.052|tagging_loss_fusion: 11.511|total_loss: 58.749 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 1745 | tagging_loss_video: 7.498|tagging_loss_audio: 14.779|tagging_loss_text: 16.363|tagging_loss_image: 9.237|tagging_loss_fusion: 10.905|total_loss: 58.782 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 1746 | tagging_loss_video: 8.443|tagging_loss_audio: 14.391|tagging_loss_text: 13.863|tagging_loss_image: 9.676|tagging_loss_fusion: 9.892|total_loss: 56.264 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 1747 | tagging_loss_video: 7.068|tagging_loss_audio: 10.243|tagging_loss_text: 16.707|tagging_loss_image: 9.266|tagging_loss_fusion: 7.839|total_loss: 51.124 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 1748 | tagging_loss_video: 9.050|tagging_loss_audio: 11.404|tagging_loss_text: 14.441|tagging_loss_image: 11.150|tagging_loss_fusion: 12.040|total_loss: 58.085 | 62.13 Examples/sec\n",
      "INFO:tensorflow:training step 1749 | tagging_loss_video: 7.495|tagging_loss_audio: 12.389|tagging_loss_text: 14.863|tagging_loss_image: 8.075|tagging_loss_fusion: 9.681|total_loss: 52.503 | 69.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1750 |tagging_loss_video: 8.037|tagging_loss_audio: 11.381|tagging_loss_text: 12.290|tagging_loss_image: 9.353|tagging_loss_fusion: 8.393|total_loss: 49.454 | Examples/sec: 67.90\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.68 | precision@0.5: 0.88 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1751 | tagging_loss_video: 8.478|tagging_loss_audio: 11.257|tagging_loss_text: 13.688|tagging_loss_image: 10.136|tagging_loss_fusion: 9.562|total_loss: 53.121 | 68.39 Examples/sec\n",
      "INFO:tensorflow:training step 1752 | tagging_loss_video: 8.698|tagging_loss_audio: 14.230|tagging_loss_text: 14.656|tagging_loss_image: 10.404|tagging_loss_fusion: 9.496|total_loss: 57.485 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 1753 | tagging_loss_video: 7.450|tagging_loss_audio: 12.429|tagging_loss_text: 19.348|tagging_loss_image: 9.765|tagging_loss_fusion: 7.877|total_loss: 56.869 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 1754 | tagging_loss_video: 8.412|tagging_loss_audio: 9.946|tagging_loss_text: 16.119|tagging_loss_image: 9.459|tagging_loss_fusion: 9.877|total_loss: 53.812 | 61.52 Examples/sec\n",
      "INFO:tensorflow:training step 1755 | tagging_loss_video: 8.974|tagging_loss_audio: 12.620|tagging_loss_text: 17.152|tagging_loss_image: 9.639|tagging_loss_fusion: 9.944|total_loss: 58.330 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 1756 | tagging_loss_video: 8.097|tagging_loss_audio: 10.523|tagging_loss_text: 13.302|tagging_loss_image: 8.041|tagging_loss_fusion: 10.602|total_loss: 50.565 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 1757 | tagging_loss_video: 8.726|tagging_loss_audio: 12.703|tagging_loss_text: 15.424|tagging_loss_image: 9.261|tagging_loss_fusion: 10.388|total_loss: 56.502 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 1758 | tagging_loss_video: 9.157|tagging_loss_audio: 13.472|tagging_loss_text: 12.719|tagging_loss_image: 9.588|tagging_loss_fusion: 11.107|total_loss: 56.044 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 1759 | tagging_loss_video: 7.814|tagging_loss_audio: 12.476|tagging_loss_text: 15.484|tagging_loss_image: 10.147|tagging_loss_fusion: 9.267|total_loss: 55.187 | 62.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1760 |tagging_loss_video: 7.687|tagging_loss_audio: 12.620|tagging_loss_text: 14.845|tagging_loss_image: 10.901|tagging_loss_fusion: 11.414|total_loss: 57.466 | Examples/sec: 70.49\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.66 | precision@0.5: 0.88 |recall@0.1: 0.94 | recall@0.5: 0.76\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1761 | tagging_loss_video: 9.174|tagging_loss_audio: 12.831|tagging_loss_text: 15.378|tagging_loss_image: 10.491|tagging_loss_fusion: 10.358|total_loss: 58.232 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 1762 | tagging_loss_video: 8.510|tagging_loss_audio: 11.809|tagging_loss_text: 15.983|tagging_loss_image: 9.909|tagging_loss_fusion: 8.372|total_loss: 54.582 | 63.49 Examples/sec\n",
      "INFO:tensorflow:training step 1763 | tagging_loss_video: 7.470|tagging_loss_audio: 10.759|tagging_loss_text: 17.336|tagging_loss_image: 9.160|tagging_loss_fusion: 10.144|total_loss: 54.869 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 1764 | tagging_loss_video: 7.342|tagging_loss_audio: 12.067|tagging_loss_text: 16.216|tagging_loss_image: 10.605|tagging_loss_fusion: 9.375|total_loss: 55.604 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 1765 | tagging_loss_video: 8.509|tagging_loss_audio: 11.932|tagging_loss_text: 18.451|tagging_loss_image: 10.816|tagging_loss_fusion: 9.905|total_loss: 59.613 | 64.05 Examples/sec\n",
      "INFO:tensorflow:training step 1766 | tagging_loss_video: 7.705|tagging_loss_audio: 11.897|tagging_loss_text: 12.959|tagging_loss_image: 9.100|tagging_loss_fusion: 10.064|total_loss: 51.725 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 1767 | tagging_loss_video: 8.798|tagging_loss_audio: 12.007|tagging_loss_text: 17.894|tagging_loss_image: 9.596|tagging_loss_fusion: 9.596|total_loss: 57.890 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 1768 | tagging_loss_video: 8.822|tagging_loss_audio: 12.785|tagging_loss_text: 12.790|tagging_loss_image: 9.426|tagging_loss_fusion: 7.843|total_loss: 51.666 | 65.22 Examples/sec\n",
      "INFO:tensorflow:training step 1769 | tagging_loss_video: 8.682|tagging_loss_audio: 12.105|tagging_loss_text: 14.683|tagging_loss_image: 9.791|tagging_loss_fusion: 9.671|total_loss: 54.932 | 68.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1770 |tagging_loss_video: 7.576|tagging_loss_audio: 11.839|tagging_loss_text: 15.666|tagging_loss_image: 8.775|tagging_loss_fusion: 7.885|total_loss: 51.741 | Examples/sec: 70.44\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.66 | precision@0.5: 0.86 |recall@0.1: 0.97 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 1771 | tagging_loss_video: 8.313|tagging_loss_audio: 12.367|tagging_loss_text: 14.398|tagging_loss_image: 10.217|tagging_loss_fusion: 8.885|total_loss: 54.180 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 1772 | tagging_loss_video: 8.597|tagging_loss_audio: 14.581|tagging_loss_text: 16.417|tagging_loss_image: 11.410|tagging_loss_fusion: 8.460|total_loss: 59.466 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 1773 | tagging_loss_video: 9.061|tagging_loss_audio: 13.187|tagging_loss_text: 16.051|tagging_loss_image: 10.703|tagging_loss_fusion: 10.559|total_loss: 59.561 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 1774 | tagging_loss_video: 8.352|tagging_loss_audio: 10.524|tagging_loss_text: 17.025|tagging_loss_image: 10.050|tagging_loss_fusion: 9.715|total_loss: 55.667 | 66.67 Examples/sec\n",
      "INFO:tensorflow:training step 1775 | tagging_loss_video: 8.872|tagging_loss_audio: 13.760|tagging_loss_text: 17.609|tagging_loss_image: 11.343|tagging_loss_fusion: 11.055|total_loss: 62.639 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 1776 | tagging_loss_video: 7.273|tagging_loss_audio: 12.640|tagging_loss_text: 11.970|tagging_loss_image: 8.689|tagging_loss_fusion: 9.747|total_loss: 50.320 | 63.65 Examples/sec\n",
      "INFO:tensorflow:training step 1777 | tagging_loss_video: 7.070|tagging_loss_audio: 11.948|tagging_loss_text: 15.810|tagging_loss_image: 8.427|tagging_loss_fusion: 8.138|total_loss: 51.392 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 1778 | tagging_loss_video: 8.073|tagging_loss_audio: 12.358|tagging_loss_text: 14.855|tagging_loss_image: 9.323|tagging_loss_fusion: 7.662|total_loss: 52.272 | 67.37 Examples/sec\n",
      "INFO:tensorflow:training step 1779 | tagging_loss_video: 7.181|tagging_loss_audio: 11.408|tagging_loss_text: 13.522|tagging_loss_image: 9.370|tagging_loss_fusion: 9.470|total_loss: 50.950 | 65.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1780 |tagging_loss_video: 7.680|tagging_loss_audio: 14.104|tagging_loss_text: 16.136|tagging_loss_image: 10.346|tagging_loss_fusion: 8.575|total_loss: 56.841 | Examples/sec: 70.77\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.72 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 1781 | tagging_loss_video: 7.784|tagging_loss_audio: 13.136|tagging_loss_text: 15.193|tagging_loss_image: 10.061|tagging_loss_fusion: 8.559|total_loss: 54.733 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 1782 | tagging_loss_video: 8.324|tagging_loss_audio: 11.614|tagging_loss_text: 14.643|tagging_loss_image: 9.945|tagging_loss_fusion: 10.026|total_loss: 54.551 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 1783 | tagging_loss_video: 6.524|tagging_loss_audio: 11.498|tagging_loss_text: 15.991|tagging_loss_image: 8.223|tagging_loss_fusion: 8.425|total_loss: 50.661 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 1784 | tagging_loss_video: 9.738|tagging_loss_audio: 14.498|tagging_loss_text: 16.289|tagging_loss_image: 11.388|tagging_loss_fusion: 10.838|total_loss: 62.751 | 61.16 Examples/sec\n",
      "INFO:tensorflow:training step 1785 | tagging_loss_video: 7.213|tagging_loss_audio: 13.048|tagging_loss_text: 14.479|tagging_loss_image: 9.400|tagging_loss_fusion: 8.884|total_loss: 53.024 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 1786 | tagging_loss_video: 7.421|tagging_loss_audio: 13.494|tagging_loss_text: 13.003|tagging_loss_image: 8.522|tagging_loss_fusion: 7.389|total_loss: 49.829 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 1787 | tagging_loss_video: 7.448|tagging_loss_audio: 10.272|tagging_loss_text: 14.177|tagging_loss_image: 7.746|tagging_loss_fusion: 8.365|total_loss: 48.009 | 62.15 Examples/sec\n",
      "INFO:tensorflow:training step 1788 | tagging_loss_video: 8.429|tagging_loss_audio: 12.617|tagging_loss_text: 13.830|tagging_loss_image: 9.873|tagging_loss_fusion: 8.880|total_loss: 53.630 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 1789 | tagging_loss_video: 7.161|tagging_loss_audio: 11.603|tagging_loss_text: 15.435|tagging_loss_image: 10.481|tagging_loss_fusion: 7.286|total_loss: 51.965 | 70.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1790 |tagging_loss_video: 7.371|tagging_loss_audio: 12.106|tagging_loss_text: 17.509|tagging_loss_image: 9.011|tagging_loss_fusion: 8.266|total_loss: 54.264 | Examples/sec: 63.92\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.68 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1791 | tagging_loss_video: 8.911|tagging_loss_audio: 14.098|tagging_loss_text: 14.115|tagging_loss_image: 10.359|tagging_loss_fusion: 11.233|total_loss: 58.715 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 1792 | tagging_loss_video: 7.284|tagging_loss_audio: 11.629|tagging_loss_text: 16.578|tagging_loss_image: 9.366|tagging_loss_fusion: 7.876|total_loss: 52.734 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 1793 | tagging_loss_video: 7.654|tagging_loss_audio: 10.421|tagging_loss_text: 12.315|tagging_loss_image: 9.137|tagging_loss_fusion: 8.577|total_loss: 48.104 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 1794 | tagging_loss_video: 7.643|tagging_loss_audio: 11.232|tagging_loss_text: 14.403|tagging_loss_image: 8.904|tagging_loss_fusion: 8.199|total_loss: 50.381 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 1795 | tagging_loss_video: 7.342|tagging_loss_audio: 12.241|tagging_loss_text: 13.903|tagging_loss_image: 9.568|tagging_loss_fusion: 10.926|total_loss: 53.980 | 65.14 Examples/sec\n",
      "INFO:tensorflow:training step 1796 | tagging_loss_video: 8.174|tagging_loss_audio: 9.914|tagging_loss_text: 17.365|tagging_loss_image: 8.900|tagging_loss_fusion: 10.925|total_loss: 55.278 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 1797 | tagging_loss_video: 9.135|tagging_loss_audio: 10.323|tagging_loss_text: 17.456|tagging_loss_image: 9.774|tagging_loss_fusion: 7.721|total_loss: 54.409 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 1798 | tagging_loss_video: 7.637|tagging_loss_audio: 12.263|tagging_loss_text: 18.078|tagging_loss_image: 9.942|tagging_loss_fusion: 7.792|total_loss: 55.712 | 66.05 Examples/sec\n",
      "INFO:tensorflow:training step 1799 | tagging_loss_video: 8.674|tagging_loss_audio: 13.195|tagging_loss_text: 16.876|tagging_loss_image: 10.464|tagging_loss_fusion: 10.066|total_loss: 59.275 | 61.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1800 |tagging_loss_video: 8.595|tagging_loss_audio: 12.390|tagging_loss_text: 16.281|tagging_loss_image: 9.820|tagging_loss_fusion: 9.240|total_loss: 56.326 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.61 | precision@0.5: 0.87 |recall@0.1: 0.97 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1801 | tagging_loss_video: 8.670|tagging_loss_audio: 12.897|tagging_loss_text: 14.366|tagging_loss_image: 9.453|tagging_loss_fusion: 13.491|total_loss: 58.877 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 1802 | tagging_loss_video: 8.074|tagging_loss_audio: 11.862|tagging_loss_text: 15.247|tagging_loss_image: 9.456|tagging_loss_fusion: 8.525|total_loss: 53.164 | 64.68 Examples/sec\n",
      "INFO:tensorflow:training step 1803 | tagging_loss_video: 6.536|tagging_loss_audio: 11.622|tagging_loss_text: 14.953|tagging_loss_image: 8.920|tagging_loss_fusion: 8.087|total_loss: 50.117 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 1804 | tagging_loss_video: 8.210|tagging_loss_audio: 12.065|tagging_loss_text: 16.044|tagging_loss_image: 10.040|tagging_loss_fusion: 9.430|total_loss: 55.791 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 1805 | tagging_loss_video: 7.353|tagging_loss_audio: 11.518|tagging_loss_text: 14.235|tagging_loss_image: 8.924|tagging_loss_fusion: 7.585|total_loss: 49.615 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 1806 | tagging_loss_video: 7.744|tagging_loss_audio: 10.293|tagging_loss_text: 15.237|tagging_loss_image: 10.065|tagging_loss_fusion: 9.801|total_loss: 53.139 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 1807 | tagging_loss_video: 8.249|tagging_loss_audio: 13.584|tagging_loss_text: 17.919|tagging_loss_image: 8.522|tagging_loss_fusion: 8.431|total_loss: 56.705 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 1808 | tagging_loss_video: 8.505|tagging_loss_audio: 10.986|tagging_loss_text: 14.790|tagging_loss_image: 8.559|tagging_loss_fusion: 7.863|total_loss: 50.703 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 1809 | tagging_loss_video: 8.450|tagging_loss_audio: 14.296|tagging_loss_text: 12.565|tagging_loss_image: 10.809|tagging_loss_fusion: 9.516|total_loss: 55.635 | 66.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1810 |tagging_loss_video: 7.691|tagging_loss_audio: 14.488|tagging_loss_text: 16.344|tagging_loss_image: 8.429|tagging_loss_fusion: 9.098|total_loss: 56.049 | Examples/sec: 68.67\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.67 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1811 | tagging_loss_video: 8.743|tagging_loss_audio: 12.131|tagging_loss_text: 18.239|tagging_loss_image: 10.800|tagging_loss_fusion: 10.716|total_loss: 60.629 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 1812 | tagging_loss_video: 8.671|tagging_loss_audio: 13.541|tagging_loss_text: 16.375|tagging_loss_image: 11.006|tagging_loss_fusion: 10.734|total_loss: 60.327 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 1813 | tagging_loss_video: 8.243|tagging_loss_audio: 10.616|tagging_loss_text: 15.839|tagging_loss_image: 9.603|tagging_loss_fusion: 8.041|total_loss: 52.341 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 1814 | tagging_loss_video: 8.225|tagging_loss_audio: 10.535|tagging_loss_text: 13.280|tagging_loss_image: 9.917|tagging_loss_fusion: 8.895|total_loss: 50.851 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 1815 | tagging_loss_video: 7.757|tagging_loss_audio: 11.820|tagging_loss_text: 14.425|tagging_loss_image: 8.617|tagging_loss_fusion: 7.194|total_loss: 49.812 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 1816 | tagging_loss_video: 7.154|tagging_loss_audio: 11.159|tagging_loss_text: 11.731|tagging_loss_image: 9.143|tagging_loss_fusion: 8.474|total_loss: 47.660 | 63.48 Examples/sec\n",
      "INFO:tensorflow:training step 1817 | tagging_loss_video: 8.020|tagging_loss_audio: 11.645|tagging_loss_text: 15.719|tagging_loss_image: 8.969|tagging_loss_fusion: 9.576|total_loss: 53.928 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 1818 | tagging_loss_video: 7.015|tagging_loss_audio: 11.937|tagging_loss_text: 15.576|tagging_loss_image: 8.999|tagging_loss_fusion: 9.749|total_loss: 53.275 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 1819 | tagging_loss_video: 8.202|tagging_loss_audio: 12.665|tagging_loss_text: 16.051|tagging_loss_image: 10.312|tagging_loss_fusion: 9.110|total_loss: 56.340 | 63.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1820 |tagging_loss_video: 8.408|tagging_loss_audio: 13.129|tagging_loss_text: 14.872|tagging_loss_image: 9.140|tagging_loss_fusion: 10.046|total_loss: 55.594 | Examples/sec: 67.35\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.62 | precision@0.5: 0.85 |recall@0.1: 0.94 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1821 | tagging_loss_video: 8.864|tagging_loss_audio: 12.798|tagging_loss_text: 16.387|tagging_loss_image: 10.120|tagging_loss_fusion: 9.257|total_loss: 57.427 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 1822 | tagging_loss_video: 8.021|tagging_loss_audio: 14.216|tagging_loss_text: 17.224|tagging_loss_image: 10.832|tagging_loss_fusion: 9.990|total_loss: 60.283 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 1823 | tagging_loss_video: 9.033|tagging_loss_audio: 11.954|tagging_loss_text: 15.311|tagging_loss_image: 10.888|tagging_loss_fusion: 10.474|total_loss: 57.660 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 1824 | tagging_loss_video: 7.183|tagging_loss_audio: 10.763|tagging_loss_text: 15.049|tagging_loss_image: 9.619|tagging_loss_fusion: 10.523|total_loss: 53.136 | 60.19 Examples/sec\n",
      "INFO:tensorflow:training step 1825 | tagging_loss_video: 8.553|tagging_loss_audio: 13.617|tagging_loss_text: 15.233|tagging_loss_image: 10.805|tagging_loss_fusion: 9.570|total_loss: 57.778 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 1826 | tagging_loss_video: 8.859|tagging_loss_audio: 13.910|tagging_loss_text: 13.286|tagging_loss_image: 8.161|tagging_loss_fusion: 9.108|total_loss: 53.323 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 1827 | tagging_loss_video: 9.103|tagging_loss_audio: 12.274|tagging_loss_text: 14.464|tagging_loss_image: 9.840|tagging_loss_fusion: 11.350|total_loss: 57.031 | 61.69 Examples/sec\n",
      "INFO:tensorflow:training step 1828 | tagging_loss_video: 9.264|tagging_loss_audio: 11.495|tagging_loss_text: 15.698|tagging_loss_image: 10.362|tagging_loss_fusion: 10.759|total_loss: 57.578 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 1829 | tagging_loss_video: 7.841|tagging_loss_audio: 9.466|tagging_loss_text: 17.050|tagging_loss_image: 9.811|tagging_loss_fusion: 7.172|total_loss: 51.340 | 71.57 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1830 |tagging_loss_video: 8.270|tagging_loss_audio: 13.993|tagging_loss_text: 15.936|tagging_loss_image: 9.809|tagging_loss_fusion: 9.475|total_loss: 57.484 | Examples/sec: 59.74\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.66 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1831 | tagging_loss_video: 8.120|tagging_loss_audio: 10.373|tagging_loss_text: 13.920|tagging_loss_image: 8.971|tagging_loss_fusion: 8.710|total_loss: 50.094 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 1832 | tagging_loss_video: 10.366|tagging_loss_audio: 13.191|tagging_loss_text: 13.048|tagging_loss_image: 11.081|tagging_loss_fusion: 12.188|total_loss: 59.874 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 1833 | tagging_loss_video: 7.754|tagging_loss_audio: 13.256|tagging_loss_text: 16.173|tagging_loss_image: 9.888|tagging_loss_fusion: 9.712|total_loss: 56.784 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 1834 | tagging_loss_video: 9.743|tagging_loss_audio: 12.489|tagging_loss_text: 16.546|tagging_loss_image: 13.152|tagging_loss_fusion: 11.800|total_loss: 63.731 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 1835 | tagging_loss_video: 8.456|tagging_loss_audio: 10.392|tagging_loss_text: 15.825|tagging_loss_image: 9.336|tagging_loss_fusion: 10.203|total_loss: 54.212 | 71.97 Examples/sec\n",
      "INFO:tensorflow:training step 1836 | tagging_loss_video: 9.111|tagging_loss_audio: 14.054|tagging_loss_text: 12.743|tagging_loss_image: 10.271|tagging_loss_fusion: 8.257|total_loss: 54.435 | 67.77 Examples/sec\n",
      "INFO:tensorflow:training step 1837 | tagging_loss_video: 7.545|tagging_loss_audio: 11.106|tagging_loss_text: 14.238|tagging_loss_image: 8.605|tagging_loss_fusion: 8.224|total_loss: 49.719 | 72.01 Examples/sec\n",
      "INFO:tensorflow:training step 1838 | tagging_loss_video: 8.873|tagging_loss_audio: 13.256|tagging_loss_text: 12.320|tagging_loss_image: 10.309|tagging_loss_fusion: 12.492|total_loss: 57.250 | 61.75 Examples/sec\n",
      "INFO:tensorflow:training step 1839 | tagging_loss_video: 9.543|tagging_loss_audio: 13.378|tagging_loss_text: 13.697|tagging_loss_image: 10.971|tagging_loss_fusion: 10.989|total_loss: 58.579 | 70.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1840 |tagging_loss_video: 7.773|tagging_loss_audio: 12.038|tagging_loss_text: 15.254|tagging_loss_image: 10.002|tagging_loss_fusion: 9.247|total_loss: 54.314 | Examples/sec: 69.98\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.69 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1841 | tagging_loss_video: 8.320|tagging_loss_audio: 12.424|tagging_loss_text: 14.765|tagging_loss_image: 10.361|tagging_loss_fusion: 9.485|total_loss: 55.356 | 63.45 Examples/sec\n",
      "INFO:tensorflow:training step 1842 | tagging_loss_video: 9.266|tagging_loss_audio: 13.985|tagging_loss_text: 14.530|tagging_loss_image: 11.110|tagging_loss_fusion: 10.810|total_loss: 59.700 | 72.55 Examples/sec\n",
      "INFO:tensorflow:training step 1843 | tagging_loss_video: 8.114|tagging_loss_audio: 10.860|tagging_loss_text: 16.689|tagging_loss_image: 8.943|tagging_loss_fusion: 8.759|total_loss: 53.365 | 64.21 Examples/sec\n",
      "INFO:tensorflow:training step 1844 | tagging_loss_video: 8.166|tagging_loss_audio: 11.805|tagging_loss_text: 12.682|tagging_loss_image: 10.076|tagging_loss_fusion: 7.891|total_loss: 50.619 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 1845 | tagging_loss_video: 8.375|tagging_loss_audio: 13.351|tagging_loss_text: 14.762|tagging_loss_image: 10.279|tagging_loss_fusion: 10.464|total_loss: 57.231 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 1846 | tagging_loss_video: 9.159|tagging_loss_audio: 12.141|tagging_loss_text: 17.165|tagging_loss_image: 9.190|tagging_loss_fusion: 9.167|total_loss: 56.822 | 71.71 Examples/sec\n",
      "INFO:tensorflow:training step 1847 | tagging_loss_video: 7.804|tagging_loss_audio: 11.151|tagging_loss_text: 15.367|tagging_loss_image: 9.736|tagging_loss_fusion: 7.631|total_loss: 51.688 | 67.07 Examples/sec\n",
      "INFO:tensorflow:training step 1848 | tagging_loss_video: 9.059|tagging_loss_audio: 16.175|tagging_loss_text: 15.553|tagging_loss_image: 10.139|tagging_loss_fusion: 9.697|total_loss: 60.622 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 1849 | tagging_loss_video: 8.334|tagging_loss_audio: 10.179|tagging_loss_text: 17.025|tagging_loss_image: 11.014|tagging_loss_fusion: 11.838|total_loss: 58.391 | 61.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1850 |tagging_loss_video: 8.052|tagging_loss_audio: 11.495|tagging_loss_text: 16.475|tagging_loss_image: 9.582|tagging_loss_fusion: 7.932|total_loss: 53.536 | Examples/sec: 71.97\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.68 | precision@0.5: 0.88 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 1851 | tagging_loss_video: 7.945|tagging_loss_audio: 9.889|tagging_loss_text: 17.094|tagging_loss_image: 9.325|tagging_loss_fusion: 8.200|total_loss: 52.452 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 1852 | tagging_loss_video: 7.550|tagging_loss_audio: 12.325|tagging_loss_text: 11.059|tagging_loss_image: 9.130|tagging_loss_fusion: 8.751|total_loss: 48.815 | 62.63 Examples/sec\n",
      "INFO:tensorflow:training step 1853 | tagging_loss_video: 8.565|tagging_loss_audio: 14.526|tagging_loss_text: 15.316|tagging_loss_image: 10.620|tagging_loss_fusion: 10.707|total_loss: 59.733 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 1854 | tagging_loss_video: 8.327|tagging_loss_audio: 12.281|tagging_loss_text: 15.149|tagging_loss_image: 10.075|tagging_loss_fusion: 10.116|total_loss: 55.949 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 1855 | tagging_loss_video: 7.953|tagging_loss_audio: 12.281|tagging_loss_text: 14.524|tagging_loss_image: 8.964|tagging_loss_fusion: 10.478|total_loss: 54.201 | 66.85 Examples/sec\n",
      "INFO:tensorflow:training step 1856 | tagging_loss_video: 7.030|tagging_loss_audio: 12.429|tagging_loss_text: 12.676|tagging_loss_image: 9.562|tagging_loss_fusion: 7.804|total_loss: 49.502 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 1857 | tagging_loss_video: 7.854|tagging_loss_audio: 12.352|tagging_loss_text: 16.985|tagging_loss_image: 8.631|tagging_loss_fusion: 8.879|total_loss: 54.700 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 1858 | tagging_loss_video: 7.471|tagging_loss_audio: 12.218|tagging_loss_text: 14.524|tagging_loss_image: 7.832|tagging_loss_fusion: 8.834|total_loss: 50.878 | 65.40 Examples/sec\n",
      "INFO:tensorflow:training step 1859 | tagging_loss_video: 7.561|tagging_loss_audio: 11.093|tagging_loss_text: 16.340|tagging_loss_image: 6.689|tagging_loss_fusion: 8.736|total_loss: 50.419 | 67.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1860 |tagging_loss_video: 7.635|tagging_loss_audio: 11.780|tagging_loss_text: 16.134|tagging_loss_image: 8.898|tagging_loss_fusion: 7.559|total_loss: 52.006 | Examples/sec: 66.58\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.66 | precision@0.5: 0.88 |recall@0.1: 0.98 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 1861 | tagging_loss_video: 8.471|tagging_loss_audio: 11.030|tagging_loss_text: 14.022|tagging_loss_image: 9.399|tagging_loss_fusion: 7.424|total_loss: 50.346 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 1862 | tagging_loss_video: 7.853|tagging_loss_audio: 9.899|tagging_loss_text: 15.249|tagging_loss_image: 9.773|tagging_loss_fusion: 8.760|total_loss: 51.534 | 67.31 Examples/sec\n",
      "INFO:tensorflow:training step 1863 | tagging_loss_video: 7.420|tagging_loss_audio: 11.538|tagging_loss_text: 16.842|tagging_loss_image: 7.926|tagging_loss_fusion: 8.929|total_loss: 52.654 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 1864 | tagging_loss_video: 8.011|tagging_loss_audio: 11.496|tagging_loss_text: 14.024|tagging_loss_image: 9.051|tagging_loss_fusion: 8.410|total_loss: 50.993 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 1865 | tagging_loss_video: 7.834|tagging_loss_audio: 11.880|tagging_loss_text: 11.943|tagging_loss_image: 9.683|tagging_loss_fusion: 8.646|total_loss: 49.985 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 1866 | tagging_loss_video: 7.304|tagging_loss_audio: 9.766|tagging_loss_text: 16.238|tagging_loss_image: 9.922|tagging_loss_fusion: 8.181|total_loss: 51.410 | 65.80 Examples/sec\n",
      "INFO:tensorflow:training step 1867 | tagging_loss_video: 8.307|tagging_loss_audio: 11.381|tagging_loss_text: 14.711|tagging_loss_image: 10.861|tagging_loss_fusion: 10.871|total_loss: 56.131 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 1868 | tagging_loss_video: 9.083|tagging_loss_audio: 14.501|tagging_loss_text: 20.117|tagging_loss_image: 10.822|tagging_loss_fusion: 10.535|total_loss: 65.059 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 1869 | tagging_loss_video: 7.676|tagging_loss_audio: 14.159|tagging_loss_text: 13.576|tagging_loss_image: 10.538|tagging_loss_fusion: 12.327|total_loss: 58.276 | 61.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1870 |tagging_loss_video: 7.601|tagging_loss_audio: 13.160|tagging_loss_text: 14.497|tagging_loss_image: 8.679|tagging_loss_fusion: 8.249|total_loss: 52.187 | Examples/sec: 70.93\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.71 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1871 | tagging_loss_video: 8.254|tagging_loss_audio: 12.731|tagging_loss_text: 15.742|tagging_loss_image: 10.368|tagging_loss_fusion: 11.151|total_loss: 58.245 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 1872 | tagging_loss_video: 8.537|tagging_loss_audio: 9.477|tagging_loss_text: 10.718|tagging_loss_image: 8.352|tagging_loss_fusion: 9.112|total_loss: 46.195 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 1873 | tagging_loss_video: 8.169|tagging_loss_audio: 14.438|tagging_loss_text: 16.529|tagging_loss_image: 10.609|tagging_loss_fusion: 9.908|total_loss: 59.654 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 1874 | tagging_loss_video: 7.467|tagging_loss_audio: 9.816|tagging_loss_text: 15.243|tagging_loss_image: 8.962|tagging_loss_fusion: 7.738|total_loss: 49.226 | 60.11 Examples/sec\n",
      "INFO:tensorflow:training step 1875 | tagging_loss_video: 8.372|tagging_loss_audio: 12.961|tagging_loss_text: 18.014|tagging_loss_image: 9.848|tagging_loss_fusion: 9.096|total_loss: 58.290 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 1876 | tagging_loss_video: 8.835|tagging_loss_audio: 11.711|tagging_loss_text: 16.045|tagging_loss_image: 10.369|tagging_loss_fusion: 7.796|total_loss: 54.756 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 1877 | tagging_loss_video: 6.878|tagging_loss_audio: 12.137|tagging_loss_text: 9.340|tagging_loss_image: 8.539|tagging_loss_fusion: 7.835|total_loss: 44.729 | 63.20 Examples/sec\n",
      "INFO:tensorflow:training step 1878 | tagging_loss_video: 8.228|tagging_loss_audio: 12.293|tagging_loss_text: 14.953|tagging_loss_image: 9.896|tagging_loss_fusion: 7.422|total_loss: 52.792 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 1879 | tagging_loss_video: 8.087|tagging_loss_audio: 12.047|tagging_loss_text: 15.966|tagging_loss_image: 9.970|tagging_loss_fusion: 8.020|total_loss: 54.090 | 70.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1880 |tagging_loss_video: 7.463|tagging_loss_audio: 11.702|tagging_loss_text: 10.777|tagging_loss_image: 9.129|tagging_loss_fusion: 7.846|total_loss: 46.917 | Examples/sec: 66.78\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 1881 | tagging_loss_video: 8.738|tagging_loss_audio: 12.428|tagging_loss_text: 17.120|tagging_loss_image: 9.253|tagging_loss_fusion: 11.628|total_loss: 59.168 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 1882 | tagging_loss_video: 7.798|tagging_loss_audio: 11.696|tagging_loss_text: 15.483|tagging_loss_image: 9.142|tagging_loss_fusion: 8.735|total_loss: 52.854 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 1883 | tagging_loss_video: 8.176|tagging_loss_audio: 11.706|tagging_loss_text: 14.380|tagging_loss_image: 8.810|tagging_loss_fusion: 10.282|total_loss: 53.353 | 65.59 Examples/sec\n",
      "INFO:tensorflow:training step 1884 | tagging_loss_video: 8.256|tagging_loss_audio: 9.377|tagging_loss_text: 15.555|tagging_loss_image: 10.543|tagging_loss_fusion: 11.715|total_loss: 55.447 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 1885 | tagging_loss_video: 7.950|tagging_loss_audio: 11.912|tagging_loss_text: 15.716|tagging_loss_image: 8.501|tagging_loss_fusion: 8.583|total_loss: 52.662 | 66.66 Examples/sec\n",
      "INFO:tensorflow:training step 1886 | tagging_loss_video: 7.820|tagging_loss_audio: 12.030|tagging_loss_text: 13.199|tagging_loss_image: 9.162|tagging_loss_fusion: 9.410|total_loss: 51.621 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 1887 | tagging_loss_video: 6.134|tagging_loss_audio: 11.004|tagging_loss_text: 16.077|tagging_loss_image: 8.498|tagging_loss_fusion: 6.639|total_loss: 48.353 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 1888 | tagging_loss_video: 8.462|tagging_loss_audio: 11.776|tagging_loss_text: 16.453|tagging_loss_image: 9.256|tagging_loss_fusion: 7.759|total_loss: 53.706 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 1889 | tagging_loss_video: 8.662|tagging_loss_audio: 9.598|tagging_loss_text: 16.957|tagging_loss_image: 8.544|tagging_loss_fusion: 8.108|total_loss: 51.869 | 67.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1890 |tagging_loss_video: 8.284|tagging_loss_audio: 10.190|tagging_loss_text: 14.309|tagging_loss_image: 9.775|tagging_loss_fusion: 8.967|total_loss: 51.524 | Examples/sec: 71.37\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.88 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 1891 | tagging_loss_video: 7.605|tagging_loss_audio: 13.680|tagging_loss_text: 15.638|tagging_loss_image: 9.163|tagging_loss_fusion: 11.399|total_loss: 57.485 | 62.22 Examples/sec\n",
      "INFO:tensorflow:training step 1892 | tagging_loss_video: 8.011|tagging_loss_audio: 13.225|tagging_loss_text: 15.331|tagging_loss_image: 9.103|tagging_loss_fusion: 8.960|total_loss: 54.630 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 1893 | tagging_loss_video: 8.086|tagging_loss_audio: 11.108|tagging_loss_text: 14.501|tagging_loss_image: 9.903|tagging_loss_fusion: 9.109|total_loss: 52.707 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 1894 | tagging_loss_video: 7.608|tagging_loss_audio: 11.139|tagging_loss_text: 14.684|tagging_loss_image: 10.094|tagging_loss_fusion: 9.462|total_loss: 52.986 | 62.86 Examples/sec\n",
      "INFO:tensorflow:training step 1895 | tagging_loss_video: 8.414|tagging_loss_audio: 11.109|tagging_loss_text: 19.384|tagging_loss_image: 10.857|tagging_loss_fusion: 9.309|total_loss: 59.074 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 1896 | tagging_loss_video: 8.038|tagging_loss_audio: 12.776|tagging_loss_text: 14.648|tagging_loss_image: 9.529|tagging_loss_fusion: 8.625|total_loss: 53.617 | 71.10 Examples/sec\n",
      "INFO:tensorflow:training step 1897 | tagging_loss_video: 8.169|tagging_loss_audio: 13.276|tagging_loss_text: 18.175|tagging_loss_image: 10.173|tagging_loss_fusion: 7.204|total_loss: 56.998 | 72.28 Examples/sec\n",
      "INFO:tensorflow:training step 1898 | tagging_loss_video: 7.570|tagging_loss_audio: 12.416|tagging_loss_text: 15.758|tagging_loss_image: 10.389|tagging_loss_fusion: 6.784|total_loss: 52.919 | 66.41 Examples/sec\n",
      "INFO:tensorflow:training step 1899 | tagging_loss_video: 8.470|tagging_loss_audio: 12.489|tagging_loss_text: 14.161|tagging_loss_image: 8.524|tagging_loss_fusion: 8.600|total_loss: 52.244 | 69.33 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1900 |tagging_loss_video: 8.592|tagging_loss_audio: 13.519|tagging_loss_text: 17.002|tagging_loss_image: 10.305|tagging_loss_fusion: 9.112|total_loss: 58.530 | Examples/sec: 68.76\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.72 | precision@0.5: 0.91 |recall@0.1: 0.94 | recall@0.5: 0.82\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 1901 | tagging_loss_video: 9.016|tagging_loss_audio: 13.385|tagging_loss_text: 13.989|tagging_loss_image: 8.977|tagging_loss_fusion: 9.035|total_loss: 54.402 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 1902 | tagging_loss_video: 7.341|tagging_loss_audio: 12.412|tagging_loss_text: 15.913|tagging_loss_image: 10.150|tagging_loss_fusion: 8.449|total_loss: 54.265 | 63.58 Examples/sec\n",
      "INFO:tensorflow:training step 1903 | tagging_loss_video: 7.064|tagging_loss_audio: 13.802|tagging_loss_text: 13.176|tagging_loss_image: 8.571|tagging_loss_fusion: 6.959|total_loss: 49.572 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 1904 | tagging_loss_video: 8.273|tagging_loss_audio: 13.541|tagging_loss_text: 14.813|tagging_loss_image: 9.152|tagging_loss_fusion: 9.765|total_loss: 55.544 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 1905 | tagging_loss_video: 8.922|tagging_loss_audio: 12.631|tagging_loss_text: 16.081|tagging_loss_image: 8.666|tagging_loss_fusion: 10.937|total_loss: 57.237 | 60.19 Examples/sec\n",
      "INFO:tensorflow:training step 1906 | tagging_loss_video: 8.770|tagging_loss_audio: 11.785|tagging_loss_text: 13.641|tagging_loss_image: 8.697|tagging_loss_fusion: 10.218|total_loss: 53.112 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 1907 | tagging_loss_video: 7.559|tagging_loss_audio: 9.451|tagging_loss_text: 16.533|tagging_loss_image: 9.806|tagging_loss_fusion: 9.843|total_loss: 53.192 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 1908 | tagging_loss_video: 8.376|tagging_loss_audio: 9.278|tagging_loss_text: 11.490|tagging_loss_image: 10.072|tagging_loss_fusion: 8.833|total_loss: 48.049 | 63.67 Examples/sec\n",
      "INFO:tensorflow:training step 1909 | tagging_loss_video: 7.978|tagging_loss_audio: 12.511|tagging_loss_text: 14.079|tagging_loss_image: 10.503|tagging_loss_fusion: 7.416|total_loss: 52.488 | 69.35 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1910 |tagging_loss_video: 7.217|tagging_loss_audio: 10.803|tagging_loss_text: 15.717|tagging_loss_image: 9.474|tagging_loss_fusion: 9.445|total_loss: 52.656 | Examples/sec: 69.26\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.65 | precision@0.5: 0.86 |recall@0.1: 0.96 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 1911 | tagging_loss_video: 8.766|tagging_loss_audio: 11.215|tagging_loss_text: 15.017|tagging_loss_image: 9.699|tagging_loss_fusion: 9.949|total_loss: 54.647 | 67.12 Examples/sec\n",
      "INFO:tensorflow:training step 1912 | tagging_loss_video: 7.405|tagging_loss_audio: 12.974|tagging_loss_text: 17.975|tagging_loss_image: 10.597|tagging_loss_fusion: 8.328|total_loss: 57.279 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 1913 | tagging_loss_video: 8.577|tagging_loss_audio: 12.294|tagging_loss_text: 17.516|tagging_loss_image: 10.377|tagging_loss_fusion: 11.319|total_loss: 60.083 | 66.84 Examples/sec\n",
      "INFO:tensorflow:training step 1914 | tagging_loss_video: 7.865|tagging_loss_audio: 10.613|tagging_loss_text: 16.898|tagging_loss_image: 10.497|tagging_loss_fusion: 7.396|total_loss: 53.271 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 1915 | tagging_loss_video: 9.619|tagging_loss_audio: 12.467|tagging_loss_text: 15.265|tagging_loss_image: 9.946|tagging_loss_fusion: 11.723|total_loss: 59.019 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 1916 | tagging_loss_video: 6.962|tagging_loss_audio: 10.382|tagging_loss_text: 13.854|tagging_loss_image: 7.956|tagging_loss_fusion: 8.161|total_loss: 47.315 | 60.50 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 1917.\n",
      "INFO:tensorflow:training step 1917 | tagging_loss_video: 7.649|tagging_loss_audio: 12.184|tagging_loss_text: 15.872|tagging_loss_image: 8.702|tagging_loss_fusion: 7.252|total_loss: 51.658 | 51.58 Examples/sec\n",
      "INFO:tensorflow:training step 1918 | tagging_loss_video: 6.811|tagging_loss_audio: 9.789|tagging_loss_text: 15.953|tagging_loss_image: 8.642|tagging_loss_fusion: 6.665|total_loss: 47.860 | 57.74 Examples/sec\n",
      "INFO:tensorflow:training step 1919 | tagging_loss_video: 7.506|tagging_loss_audio: 12.181|tagging_loss_text: 17.546|tagging_loss_image: 9.726|tagging_loss_fusion: 10.313|total_loss: 57.271 | 69.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1920 |tagging_loss_video: 6.905|tagging_loss_audio: 13.017|tagging_loss_text: 15.851|tagging_loss_image: 10.123|tagging_loss_fusion: 10.072|total_loss: 55.967 | Examples/sec: 72.32\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.60 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1921 | tagging_loss_video: 6.925|tagging_loss_audio: 10.240|tagging_loss_text: 12.652|tagging_loss_image: 9.529|tagging_loss_fusion: 6.519|total_loss: 45.864 | 68.62 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 2.09995\n",
      "INFO:tensorflow:training step 1922 | tagging_loss_video: 6.358|tagging_loss_audio: 11.771|tagging_loss_text: 12.792|tagging_loss_image: 8.763|tagging_loss_fusion: 6.465|total_loss: 46.150 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 1923 | tagging_loss_video: 8.745|tagging_loss_audio: 12.664|tagging_loss_text: 18.467|tagging_loss_image: 9.427|tagging_loss_fusion: 11.638|total_loss: 60.940 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 1924 | tagging_loss_video: 7.532|tagging_loss_audio: 13.433|tagging_loss_text: 19.189|tagging_loss_image: 8.878|tagging_loss_fusion: 6.754|total_loss: 55.786 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 1925 | tagging_loss_video: 6.635|tagging_loss_audio: 9.581|tagging_loss_text: 14.887|tagging_loss_image: 8.397|tagging_loss_fusion: 6.279|total_loss: 45.780 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 1926 | tagging_loss_video: 6.335|tagging_loss_audio: 11.966|tagging_loss_text: 11.167|tagging_loss_image: 8.061|tagging_loss_fusion: 8.081|total_loss: 45.610 | 60.72 Examples/sec\n",
      "INFO:tensorflow:training step 1927 | tagging_loss_video: 8.489|tagging_loss_audio: 13.632|tagging_loss_text: 15.997|tagging_loss_image: 8.766|tagging_loss_fusion: 8.534|total_loss: 55.418 | 67.16 Examples/sec\n",
      "INFO:tensorflow:training step 1928 | tagging_loss_video: 8.289|tagging_loss_audio: 10.154|tagging_loss_text: 16.824|tagging_loss_image: 10.171|tagging_loss_fusion: 9.082|total_loss: 54.521 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 1929 | tagging_loss_video: 7.418|tagging_loss_audio: 9.813|tagging_loss_text: 15.520|tagging_loss_image: 8.024|tagging_loss_fusion: 7.773|total_loss: 48.548 | 70.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1930 |tagging_loss_video: 7.270|tagging_loss_audio: 12.287|tagging_loss_text: 13.505|tagging_loss_image: 9.832|tagging_loss_fusion: 8.378|total_loss: 51.271 | Examples/sec: 69.46\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.70 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 1931 | tagging_loss_video: 7.043|tagging_loss_audio: 12.029|tagging_loss_text: 13.791|tagging_loss_image: 9.007|tagging_loss_fusion: 7.474|total_loss: 49.343 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 1932 | tagging_loss_video: 6.963|tagging_loss_audio: 12.145|tagging_loss_text: 9.788|tagging_loss_image: 8.446|tagging_loss_fusion: 9.164|total_loss: 46.505 | 66.44 Examples/sec\n",
      "INFO:tensorflow:training step 1933 | tagging_loss_video: 8.000|tagging_loss_audio: 12.660|tagging_loss_text: 15.701|tagging_loss_image: 8.457|tagging_loss_fusion: 7.996|total_loss: 52.815 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 1934 | tagging_loss_video: 7.851|tagging_loss_audio: 12.941|tagging_loss_text: 16.621|tagging_loss_image: 9.684|tagging_loss_fusion: 8.577|total_loss: 55.675 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 1935 | tagging_loss_video: 7.825|tagging_loss_audio: 12.696|tagging_loss_text: 16.010|tagging_loss_image: 9.622|tagging_loss_fusion: 8.609|total_loss: 54.762 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 1936 | tagging_loss_video: 8.115|tagging_loss_audio: 12.072|tagging_loss_text: 16.742|tagging_loss_image: 9.045|tagging_loss_fusion: 9.647|total_loss: 55.622 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 1937 | tagging_loss_video: 7.951|tagging_loss_audio: 13.510|tagging_loss_text: 16.033|tagging_loss_image: 9.535|tagging_loss_fusion: 8.508|total_loss: 55.537 | 66.93 Examples/sec\n",
      "INFO:tensorflow:training step 1938 | tagging_loss_video: 8.918|tagging_loss_audio: 13.706|tagging_loss_text: 13.408|tagging_loss_image: 9.379|tagging_loss_fusion: 9.171|total_loss: 54.582 | 61.19 Examples/sec\n",
      "INFO:tensorflow:training step 1939 | tagging_loss_video: 7.632|tagging_loss_audio: 11.060|tagging_loss_text: 16.332|tagging_loss_image: 9.898|tagging_loss_fusion: 7.758|total_loss: 52.680 | 68.57 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1940 |tagging_loss_video: 8.726|tagging_loss_audio: 12.044|tagging_loss_text: 18.563|tagging_loss_image: 9.602|tagging_loss_fusion: 12.454|total_loss: 61.388 | Examples/sec: 70.28\n",
      "INFO:tensorflow:GAP: 0.83 | precision@0.1: 0.65 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.73\n",
      "INFO:tensorflow:training step 1941 | tagging_loss_video: 6.581|tagging_loss_audio: 11.508|tagging_loss_text: 14.010|tagging_loss_image: 8.383|tagging_loss_fusion: 11.026|total_loss: 51.509 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 1942 | tagging_loss_video: 6.420|tagging_loss_audio: 10.516|tagging_loss_text: 14.333|tagging_loss_image: 7.627|tagging_loss_fusion: 8.738|total_loss: 47.634 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 1943 | tagging_loss_video: 7.898|tagging_loss_audio: 12.316|tagging_loss_text: 14.649|tagging_loss_image: 9.042|tagging_loss_fusion: 8.833|total_loss: 52.736 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 1944 | tagging_loss_video: 6.721|tagging_loss_audio: 13.379|tagging_loss_text: 10.975|tagging_loss_image: 9.063|tagging_loss_fusion: 6.776|total_loss: 46.913 | 62.77 Examples/sec\n",
      "INFO:tensorflow:training step 1945 | tagging_loss_video: 7.915|tagging_loss_audio: 14.189|tagging_loss_text: 13.119|tagging_loss_image: 9.001|tagging_loss_fusion: 6.650|total_loss: 50.874 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 1946 | tagging_loss_video: 7.935|tagging_loss_audio: 11.002|tagging_loss_text: 16.594|tagging_loss_image: 8.773|tagging_loss_fusion: 8.498|total_loss: 52.802 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 1947 | tagging_loss_video: 8.469|tagging_loss_audio: 11.791|tagging_loss_text: 12.258|tagging_loss_image: 8.273|tagging_loss_fusion: 7.567|total_loss: 48.359 | 71.81 Examples/sec\n",
      "INFO:tensorflow:training step 1948 | tagging_loss_video: 8.035|tagging_loss_audio: 11.865|tagging_loss_text: 18.260|tagging_loss_image: 10.290|tagging_loss_fusion: 9.794|total_loss: 58.243 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 1949 | tagging_loss_video: 7.529|tagging_loss_audio: 10.661|tagging_loss_text: 16.374|tagging_loss_image: 10.061|tagging_loss_fusion: 9.576|total_loss: 54.201 | 69.37 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1950 |tagging_loss_video: 8.670|tagging_loss_audio: 14.560|tagging_loss_text: 14.752|tagging_loss_image: 9.906|tagging_loss_fusion: 9.532|total_loss: 57.418 | Examples/sec: 70.02\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.69 | precision@0.5: 0.90 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1951 | tagging_loss_video: 9.020|tagging_loss_audio: 13.280|tagging_loss_text: 21.547|tagging_loss_image: 9.750|tagging_loss_fusion: 7.928|total_loss: 61.525 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 1952 | tagging_loss_video: 8.593|tagging_loss_audio: 11.884|tagging_loss_text: 16.585|tagging_loss_image: 9.329|tagging_loss_fusion: 9.043|total_loss: 55.434 | 61.68 Examples/sec\n",
      "INFO:tensorflow:training step 1953 | tagging_loss_video: 6.873|tagging_loss_audio: 9.717|tagging_loss_text: 15.275|tagging_loss_image: 8.373|tagging_loss_fusion: 7.228|total_loss: 47.466 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 1954 | tagging_loss_video: 6.506|tagging_loss_audio: 10.164|tagging_loss_text: 13.110|tagging_loss_image: 8.196|tagging_loss_fusion: 6.818|total_loss: 44.795 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 1955 | tagging_loss_video: 7.172|tagging_loss_audio: 8.828|tagging_loss_text: 15.519|tagging_loss_image: 8.847|tagging_loss_fusion: 6.473|total_loss: 46.838 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 1956 | tagging_loss_video: 8.498|tagging_loss_audio: 11.311|tagging_loss_text: 14.275|tagging_loss_image: 8.985|tagging_loss_fusion: 9.379|total_loss: 52.448 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 1957 | tagging_loss_video: 7.319|tagging_loss_audio: 11.516|tagging_loss_text: 12.147|tagging_loss_image: 8.836|tagging_loss_fusion: 8.706|total_loss: 48.524 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 1958 | tagging_loss_video: 8.609|tagging_loss_audio: 11.821|tagging_loss_text: 12.618|tagging_loss_image: 10.844|tagging_loss_fusion: 8.845|total_loss: 52.737 | 64.05 Examples/sec\n",
      "INFO:tensorflow:training step 1959 | tagging_loss_video: 6.946|tagging_loss_audio: 11.154|tagging_loss_text: 16.911|tagging_loss_image: 8.424|tagging_loss_fusion: 9.628|total_loss: 53.063 | 69.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1960 |tagging_loss_video: 8.560|tagging_loss_audio: 12.409|tagging_loss_text: 13.495|tagging_loss_image: 9.695|tagging_loss_fusion: 9.167|total_loss: 53.326 | Examples/sec: 72.03\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.68 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 1961 | tagging_loss_video: 8.497|tagging_loss_audio: 11.056|tagging_loss_text: 15.129|tagging_loss_image: 10.675|tagging_loss_fusion: 9.933|total_loss: 55.291 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 1962 | tagging_loss_video: 9.317|tagging_loss_audio: 11.912|tagging_loss_text: 15.374|tagging_loss_image: 9.932|tagging_loss_fusion: 10.290|total_loss: 56.825 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 1963 | tagging_loss_video: 7.433|tagging_loss_audio: 12.177|tagging_loss_text: 17.049|tagging_loss_image: 8.365|tagging_loss_fusion: 11.211|total_loss: 56.235 | 60.74 Examples/sec\n",
      "INFO:tensorflow:training step 1964 | tagging_loss_video: 8.431|tagging_loss_audio: 13.716|tagging_loss_text: 15.893|tagging_loss_image: 10.087|tagging_loss_fusion: 9.749|total_loss: 57.877 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 1965 | tagging_loss_video: 8.117|tagging_loss_audio: 9.869|tagging_loss_text: 12.572|tagging_loss_image: 10.258|tagging_loss_fusion: 7.756|total_loss: 48.572 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 1966 | tagging_loss_video: 8.491|tagging_loss_audio: 11.295|tagging_loss_text: 16.566|tagging_loss_image: 9.432|tagging_loss_fusion: 9.698|total_loss: 55.481 | 61.56 Examples/sec\n",
      "INFO:tensorflow:training step 1967 | tagging_loss_video: 9.121|tagging_loss_audio: 16.504|tagging_loss_text: 14.135|tagging_loss_image: 10.312|tagging_loss_fusion: 8.826|total_loss: 58.898 | 72.04 Examples/sec\n",
      "INFO:tensorflow:training step 1968 | tagging_loss_video: 7.481|tagging_loss_audio: 12.512|tagging_loss_text: 17.092|tagging_loss_image: 10.080|tagging_loss_fusion: 8.819|total_loss: 55.984 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 1969 | tagging_loss_video: 7.273|tagging_loss_audio: 11.170|tagging_loss_text: 17.232|tagging_loss_image: 8.787|tagging_loss_fusion: 13.046|total_loss: 57.507 | 68.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1970 |tagging_loss_video: 8.844|tagging_loss_audio: 12.541|tagging_loss_text: 17.322|tagging_loss_image: 9.910|tagging_loss_fusion: 7.649|total_loss: 56.266 | Examples/sec: 68.48\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.70 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 1971 | tagging_loss_video: 8.872|tagging_loss_audio: 12.226|tagging_loss_text: 15.939|tagging_loss_image: 10.775|tagging_loss_fusion: 9.242|total_loss: 57.053 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 1972 | tagging_loss_video: 8.909|tagging_loss_audio: 13.135|tagging_loss_text: 14.092|tagging_loss_image: 10.802|tagging_loss_fusion: 10.143|total_loss: 57.081 | 66.83 Examples/sec\n",
      "INFO:tensorflow:training step 1973 | tagging_loss_video: 9.712|tagging_loss_audio: 11.636|tagging_loss_text: 13.497|tagging_loss_image: 11.209|tagging_loss_fusion: 8.609|total_loss: 54.664 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 1974 | tagging_loss_video: 8.238|tagging_loss_audio: 13.972|tagging_loss_text: 15.186|tagging_loss_image: 9.835|tagging_loss_fusion: 7.928|total_loss: 55.158 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 1975 | tagging_loss_video: 9.051|tagging_loss_audio: 13.605|tagging_loss_text: 17.653|tagging_loss_image: 9.981|tagging_loss_fusion: 10.254|total_loss: 60.543 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 1976 | tagging_loss_video: 7.497|tagging_loss_audio: 11.225|tagging_loss_text: 14.273|tagging_loss_image: 8.060|tagging_loss_fusion: 8.392|total_loss: 49.448 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 1977 | tagging_loss_video: 8.523|tagging_loss_audio: 13.679|tagging_loss_text: 16.586|tagging_loss_image: 10.119|tagging_loss_fusion: 9.901|total_loss: 58.809 | 60.79 Examples/sec\n",
      "INFO:tensorflow:training step 1978 | tagging_loss_video: 9.920|tagging_loss_audio: 13.897|tagging_loss_text: 18.834|tagging_loss_image: 11.918|tagging_loss_fusion: 8.709|total_loss: 63.279 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 1979 | tagging_loss_video: 8.764|tagging_loss_audio: 14.685|tagging_loss_text: 16.030|tagging_loss_image: 9.546|tagging_loss_fusion: 9.745|total_loss: 58.771 | 71.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1980 |tagging_loss_video: 8.037|tagging_loss_audio: 11.299|tagging_loss_text: 14.803|tagging_loss_image: 9.416|tagging_loss_fusion: 9.103|total_loss: 52.659 | Examples/sec: 61.85\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.66 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 1981 | tagging_loss_video: 9.088|tagging_loss_audio: 14.757|tagging_loss_text: 22.080|tagging_loss_image: 10.621|tagging_loss_fusion: 11.198|total_loss: 67.744 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 1982 | tagging_loss_video: 8.075|tagging_loss_audio: 12.565|tagging_loss_text: 18.070|tagging_loss_image: 10.486|tagging_loss_fusion: 10.745|total_loss: 59.941 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 1983 | tagging_loss_video: 7.573|tagging_loss_audio: 11.255|tagging_loss_text: 14.711|tagging_loss_image: 8.714|tagging_loss_fusion: 8.654|total_loss: 50.907 | 64.78 Examples/sec\n",
      "INFO:tensorflow:training step 1984 | tagging_loss_video: 8.419|tagging_loss_audio: 11.416|tagging_loss_text: 14.799|tagging_loss_image: 10.212|tagging_loss_fusion: 10.800|total_loss: 55.646 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 1985 | tagging_loss_video: 7.871|tagging_loss_audio: 11.234|tagging_loss_text: 18.514|tagging_loss_image: 9.421|tagging_loss_fusion: 8.647|total_loss: 55.687 | 66.80 Examples/sec\n",
      "INFO:tensorflow:training step 1986 | tagging_loss_video: 7.914|tagging_loss_audio: 11.913|tagging_loss_text: 12.665|tagging_loss_image: 8.817|tagging_loss_fusion: 9.540|total_loss: 50.848 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 1987 | tagging_loss_video: 8.279|tagging_loss_audio: 13.827|tagging_loss_text: 16.349|tagging_loss_image: 11.506|tagging_loss_fusion: 7.517|total_loss: 57.479 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 1988 | tagging_loss_video: 9.251|tagging_loss_audio: 13.092|tagging_loss_text: 20.013|tagging_loss_image: 10.133|tagging_loss_fusion: 8.901|total_loss: 61.390 | 62.03 Examples/sec\n",
      "INFO:tensorflow:training step 1989 | tagging_loss_video: 7.507|tagging_loss_audio: 10.946|tagging_loss_text: 13.218|tagging_loss_image: 9.491|tagging_loss_fusion: 10.728|total_loss: 51.890 | 71.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 1990 |tagging_loss_video: 7.738|tagging_loss_audio: 11.060|tagging_loss_text: 15.008|tagging_loss_image: 8.449|tagging_loss_fusion: 9.078|total_loss: 51.333 | Examples/sec: 66.83\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.67 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 1991 | tagging_loss_video: 7.726|tagging_loss_audio: 11.916|tagging_loss_text: 16.184|tagging_loss_image: 9.094|tagging_loss_fusion: 8.263|total_loss: 53.184 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 1992 | tagging_loss_video: 8.649|tagging_loss_audio: 15.710|tagging_loss_text: 15.463|tagging_loss_image: 10.054|tagging_loss_fusion: 8.488|total_loss: 58.365 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 1993 | tagging_loss_video: 7.748|tagging_loss_audio: 12.752|tagging_loss_text: 11.659|tagging_loss_image: 9.698|tagging_loss_fusion: 8.277|total_loss: 50.134 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 1994 | tagging_loss_video: 7.632|tagging_loss_audio: 10.876|tagging_loss_text: 15.920|tagging_loss_image: 8.784|tagging_loss_fusion: 9.134|total_loss: 52.347 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 1995 | tagging_loss_video: 7.240|tagging_loss_audio: 13.469|tagging_loss_text: 18.019|tagging_loss_image: 9.461|tagging_loss_fusion: 8.046|total_loss: 56.235 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 1996 | tagging_loss_video: 7.068|tagging_loss_audio: 12.971|tagging_loss_text: 14.153|tagging_loss_image: 8.896|tagging_loss_fusion: 9.286|total_loss: 52.373 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 1997 | tagging_loss_video: 7.790|tagging_loss_audio: 12.240|tagging_loss_text: 17.192|tagging_loss_image: 9.500|tagging_loss_fusion: 7.756|total_loss: 54.477 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 1998 | tagging_loss_video: 6.989|tagging_loss_audio: 11.770|tagging_loss_text: 16.391|tagging_loss_image: 8.606|tagging_loss_fusion: 6.980|total_loss: 50.737 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 1999 | tagging_loss_video: 6.406|tagging_loss_audio: 10.557|tagging_loss_text: 13.361|tagging_loss_image: 8.644|tagging_loss_fusion: 6.269|total_loss: 45.237 | 70.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2000 |tagging_loss_video: 7.735|tagging_loss_audio: 13.246|tagging_loss_text: 13.363|tagging_loss_image: 9.367|tagging_loss_fusion: 7.250|total_loss: 50.960 | Examples/sec: 71.25\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.71 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.675|loss: 24.046|GAP: 0.684|examples_per_second: 88.210\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.689|loss: 23.394|GAP: 0.718|examples_per_second: 100.161\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 0.969|perr: 0.667|loss: 30.501|GAP: 0.658|examples_per_second: 101.267\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.736|loss: 20.797|GAP: 0.748|examples_per_second: 99.277\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.744|loss: 21.910|GAP: 0.732|examples_per_second: 100.323\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.681|loss: 24.293|GAP: 0.684|examples_per_second: 93.556\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.698|loss: 26.416|GAP: 0.691|examples_per_second: 97.211\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.709|loss: 23.198|GAP: 0.719|examples_per_second: 92.238\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 0.969|perr: 0.693|loss: 24.208|GAP: 0.710|examples_per_second: 89.661\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.631|loss: 26.323|GAP: 0.661|examples_per_second: 92.052\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 0.969|perr: 0.731|loss: 21.228|GAP: 0.734|examples_per_second: 94.057\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 1.000|perr: 0.727|loss: 21.875|GAP: 0.740|examples_per_second: 92.083\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.711|loss: 22.918|GAP: 0.720|examples_per_second: 88.998\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.702|loss: 23.907|GAP: 0.703|examples_per_second: 95.910\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.705|loss: 23.526|GAP: 0.699|examples_per_second: 91.628\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 2000 | MAP: 0.233 | GAP: 0.670 | p@0.1: 0.566 | p@0.5:0.800 | r@0.1:0.772 | r@0.5: 0.554 | Avg_Loss: 22.964217\n",
      "INFO:tensorflow:epoch/eval number 2000 | MAP: 0.226 | GAP: 0.670 | p@0.1: 0.435 | p@0.5:0.790 | r@0.1:0.878 | r@0.5: 0.541 | Avg_Loss: 18.988651\n",
      "INFO:tensorflow:epoch/eval number 2000 | MAP: 0.110 | GAP: 0.564 | p@0.1: 0.359 | p@0.5:0.766 | r@0.1:0.827 | r@0.5: 0.397 | Avg_Loss: 22.694331\n",
      "INFO:tensorflow:epoch/eval number 2000 | MAP: 0.252 | GAP: 0.644 | p@0.1: 0.531 | p@0.5:0.706 | r@0.1:0.783 | r@0.5: 0.617 | Avg_Loss: 25.692114\n",
      "INFO:tensorflow:epoch/eval number 2000 | MAP: 0.285 | GAP: 0.706 | p@0.1: 0.642 | p@0.5:0.794 | r@0.1:0.754 | r@0.5: 0.606 | Avg_Loss: 23.902562\n",
      "INFO:tensorflow:validation score on val799 is : 0.7064\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-2000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_2000_0.7064/saved_model.pb\n",
      "INFO:tensorflow:training step 2001 | tagging_loss_video: 6.475|tagging_loss_audio: 7.634|tagging_loss_text: 15.919|tagging_loss_image: 7.955|tagging_loss_fusion: 6.096|total_loss: 44.078 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 2002 | tagging_loss_video: 8.614|tagging_loss_audio: 12.125|tagging_loss_text: 12.239|tagging_loss_image: 8.393|tagging_loss_fusion: 10.193|total_loss: 51.564 | 71.74 Examples/sec\n",
      "INFO:tensorflow:training step 2003 | tagging_loss_video: 8.351|tagging_loss_audio: 11.066|tagging_loss_text: 16.568|tagging_loss_image: 9.213|tagging_loss_fusion: 10.644|total_loss: 55.842 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 2004 | tagging_loss_video: 7.552|tagging_loss_audio: 11.042|tagging_loss_text: 19.028|tagging_loss_image: 8.741|tagging_loss_fusion: 7.403|total_loss: 53.766 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 2005 | tagging_loss_video: 8.306|tagging_loss_audio: 12.910|tagging_loss_text: 18.694|tagging_loss_image: 10.004|tagging_loss_fusion: 9.353|total_loss: 59.267 | 72.04 Examples/sec\n",
      "INFO:tensorflow:training step 2006 | tagging_loss_video: 7.111|tagging_loss_audio: 12.583|tagging_loss_text: 14.131|tagging_loss_image: 8.986|tagging_loss_fusion: 9.208|total_loss: 52.019 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 2007 | tagging_loss_video: 8.515|tagging_loss_audio: 12.502|tagging_loss_text: 14.290|tagging_loss_image: 9.637|tagging_loss_fusion: 8.221|total_loss: 53.164 | 70.35 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2008 | tagging_loss_video: 8.837|tagging_loss_audio: 12.320|tagging_loss_text: 15.762|tagging_loss_image: 10.040|tagging_loss_fusion: 9.494|total_loss: 56.453 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 2009 | tagging_loss_video: 8.020|tagging_loss_audio: 11.678|tagging_loss_text: 17.575|tagging_loss_image: 9.032|tagging_loss_fusion: 7.809|total_loss: 54.113 | 70.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2010 |tagging_loss_video: 8.160|tagging_loss_audio: 10.756|tagging_loss_text: 11.798|tagging_loss_image: 10.069|tagging_loss_fusion: 8.893|total_loss: 49.677 | Examples/sec: 67.00\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.67 | precision@0.5: 0.86 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2011 | tagging_loss_video: 6.625|tagging_loss_audio: 10.077|tagging_loss_text: 14.709|tagging_loss_image: 9.599|tagging_loss_fusion: 7.164|total_loss: 48.173 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 2012 | tagging_loss_video: 8.510|tagging_loss_audio: 12.521|tagging_loss_text: 17.220|tagging_loss_image: 8.520|tagging_loss_fusion: 8.103|total_loss: 54.874 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 2013 | tagging_loss_video: 8.055|tagging_loss_audio: 12.571|tagging_loss_text: 16.157|tagging_loss_image: 7.799|tagging_loss_fusion: 8.708|total_loss: 53.290 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 2014 | tagging_loss_video: 7.876|tagging_loss_audio: 11.983|tagging_loss_text: 14.368|tagging_loss_image: 9.719|tagging_loss_fusion: 9.742|total_loss: 53.688 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 2015 | tagging_loss_video: 7.853|tagging_loss_audio: 10.072|tagging_loss_text: 12.498|tagging_loss_image: 9.346|tagging_loss_fusion: 8.938|total_loss: 48.707 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 2016 | tagging_loss_video: 7.167|tagging_loss_audio: 11.278|tagging_loss_text: 21.556|tagging_loss_image: 9.298|tagging_loss_fusion: 10.525|total_loss: 59.824 | 66.64 Examples/sec\n",
      "INFO:tensorflow:training step 2017 | tagging_loss_video: 6.995|tagging_loss_audio: 10.365|tagging_loss_text: 13.798|tagging_loss_image: 9.197|tagging_loss_fusion: 8.442|total_loss: 48.798 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 2018 | tagging_loss_video: 8.395|tagging_loss_audio: 12.416|tagging_loss_text: 13.257|tagging_loss_image: 9.027|tagging_loss_fusion: 8.871|total_loss: 51.965 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 2019 | tagging_loss_video: 7.792|tagging_loss_audio: 12.723|tagging_loss_text: 20.753|tagging_loss_image: 10.888|tagging_loss_fusion: 9.460|total_loss: 61.616 | 71.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2020 |tagging_loss_video: 7.420|tagging_loss_audio: 10.672|tagging_loss_text: 12.576|tagging_loss_image: 9.268|tagging_loss_fusion: 8.105|total_loss: 48.041 | Examples/sec: 68.02\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.70 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2021 | tagging_loss_video: 8.479|tagging_loss_audio: 11.278|tagging_loss_text: 10.508|tagging_loss_image: 9.858|tagging_loss_fusion: 8.780|total_loss: 48.904 | 71.44 Examples/sec\n",
      "INFO:tensorflow:training step 2022 | tagging_loss_video: 8.764|tagging_loss_audio: 10.478|tagging_loss_text: 17.021|tagging_loss_image: 9.525|tagging_loss_fusion: 9.530|total_loss: 55.318 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 2023 | tagging_loss_video: 7.467|tagging_loss_audio: 11.508|tagging_loss_text: 16.168|tagging_loss_image: 9.604|tagging_loss_fusion: 7.507|total_loss: 52.254 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 2024 | tagging_loss_video: 9.057|tagging_loss_audio: 12.661|tagging_loss_text: 12.066|tagging_loss_image: 11.197|tagging_loss_fusion: 9.860|total_loss: 54.841 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 2025 | tagging_loss_video: 7.951|tagging_loss_audio: 11.707|tagging_loss_text: 16.843|tagging_loss_image: 9.700|tagging_loss_fusion: 7.280|total_loss: 53.482 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 2026 | tagging_loss_video: 7.791|tagging_loss_audio: 11.571|tagging_loss_text: 12.492|tagging_loss_image: 9.972|tagging_loss_fusion: 7.863|total_loss: 49.689 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 2027 | tagging_loss_video: 7.584|tagging_loss_audio: 12.685|tagging_loss_text: 13.840|tagging_loss_image: 9.145|tagging_loss_fusion: 8.002|total_loss: 51.255 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 2028 | tagging_loss_video: 7.602|tagging_loss_audio: 10.517|tagging_loss_text: 16.191|tagging_loss_image: 10.053|tagging_loss_fusion: 7.126|total_loss: 51.490 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 2029 | tagging_loss_video: 8.053|tagging_loss_audio: 11.971|tagging_loss_text: 12.545|tagging_loss_image: 10.088|tagging_loss_fusion: 9.242|total_loss: 51.899 | 71.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2030 |tagging_loss_video: 8.163|tagging_loss_audio: 11.464|tagging_loss_text: 14.235|tagging_loss_image: 8.266|tagging_loss_fusion: 8.667|total_loss: 50.795 | Examples/sec: 71.04\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.67 | precision@0.5: 0.86 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2031 | tagging_loss_video: 7.698|tagging_loss_audio: 13.145|tagging_loss_text: 14.483|tagging_loss_image: 9.803|tagging_loss_fusion: 8.000|total_loss: 53.130 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 2032 | tagging_loss_video: 7.983|tagging_loss_audio: 9.992|tagging_loss_text: 14.808|tagging_loss_image: 9.180|tagging_loss_fusion: 10.257|total_loss: 52.220 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 2033 | tagging_loss_video: 7.575|tagging_loss_audio: 10.101|tagging_loss_text: 17.458|tagging_loss_image: 9.669|tagging_loss_fusion: 9.090|total_loss: 53.894 | 71.68 Examples/sec\n",
      "INFO:tensorflow:training step 2034 | tagging_loss_video: 7.638|tagging_loss_audio: 11.481|tagging_loss_text: 15.012|tagging_loss_image: 8.077|tagging_loss_fusion: 9.995|total_loss: 52.204 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 2035 | tagging_loss_video: 8.149|tagging_loss_audio: 11.107|tagging_loss_text: 17.091|tagging_loss_image: 9.911|tagging_loss_fusion: 8.005|total_loss: 54.263 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 2036 | tagging_loss_video: 7.772|tagging_loss_audio: 13.928|tagging_loss_text: 16.370|tagging_loss_image: 10.517|tagging_loss_fusion: 9.000|total_loss: 57.587 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 2037 | tagging_loss_video: 8.118|tagging_loss_audio: 15.068|tagging_loss_text: 13.810|tagging_loss_image: 10.409|tagging_loss_fusion: 10.062|total_loss: 57.467 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 2038 | tagging_loss_video: 6.447|tagging_loss_audio: 12.286|tagging_loss_text: 14.090|tagging_loss_image: 10.221|tagging_loss_fusion: 7.146|total_loss: 50.190 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 2039 | tagging_loss_video: 8.275|tagging_loss_audio: 14.038|tagging_loss_text: 16.698|tagging_loss_image: 11.052|tagging_loss_fusion: 9.717|total_loss: 59.781 | 71.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2040 |tagging_loss_video: 7.295|tagging_loss_audio: 10.038|tagging_loss_text: 15.735|tagging_loss_image: 9.615|tagging_loss_fusion: 5.962|total_loss: 48.645 | Examples/sec: 68.56\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.70 | precision@0.5: 0.89 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2041 | tagging_loss_video: 7.012|tagging_loss_audio: 11.773|tagging_loss_text: 15.078|tagging_loss_image: 7.935|tagging_loss_fusion: 6.284|total_loss: 48.082 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 2042 | tagging_loss_video: 8.997|tagging_loss_audio: 13.408|tagging_loss_text: 17.503|tagging_loss_image: 8.888|tagging_loss_fusion: 8.000|total_loss: 56.796 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 2043 | tagging_loss_video: 6.886|tagging_loss_audio: 11.938|tagging_loss_text: 13.443|tagging_loss_image: 8.080|tagging_loss_fusion: 7.614|total_loss: 47.961 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 2044 | tagging_loss_video: 7.298|tagging_loss_audio: 12.154|tagging_loss_text: 16.116|tagging_loss_image: 10.412|tagging_loss_fusion: 8.054|total_loss: 54.034 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 2045 | tagging_loss_video: 7.531|tagging_loss_audio: 10.832|tagging_loss_text: 15.186|tagging_loss_image: 9.697|tagging_loss_fusion: 8.002|total_loss: 51.248 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 2046 | tagging_loss_video: 7.418|tagging_loss_audio: 11.731|tagging_loss_text: 14.657|tagging_loss_image: 9.160|tagging_loss_fusion: 9.747|total_loss: 52.714 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 2047 | tagging_loss_video: 5.893|tagging_loss_audio: 11.318|tagging_loss_text: 15.325|tagging_loss_image: 7.994|tagging_loss_fusion: 5.999|total_loss: 46.530 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 2048 | tagging_loss_video: 8.290|tagging_loss_audio: 12.190|tagging_loss_text: 16.334|tagging_loss_image: 9.808|tagging_loss_fusion: 9.951|total_loss: 56.572 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 2049 | tagging_loss_video: 6.444|tagging_loss_audio: 12.342|tagging_loss_text: 18.479|tagging_loss_image: 9.085|tagging_loss_fusion: 5.812|total_loss: 52.162 | 70.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2050 |tagging_loss_video: 6.627|tagging_loss_audio: 9.185|tagging_loss_text: 11.754|tagging_loss_image: 7.789|tagging_loss_fusion: 7.745|total_loss: 43.099 | Examples/sec: 70.22\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.68 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2051 | tagging_loss_video: 7.020|tagging_loss_audio: 11.177|tagging_loss_text: 15.426|tagging_loss_image: 7.732|tagging_loss_fusion: 6.957|total_loss: 48.312 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 2052 | tagging_loss_video: 7.707|tagging_loss_audio: 12.451|tagging_loss_text: 17.354|tagging_loss_image: 9.088|tagging_loss_fusion: 8.770|total_loss: 55.369 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 2053 | tagging_loss_video: 7.615|tagging_loss_audio: 12.783|tagging_loss_text: 17.582|tagging_loss_image: 9.384|tagging_loss_fusion: 10.307|total_loss: 57.672 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 2054 | tagging_loss_video: 6.353|tagging_loss_audio: 9.578|tagging_loss_text: 14.220|tagging_loss_image: 7.790|tagging_loss_fusion: 7.749|total_loss: 45.690 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 2055 | tagging_loss_video: 7.870|tagging_loss_audio: 13.280|tagging_loss_text: 18.803|tagging_loss_image: 9.453|tagging_loss_fusion: 7.634|total_loss: 57.041 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 2056 | tagging_loss_video: 7.971|tagging_loss_audio: 11.650|tagging_loss_text: 14.421|tagging_loss_image: 8.028|tagging_loss_fusion: 7.964|total_loss: 50.035 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 2057 | tagging_loss_video: 7.490|tagging_loss_audio: 8.745|tagging_loss_text: 16.334|tagging_loss_image: 7.041|tagging_loss_fusion: 6.955|total_loss: 46.565 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 2058 | tagging_loss_video: 7.226|tagging_loss_audio: 9.383|tagging_loss_text: 11.092|tagging_loss_image: 8.594|tagging_loss_fusion: 7.358|total_loss: 43.654 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 2059 | tagging_loss_video: 7.561|tagging_loss_audio: 11.483|tagging_loss_text: 14.594|tagging_loss_image: 9.145|tagging_loss_fusion: 9.545|total_loss: 52.328 | 70.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2060 |tagging_loss_video: 7.074|tagging_loss_audio: 12.010|tagging_loss_text: 14.680|tagging_loss_image: 9.032|tagging_loss_fusion: 7.966|total_loss: 50.761 | Examples/sec: 70.37\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.93 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2061 | tagging_loss_video: 7.344|tagging_loss_audio: 9.619|tagging_loss_text: 18.090|tagging_loss_image: 9.022|tagging_loss_fusion: 8.830|total_loss: 52.905 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 2062 | tagging_loss_video: 7.376|tagging_loss_audio: 11.179|tagging_loss_text: 14.629|tagging_loss_image: 8.946|tagging_loss_fusion: 7.502|total_loss: 49.631 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 2063 | tagging_loss_video: 7.411|tagging_loss_audio: 12.384|tagging_loss_text: 16.248|tagging_loss_image: 9.861|tagging_loss_fusion: 9.555|total_loss: 55.458 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 2064 | tagging_loss_video: 7.126|tagging_loss_audio: 12.606|tagging_loss_text: 16.327|tagging_loss_image: 9.309|tagging_loss_fusion: 9.459|total_loss: 54.828 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 2065 | tagging_loss_video: 8.835|tagging_loss_audio: 11.361|tagging_loss_text: 16.482|tagging_loss_image: 9.822|tagging_loss_fusion: 9.722|total_loss: 56.221 | 68.94 Examples/sec\n",
      "INFO:tensorflow:training step 2066 | tagging_loss_video: 7.143|tagging_loss_audio: 10.204|tagging_loss_text: 13.943|tagging_loss_image: 8.796|tagging_loss_fusion: 8.169|total_loss: 48.256 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 2067 | tagging_loss_video: 7.251|tagging_loss_audio: 10.355|tagging_loss_text: 13.078|tagging_loss_image: 7.508|tagging_loss_fusion: 8.144|total_loss: 46.337 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2068 | tagging_loss_video: 8.893|tagging_loss_audio: 10.933|tagging_loss_text: 15.931|tagging_loss_image: 8.668|tagging_loss_fusion: 10.296|total_loss: 54.722 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 2069 | tagging_loss_video: 6.583|tagging_loss_audio: 11.758|tagging_loss_text: 11.585|tagging_loss_image: 8.329|tagging_loss_fusion: 6.818|total_loss: 45.074 | 70.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2070 |tagging_loss_video: 7.190|tagging_loss_audio: 10.563|tagging_loss_text: 13.765|tagging_loss_image: 9.576|tagging_loss_fusion: 7.602|total_loss: 48.696 | Examples/sec: 69.11\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.73 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2071 | tagging_loss_video: 7.691|tagging_loss_audio: 10.938|tagging_loss_text: 16.582|tagging_loss_image: 8.832|tagging_loss_fusion: 8.098|total_loss: 52.140 | 67.66 Examples/sec\n",
      "INFO:tensorflow:training step 2072 | tagging_loss_video: 7.227|tagging_loss_audio: 10.315|tagging_loss_text: 13.472|tagging_loss_image: 8.004|tagging_loss_fusion: 7.963|total_loss: 46.981 | 72.45 Examples/sec\n",
      "INFO:tensorflow:training step 2073 | tagging_loss_video: 8.166|tagging_loss_audio: 14.021|tagging_loss_text: 16.053|tagging_loss_image: 9.963|tagging_loss_fusion: 8.088|total_loss: 56.290 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 2074 | tagging_loss_video: 7.436|tagging_loss_audio: 13.573|tagging_loss_text: 16.336|tagging_loss_image: 10.036|tagging_loss_fusion: 8.514|total_loss: 55.894 | 72.43 Examples/sec\n",
      "INFO:tensorflow:training step 2075 | tagging_loss_video: 8.179|tagging_loss_audio: 12.753|tagging_loss_text: 16.227|tagging_loss_image: 8.768|tagging_loss_fusion: 7.674|total_loss: 53.601 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 2076 | tagging_loss_video: 9.118|tagging_loss_audio: 13.839|tagging_loss_text: 17.214|tagging_loss_image: 11.413|tagging_loss_fusion: 8.794|total_loss: 60.378 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 2077 | tagging_loss_video: 7.821|tagging_loss_audio: 10.659|tagging_loss_text: 11.696|tagging_loss_image: 9.235|tagging_loss_fusion: 11.020|total_loss: 50.430 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 2078 | tagging_loss_video: 6.985|tagging_loss_audio: 12.083|tagging_loss_text: 16.597|tagging_loss_image: 8.370|tagging_loss_fusion: 8.403|total_loss: 52.438 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 2079 | tagging_loss_video: 6.767|tagging_loss_audio: 11.572|tagging_loss_text: 14.332|tagging_loss_image: 7.725|tagging_loss_fusion: 8.312|total_loss: 48.708 | 71.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2080 |tagging_loss_video: 6.996|tagging_loss_audio: 11.574|tagging_loss_text: 11.688|tagging_loss_image: 7.924|tagging_loss_fusion: 8.264|total_loss: 46.445 | Examples/sec: 68.08\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.67 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2081 | tagging_loss_video: 6.886|tagging_loss_audio: 12.598|tagging_loss_text: 14.171|tagging_loss_image: 8.532|tagging_loss_fusion: 6.707|total_loss: 48.894 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 2082 | tagging_loss_video: 6.791|tagging_loss_audio: 11.658|tagging_loss_text: 14.272|tagging_loss_image: 9.113|tagging_loss_fusion: 7.296|total_loss: 49.130 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 2083 | tagging_loss_video: 8.573|tagging_loss_audio: 12.515|tagging_loss_text: 15.423|tagging_loss_image: 10.110|tagging_loss_fusion: 7.421|total_loss: 54.042 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 2084 | tagging_loss_video: 7.298|tagging_loss_audio: 10.118|tagging_loss_text: 16.816|tagging_loss_image: 9.224|tagging_loss_fusion: 8.777|total_loss: 52.233 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 2085 | tagging_loss_video: 7.812|tagging_loss_audio: 12.454|tagging_loss_text: 17.929|tagging_loss_image: 9.457|tagging_loss_fusion: 10.215|total_loss: 57.868 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 2086 | tagging_loss_video: 7.866|tagging_loss_audio: 12.099|tagging_loss_text: 13.684|tagging_loss_image: 10.174|tagging_loss_fusion: 7.769|total_loss: 51.592 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 2087 | tagging_loss_video: 8.840|tagging_loss_audio: 12.374|tagging_loss_text: 16.757|tagging_loss_image: 9.356|tagging_loss_fusion: 8.981|total_loss: 56.309 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 2088 | tagging_loss_video: 8.894|tagging_loss_audio: 11.686|tagging_loss_text: 17.792|tagging_loss_image: 8.636|tagging_loss_fusion: 9.485|total_loss: 56.493 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 2089 | tagging_loss_video: 8.041|tagging_loss_audio: 12.931|tagging_loss_text: 13.012|tagging_loss_image: 9.997|tagging_loss_fusion: 12.004|total_loss: 55.986 | 70.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2090 |tagging_loss_video: 7.322|tagging_loss_audio: 13.243|tagging_loss_text: 13.831|tagging_loss_image: 9.044|tagging_loss_fusion: 8.062|total_loss: 51.502 | Examples/sec: 70.03\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.72 | precision@0.5: 0.92 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2091 | tagging_loss_video: 6.701|tagging_loss_audio: 12.544|tagging_loss_text: 15.147|tagging_loss_image: 9.847|tagging_loss_fusion: 6.514|total_loss: 50.752 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 2092 | tagging_loss_video: 8.360|tagging_loss_audio: 14.271|tagging_loss_text: 15.670|tagging_loss_image: 10.569|tagging_loss_fusion: 8.038|total_loss: 56.907 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 2093 | tagging_loss_video: 7.442|tagging_loss_audio: 12.016|tagging_loss_text: 15.686|tagging_loss_image: 8.764|tagging_loss_fusion: 7.640|total_loss: 51.548 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 2094 | tagging_loss_video: 7.977|tagging_loss_audio: 13.688|tagging_loss_text: 16.537|tagging_loss_image: 9.762|tagging_loss_fusion: 9.742|total_loss: 57.706 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 2095 | tagging_loss_video: 8.674|tagging_loss_audio: 11.303|tagging_loss_text: 16.002|tagging_loss_image: 8.527|tagging_loss_fusion: 9.051|total_loss: 53.557 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 2096 | tagging_loss_video: 9.361|tagging_loss_audio: 11.526|tagging_loss_text: 15.329|tagging_loss_image: 10.376|tagging_loss_fusion: 8.989|total_loss: 55.580 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 2097 | tagging_loss_video: 8.867|tagging_loss_audio: 11.994|tagging_loss_text: 14.844|tagging_loss_image: 10.050|tagging_loss_fusion: 9.437|total_loss: 55.191 | 67.63 Examples/sec\n",
      "INFO:tensorflow:training step 2098 | tagging_loss_video: 8.868|tagging_loss_audio: 13.252|tagging_loss_text: 15.077|tagging_loss_image: 10.526|tagging_loss_fusion: 11.158|total_loss: 58.881 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 2099 | tagging_loss_video: 7.850|tagging_loss_audio: 12.195|tagging_loss_text: 14.529|tagging_loss_image: 8.129|tagging_loss_fusion: 9.350|total_loss: 52.053 | 63.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2100 |tagging_loss_video: 7.988|tagging_loss_audio: 11.222|tagging_loss_text: 14.775|tagging_loss_image: 9.433|tagging_loss_fusion: 6.525|total_loss: 49.943 | Examples/sec: 70.44\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.71 | precision@0.5: 0.89 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 2101 | tagging_loss_video: 7.284|tagging_loss_audio: 9.800|tagging_loss_text: 14.175|tagging_loss_image: 8.032|tagging_loss_fusion: 8.988|total_loss: 48.280 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 2102 | tagging_loss_video: 8.258|tagging_loss_audio: 12.948|tagging_loss_text: 15.210|tagging_loss_image: 9.852|tagging_loss_fusion: 9.655|total_loss: 55.924 | 63.10 Examples/sec\n",
      "INFO:tensorflow:training step 2103 | tagging_loss_video: 8.737|tagging_loss_audio: 13.937|tagging_loss_text: 15.176|tagging_loss_image: 11.521|tagging_loss_fusion: 9.483|total_loss: 58.854 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 2104 | tagging_loss_video: 7.603|tagging_loss_audio: 11.972|tagging_loss_text: 13.817|tagging_loss_image: 8.960|tagging_loss_fusion: 7.738|total_loss: 50.090 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 2105 | tagging_loss_video: 7.943|tagging_loss_audio: 11.235|tagging_loss_text: 15.412|tagging_loss_image: 9.920|tagging_loss_fusion: 9.031|total_loss: 53.541 | 58.21 Examples/sec\n",
      "INFO:tensorflow:training step 2106 | tagging_loss_video: 8.939|tagging_loss_audio: 14.368|tagging_loss_text: 14.575|tagging_loss_image: 11.525|tagging_loss_fusion: 11.143|total_loss: 60.550 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 2107 | tagging_loss_video: 7.685|tagging_loss_audio: 11.640|tagging_loss_text: 17.948|tagging_loss_image: 8.900|tagging_loss_fusion: 8.638|total_loss: 54.812 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 2108 | tagging_loss_video: 7.050|tagging_loss_audio: 10.062|tagging_loss_text: 17.383|tagging_loss_image: 8.089|tagging_loss_fusion: 7.710|total_loss: 50.294 | 66.06 Examples/sec\n",
      "INFO:tensorflow:training step 2109 | tagging_loss_video: 7.754|tagging_loss_audio: 11.210|tagging_loss_text: 16.114|tagging_loss_image: 9.852|tagging_loss_fusion: 9.832|total_loss: 54.762 | 71.50 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2110 |tagging_loss_video: 7.743|tagging_loss_audio: 12.943|tagging_loss_text: 16.375|tagging_loss_image: 9.318|tagging_loss_fusion: 8.990|total_loss: 55.369 | Examples/sec: 69.68\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.70 | precision@0.5: 0.92 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2111 | tagging_loss_video: 7.431|tagging_loss_audio: 10.942|tagging_loss_text: 16.677|tagging_loss_image: 8.415|tagging_loss_fusion: 7.616|total_loss: 51.081 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 2112 | tagging_loss_video: 8.224|tagging_loss_audio: 12.308|tagging_loss_text: 18.573|tagging_loss_image: 10.219|tagging_loss_fusion: 9.797|total_loss: 59.122 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 2113 | tagging_loss_video: 7.736|tagging_loss_audio: 12.162|tagging_loss_text: 15.537|tagging_loss_image: 9.997|tagging_loss_fusion: 7.906|total_loss: 53.339 | 64.20 Examples/sec\n",
      "INFO:tensorflow:training step 2114 | tagging_loss_video: 7.286|tagging_loss_audio: 11.254|tagging_loss_text: 16.419|tagging_loss_image: 9.094|tagging_loss_fusion: 7.812|total_loss: 51.865 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 2115 | tagging_loss_video: 7.578|tagging_loss_audio: 11.428|tagging_loss_text: 13.661|tagging_loss_image: 9.201|tagging_loss_fusion: 9.684|total_loss: 51.551 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 2116 | tagging_loss_video: 7.220|tagging_loss_audio: 10.697|tagging_loss_text: 15.587|tagging_loss_image: 8.096|tagging_loss_fusion: 8.567|total_loss: 50.167 | 64.48 Examples/sec\n",
      "INFO:tensorflow:training step 2117 | tagging_loss_video: 7.545|tagging_loss_audio: 13.490|tagging_loss_text: 16.085|tagging_loss_image: 9.780|tagging_loss_fusion: 6.882|total_loss: 53.783 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 2118 | tagging_loss_video: 7.630|tagging_loss_audio: 12.240|tagging_loss_text: 13.594|tagging_loss_image: 9.564|tagging_loss_fusion: 6.389|total_loss: 49.418 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 2119 | tagging_loss_video: 6.701|tagging_loss_audio: 12.625|tagging_loss_text: 15.215|tagging_loss_image: 8.432|tagging_loss_fusion: 6.488|total_loss: 49.460 | 61.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2120 |tagging_loss_video: 6.982|tagging_loss_audio: 12.457|tagging_loss_text: 11.370|tagging_loss_image: 8.445|tagging_loss_fusion: 7.337|total_loss: 46.592 | Examples/sec: 71.47\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.69 | precision@0.5: 0.88 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2121 | tagging_loss_video: 6.846|tagging_loss_audio: 11.536|tagging_loss_text: 15.644|tagging_loss_image: 8.080|tagging_loss_fusion: 7.515|total_loss: 49.622 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 2122 | tagging_loss_video: 7.554|tagging_loss_audio: 12.479|tagging_loss_text: 9.936|tagging_loss_image: 8.898|tagging_loss_fusion: 7.074|total_loss: 45.941 | 67.27 Examples/sec\n",
      "INFO:tensorflow:training step 2123 | tagging_loss_video: 6.760|tagging_loss_audio: 10.387|tagging_loss_text: 15.099|tagging_loss_image: 8.652|tagging_loss_fusion: 7.287|total_loss: 48.185 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 2124 | tagging_loss_video: 6.270|tagging_loss_audio: 11.174|tagging_loss_text: 12.577|tagging_loss_image: 7.958|tagging_loss_fusion: 7.094|total_loss: 45.073 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 2125 | tagging_loss_video: 7.633|tagging_loss_audio: 10.447|tagging_loss_text: 16.775|tagging_loss_image: 8.957|tagging_loss_fusion: 7.412|total_loss: 51.225 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 2126 | tagging_loss_video: 7.750|tagging_loss_audio: 11.428|tagging_loss_text: 17.960|tagging_loss_image: 9.327|tagging_loss_fusion: 8.216|total_loss: 54.681 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 2127 | tagging_loss_video: 7.247|tagging_loss_audio: 13.241|tagging_loss_text: 12.315|tagging_loss_image: 8.996|tagging_loss_fusion: 8.588|total_loss: 50.386 | 61.10 Examples/sec\n",
      "INFO:tensorflow:training step 2128 | tagging_loss_video: 7.116|tagging_loss_audio: 10.088|tagging_loss_text: 13.991|tagging_loss_image: 9.506|tagging_loss_fusion: 8.238|total_loss: 48.938 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 2129 | tagging_loss_video: 7.117|tagging_loss_audio: 12.136|tagging_loss_text: 17.324|tagging_loss_image: 9.314|tagging_loss_fusion: 6.234|total_loss: 52.126 | 71.88 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 2130.\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2130 |tagging_loss_video: 6.663|tagging_loss_audio: 12.279|tagging_loss_text: 16.174|tagging_loss_image: 8.628|tagging_loss_fusion: 8.568|total_loss: 52.313 | Examples/sec: 46.85\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.68 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 2131 | tagging_loss_video: 8.526|tagging_loss_audio: 13.532|tagging_loss_text: 17.693|tagging_loss_image: 10.757|tagging_loss_fusion: 10.809|total_loss: 61.317 | 65.55 Examples/sec\n",
      "INFO:tensorflow:training step 2132 | tagging_loss_video: 7.754|tagging_loss_audio: 11.797|tagging_loss_text: 14.797|tagging_loss_image: 10.895|tagging_loss_fusion: 9.647|total_loss: 54.890 | 63.91 Examples/sec\n",
      "INFO:tensorflow:training step 2133 | tagging_loss_video: 7.849|tagging_loss_audio: 8.284|tagging_loss_text: 14.389|tagging_loss_image: 9.063|tagging_loss_fusion: 8.671|total_loss: 48.256 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 2134 | tagging_loss_video: 8.031|tagging_loss_audio: 12.776|tagging_loss_text: 17.119|tagging_loss_image: 10.863|tagging_loss_fusion: 9.214|total_loss: 58.003 | 71.12 Examples/sec\n",
      "INFO:tensorflow:global_step/sec: 1.77488\n",
      "INFO:tensorflow:training step 2135 | tagging_loss_video: 8.234|tagging_loss_audio: 11.816|tagging_loss_text: 14.724|tagging_loss_image: 9.108|tagging_loss_fusion: 8.487|total_loss: 52.368 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 2136 | tagging_loss_video: 7.819|tagging_loss_audio: 12.907|tagging_loss_text: 17.309|tagging_loss_image: 10.201|tagging_loss_fusion: 10.031|total_loss: 58.267 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 2137 | tagging_loss_video: 7.706|tagging_loss_audio: 12.600|tagging_loss_text: 12.604|tagging_loss_image: 8.872|tagging_loss_fusion: 6.710|total_loss: 48.492 | 63.50 Examples/sec\n",
      "INFO:tensorflow:training step 2138 | tagging_loss_video: 8.264|tagging_loss_audio: 11.739|tagging_loss_text: 14.532|tagging_loss_image: 9.531|tagging_loss_fusion: 7.677|total_loss: 51.743 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 2139 | tagging_loss_video: 7.716|tagging_loss_audio: 14.112|tagging_loss_text: 15.993|tagging_loss_image: 9.918|tagging_loss_fusion: 8.373|total_loss: 56.111 | 70.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2140 |tagging_loss_video: 6.019|tagging_loss_audio: 8.669|tagging_loss_text: 12.453|tagging_loss_image: 7.767|tagging_loss_fusion: 6.151|total_loss: 41.059 | Examples/sec: 61.30\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.71 | precision@0.5: 0.89 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2141 | tagging_loss_video: 7.279|tagging_loss_audio: 10.474|tagging_loss_text: 14.948|tagging_loss_image: 8.384|tagging_loss_fusion: 9.079|total_loss: 50.164 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2142 | tagging_loss_video: 7.464|tagging_loss_audio: 11.070|tagging_loss_text: 15.963|tagging_loss_image: 9.191|tagging_loss_fusion: 8.036|total_loss: 51.724 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 2143 | tagging_loss_video: 6.598|tagging_loss_audio: 10.963|tagging_loss_text: 16.204|tagging_loss_image: 9.014|tagging_loss_fusion: 5.757|total_loss: 48.536 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 2144 | tagging_loss_video: 7.778|tagging_loss_audio: 12.730|tagging_loss_text: 14.838|tagging_loss_image: 9.929|tagging_loss_fusion: 8.896|total_loss: 54.171 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 2145 | tagging_loss_video: 7.158|tagging_loss_audio: 10.793|tagging_loss_text: 16.124|tagging_loss_image: 8.350|tagging_loss_fusion: 6.722|total_loss: 49.148 | 67.53 Examples/sec\n",
      "INFO:tensorflow:training step 2146 | tagging_loss_video: 6.851|tagging_loss_audio: 12.361|tagging_loss_text: 15.021|tagging_loss_image: 8.657|tagging_loss_fusion: 8.193|total_loss: 51.083 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 2147 | tagging_loss_video: 8.525|tagging_loss_audio: 12.103|tagging_loss_text: 16.970|tagging_loss_image: 9.265|tagging_loss_fusion: 8.500|total_loss: 55.364 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 2148 | tagging_loss_video: 7.296|tagging_loss_audio: 10.175|tagging_loss_text: 13.570|tagging_loss_image: 8.691|tagging_loss_fusion: 8.105|total_loss: 47.838 | 66.22 Examples/sec\n",
      "INFO:tensorflow:training step 2149 | tagging_loss_video: 7.901|tagging_loss_audio: 11.796|tagging_loss_text: 13.840|tagging_loss_image: 8.746|tagging_loss_fusion: 9.109|total_loss: 51.393 | 69.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2150 |tagging_loss_video: 7.029|tagging_loss_audio: 10.454|tagging_loss_text: 15.403|tagging_loss_image: 8.814|tagging_loss_fusion: 7.282|total_loss: 48.982 | Examples/sec: 70.61\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.70 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2151 | tagging_loss_video: 8.049|tagging_loss_audio: 13.263|tagging_loss_text: 13.778|tagging_loss_image: 9.502|tagging_loss_fusion: 8.554|total_loss: 53.148 | 65.55 Examples/sec\n",
      "INFO:tensorflow:training step 2152 | tagging_loss_video: 7.769|tagging_loss_audio: 12.246|tagging_loss_text: 18.185|tagging_loss_image: 7.847|tagging_loss_fusion: 6.401|total_loss: 52.449 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 2153 | tagging_loss_video: 7.086|tagging_loss_audio: 11.683|tagging_loss_text: 13.651|tagging_loss_image: 8.547|tagging_loss_fusion: 7.076|total_loss: 48.043 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2154 | tagging_loss_video: 7.194|tagging_loss_audio: 11.816|tagging_loss_text: 16.320|tagging_loss_image: 8.761|tagging_loss_fusion: 6.469|total_loss: 50.561 | 63.79 Examples/sec\n",
      "INFO:tensorflow:training step 2155 | tagging_loss_video: 7.052|tagging_loss_audio: 12.728|tagging_loss_text: 18.735|tagging_loss_image: 9.133|tagging_loss_fusion: 6.738|total_loss: 54.386 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 2156 | tagging_loss_video: 6.778|tagging_loss_audio: 9.693|tagging_loss_text: 12.424|tagging_loss_image: 8.477|tagging_loss_fusion: 6.926|total_loss: 44.298 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 2157 | tagging_loss_video: 8.171|tagging_loss_audio: 11.464|tagging_loss_text: 16.784|tagging_loss_image: 8.248|tagging_loss_fusion: 8.419|total_loss: 53.087 | 64.52 Examples/sec\n",
      "INFO:tensorflow:training step 2158 | tagging_loss_video: 8.096|tagging_loss_audio: 13.309|tagging_loss_text: 13.358|tagging_loss_image: 10.405|tagging_loss_fusion: 8.158|total_loss: 53.325 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 2159 | tagging_loss_video: 7.391|tagging_loss_audio: 11.170|tagging_loss_text: 16.647|tagging_loss_image: 8.860|tagging_loss_fusion: 7.687|total_loss: 51.755 | 68.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2160 |tagging_loss_video: 7.683|tagging_loss_audio: 11.781|tagging_loss_text: 16.082|tagging_loss_image: 9.317|tagging_loss_fusion: 8.526|total_loss: 53.390 | Examples/sec: 71.21\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2161 | tagging_loss_video: 6.884|tagging_loss_audio: 12.820|tagging_loss_text: 16.979|tagging_loss_image: 9.213|tagging_loss_fusion: 7.359|total_loss: 53.255 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 2162 | tagging_loss_video: 7.187|tagging_loss_audio: 11.240|tagging_loss_text: 12.814|tagging_loss_image: 9.127|tagging_loss_fusion: 8.165|total_loss: 48.534 | 62.41 Examples/sec\n",
      "INFO:tensorflow:training step 2163 | tagging_loss_video: 7.872|tagging_loss_audio: 12.416|tagging_loss_text: 11.989|tagging_loss_image: 9.323|tagging_loss_fusion: 7.619|total_loss: 49.218 | 68.75 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2164 | tagging_loss_video: 8.103|tagging_loss_audio: 12.751|tagging_loss_text: 14.004|tagging_loss_image: 9.298|tagging_loss_fusion: 7.190|total_loss: 51.346 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 2165 | tagging_loss_video: 7.928|tagging_loss_audio: 10.340|tagging_loss_text: 15.285|tagging_loss_image: 7.775|tagging_loss_fusion: 8.668|total_loss: 49.995 | 61.92 Examples/sec\n",
      "INFO:tensorflow:training step 2166 | tagging_loss_video: 6.434|tagging_loss_audio: 11.363|tagging_loss_text: 15.944|tagging_loss_image: 8.794|tagging_loss_fusion: 8.470|total_loss: 51.005 | 66.91 Examples/sec\n",
      "INFO:tensorflow:training step 2167 | tagging_loss_video: 7.417|tagging_loss_audio: 12.143|tagging_loss_text: 14.810|tagging_loss_image: 8.425|tagging_loss_fusion: 8.066|total_loss: 50.862 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 2168 | tagging_loss_video: 8.892|tagging_loss_audio: 12.056|tagging_loss_text: 13.211|tagging_loss_image: 10.192|tagging_loss_fusion: 9.593|total_loss: 53.944 | 64.38 Examples/sec\n",
      "INFO:tensorflow:training step 2169 | tagging_loss_video: 7.434|tagging_loss_audio: 11.414|tagging_loss_text: 12.239|tagging_loss_image: 7.666|tagging_loss_fusion: 8.166|total_loss: 46.919 | 71.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2170 |tagging_loss_video: 7.244|tagging_loss_audio: 12.976|tagging_loss_text: 14.441|tagging_loss_image: 8.906|tagging_loss_fusion: 7.098|total_loss: 50.664 | Examples/sec: 70.34\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 2171 | tagging_loss_video: 7.657|tagging_loss_audio: 10.023|tagging_loss_text: 15.433|tagging_loss_image: 8.788|tagging_loss_fusion: 6.975|total_loss: 48.875 | 72.04 Examples/sec\n",
      "INFO:tensorflow:training step 2172 | tagging_loss_video: 8.101|tagging_loss_audio: 11.469|tagging_loss_text: 20.352|tagging_loss_image: 9.063|tagging_loss_fusion: 9.524|total_loss: 58.509 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 2173 | tagging_loss_video: 7.964|tagging_loss_audio: 11.833|tagging_loss_text: 16.985|tagging_loss_image: 7.941|tagging_loss_fusion: 7.023|total_loss: 51.745 | 66.29 Examples/sec\n",
      "INFO:tensorflow:training step 2174 | tagging_loss_video: 6.971|tagging_loss_audio: 11.501|tagging_loss_text: 15.030|tagging_loss_image: 8.333|tagging_loss_fusion: 9.187|total_loss: 51.022 | 67.08 Examples/sec\n",
      "INFO:tensorflow:training step 2175 | tagging_loss_video: 8.419|tagging_loss_audio: 10.813|tagging_loss_text: 15.633|tagging_loss_image: 10.037|tagging_loss_fusion: 8.348|total_loss: 53.250 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 2176 | tagging_loss_video: 9.119|tagging_loss_audio: 11.727|tagging_loss_text: 15.969|tagging_loss_image: 10.450|tagging_loss_fusion: 9.706|total_loss: 56.971 | 63.79 Examples/sec\n",
      "INFO:tensorflow:training step 2177 | tagging_loss_video: 6.930|tagging_loss_audio: 12.005|tagging_loss_text: 15.495|tagging_loss_image: 8.489|tagging_loss_fusion: 9.989|total_loss: 52.909 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 2178 | tagging_loss_video: 8.411|tagging_loss_audio: 12.542|tagging_loss_text: 18.281|tagging_loss_image: 10.198|tagging_loss_fusion: 7.411|total_loss: 56.844 | 65.10 Examples/sec\n",
      "INFO:tensorflow:training step 2179 | tagging_loss_video: 6.293|tagging_loss_audio: 11.700|tagging_loss_text: 13.260|tagging_loss_image: 7.822|tagging_loss_fusion: 6.009|total_loss: 45.084 | 68.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2180 |tagging_loss_video: 6.974|tagging_loss_audio: 10.850|tagging_loss_text: 14.504|tagging_loss_image: 7.911|tagging_loss_fusion: 7.934|total_loss: 48.174 | Examples/sec: 70.03\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2181 | tagging_loss_video: 7.533|tagging_loss_audio: 10.361|tagging_loss_text: 14.110|tagging_loss_image: 8.289|tagging_loss_fusion: 8.687|total_loss: 48.980 | 68.93 Examples/sec\n",
      "INFO:tensorflow:training step 2182 | tagging_loss_video: 7.001|tagging_loss_audio: 11.089|tagging_loss_text: 14.759|tagging_loss_image: 7.446|tagging_loss_fusion: 7.768|total_loss: 48.063 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 2183 | tagging_loss_video: 5.885|tagging_loss_audio: 11.657|tagging_loss_text: 15.420|tagging_loss_image: 9.310|tagging_loss_fusion: 5.780|total_loss: 48.053 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 2184 | tagging_loss_video: 8.179|tagging_loss_audio: 11.601|tagging_loss_text: 15.219|tagging_loss_image: 8.298|tagging_loss_fusion: 10.417|total_loss: 53.714 | 66.54 Examples/sec\n",
      "INFO:tensorflow:training step 2185 | tagging_loss_video: 6.945|tagging_loss_audio: 11.908|tagging_loss_text: 13.388|tagging_loss_image: 9.090|tagging_loss_fusion: 6.933|total_loss: 48.263 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 2186 | tagging_loss_video: 6.367|tagging_loss_audio: 12.138|tagging_loss_text: 14.741|tagging_loss_image: 7.423|tagging_loss_fusion: 5.322|total_loss: 45.991 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 2187 | tagging_loss_video: 8.988|tagging_loss_audio: 12.561|tagging_loss_text: 16.326|tagging_loss_image: 9.490|tagging_loss_fusion: 9.373|total_loss: 56.737 | 59.29 Examples/sec\n",
      "INFO:tensorflow:training step 2188 | tagging_loss_video: 6.147|tagging_loss_audio: 12.327|tagging_loss_text: 15.067|tagging_loss_image: 8.133|tagging_loss_fusion: 6.194|total_loss: 47.868 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 2189 | tagging_loss_video: 6.694|tagging_loss_audio: 11.199|tagging_loss_text: 17.395|tagging_loss_image: 8.312|tagging_loss_fusion: 7.698|total_loss: 51.298 | 70.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2190 |tagging_loss_video: 6.050|tagging_loss_audio: 10.241|tagging_loss_text: 14.838|tagging_loss_image: 7.203|tagging_loss_fusion: 8.159|total_loss: 46.491 | Examples/sec: 64.87\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.70 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 2191 | tagging_loss_video: 7.530|tagging_loss_audio: 12.620|tagging_loss_text: 14.603|tagging_loss_image: 8.257|tagging_loss_fusion: 7.867|total_loss: 50.876 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 2192 | tagging_loss_video: 7.810|tagging_loss_audio: 11.166|tagging_loss_text: 16.863|tagging_loss_image: 9.640|tagging_loss_fusion: 7.653|total_loss: 53.132 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 2193 | tagging_loss_video: 7.696|tagging_loss_audio: 10.169|tagging_loss_text: 12.249|tagging_loss_image: 8.422|tagging_loss_fusion: 7.462|total_loss: 45.998 | 60.77 Examples/sec\n",
      "INFO:tensorflow:training step 2194 | tagging_loss_video: 7.519|tagging_loss_audio: 12.909|tagging_loss_text: 17.555|tagging_loss_image: 7.883|tagging_loss_fusion: 8.380|total_loss: 54.246 | 72.17 Examples/sec\n",
      "INFO:tensorflow:training step 2195 | tagging_loss_video: 7.145|tagging_loss_audio: 9.741|tagging_loss_text: 15.939|tagging_loss_image: 8.318|tagging_loss_fusion: 8.701|total_loss: 49.844 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 2196 | tagging_loss_video: 7.520|tagging_loss_audio: 10.782|tagging_loss_text: 15.658|tagging_loss_image: 8.174|tagging_loss_fusion: 8.296|total_loss: 50.430 | 66.22 Examples/sec\n",
      "INFO:tensorflow:training step 2197 | tagging_loss_video: 6.802|tagging_loss_audio: 7.561|tagging_loss_text: 16.931|tagging_loss_image: 8.495|tagging_loss_fusion: 8.394|total_loss: 48.182 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 2198 | tagging_loss_video: 8.466|tagging_loss_audio: 8.943|tagging_loss_text: 13.268|tagging_loss_image: 8.755|tagging_loss_fusion: 6.949|total_loss: 46.381 | 64.69 Examples/sec\n",
      "INFO:tensorflow:training step 2199 | tagging_loss_video: 7.899|tagging_loss_audio: 11.678|tagging_loss_text: 15.358|tagging_loss_image: 8.725|tagging_loss_fusion: 9.437|total_loss: 53.097 | 71.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2200 |tagging_loss_video: 7.138|tagging_loss_audio: 11.623|tagging_loss_text: 15.390|tagging_loss_image: 8.913|tagging_loss_fusion: 8.434|total_loss: 51.498 | Examples/sec: 68.16\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.69 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2201 | tagging_loss_video: 6.649|tagging_loss_audio: 11.201|tagging_loss_text: 18.157|tagging_loss_image: 8.538|tagging_loss_fusion: 6.543|total_loss: 51.088 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 2202 | tagging_loss_video: 7.199|tagging_loss_audio: 11.918|tagging_loss_text: 17.636|tagging_loss_image: 8.980|tagging_loss_fusion: 7.997|total_loss: 53.730 | 60.53 Examples/sec\n",
      "INFO:tensorflow:training step 2203 | tagging_loss_video: 7.412|tagging_loss_audio: 10.519|tagging_loss_text: 13.613|tagging_loss_image: 8.702|tagging_loss_fusion: 10.327|total_loss: 50.573 | 71.75 Examples/sec\n",
      "INFO:tensorflow:training step 2204 | tagging_loss_video: 8.020|tagging_loss_audio: 14.662|tagging_loss_text: 13.634|tagging_loss_image: 9.360|tagging_loss_fusion: 9.703|total_loss: 55.377 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 2205 | tagging_loss_video: 7.883|tagging_loss_audio: 11.148|tagging_loss_text: 12.670|tagging_loss_image: 7.803|tagging_loss_fusion: 7.358|total_loss: 46.861 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 2206 | tagging_loss_video: 6.655|tagging_loss_audio: 8.176|tagging_loss_text: 13.729|tagging_loss_image: 7.196|tagging_loss_fusion: 8.308|total_loss: 44.064 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 2207 | tagging_loss_video: 7.418|tagging_loss_audio: 10.356|tagging_loss_text: 17.332|tagging_loss_image: 8.438|tagging_loss_fusion: 9.033|total_loss: 52.577 | 67.54 Examples/sec\n",
      "INFO:tensorflow:training step 2208 | tagging_loss_video: 6.332|tagging_loss_audio: 9.531|tagging_loss_text: 13.542|tagging_loss_image: 7.913|tagging_loss_fusion: 7.549|total_loss: 44.867 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 2209 | tagging_loss_video: 7.609|tagging_loss_audio: 11.561|tagging_loss_text: 17.860|tagging_loss_image: 9.080|tagging_loss_fusion: 9.417|total_loss: 55.527 | 70.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2210 |tagging_loss_video: 8.169|tagging_loss_audio: 12.083|tagging_loss_text: 15.767|tagging_loss_image: 9.280|tagging_loss_fusion: 8.641|total_loss: 53.940 | Examples/sec: 70.28\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.69 | precision@0.5: 0.87 |recall@0.1: 0.97 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2211 | tagging_loss_video: 7.592|tagging_loss_audio: 9.116|tagging_loss_text: 14.216|tagging_loss_image: 7.631|tagging_loss_fusion: 8.814|total_loss: 47.370 | 64.81 Examples/sec\n",
      "INFO:tensorflow:training step 2212 | tagging_loss_video: 7.667|tagging_loss_audio: 10.436|tagging_loss_text: 14.683|tagging_loss_image: 9.793|tagging_loss_fusion: 9.546|total_loss: 52.125 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 2213 | tagging_loss_video: 8.092|tagging_loss_audio: 11.024|tagging_loss_text: 16.354|tagging_loss_image: 10.082|tagging_loss_fusion: 8.345|total_loss: 53.897 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 2214 | tagging_loss_video: 9.007|tagging_loss_audio: 11.987|tagging_loss_text: 16.995|tagging_loss_image: 9.922|tagging_loss_fusion: 8.945|total_loss: 56.856 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 2215 | tagging_loss_video: 8.512|tagging_loss_audio: 11.821|tagging_loss_text: 16.430|tagging_loss_image: 10.118|tagging_loss_fusion: 8.726|total_loss: 55.608 | 67.31 Examples/sec\n",
      "INFO:tensorflow:training step 2216 | tagging_loss_video: 7.751|tagging_loss_audio: 11.294|tagging_loss_text: 15.892|tagging_loss_image: 8.750|tagging_loss_fusion: 6.837|total_loss: 50.524 | 58.60 Examples/sec\n",
      "INFO:tensorflow:training step 2217 | tagging_loss_video: 6.998|tagging_loss_audio: 9.933|tagging_loss_text: 12.726|tagging_loss_image: 8.680|tagging_loss_fusion: 7.032|total_loss: 45.369 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 2218 | tagging_loss_video: 6.276|tagging_loss_audio: 9.938|tagging_loss_text: 15.028|tagging_loss_image: 7.216|tagging_loss_fusion: 5.808|total_loss: 44.267 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 2219 | tagging_loss_video: 7.474|tagging_loss_audio: 12.041|tagging_loss_text: 14.958|tagging_loss_image: 7.854|tagging_loss_fusion: 5.923|total_loss: 48.251 | 61.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2220 |tagging_loss_video: 7.790|tagging_loss_audio: 11.702|tagging_loss_text: 15.708|tagging_loss_image: 8.662|tagging_loss_fusion: 8.972|total_loss: 52.834 | Examples/sec: 68.73\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.72 | precision@0.5: 0.88 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 2221 | tagging_loss_video: 6.887|tagging_loss_audio: 10.317|tagging_loss_text: 18.256|tagging_loss_image: 8.131|tagging_loss_fusion: 7.772|total_loss: 51.362 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 2222 | tagging_loss_video: 7.482|tagging_loss_audio: 13.778|tagging_loss_text: 15.412|tagging_loss_image: 10.058|tagging_loss_fusion: 9.410|total_loss: 56.140 | 60.76 Examples/sec\n",
      "INFO:tensorflow:training step 2223 | tagging_loss_video: 6.971|tagging_loss_audio: 10.165|tagging_loss_text: 16.822|tagging_loss_image: 7.782|tagging_loss_fusion: 8.402|total_loss: 50.142 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 2224 | tagging_loss_video: 6.208|tagging_loss_audio: 11.276|tagging_loss_text: 16.404|tagging_loss_image: 9.266|tagging_loss_fusion: 7.247|total_loss: 50.401 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 2225 | tagging_loss_video: 7.810|tagging_loss_audio: 12.902|tagging_loss_text: 17.305|tagging_loss_image: 9.865|tagging_loss_fusion: 8.119|total_loss: 56.000 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 2226 | tagging_loss_video: 8.520|tagging_loss_audio: 13.803|tagging_loss_text: 13.271|tagging_loss_image: 7.899|tagging_loss_fusion: 10.751|total_loss: 54.245 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 2227 | tagging_loss_video: 8.167|tagging_loss_audio: 12.765|tagging_loss_text: 15.716|tagging_loss_image: 8.607|tagging_loss_fusion: 6.264|total_loss: 51.519 | 60.41 Examples/sec\n",
      "INFO:tensorflow:training step 2228 | tagging_loss_video: 8.340|tagging_loss_audio: 12.402|tagging_loss_text: 15.913|tagging_loss_image: 10.406|tagging_loss_fusion: 10.515|total_loss: 57.576 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 2229 | tagging_loss_video: 7.111|tagging_loss_audio: 9.663|tagging_loss_text: 13.276|tagging_loss_image: 9.050|tagging_loss_fusion: 7.227|total_loss: 46.326 | 68.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2230 |tagging_loss_video: 7.501|tagging_loss_audio: 11.176|tagging_loss_text: 12.367|tagging_loss_image: 9.351|tagging_loss_fusion: 9.877|total_loss: 50.272 | Examples/sec: 66.50\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.67 | precision@0.5: 0.87 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2231 | tagging_loss_video: 9.014|tagging_loss_audio: 13.851|tagging_loss_text: 15.616|tagging_loss_image: 9.589|tagging_loss_fusion: 9.774|total_loss: 57.844 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 2232 | tagging_loss_video: 6.274|tagging_loss_audio: 10.741|tagging_loss_text: 16.386|tagging_loss_image: 9.106|tagging_loss_fusion: 6.558|total_loss: 49.064 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 2233 | tagging_loss_video: 7.564|tagging_loss_audio: 11.248|tagging_loss_text: 15.184|tagging_loss_image: 8.382|tagging_loss_fusion: 9.404|total_loss: 51.781 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 2234 | tagging_loss_video: 8.089|tagging_loss_audio: 12.731|tagging_loss_text: 16.602|tagging_loss_image: 8.429|tagging_loss_fusion: 7.402|total_loss: 53.253 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 2235 | tagging_loss_video: 9.009|tagging_loss_audio: 11.787|tagging_loss_text: 16.746|tagging_loss_image: 9.323|tagging_loss_fusion: 8.305|total_loss: 55.170 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 2236 | tagging_loss_video: 8.218|tagging_loss_audio: 12.789|tagging_loss_text: 16.258|tagging_loss_image: 10.255|tagging_loss_fusion: 8.364|total_loss: 55.884 | 66.59 Examples/sec\n",
      "INFO:tensorflow:training step 2237 | tagging_loss_video: 8.407|tagging_loss_audio: 12.453|tagging_loss_text: 13.512|tagging_loss_image: 7.951|tagging_loss_fusion: 10.347|total_loss: 52.671 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 2238 | tagging_loss_video: 6.981|tagging_loss_audio: 9.691|tagging_loss_text: 15.861|tagging_loss_image: 8.218|tagging_loss_fusion: 7.862|total_loss: 48.612 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 2239 | tagging_loss_video: 7.242|tagging_loss_audio: 11.503|tagging_loss_text: 16.907|tagging_loss_image: 9.174|tagging_loss_fusion: 7.076|total_loss: 51.903 | 65.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2240 |tagging_loss_video: 7.362|tagging_loss_audio: 10.940|tagging_loss_text: 13.615|tagging_loss_image: 9.511|tagging_loss_fusion: 7.843|total_loss: 49.271 | Examples/sec: 69.01\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.63 | precision@0.5: 0.86 |recall@0.1: 0.98 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 2241 | tagging_loss_video: 8.034|tagging_loss_audio: 11.408|tagging_loss_text: 16.631|tagging_loss_image: 8.949|tagging_loss_fusion: 9.073|total_loss: 54.096 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 2242 | tagging_loss_video: 8.335|tagging_loss_audio: 12.974|tagging_loss_text: 16.559|tagging_loss_image: 10.123|tagging_loss_fusion: 7.457|total_loss: 55.448 | 69.30 Examples/sec\n",
      "INFO:tensorflow:training step 2243 | tagging_loss_video: 7.640|tagging_loss_audio: 12.070|tagging_loss_text: 13.796|tagging_loss_image: 9.649|tagging_loss_fusion: 9.203|total_loss: 52.357 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 2244 | tagging_loss_video: 8.600|tagging_loss_audio: 11.073|tagging_loss_text: 13.989|tagging_loss_image: 8.889|tagging_loss_fusion: 8.326|total_loss: 50.877 | 63.29 Examples/sec\n",
      "INFO:tensorflow:training step 2245 | tagging_loss_video: 9.785|tagging_loss_audio: 13.965|tagging_loss_text: 15.245|tagging_loss_image: 10.154|tagging_loss_fusion: 10.083|total_loss: 59.233 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 2246 | tagging_loss_video: 7.294|tagging_loss_audio: 12.665|tagging_loss_text: 15.268|tagging_loss_image: 8.738|tagging_loss_fusion: 7.335|total_loss: 51.301 | 72.37 Examples/sec\n",
      "INFO:tensorflow:training step 2247 | tagging_loss_video: 6.917|tagging_loss_audio: 10.197|tagging_loss_text: 19.502|tagging_loss_image: 8.796|tagging_loss_fusion: 7.272|total_loss: 52.685 | 59.32 Examples/sec\n",
      "INFO:tensorflow:training step 2248 | tagging_loss_video: 7.762|tagging_loss_audio: 11.376|tagging_loss_text: 14.828|tagging_loss_image: 8.267|tagging_loss_fusion: 7.368|total_loss: 49.599 | 66.46 Examples/sec\n",
      "INFO:tensorflow:training step 2249 | tagging_loss_video: 7.615|tagging_loss_audio: 11.244|tagging_loss_text: 15.760|tagging_loss_image: 9.143|tagging_loss_fusion: 7.327|total_loss: 51.089 | 67.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2250 |tagging_loss_video: 6.891|tagging_loss_audio: 12.808|tagging_loss_text: 12.695|tagging_loss_image: 7.097|tagging_loss_fusion: 8.617|total_loss: 48.108 | Examples/sec: 69.41\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.69 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2251 | tagging_loss_video: 7.724|tagging_loss_audio: 13.840|tagging_loss_text: 16.346|tagging_loss_image: 10.102|tagging_loss_fusion: 8.606|total_loss: 56.619 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 2252 | tagging_loss_video: 8.543|tagging_loss_audio: 12.335|tagging_loss_text: 12.603|tagging_loss_image: 8.928|tagging_loss_fusion: 10.220|total_loss: 52.630 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 2253 | tagging_loss_video: 7.401|tagging_loss_audio: 11.642|tagging_loss_text: 14.454|tagging_loss_image: 9.129|tagging_loss_fusion: 9.306|total_loss: 51.932 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 2254 | tagging_loss_video: 7.687|tagging_loss_audio: 10.519|tagging_loss_text: 17.826|tagging_loss_image: 9.187|tagging_loss_fusion: 7.610|total_loss: 52.829 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 2255 | tagging_loss_video: 7.013|tagging_loss_audio: 11.351|tagging_loss_text: 16.841|tagging_loss_image: 8.241|tagging_loss_fusion: 7.123|total_loss: 50.568 | 62.84 Examples/sec\n",
      "INFO:tensorflow:training step 2256 | tagging_loss_video: 8.117|tagging_loss_audio: 13.385|tagging_loss_text: 14.692|tagging_loss_image: 10.406|tagging_loss_fusion: 9.875|total_loss: 56.474 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 2257 | tagging_loss_video: 7.295|tagging_loss_audio: 11.627|tagging_loss_text: 13.626|tagging_loss_image: 8.811|tagging_loss_fusion: 8.193|total_loss: 49.552 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 2258 | tagging_loss_video: 7.705|tagging_loss_audio: 11.591|tagging_loss_text: 17.200|tagging_loss_image: 8.849|tagging_loss_fusion: 6.236|total_loss: 51.581 | 62.59 Examples/sec\n",
      "INFO:tensorflow:training step 2259 | tagging_loss_video: 6.957|tagging_loss_audio: 12.255|tagging_loss_text: 16.000|tagging_loss_image: 8.470|tagging_loss_fusion: 8.834|total_loss: 52.516 | 71.16 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2260 |tagging_loss_video: 7.172|tagging_loss_audio: 10.259|tagging_loss_text: 14.271|tagging_loss_image: 9.017|tagging_loss_fusion: 9.794|total_loss: 50.514 | Examples/sec: 69.34\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.66 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.80\n",
      "INFO:tensorflow:training step 2261 | tagging_loss_video: 6.561|tagging_loss_audio: 10.840|tagging_loss_text: 15.198|tagging_loss_image: 7.677|tagging_loss_fusion: 8.082|total_loss: 48.358 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 2262 | tagging_loss_video: 7.061|tagging_loss_audio: 7.530|tagging_loss_text: 14.469|tagging_loss_image: 8.144|tagging_loss_fusion: 7.393|total_loss: 44.597 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 2263 | tagging_loss_video: 7.566|tagging_loss_audio: 11.325|tagging_loss_text: 18.248|tagging_loss_image: 8.257|tagging_loss_fusion: 6.027|total_loss: 51.423 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 2264 | tagging_loss_video: 7.150|tagging_loss_audio: 9.500|tagging_loss_text: 14.762|tagging_loss_image: 9.105|tagging_loss_fusion: 8.924|total_loss: 49.440 | 66.63 Examples/sec\n",
      "INFO:tensorflow:training step 2265 | tagging_loss_video: 6.562|tagging_loss_audio: 10.366|tagging_loss_text: 11.295|tagging_loss_image: 8.881|tagging_loss_fusion: 6.491|total_loss: 43.595 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 2266 | tagging_loss_video: 7.427|tagging_loss_audio: 11.336|tagging_loss_text: 14.243|tagging_loss_image: 9.068|tagging_loss_fusion: 7.925|total_loss: 49.998 | 63.16 Examples/sec\n",
      "INFO:tensorflow:training step 2267 | tagging_loss_video: 7.106|tagging_loss_audio: 11.162|tagging_loss_text: 14.028|tagging_loss_image: 8.961|tagging_loss_fusion: 5.920|total_loss: 47.177 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 2268 | tagging_loss_video: 6.509|tagging_loss_audio: 11.454|tagging_loss_text: 14.617|tagging_loss_image: 8.148|tagging_loss_fusion: 8.442|total_loss: 49.170 | 67.17 Examples/sec\n",
      "INFO:tensorflow:training step 2269 | tagging_loss_video: 7.336|tagging_loss_audio: 8.938|tagging_loss_text: 18.243|tagging_loss_image: 8.758|tagging_loss_fusion: 7.904|total_loss: 51.179 | 67.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2270 |tagging_loss_video: 9.301|tagging_loss_audio: 13.958|tagging_loss_text: 16.038|tagging_loss_image: 10.002|tagging_loss_fusion: 10.417|total_loss: 59.716 | Examples/sec: 69.31\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.72 | precision@0.5: 0.90 |recall@0.1: 0.92 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2271 | tagging_loss_video: 8.393|tagging_loss_audio: 13.519|tagging_loss_text: 18.329|tagging_loss_image: 10.767|tagging_loss_fusion: 9.953|total_loss: 60.961 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 2272 | tagging_loss_video: 9.204|tagging_loss_audio: 12.614|tagging_loss_text: 12.140|tagging_loss_image: 11.345|tagging_loss_fusion: 9.586|total_loss: 54.890 | 63.53 Examples/sec\n",
      "INFO:tensorflow:training step 2273 | tagging_loss_video: 7.454|tagging_loss_audio: 11.786|tagging_loss_text: 14.445|tagging_loss_image: 8.727|tagging_loss_fusion: 8.929|total_loss: 51.340 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 2274 | tagging_loss_video: 8.226|tagging_loss_audio: 12.461|tagging_loss_text: 17.861|tagging_loss_image: 9.814|tagging_loss_fusion: 10.357|total_loss: 58.718 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 2275 | tagging_loss_video: 7.806|tagging_loss_audio: 10.245|tagging_loss_text: 14.719|tagging_loss_image: 8.798|tagging_loss_fusion: 6.744|total_loss: 48.312 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 2276 | tagging_loss_video: 7.568|tagging_loss_audio: 11.861|tagging_loss_text: 14.387|tagging_loss_image: 9.893|tagging_loss_fusion: 8.318|total_loss: 52.027 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 2277 | tagging_loss_video: 7.280|tagging_loss_audio: 9.739|tagging_loss_text: 16.559|tagging_loss_image: 8.683|tagging_loss_fusion: 8.848|total_loss: 51.109 | 62.75 Examples/sec\n",
      "INFO:tensorflow:training step 2278 | tagging_loss_video: 8.097|tagging_loss_audio: 12.674|tagging_loss_text: 17.254|tagging_loss_image: 9.098|tagging_loss_fusion: 9.056|total_loss: 56.179 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 2279 | tagging_loss_video: 8.253|tagging_loss_audio: 11.072|tagging_loss_text: 17.378|tagging_loss_image: 9.378|tagging_loss_fusion: 9.169|total_loss: 55.250 | 70.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2280 |tagging_loss_video: 5.905|tagging_loss_audio: 9.450|tagging_loss_text: 14.899|tagging_loss_image: 7.488|tagging_loss_fusion: 4.898|total_loss: 42.640 | Examples/sec: 62.24\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.73 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 2281 | tagging_loss_video: 6.776|tagging_loss_audio: 11.651|tagging_loss_text: 14.942|tagging_loss_image: 8.860|tagging_loss_fusion: 5.817|total_loss: 48.046 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 2282 | tagging_loss_video: 7.386|tagging_loss_audio: 12.699|tagging_loss_text: 14.492|tagging_loss_image: 9.485|tagging_loss_fusion: 6.517|total_loss: 50.579 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 2283 | tagging_loss_video: 7.298|tagging_loss_audio: 10.367|tagging_loss_text: 17.568|tagging_loss_image: 9.099|tagging_loss_fusion: 8.559|total_loss: 52.891 | 67.09 Examples/sec\n",
      "INFO:tensorflow:training step 2284 | tagging_loss_video: 8.131|tagging_loss_audio: 12.081|tagging_loss_text: 18.583|tagging_loss_image: 9.531|tagging_loss_fusion: 6.556|total_loss: 54.883 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 2285 | tagging_loss_video: 8.384|tagging_loss_audio: 9.900|tagging_loss_text: 15.483|tagging_loss_image: 8.758|tagging_loss_fusion: 9.472|total_loss: 51.997 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 2286 | tagging_loss_video: 7.339|tagging_loss_audio: 12.510|tagging_loss_text: 17.094|tagging_loss_image: 8.364|tagging_loss_fusion: 7.537|total_loss: 52.844 | 66.09 Examples/sec\n",
      "INFO:tensorflow:training step 2287 | tagging_loss_video: 8.806|tagging_loss_audio: 11.870|tagging_loss_text: 14.899|tagging_loss_image: 9.176|tagging_loss_fusion: 8.057|total_loss: 52.807 | 68.22 Examples/sec\n",
      "INFO:tensorflow:training step 2288 | tagging_loss_video: 7.870|tagging_loss_audio: 10.716|tagging_loss_text: 14.278|tagging_loss_image: 8.453|tagging_loss_fusion: 7.581|total_loss: 48.898 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 2289 | tagging_loss_video: 7.611|tagging_loss_audio: 11.317|tagging_loss_text: 15.907|tagging_loss_image: 9.261|tagging_loss_fusion: 8.459|total_loss: 52.555 | 66.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2290 |tagging_loss_video: 6.723|tagging_loss_audio: 9.602|tagging_loss_text: 15.426|tagging_loss_image: 8.561|tagging_loss_fusion: 5.885|total_loss: 46.197 | Examples/sec: 70.75\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.74 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2291 | tagging_loss_video: 6.968|tagging_loss_audio: 12.726|tagging_loss_text: 15.151|tagging_loss_image: 8.565|tagging_loss_fusion: 8.184|total_loss: 51.595 | 65.60 Examples/sec\n",
      "INFO:tensorflow:training step 2292 | tagging_loss_video: 6.704|tagging_loss_audio: 9.960|tagging_loss_text: 16.152|tagging_loss_image: 7.323|tagging_loss_fusion: 5.072|total_loss: 45.212 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 2293 | tagging_loss_video: 7.125|tagging_loss_audio: 10.747|tagging_loss_text: 14.394|tagging_loss_image: 7.734|tagging_loss_fusion: 8.864|total_loss: 48.864 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 2294 | tagging_loss_video: 7.152|tagging_loss_audio: 12.236|tagging_loss_text: 15.659|tagging_loss_image: 10.253|tagging_loss_fusion: 6.333|total_loss: 51.633 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 2295 | tagging_loss_video: 7.930|tagging_loss_audio: 13.215|tagging_loss_text: 15.987|tagging_loss_image: 9.453|tagging_loss_fusion: 9.730|total_loss: 56.316 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 2296 | tagging_loss_video: 6.394|tagging_loss_audio: 11.065|tagging_loss_text: 16.613|tagging_loss_image: 9.455|tagging_loss_fusion: 6.251|total_loss: 49.777 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 2297 | tagging_loss_video: 6.893|tagging_loss_audio: 11.219|tagging_loss_text: 14.667|tagging_loss_image: 9.183|tagging_loss_fusion: 6.327|total_loss: 48.290 | 66.58 Examples/sec\n",
      "INFO:tensorflow:training step 2298 | tagging_loss_video: 8.444|tagging_loss_audio: 8.548|tagging_loss_text: 16.362|tagging_loss_image: 9.405|tagging_loss_fusion: 8.656|total_loss: 51.415 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 2299 | tagging_loss_video: 7.178|tagging_loss_audio: 10.019|tagging_loss_text: 15.270|tagging_loss_image: 7.848|tagging_loss_fusion: 8.418|total_loss: 48.733 | 69.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2300 |tagging_loss_video: 7.292|tagging_loss_audio: 12.214|tagging_loss_text: 14.668|tagging_loss_image: 9.319|tagging_loss_fusion: 9.361|total_loss: 52.854 | Examples/sec: 69.04\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.73 | precision@0.5: 0.88 |recall@0.1: 0.93 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 2301 | tagging_loss_video: 7.783|tagging_loss_audio: 12.154|tagging_loss_text: 16.998|tagging_loss_image: 9.485|tagging_loss_fusion: 10.141|total_loss: 56.560 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 2302 | tagging_loss_video: 7.709|tagging_loss_audio: 12.079|tagging_loss_text: 16.888|tagging_loss_image: 9.398|tagging_loss_fusion: 7.164|total_loss: 53.237 | 67.42 Examples/sec\n",
      "INFO:tensorflow:training step 2303 | tagging_loss_video: 7.066|tagging_loss_audio: 11.754|tagging_loss_text: 14.192|tagging_loss_image: 9.361|tagging_loss_fusion: 6.788|total_loss: 49.160 | 69.48 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2304 | tagging_loss_video: 6.948|tagging_loss_audio: 11.245|tagging_loss_text: 17.554|tagging_loss_image: 9.043|tagging_loss_fusion: 6.787|total_loss: 51.576 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 2305 | tagging_loss_video: 7.657|tagging_loss_audio: 11.529|tagging_loss_text: 15.279|tagging_loss_image: 8.734|tagging_loss_fusion: 7.577|total_loss: 50.776 | 58.63 Examples/sec\n",
      "INFO:tensorflow:training step 2306 | tagging_loss_video: 7.096|tagging_loss_audio: 11.438|tagging_loss_text: 14.518|tagging_loss_image: 8.923|tagging_loss_fusion: 7.788|total_loss: 49.763 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 2307 | tagging_loss_video: 6.332|tagging_loss_audio: 10.673|tagging_loss_text: 14.866|tagging_loss_image: 8.592|tagging_loss_fusion: 5.593|total_loss: 46.055 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 2308 | tagging_loss_video: 8.469|tagging_loss_audio: 12.991|tagging_loss_text: 14.725|tagging_loss_image: 10.012|tagging_loss_fusion: 8.566|total_loss: 54.762 | 63.47 Examples/sec\n",
      "INFO:tensorflow:training step 2309 | tagging_loss_video: 7.179|tagging_loss_audio: 9.666|tagging_loss_text: 12.204|tagging_loss_image: 8.384|tagging_loss_fusion: 8.002|total_loss: 45.435 | 69.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2310 |tagging_loss_video: 7.191|tagging_loss_audio: 12.708|tagging_loss_text: 15.904|tagging_loss_image: 8.785|tagging_loss_fusion: 7.049|total_loss: 51.638 | Examples/sec: 70.64\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.75 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2311 | tagging_loss_video: 7.252|tagging_loss_audio: 10.720|tagging_loss_text: 12.801|tagging_loss_image: 9.185|tagging_loss_fusion: 9.445|total_loss: 49.403 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 2312 | tagging_loss_video: 7.057|tagging_loss_audio: 10.970|tagging_loss_text: 15.333|tagging_loss_image: 8.917|tagging_loss_fusion: 6.628|total_loss: 48.905 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 2313 | tagging_loss_video: 6.446|tagging_loss_audio: 11.473|tagging_loss_text: 14.353|tagging_loss_image: 7.980|tagging_loss_fusion: 5.556|total_loss: 45.809 | 66.12 Examples/sec\n",
      "INFO:tensorflow:training step 2314 | tagging_loss_video: 6.481|tagging_loss_audio: 10.863|tagging_loss_text: 13.012|tagging_loss_image: 9.640|tagging_loss_fusion: 5.758|total_loss: 45.754 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 2315 | tagging_loss_video: 7.605|tagging_loss_audio: 12.045|tagging_loss_text: 15.680|tagging_loss_image: 9.959|tagging_loss_fusion: 7.665|total_loss: 52.954 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 2316 | tagging_loss_video: 7.260|tagging_loss_audio: 11.814|tagging_loss_text: 18.229|tagging_loss_image: 9.762|tagging_loss_fusion: 5.818|total_loss: 52.884 | 62.79 Examples/sec\n",
      "INFO:tensorflow:training step 2317 | tagging_loss_video: 6.723|tagging_loss_audio: 12.033|tagging_loss_text: 14.788|tagging_loss_image: 8.985|tagging_loss_fusion: 6.918|total_loss: 49.446 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 2318 | tagging_loss_video: 7.652|tagging_loss_audio: 13.676|tagging_loss_text: 18.329|tagging_loss_image: 10.880|tagging_loss_fusion: 9.630|total_loss: 60.167 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 2319 | tagging_loss_video: 6.294|tagging_loss_audio: 11.039|tagging_loss_text: 14.499|tagging_loss_image: 7.498|tagging_loss_fusion: 7.011|total_loss: 46.341 | 63.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2320 |tagging_loss_video: 6.820|tagging_loss_audio: 10.527|tagging_loss_text: 14.522|tagging_loss_image: 7.626|tagging_loss_fusion: 7.363|total_loss: 46.857 | Examples/sec: 68.52\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.73 | precision@0.5: 0.92 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2321 | tagging_loss_video: 7.713|tagging_loss_audio: 11.188|tagging_loss_text: 12.176|tagging_loss_image: 7.809|tagging_loss_fusion: 9.774|total_loss: 48.659 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 2322 | tagging_loss_video: 7.069|tagging_loss_audio: 10.638|tagging_loss_text: 12.217|tagging_loss_image: 7.307|tagging_loss_fusion: 5.853|total_loss: 43.083 | 64.61 Examples/sec\n",
      "INFO:tensorflow:training step 2323 | tagging_loss_video: 6.288|tagging_loss_audio: 11.309|tagging_loss_text: 16.834|tagging_loss_image: 9.163|tagging_loss_fusion: 7.091|total_loss: 50.685 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 2324 | tagging_loss_video: 7.490|tagging_loss_audio: 10.131|tagging_loss_text: 15.142|tagging_loss_image: 8.515|tagging_loss_fusion: 6.491|total_loss: 47.769 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 2325 | tagging_loss_video: 7.365|tagging_loss_audio: 11.641|tagging_loss_text: 17.364|tagging_loss_image: 8.837|tagging_loss_fusion: 8.097|total_loss: 53.305 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 2326 | tagging_loss_video: 6.529|tagging_loss_audio: 11.827|tagging_loss_text: 14.816|tagging_loss_image: 6.991|tagging_loss_fusion: 7.357|total_loss: 47.520 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 2327 | tagging_loss_video: 8.217|tagging_loss_audio: 10.044|tagging_loss_text: 14.843|tagging_loss_image: 9.517|tagging_loss_fusion: 7.011|total_loss: 49.632 | 61.06 Examples/sec\n",
      "INFO:tensorflow:training step 2328 | tagging_loss_video: 7.059|tagging_loss_audio: 10.160|tagging_loss_text: 15.014|tagging_loss_image: 8.259|tagging_loss_fusion: 6.898|total_loss: 47.391 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 2329 | tagging_loss_video: 6.392|tagging_loss_audio: 9.246|tagging_loss_text: 13.654|tagging_loss_image: 7.221|tagging_loss_fusion: 6.165|total_loss: 42.678 | 68.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2330 |tagging_loss_video: 5.726|tagging_loss_audio: 9.850|tagging_loss_text: 14.795|tagging_loss_image: 7.905|tagging_loss_fusion: 6.200|total_loss: 44.477 | Examples/sec: 62.01\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.72 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2331 | tagging_loss_video: 7.509|tagging_loss_audio: 9.902|tagging_loss_text: 12.529|tagging_loss_image: 8.169|tagging_loss_fusion: 6.662|total_loss: 44.772 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 2332 | tagging_loss_video: 8.049|tagging_loss_audio: 11.253|tagging_loss_text: 15.521|tagging_loss_image: 8.926|tagging_loss_fusion: 9.110|total_loss: 52.859 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 2333 | tagging_loss_video: 6.028|tagging_loss_audio: 10.161|tagging_loss_text: 14.827|tagging_loss_image: 7.755|tagging_loss_fusion: 6.430|total_loss: 45.201 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 2334 | tagging_loss_video: 7.116|tagging_loss_audio: 11.207|tagging_loss_text: 15.482|tagging_loss_image: 8.825|tagging_loss_fusion: 7.533|total_loss: 50.164 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2335 | tagging_loss_video: 6.978|tagging_loss_audio: 9.649|tagging_loss_text: 14.573|tagging_loss_image: 7.703|tagging_loss_fusion: 7.296|total_loss: 46.199 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2336 | tagging_loss_video: 7.328|tagging_loss_audio: 9.749|tagging_loss_text: 11.778|tagging_loss_image: 8.325|tagging_loss_fusion: 8.219|total_loss: 45.399 | 65.32 Examples/sec\n",
      "INFO:tensorflow:training step 2337 | tagging_loss_video: 7.965|tagging_loss_audio: 8.135|tagging_loss_text: 11.104|tagging_loss_image: 7.490|tagging_loss_fusion: 7.992|total_loss: 42.686 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 2338 | tagging_loss_video: 7.109|tagging_loss_audio: 11.570|tagging_loss_text: 16.543|tagging_loss_image: 8.962|tagging_loss_fusion: 7.870|total_loss: 52.053 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 2339 | tagging_loss_video: 7.757|tagging_loss_audio: 11.465|tagging_loss_text: 16.002|tagging_loss_image: 7.958|tagging_loss_fusion: 7.851|total_loss: 51.034 | 71.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2340 |tagging_loss_video: 6.662|tagging_loss_audio: 11.615|tagging_loss_text: 15.444|tagging_loss_image: 7.935|tagging_loss_fusion: 7.455|total_loss: 49.111 | Examples/sec: 69.02\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.72 | precision@0.5: 0.90 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2341 | tagging_loss_video: 7.054|tagging_loss_audio: 13.834|tagging_loss_text: 12.632|tagging_loss_image: 8.633|tagging_loss_fusion: 6.038|total_loss: 48.191 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 2342 | tagging_loss_video: 7.351|tagging_loss_audio: 11.738|tagging_loss_text: 16.955|tagging_loss_image: 9.067|tagging_loss_fusion: 8.482|total_loss: 53.592 | 63.24 Examples/sec\n",
      "INFO:tensorflow:training step 2343 | tagging_loss_video: 7.206|tagging_loss_audio: 11.215|tagging_loss_text: 17.710|tagging_loss_image: 9.258|tagging_loss_fusion: 7.744|total_loss: 53.134 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 2344 | tagging_loss_video: 8.250|tagging_loss_audio: 12.381|tagging_loss_text: 14.369|tagging_loss_image: 9.840|tagging_loss_fusion: 8.654|total_loss: 53.494 | 67.69 Examples/sec\n",
      "INFO:tensorflow:training step 2345 | tagging_loss_video: 7.335|tagging_loss_audio: 9.682|tagging_loss_text: 13.933|tagging_loss_image: 8.052|tagging_loss_fusion: 7.884|total_loss: 46.886 | 65.73 Examples/sec\n",
      "INFO:tensorflow:training step 2346 | tagging_loss_video: 7.089|tagging_loss_audio: 9.419|tagging_loss_text: 14.260|tagging_loss_image: 6.944|tagging_loss_fusion: 7.986|total_loss: 45.697 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 2347 | tagging_loss_video: 7.444|tagging_loss_audio: 12.017|tagging_loss_text: 12.009|tagging_loss_image: 8.877|tagging_loss_fusion: 9.766|total_loss: 50.113 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 2348 | tagging_loss_video: 6.820|tagging_loss_audio: 11.384|tagging_loss_text: 18.009|tagging_loss_image: 7.495|tagging_loss_fusion: 6.349|total_loss: 50.057 | 61.25 Examples/sec\n",
      "INFO:tensorflow:training step 2349 | tagging_loss_video: 6.633|tagging_loss_audio: 11.854|tagging_loss_text: 16.529|tagging_loss_image: 9.230|tagging_loss_fusion: 7.183|total_loss: 51.429 | 69.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2350 |tagging_loss_video: 7.606|tagging_loss_audio: 10.366|tagging_loss_text: 13.931|tagging_loss_image: 8.470|tagging_loss_fusion: 7.782|total_loss: 48.155 | Examples/sec: 71.51\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 2351 | tagging_loss_video: 6.893|tagging_loss_audio: 12.275|tagging_loss_text: 12.812|tagging_loss_image: 8.328|tagging_loss_fusion: 6.632|total_loss: 46.940 | 64.61 Examples/sec\n",
      "INFO:tensorflow:training step 2352 | tagging_loss_video: 7.694|tagging_loss_audio: 11.811|tagging_loss_text: 16.050|tagging_loss_image: 9.069|tagging_loss_fusion: 7.990|total_loss: 52.614 | 69.30 Examples/sec\n",
      "INFO:tensorflow:training step 2353 | tagging_loss_video: 6.955|tagging_loss_audio: 12.496|tagging_loss_text: 15.651|tagging_loss_image: 7.858|tagging_loss_fusion: 6.151|total_loss: 49.111 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 2354 | tagging_loss_video: 6.789|tagging_loss_audio: 13.532|tagging_loss_text: 14.083|tagging_loss_image: 9.080|tagging_loss_fusion: 8.799|total_loss: 52.283 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 2355 | tagging_loss_video: 7.175|tagging_loss_audio: 12.076|tagging_loss_text: 19.269|tagging_loss_image: 10.353|tagging_loss_fusion: 8.646|total_loss: 57.520 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 2356 | tagging_loss_video: 7.128|tagging_loss_audio: 10.001|tagging_loss_text: 15.159|tagging_loss_image: 8.936|tagging_loss_fusion: 7.019|total_loss: 48.244 | 56.84 Examples/sec\n",
      "INFO:tensorflow:training step 2357 | tagging_loss_video: 6.345|tagging_loss_audio: 11.896|tagging_loss_text: 12.702|tagging_loss_image: 8.249|tagging_loss_fusion: 5.056|total_loss: 44.249 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 2358 | tagging_loss_video: 6.106|tagging_loss_audio: 8.593|tagging_loss_text: 15.041|tagging_loss_image: 7.336|tagging_loss_fusion: 7.476|total_loss: 44.553 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 2359 | tagging_loss_video: 6.131|tagging_loss_audio: 13.129|tagging_loss_text: 15.595|tagging_loss_image: 7.581|tagging_loss_fusion: 5.972|total_loss: 48.408 | 62.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2360 |tagging_loss_video: 8.146|tagging_loss_audio: 11.460|tagging_loss_text: 19.231|tagging_loss_image: 8.515|tagging_loss_fusion: 7.869|total_loss: 55.220 | Examples/sec: 70.96\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 2361 | tagging_loss_video: 7.269|tagging_loss_audio: 9.878|tagging_loss_text: 14.947|tagging_loss_image: 8.480|tagging_loss_fusion: 8.253|total_loss: 48.827 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 2362 | tagging_loss_video: 7.622|tagging_loss_audio: 10.926|tagging_loss_text: 16.133|tagging_loss_image: 9.963|tagging_loss_fusion: 8.841|total_loss: 53.484 | 66.82 Examples/sec\n",
      "INFO:tensorflow:training step 2363 | tagging_loss_video: 5.826|tagging_loss_audio: 8.729|tagging_loss_text: 14.851|tagging_loss_image: 8.191|tagging_loss_fusion: 4.832|total_loss: 42.428 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 2364 | tagging_loss_video: 6.655|tagging_loss_audio: 12.166|tagging_loss_text: 16.405|tagging_loss_image: 9.211|tagging_loss_fusion: 6.202|total_loss: 50.639 | 65.52 Examples/sec\n",
      "INFO:tensorflow:training step 2365 | tagging_loss_video: 7.365|tagging_loss_audio: 10.712|tagging_loss_text: 15.865|tagging_loss_image: 8.969|tagging_loss_fusion: 11.326|total_loss: 54.236 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 2366 | tagging_loss_video: 8.376|tagging_loss_audio: 10.611|tagging_loss_text: 15.988|tagging_loss_image: 9.304|tagging_loss_fusion: 7.063|total_loss: 51.342 | 68.85 Examples/sec\n",
      "INFO:tensorflow:training step 2367 | tagging_loss_video: 7.339|tagging_loss_audio: 11.647|tagging_loss_text: 13.729|tagging_loss_image: 9.169|tagging_loss_fusion: 8.037|total_loss: 49.921 | 61.94 Examples/sec\n",
      "INFO:tensorflow:training step 2368 | tagging_loss_video: 8.528|tagging_loss_audio: 11.519|tagging_loss_text: 13.708|tagging_loss_image: 9.370|tagging_loss_fusion: 6.011|total_loss: 49.137 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 2369 | tagging_loss_video: 6.769|tagging_loss_audio: 11.039|tagging_loss_text: 16.598|tagging_loss_image: 8.351|tagging_loss_fusion: 7.578|total_loss: 50.335 | 69.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2370 |tagging_loss_video: 7.078|tagging_loss_audio: 10.443|tagging_loss_text: 12.396|tagging_loss_image: 9.593|tagging_loss_fusion: 10.645|total_loss: 50.154 | Examples/sec: 67.75\n",
      "INFO:tensorflow:GAP: 0.86 | precision@0.1: 0.63 | precision@0.5: 0.87 |recall@0.1: 0.93 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 2371 | tagging_loss_video: 7.951|tagging_loss_audio: 13.577|tagging_loss_text: 17.941|tagging_loss_image: 10.453|tagging_loss_fusion: 9.282|total_loss: 59.205 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 2372 | tagging_loss_video: 7.417|tagging_loss_audio: 12.285|tagging_loss_text: 15.709|tagging_loss_image: 8.024|tagging_loss_fusion: 7.325|total_loss: 50.760 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 2373 | tagging_loss_video: 7.477|tagging_loss_audio: 10.064|tagging_loss_text: 15.899|tagging_loss_image: 9.520|tagging_loss_fusion: 8.299|total_loss: 51.258 | 66.05 Examples/sec\n",
      "INFO:tensorflow:training step 2374 | tagging_loss_video: 7.480|tagging_loss_audio: 13.130|tagging_loss_text: 13.185|tagging_loss_image: 8.110|tagging_loss_fusion: 7.697|total_loss: 49.601 | 68.48 Examples/sec\n",
      "INFO:tensorflow:training step 2375 | tagging_loss_video: 8.726|tagging_loss_audio: 12.970|tagging_loss_text: 16.598|tagging_loss_image: 9.720|tagging_loss_fusion: 8.586|total_loss: 56.600 | 66.93 Examples/sec\n",
      "INFO:tensorflow:training step 2376 | tagging_loss_video: 7.566|tagging_loss_audio: 10.712|tagging_loss_text: 15.515|tagging_loss_image: 10.053|tagging_loss_fusion: 8.904|total_loss: 52.750 | 66.23 Examples/sec\n",
      "INFO:tensorflow:training step 2377 | tagging_loss_video: 8.952|tagging_loss_audio: 13.794|tagging_loss_text: 18.076|tagging_loss_image: 11.170|tagging_loss_fusion: 10.716|total_loss: 62.709 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 2378 | tagging_loss_video: 7.331|tagging_loss_audio: 11.854|tagging_loss_text: 15.827|tagging_loss_image: 8.914|tagging_loss_fusion: 7.199|total_loss: 51.126 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 2379 | tagging_loss_video: 6.825|tagging_loss_audio: 10.914|tagging_loss_text: 17.679|tagging_loss_image: 9.861|tagging_loss_fusion: 8.254|total_loss: 53.533 | 69.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2380 |tagging_loss_video: 7.220|tagging_loss_audio: 11.588|tagging_loss_text: 11.694|tagging_loss_image: 7.818|tagging_loss_fusion: 6.923|total_loss: 45.243 | Examples/sec: 68.15\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 2381 | tagging_loss_video: 8.793|tagging_loss_audio: 14.483|tagging_loss_text: 15.936|tagging_loss_image: 9.000|tagging_loss_fusion: 9.007|total_loss: 57.219 | 66.45 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 2381.\n",
      "INFO:tensorflow:training step 2382 | tagging_loss_video: 7.272|tagging_loss_audio: 10.991|tagging_loss_text: 17.571|tagging_loss_image: 8.968|tagging_loss_fusion: 7.599|total_loss: 52.401 | 49.22 Examples/sec\n",
      "INFO:tensorflow:training step 2383 | tagging_loss_video: 8.561|tagging_loss_audio: 13.251|tagging_loss_text: 14.069|tagging_loss_image: 9.207|tagging_loss_fusion: 8.253|total_loss: 53.341 | 66.45 Examples/sec\n",
      "INFO:tensorflow:training step 2384 | tagging_loss_video: 9.257|tagging_loss_audio: 13.041|tagging_loss_text: 16.795|tagging_loss_image: 10.568|tagging_loss_fusion: 8.476|total_loss: 58.137 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 2385 | tagging_loss_video: 6.776|tagging_loss_audio: 11.615|tagging_loss_text: 18.056|tagging_loss_image: 8.553|tagging_loss_fusion: 7.193|total_loss: 52.193 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 2386 | tagging_loss_video: 7.642|tagging_loss_audio: 10.173|tagging_loss_text: 15.991|tagging_loss_image: 8.109|tagging_loss_fusion: 7.261|total_loss: 49.176 | 65.98 Examples/sec\n",
      "INFO:tensorflow:training step 2387 | tagging_loss_video: 7.473|tagging_loss_audio: 10.743|tagging_loss_text: 14.753|tagging_loss_image: 8.902|tagging_loss_fusion: 7.331|total_loss: 49.204 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 2388 | tagging_loss_video: 7.360|tagging_loss_audio: 11.289|tagging_loss_text: 14.388|tagging_loss_image: 8.958|tagging_loss_fusion: 7.559|total_loss: 49.555 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 2389 | tagging_loss_video: 7.058|tagging_loss_audio: 10.318|tagging_loss_text: 15.300|tagging_loss_image: 8.572|tagging_loss_fusion: 8.155|total_loss: 49.403 | 67.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2390 |tagging_loss_video: 8.100|tagging_loss_audio: 10.975|tagging_loss_text: 16.391|tagging_loss_image: 8.804|tagging_loss_fusion: 12.996|total_loss: 57.265 | Examples/sec: 71.63\n",
      "INFO:tensorflow:GAP: 0.84 | precision@0.1: 0.68 | precision@0.5: 0.87 |recall@0.1: 0.91 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 2391 | tagging_loss_video: 7.636|tagging_loss_audio: 12.283|tagging_loss_text: 17.849|tagging_loss_image: 9.716|tagging_loss_fusion: 5.807|total_loss: 53.291 | 66.82 Examples/sec\n",
      "INFO:tensorflow:training step 2392 | tagging_loss_video: 7.412|tagging_loss_audio: 11.402|tagging_loss_text: 15.171|tagging_loss_image: 8.017|tagging_loss_fusion: 8.989|total_loss: 50.993 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 2393 | tagging_loss_video: 6.947|tagging_loss_audio: 11.366|tagging_loss_text: 14.292|tagging_loss_image: 8.468|tagging_loss_fusion: 7.414|total_loss: 48.486 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 2394 | tagging_loss_video: 6.874|tagging_loss_audio: 10.642|tagging_loss_text: 13.680|tagging_loss_image: 7.893|tagging_loss_fusion: 6.936|total_loss: 46.026 | 65.53 Examples/sec\n",
      "INFO:tensorflow:training step 2395 | tagging_loss_video: 7.408|tagging_loss_audio: 12.826|tagging_loss_text: 17.590|tagging_loss_image: 8.979|tagging_loss_fusion: 7.423|total_loss: 54.225 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 2396 | tagging_loss_video: 7.134|tagging_loss_audio: 11.024|tagging_loss_text: 13.658|tagging_loss_image: 8.802|tagging_loss_fusion: 5.853|total_loss: 46.471 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 2397 | tagging_loss_video: 6.723|tagging_loss_audio: 9.948|tagging_loss_text: 11.945|tagging_loss_image: 8.356|tagging_loss_fusion: 7.819|total_loss: 44.791 | 64.65 Examples/sec\n",
      "INFO:tensorflow:training step 2398 | tagging_loss_video: 7.274|tagging_loss_audio: 11.001|tagging_loss_text: 14.682|tagging_loss_image: 8.590|tagging_loss_fusion: 7.473|total_loss: 49.020 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2399 | tagging_loss_video: 7.844|tagging_loss_audio: 12.769|tagging_loss_text: 15.587|tagging_loss_image: 7.888|tagging_loss_fusion: 7.678|total_loss: 51.767 | 67.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2400 |tagging_loss_video: 7.193|tagging_loss_audio: 11.943|tagging_loss_text: 13.233|tagging_loss_image: 8.450|tagging_loss_fusion: 5.652|total_loss: 46.471 | Examples/sec: 67.48\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.76 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2401 | tagging_loss_video: 6.957|tagging_loss_audio: 10.933|tagging_loss_text: 15.850|tagging_loss_image: 7.843|tagging_loss_fusion: 6.426|total_loss: 48.009 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 2402 | tagging_loss_video: 6.815|tagging_loss_audio: 12.105|tagging_loss_text: 16.778|tagging_loss_image: 8.622|tagging_loss_fusion: 7.198|total_loss: 51.517 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 2403 | tagging_loss_video: 7.004|tagging_loss_audio: 11.530|tagging_loss_text: 14.685|tagging_loss_image: 8.272|tagging_loss_fusion: 7.050|total_loss: 48.541 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 2404 | tagging_loss_video: 7.695|tagging_loss_audio: 10.430|tagging_loss_text: 13.197|tagging_loss_image: 8.889|tagging_loss_fusion: 6.247|total_loss: 46.458 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 2405 | tagging_loss_video: 7.392|tagging_loss_audio: 12.676|tagging_loss_text: 12.910|tagging_loss_image: 8.764|tagging_loss_fusion: 8.927|total_loss: 50.669 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 2406 | tagging_loss_video: 6.979|tagging_loss_audio: 11.375|tagging_loss_text: 14.686|tagging_loss_image: 8.087|tagging_loss_fusion: 7.991|total_loss: 49.117 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 2407 | tagging_loss_video: 7.979|tagging_loss_audio: 11.766|tagging_loss_text: 11.969|tagging_loss_image: 8.421|tagging_loss_fusion: 6.075|total_loss: 46.210 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 2408 | tagging_loss_video: 7.359|tagging_loss_audio: 10.256|tagging_loss_text: 13.542|tagging_loss_image: 8.683|tagging_loss_fusion: 6.100|total_loss: 45.939 | 67.33 Examples/sec\n",
      "INFO:tensorflow:training step 2409 | tagging_loss_video: 7.464|tagging_loss_audio: 10.661|tagging_loss_text: 16.115|tagging_loss_image: 10.162|tagging_loss_fusion: 8.386|total_loss: 52.788 | 70.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2410 |tagging_loss_video: 8.491|tagging_loss_audio: 13.528|tagging_loss_text: 16.142|tagging_loss_image: 11.024|tagging_loss_fusion: 7.445|total_loss: 56.629 | Examples/sec: 67.38\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.95 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 2411 | tagging_loss_video: 8.346|tagging_loss_audio: 11.877|tagging_loss_text: 14.780|tagging_loss_image: 9.714|tagging_loss_fusion: 7.189|total_loss: 51.906 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 2412 | tagging_loss_video: 6.813|tagging_loss_audio: 13.031|tagging_loss_text: 13.724|tagging_loss_image: 8.801|tagging_loss_fusion: 8.511|total_loss: 50.879 | 71.85 Examples/sec\n",
      "INFO:tensorflow:training step 2413 | tagging_loss_video: 8.038|tagging_loss_audio: 11.744|tagging_loss_text: 15.025|tagging_loss_image: 10.524|tagging_loss_fusion: 8.116|total_loss: 53.446 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 2414 | tagging_loss_video: 7.296|tagging_loss_audio: 11.658|tagging_loss_text: 14.736|tagging_loss_image: 7.941|tagging_loss_fusion: 7.499|total_loss: 49.130 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 2415 | tagging_loss_video: 7.885|tagging_loss_audio: 14.480|tagging_loss_text: 15.840|tagging_loss_image: 9.415|tagging_loss_fusion: 7.629|total_loss: 55.249 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 2416 | tagging_loss_video: 7.265|tagging_loss_audio: 12.080|tagging_loss_text: 11.886|tagging_loss_image: 7.955|tagging_loss_fusion: 6.506|total_loss: 45.691 | 60.98 Examples/sec\n",
      "INFO:tensorflow:training step 2417 | tagging_loss_video: 6.507|tagging_loss_audio: 12.726|tagging_loss_text: 16.568|tagging_loss_image: 9.447|tagging_loss_fusion: 7.062|total_loss: 52.309 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 2418 | tagging_loss_video: 7.871|tagging_loss_audio: 11.660|tagging_loss_text: 13.235|tagging_loss_image: 9.322|tagging_loss_fusion: 6.963|total_loss: 49.052 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 2419 | tagging_loss_video: 6.027|tagging_loss_audio: 9.797|tagging_loss_text: 13.632|tagging_loss_image: 7.224|tagging_loss_fusion: 6.815|total_loss: 43.495 | 63.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2420 |tagging_loss_video: 5.902|tagging_loss_audio: 11.565|tagging_loss_text: 15.551|tagging_loss_image: 7.974|tagging_loss_fusion: 5.711|total_loss: 46.703 | Examples/sec: 66.55\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 2421 | tagging_loss_video: 6.819|tagging_loss_audio: 11.165|tagging_loss_text: 13.871|tagging_loss_image: 8.791|tagging_loss_fusion: 7.156|total_loss: 47.803 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 2422 | tagging_loss_video: 6.725|tagging_loss_audio: 11.547|tagging_loss_text: 12.124|tagging_loss_image: 8.686|tagging_loss_fusion: 8.298|total_loss: 47.381 | 63.12 Examples/sec\n",
      "INFO:tensorflow:training step 2423 | tagging_loss_video: 7.188|tagging_loss_audio: 11.963|tagging_loss_text: 18.557|tagging_loss_image: 9.453|tagging_loss_fusion: 7.409|total_loss: 54.570 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 2424 | tagging_loss_video: 7.595|tagging_loss_audio: 11.077|tagging_loss_text: 14.864|tagging_loss_image: 8.081|tagging_loss_fusion: 7.430|total_loss: 49.047 | 66.58 Examples/sec\n",
      "INFO:tensorflow:training step 2425 | tagging_loss_video: 6.965|tagging_loss_audio: 11.282|tagging_loss_text: 16.462|tagging_loss_image: 8.423|tagging_loss_fusion: 7.130|total_loss: 50.262 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 2426 | tagging_loss_video: 7.975|tagging_loss_audio: 11.149|tagging_loss_text: 16.311|tagging_loss_image: 9.390|tagging_loss_fusion: 9.511|total_loss: 54.335 | 67.67 Examples/sec\n",
      "INFO:tensorflow:training step 2427 | tagging_loss_video: 7.076|tagging_loss_audio: 11.535|tagging_loss_text: 14.939|tagging_loss_image: 8.313|tagging_loss_fusion: 9.424|total_loss: 51.289 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 2428 | tagging_loss_video: 8.080|tagging_loss_audio: 10.975|tagging_loss_text: 13.882|tagging_loss_image: 9.085|tagging_loss_fusion: 7.082|total_loss: 49.104 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 2429 | tagging_loss_video: 6.789|tagging_loss_audio: 10.617|tagging_loss_text: 14.733|tagging_loss_image: 8.189|tagging_loss_fusion: 7.239|total_loss: 47.566 | 70.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2430 |tagging_loss_video: 6.526|tagging_loss_audio: 11.481|tagging_loss_text: 17.854|tagging_loss_image: 8.625|tagging_loss_fusion: 5.452|total_loss: 49.938 | Examples/sec: 62.38\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2431 | tagging_loss_video: 6.245|tagging_loss_audio: 10.402|tagging_loss_text: 14.207|tagging_loss_image: 7.692|tagging_loss_fusion: 6.954|total_loss: 45.499 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 2432 | tagging_loss_video: 7.034|tagging_loss_audio: 9.736|tagging_loss_text: 14.987|tagging_loss_image: 8.660|tagging_loss_fusion: 6.593|total_loss: 47.009 | 71.81 Examples/sec\n",
      "INFO:tensorflow:training step 2433 | tagging_loss_video: 7.241|tagging_loss_audio: 12.256|tagging_loss_text: 15.628|tagging_loss_image: 8.287|tagging_loss_fusion: 8.010|total_loss: 51.423 | 63.67 Examples/sec\n",
      "INFO:tensorflow:training step 2434 | tagging_loss_video: 7.932|tagging_loss_audio: 8.457|tagging_loss_text: 14.589|tagging_loss_image: 8.090|tagging_loss_fusion: 7.865|total_loss: 46.934 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2435 | tagging_loss_video: 6.469|tagging_loss_audio: 10.704|tagging_loss_text: 16.559|tagging_loss_image: 8.365|tagging_loss_fusion: 6.227|total_loss: 48.323 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 2436 | tagging_loss_video: 7.891|tagging_loss_audio: 11.574|tagging_loss_text: 16.118|tagging_loss_image: 8.604|tagging_loss_fusion: 9.958|total_loss: 54.145 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2437 | tagging_loss_video: 8.025|tagging_loss_audio: 12.326|tagging_loss_text: 16.390|tagging_loss_image: 9.185|tagging_loss_fusion: 7.297|total_loss: 53.224 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 2438 | tagging_loss_video: 7.008|tagging_loss_audio: 9.427|tagging_loss_text: 11.296|tagging_loss_image: 9.154|tagging_loss_fusion: 7.471|total_loss: 44.355 | 65.99 Examples/sec\n",
      "INFO:tensorflow:training step 2439 | tagging_loss_video: 7.573|tagging_loss_audio: 10.779|tagging_loss_text: 15.378|tagging_loss_image: 9.406|tagging_loss_fusion: 7.278|total_loss: 50.414 | 71.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2440 |tagging_loss_video: 7.399|tagging_loss_audio: 11.276|tagging_loss_text: 18.486|tagging_loss_image: 9.081|tagging_loss_fusion: 8.497|total_loss: 54.740 | Examples/sec: 71.39\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.75 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 2441 | tagging_loss_video: 6.887|tagging_loss_audio: 9.938|tagging_loss_text: 14.872|tagging_loss_image: 8.678|tagging_loss_fusion: 6.789|total_loss: 47.164 | 62.19 Examples/sec\n",
      "INFO:tensorflow:training step 2442 | tagging_loss_video: 6.701|tagging_loss_audio: 11.701|tagging_loss_text: 19.853|tagging_loss_image: 9.140|tagging_loss_fusion: 6.165|total_loss: 53.560 | 71.28 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2443 | tagging_loss_video: 7.586|tagging_loss_audio: 12.743|tagging_loss_text: 14.661|tagging_loss_image: 9.284|tagging_loss_fusion: 8.368|total_loss: 52.642 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 2444 | tagging_loss_video: 7.332|tagging_loss_audio: 11.203|tagging_loss_text: 15.255|tagging_loss_image: 8.700|tagging_loss_fusion: 11.126|total_loss: 53.617 | 60.66 Examples/sec\n",
      "INFO:tensorflow:training step 2445 | tagging_loss_video: 7.427|tagging_loss_audio: 10.290|tagging_loss_text: 18.710|tagging_loss_image: 8.573|tagging_loss_fusion: 7.401|total_loss: 52.401 | 72.15 Examples/sec\n",
      "INFO:tensorflow:training step 2446 | tagging_loss_video: 6.139|tagging_loss_audio: 10.645|tagging_loss_text: 14.877|tagging_loss_image: 9.007|tagging_loss_fusion: 6.341|total_loss: 47.009 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2447 | tagging_loss_video: 8.122|tagging_loss_audio: 11.637|tagging_loss_text: 14.704|tagging_loss_image: 9.077|tagging_loss_fusion: 8.384|total_loss: 51.924 | 64.78 Examples/sec\n",
      "INFO:tensorflow:training step 2448 | tagging_loss_video: 7.141|tagging_loss_audio: 9.159|tagging_loss_text: 17.430|tagging_loss_image: 7.712|tagging_loss_fusion: 8.363|total_loss: 49.806 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 2449 | tagging_loss_video: 7.600|tagging_loss_audio: 11.455|tagging_loss_text: 15.826|tagging_loss_image: 7.899|tagging_loss_fusion: 9.383|total_loss: 52.162 | 71.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2450 |tagging_loss_video: 7.371|tagging_loss_audio: 11.650|tagging_loss_text: 14.112|tagging_loss_image: 8.687|tagging_loss_fusion: 9.130|total_loss: 50.950 | Examples/sec: 66.77\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.66 | precision@0.5: 0.87 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2451 | tagging_loss_video: 7.740|tagging_loss_audio: 11.715|tagging_loss_text: 15.397|tagging_loss_image: 9.145|tagging_loss_fusion: 8.457|total_loss: 52.454 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 2452 | tagging_loss_video: 7.743|tagging_loss_audio: 10.148|tagging_loss_text: 12.388|tagging_loss_image: 7.710|tagging_loss_fusion: 8.553|total_loss: 46.542 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 2453 | tagging_loss_video: 7.607|tagging_loss_audio: 11.181|tagging_loss_text: 18.486|tagging_loss_image: 8.744|tagging_loss_fusion: 7.719|total_loss: 53.736 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 2454 | tagging_loss_video: 7.498|tagging_loss_audio: 10.691|tagging_loss_text: 16.496|tagging_loss_image: 8.898|tagging_loss_fusion: 5.972|total_loss: 49.554 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 2455 | tagging_loss_video: 7.657|tagging_loss_audio: 11.294|tagging_loss_text: 17.446|tagging_loss_image: 9.685|tagging_loss_fusion: 8.435|total_loss: 54.516 | 64.27 Examples/sec\n",
      "INFO:tensorflow:training step 2456 | tagging_loss_video: 7.172|tagging_loss_audio: 11.508|tagging_loss_text: 16.864|tagging_loss_image: 9.699|tagging_loss_fusion: 8.514|total_loss: 53.757 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 2457 | tagging_loss_video: 8.251|tagging_loss_audio: 11.215|tagging_loss_text: 16.034|tagging_loss_image: 9.035|tagging_loss_fusion: 8.558|total_loss: 53.093 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 2458 | tagging_loss_video: 6.256|tagging_loss_audio: 9.553|tagging_loss_text: 11.925|tagging_loss_image: 7.488|tagging_loss_fusion: 4.815|total_loss: 40.038 | 62.60 Examples/sec\n",
      "INFO:tensorflow:training step 2459 | tagging_loss_video: 6.663|tagging_loss_audio: 10.552|tagging_loss_text: 12.640|tagging_loss_image: 7.500|tagging_loss_fusion: 5.440|total_loss: 42.795 | 71.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2460 |tagging_loss_video: 6.590|tagging_loss_audio: 11.595|tagging_loss_text: 14.818|tagging_loss_image: 8.324|tagging_loss_fusion: 6.622|total_loss: 47.950 | Examples/sec: 69.99\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2461 | tagging_loss_video: 6.861|tagging_loss_audio: 10.391|tagging_loss_text: 12.284|tagging_loss_image: 7.431|tagging_loss_fusion: 7.600|total_loss: 44.567 | 66.81 Examples/sec\n",
      "INFO:tensorflow:training step 2462 | tagging_loss_video: 5.982|tagging_loss_audio: 11.495|tagging_loss_text: 16.142|tagging_loss_image: 8.738|tagging_loss_fusion: 7.130|total_loss: 49.487 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 2463 | tagging_loss_video: 6.638|tagging_loss_audio: 10.222|tagging_loss_text: 14.522|tagging_loss_image: 8.518|tagging_loss_fusion: 7.048|total_loss: 46.948 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 2464 | tagging_loss_video: 7.231|tagging_loss_audio: 11.964|tagging_loss_text: 11.333|tagging_loss_image: 7.988|tagging_loss_fusion: 8.108|total_loss: 46.624 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 2465 | tagging_loss_video: 6.466|tagging_loss_audio: 10.340|tagging_loss_text: 14.702|tagging_loss_image: 7.702|tagging_loss_fusion: 6.944|total_loss: 46.153 | 67.41 Examples/sec\n",
      "INFO:tensorflow:training step 2466 | tagging_loss_video: 7.628|tagging_loss_audio: 11.271|tagging_loss_text: 14.806|tagging_loss_image: 9.115|tagging_loss_fusion: 11.462|total_loss: 54.282 | 63.83 Examples/sec\n",
      "INFO:tensorflow:training step 2467 | tagging_loss_video: 6.519|tagging_loss_audio: 10.645|tagging_loss_text: 13.056|tagging_loss_image: 8.386|tagging_loss_fusion: 5.870|total_loss: 44.477 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 2468 | tagging_loss_video: 5.765|tagging_loss_audio: 10.516|tagging_loss_text: 14.313|tagging_loss_image: 7.876|tagging_loss_fusion: 6.550|total_loss: 45.020 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 2469 | tagging_loss_video: 6.703|tagging_loss_audio: 9.319|tagging_loss_text: 12.976|tagging_loss_image: 7.642|tagging_loss_fusion: 6.590|total_loss: 43.229 | 61.36 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2470 |tagging_loss_video: 7.880|tagging_loss_audio: 11.001|tagging_loss_text: 14.642|tagging_loss_image: 7.863|tagging_loss_fusion: 8.888|total_loss: 50.273 | Examples/sec: 68.67\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.73 | precision@0.5: 0.92 |recall@0.1: 0.95 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 2471 | tagging_loss_video: 7.345|tagging_loss_audio: 10.930|tagging_loss_text: 18.266|tagging_loss_image: 8.842|tagging_loss_fusion: 8.079|total_loss: 53.462 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 2472 | tagging_loss_video: 7.037|tagging_loss_audio: 11.311|tagging_loss_text: 12.906|tagging_loss_image: 8.947|tagging_loss_fusion: 7.856|total_loss: 48.057 | 65.26 Examples/sec\n",
      "INFO:tensorflow:training step 2473 | tagging_loss_video: 6.971|tagging_loss_audio: 11.446|tagging_loss_text: 12.081|tagging_loss_image: 8.057|tagging_loss_fusion: 7.403|total_loss: 45.957 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 2474 | tagging_loss_video: 6.492|tagging_loss_audio: 11.194|tagging_loss_text: 12.443|tagging_loss_image: 8.632|tagging_loss_fusion: 6.584|total_loss: 45.345 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 2475 | tagging_loss_video: 6.885|tagging_loss_audio: 10.788|tagging_loss_text: 13.664|tagging_loss_image: 8.050|tagging_loss_fusion: 7.692|total_loss: 47.079 | 66.22 Examples/sec\n",
      "INFO:tensorflow:training step 2476 | tagging_loss_video: 7.197|tagging_loss_audio: 11.229|tagging_loss_text: 13.716|tagging_loss_image: 8.086|tagging_loss_fusion: 9.155|total_loss: 49.383 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 2477 | tagging_loss_video: 6.656|tagging_loss_audio: 10.758|tagging_loss_text: 15.895|tagging_loss_image: 8.920|tagging_loss_fusion: 8.085|total_loss: 50.314 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 2478 | tagging_loss_video: 7.236|tagging_loss_audio: 9.678|tagging_loss_text: 12.652|tagging_loss_image: 8.896|tagging_loss_fusion: 8.603|total_loss: 47.066 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 2479 | tagging_loss_video: 6.464|tagging_loss_audio: 9.661|tagging_loss_text: 18.756|tagging_loss_image: 8.544|tagging_loss_fusion: 8.593|total_loss: 52.018 | 70.32 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2480 |tagging_loss_video: 7.115|tagging_loss_audio: 10.537|tagging_loss_text: 15.351|tagging_loss_image: 7.866|tagging_loss_fusion: 6.834|total_loss: 47.704 | Examples/sec: 63.39\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2481 | tagging_loss_video: 7.442|tagging_loss_audio: 12.217|tagging_loss_text: 16.115|tagging_loss_image: 8.697|tagging_loss_fusion: 7.368|total_loss: 51.838 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 2482 | tagging_loss_video: 7.542|tagging_loss_audio: 9.982|tagging_loss_text: 16.245|tagging_loss_image: 8.362|tagging_loss_fusion: 7.655|total_loss: 49.786 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 2483 | tagging_loss_video: 7.606|tagging_loss_audio: 11.895|tagging_loss_text: 17.895|tagging_loss_image: 8.927|tagging_loss_fusion: 8.874|total_loss: 55.198 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 2484 | tagging_loss_video: 7.613|tagging_loss_audio: 11.360|tagging_loss_text: 15.154|tagging_loss_image: 7.623|tagging_loss_fusion: 6.215|total_loss: 47.966 | 62.55 Examples/sec\n",
      "INFO:tensorflow:training step 2485 | tagging_loss_video: 6.233|tagging_loss_audio: 9.425|tagging_loss_text: 14.894|tagging_loss_image: 6.785|tagging_loss_fusion: 6.076|total_loss: 43.413 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 2486 | tagging_loss_video: 7.331|tagging_loss_audio: 10.193|tagging_loss_text: 15.968|tagging_loss_image: 8.051|tagging_loss_fusion: 6.400|total_loss: 47.943 | 69.30 Examples/sec\n",
      "INFO:tensorflow:training step 2487 | tagging_loss_video: 5.795|tagging_loss_audio: 12.217|tagging_loss_text: 13.585|tagging_loss_image: 7.649|tagging_loss_fusion: 4.964|total_loss: 44.209 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2488 | tagging_loss_video: 7.324|tagging_loss_audio: 10.975|tagging_loss_text: 15.182|tagging_loss_image: 8.287|tagging_loss_fusion: 9.089|total_loss: 50.857 | 68.25 Examples/sec\n",
      "INFO:tensorflow:training step 2489 | tagging_loss_video: 7.704|tagging_loss_audio: 11.942|tagging_loss_text: 15.896|tagging_loss_image: 8.341|tagging_loss_fusion: 9.438|total_loss: 53.321 | 70.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2490 |tagging_loss_video: 6.757|tagging_loss_audio: 10.020|tagging_loss_text: 16.017|tagging_loss_image: 8.119|tagging_loss_fusion: 6.457|total_loss: 47.369 | Examples/sec: 67.59\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.74 | precision@0.5: 0.90 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2491 | tagging_loss_video: 7.406|tagging_loss_audio: 11.660|tagging_loss_text: 16.796|tagging_loss_image: 9.077|tagging_loss_fusion: 8.628|total_loss: 53.567 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2492 | tagging_loss_video: 6.469|tagging_loss_audio: 12.094|tagging_loss_text: 15.030|tagging_loss_image: 9.095|tagging_loss_fusion: 6.935|total_loss: 49.623 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 2493 | tagging_loss_video: 7.556|tagging_loss_audio: 11.811|tagging_loss_text: 15.487|tagging_loss_image: 8.365|tagging_loss_fusion: 7.510|total_loss: 50.729 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 2494 | tagging_loss_video: 7.344|tagging_loss_audio: 12.496|tagging_loss_text: 17.159|tagging_loss_image: 9.415|tagging_loss_fusion: 6.479|total_loss: 52.894 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 2495 | tagging_loss_video: 7.083|tagging_loss_audio: 11.595|tagging_loss_text: 14.518|tagging_loss_image: 9.113|tagging_loss_fusion: 8.004|total_loss: 50.313 | 63.51 Examples/sec\n",
      "INFO:tensorflow:training step 2496 | tagging_loss_video: 6.474|tagging_loss_audio: 10.558|tagging_loss_text: 14.562|tagging_loss_image: 7.739|tagging_loss_fusion: 5.652|total_loss: 44.985 | 72.23 Examples/sec\n",
      "INFO:tensorflow:training step 2497 | tagging_loss_video: 6.688|tagging_loss_audio: 10.127|tagging_loss_text: 13.761|tagging_loss_image: 7.518|tagging_loss_fusion: 6.843|total_loss: 44.937 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 2498 | tagging_loss_video: 7.410|tagging_loss_audio: 9.701|tagging_loss_text: 13.652|tagging_loss_image: 7.334|tagging_loss_fusion: 6.706|total_loss: 44.803 | 62.78 Examples/sec\n",
      "INFO:tensorflow:training step 2499 | tagging_loss_video: 7.606|tagging_loss_audio: 10.664|tagging_loss_text: 14.971|tagging_loss_image: 8.163|tagging_loss_fusion: 8.129|total_loss: 49.532 | 69.75 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2500 |tagging_loss_video: 7.273|tagging_loss_audio: 11.933|tagging_loss_text: 17.581|tagging_loss_image: 8.165|tagging_loss_fusion: 7.236|total_loss: 52.187 | Examples/sec: 72.76\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2501 | tagging_loss_video: 7.277|tagging_loss_audio: 11.743|tagging_loss_text: 15.362|tagging_loss_image: 9.714|tagging_loss_fusion: 7.882|total_loss: 51.978 | 64.38 Examples/sec\n",
      "INFO:tensorflow:training step 2502 | tagging_loss_video: 7.190|tagging_loss_audio: 9.432|tagging_loss_text: 13.665|tagging_loss_image: 8.436|tagging_loss_fusion: 7.493|total_loss: 46.216 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 2503 | tagging_loss_video: 8.051|tagging_loss_audio: 10.702|tagging_loss_text: 18.560|tagging_loss_image: 8.999|tagging_loss_fusion: 9.425|total_loss: 55.737 | 67.46 Examples/sec\n",
      "INFO:tensorflow:training step 2504 | tagging_loss_video: 8.536|tagging_loss_audio: 10.575|tagging_loss_text: 13.698|tagging_loss_image: 9.202|tagging_loss_fusion: 8.986|total_loss: 50.997 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 2505 | tagging_loss_video: 8.180|tagging_loss_audio: 12.259|tagging_loss_text: 13.306|tagging_loss_image: 8.649|tagging_loss_fusion: 7.692|total_loss: 50.087 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 2506 | tagging_loss_video: 7.253|tagging_loss_audio: 10.099|tagging_loss_text: 14.323|tagging_loss_image: 7.172|tagging_loss_fusion: 7.582|total_loss: 46.430 | 61.31 Examples/sec\n",
      "INFO:tensorflow:training step 2507 | tagging_loss_video: 7.193|tagging_loss_audio: 11.965|tagging_loss_text: 15.910|tagging_loss_image: 9.284|tagging_loss_fusion: 6.846|total_loss: 51.199 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 2508 | tagging_loss_video: 6.652|tagging_loss_audio: 11.912|tagging_loss_text: 16.543|tagging_loss_image: 8.635|tagging_loss_fusion: 6.339|total_loss: 50.082 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 2509 | tagging_loss_video: 8.166|tagging_loss_audio: 12.575|tagging_loss_text: 14.495|tagging_loss_image: 9.712|tagging_loss_fusion: 6.956|total_loss: 51.903 | 62.37 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2510 |tagging_loss_video: 8.053|tagging_loss_audio: 12.486|tagging_loss_text: 14.891|tagging_loss_image: 9.301|tagging_loss_fusion: 9.096|total_loss: 53.827 | Examples/sec: 68.85\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2511 | tagging_loss_video: 7.150|tagging_loss_audio: 11.803|tagging_loss_text: 15.106|tagging_loss_image: 8.803|tagging_loss_fusion: 9.760|total_loss: 52.622 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 2512 | tagging_loss_video: 7.422|tagging_loss_audio: 10.156|tagging_loss_text: 15.226|tagging_loss_image: 8.497|tagging_loss_fusion: 7.522|total_loss: 48.823 | 66.11 Examples/sec\n",
      "INFO:tensorflow:training step 2513 | tagging_loss_video: 7.339|tagging_loss_audio: 12.492|tagging_loss_text: 17.964|tagging_loss_image: 8.588|tagging_loss_fusion: 6.731|total_loss: 53.115 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 2514 | tagging_loss_video: 7.657|tagging_loss_audio: 13.325|tagging_loss_text: 15.945|tagging_loss_image: 8.527|tagging_loss_fusion: 8.950|total_loss: 54.404 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 2515 | tagging_loss_video: 7.960|tagging_loss_audio: 11.786|tagging_loss_text: 16.866|tagging_loss_image: 9.405|tagging_loss_fusion: 8.587|total_loss: 54.604 | 63.36 Examples/sec\n",
      "INFO:tensorflow:training step 2516 | tagging_loss_video: 8.322|tagging_loss_audio: 12.756|tagging_loss_text: 15.788|tagging_loss_image: 9.875|tagging_loss_fusion: 9.507|total_loss: 56.248 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 2517 | tagging_loss_video: 6.830|tagging_loss_audio: 9.636|tagging_loss_text: 14.560|tagging_loss_image: 8.926|tagging_loss_fusion: 7.620|total_loss: 47.572 | 65.75 Examples/sec\n",
      "INFO:tensorflow:training step 2518 | tagging_loss_video: 7.739|tagging_loss_audio: 12.406|tagging_loss_text: 16.847|tagging_loss_image: 8.716|tagging_loss_fusion: 6.895|total_loss: 52.603 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 2519 | tagging_loss_video: 6.641|tagging_loss_audio: 8.088|tagging_loss_text: 11.769|tagging_loss_image: 8.308|tagging_loss_fusion: 6.717|total_loss: 41.523 | 70.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2520 |tagging_loss_video: 8.821|tagging_loss_audio: 11.921|tagging_loss_text: 14.528|tagging_loss_image: 9.286|tagging_loss_fusion: 8.418|total_loss: 52.975 | Examples/sec: 61.85\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2521 | tagging_loss_video: 7.362|tagging_loss_audio: 12.956|tagging_loss_text: 13.736|tagging_loss_image: 9.520|tagging_loss_fusion: 7.857|total_loss: 51.430 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 2522 | tagging_loss_video: 7.324|tagging_loss_audio: 10.853|tagging_loss_text: 12.392|tagging_loss_image: 9.026|tagging_loss_fusion: 7.148|total_loss: 46.744 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 2523 | tagging_loss_video: 7.678|tagging_loss_audio: 11.111|tagging_loss_text: 12.616|tagging_loss_image: 9.081|tagging_loss_fusion: 6.543|total_loss: 47.028 | 65.45 Examples/sec\n",
      "INFO:tensorflow:training step 2524 | tagging_loss_video: 7.926|tagging_loss_audio: 9.854|tagging_loss_text: 19.099|tagging_loss_image: 9.842|tagging_loss_fusion: 9.219|total_loss: 55.940 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 2525 | tagging_loss_video: 7.287|tagging_loss_audio: 11.261|tagging_loss_text: 13.811|tagging_loss_image: 8.534|tagging_loss_fusion: 8.087|total_loss: 48.981 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 2526 | tagging_loss_video: 7.472|tagging_loss_audio: 11.013|tagging_loss_text: 16.057|tagging_loss_image: 8.633|tagging_loss_fusion: 6.854|total_loss: 50.029 | 67.17 Examples/sec\n",
      "INFO:tensorflow:training step 2527 | tagging_loss_video: 6.496|tagging_loss_audio: 11.906|tagging_loss_text: 16.126|tagging_loss_image: 9.135|tagging_loss_fusion: 6.846|total_loss: 50.509 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 2528 | tagging_loss_video: 7.542|tagging_loss_audio: 10.672|tagging_loss_text: 12.988|tagging_loss_image: 8.478|tagging_loss_fusion: 7.613|total_loss: 47.293 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 2529 | tagging_loss_video: 7.241|tagging_loss_audio: 11.246|tagging_loss_text: 18.710|tagging_loss_image: 8.024|tagging_loss_fusion: 6.027|total_loss: 51.247 | 70.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2530 |tagging_loss_video: 7.836|tagging_loss_audio: 13.719|tagging_loss_text: 14.096|tagging_loss_image: 9.190|tagging_loss_fusion: 7.809|total_loss: 52.649 | Examples/sec: 69.94\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.74 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2531 | tagging_loss_video: 8.074|tagging_loss_audio: 10.573|tagging_loss_text: 14.858|tagging_loss_image: 8.214|tagging_loss_fusion: 8.865|total_loss: 50.584 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 2532 | tagging_loss_video: 6.833|tagging_loss_audio: 9.886|tagging_loss_text: 13.178|tagging_loss_image: 7.816|tagging_loss_fusion: 6.474|total_loss: 44.186 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 2533 | tagging_loss_video: 6.610|tagging_loss_audio: 12.538|tagging_loss_text: 17.061|tagging_loss_image: 8.774|tagging_loss_fusion: 6.359|total_loss: 51.342 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 2534 | tagging_loss_video: 7.205|tagging_loss_audio: 10.028|tagging_loss_text: 16.190|tagging_loss_image: 7.675|tagging_loss_fusion: 8.519|total_loss: 49.617 | 63.40 Examples/sec\n",
      "INFO:tensorflow:training step 2535 | tagging_loss_video: 7.258|tagging_loss_audio: 14.053|tagging_loss_text: 16.851|tagging_loss_image: 9.092|tagging_loss_fusion: 8.314|total_loss: 55.567 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 2536 | tagging_loss_video: 8.204|tagging_loss_audio: 10.251|tagging_loss_text: 15.690|tagging_loss_image: 8.781|tagging_loss_fusion: 8.933|total_loss: 51.859 | 65.33 Examples/sec\n",
      "INFO:tensorflow:training step 2537 | tagging_loss_video: 6.472|tagging_loss_audio: 10.043|tagging_loss_text: 15.171|tagging_loss_image: 7.777|tagging_loss_fusion: 6.381|total_loss: 45.845 | 66.14 Examples/sec\n",
      "INFO:tensorflow:training step 2538 | tagging_loss_video: 6.755|tagging_loss_audio: 11.730|tagging_loss_text: 14.651|tagging_loss_image: 7.587|tagging_loss_fusion: 6.532|total_loss: 47.255 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 2539 | tagging_loss_video: 7.229|tagging_loss_audio: 11.903|tagging_loss_text: 14.186|tagging_loss_image: 7.937|tagging_loss_fusion: 6.865|total_loss: 48.120 | 69.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2540 |tagging_loss_video: 5.928|tagging_loss_audio: 10.360|tagging_loss_text: 14.577|tagging_loss_image: 7.655|tagging_loss_fusion: 5.465|total_loss: 43.985 | Examples/sec: 67.07\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2541 | tagging_loss_video: 6.941|tagging_loss_audio: 12.579|tagging_loss_text: 12.536|tagging_loss_image: 7.911|tagging_loss_fusion: 7.486|total_loss: 47.452 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 2542 | tagging_loss_video: 6.406|tagging_loss_audio: 10.674|tagging_loss_text: 16.031|tagging_loss_image: 8.182|tagging_loss_fusion: 6.644|total_loss: 47.937 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 2543 | tagging_loss_video: 6.959|tagging_loss_audio: 11.808|tagging_loss_text: 14.736|tagging_loss_image: 8.001|tagging_loss_fusion: 5.625|total_loss: 47.129 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 2544 | tagging_loss_video: 6.913|tagging_loss_audio: 11.152|tagging_loss_text: 13.907|tagging_loss_image: 8.759|tagging_loss_fusion: 6.959|total_loss: 47.690 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 2545 | tagging_loss_video: 7.336|tagging_loss_audio: 11.709|tagging_loss_text: 14.987|tagging_loss_image: 7.998|tagging_loss_fusion: 7.703|total_loss: 49.733 | 64.25 Examples/sec\n",
      "INFO:tensorflow:training step 2546 | tagging_loss_video: 6.704|tagging_loss_audio: 10.124|tagging_loss_text: 12.055|tagging_loss_image: 7.837|tagging_loss_fusion: 6.278|total_loss: 42.998 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 2547 | tagging_loss_video: 7.300|tagging_loss_audio: 10.599|tagging_loss_text: 14.764|tagging_loss_image: 7.792|tagging_loss_fusion: 6.975|total_loss: 47.430 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 2548 | tagging_loss_video: 6.865|tagging_loss_audio: 10.594|tagging_loss_text: 14.894|tagging_loss_image: 7.898|tagging_loss_fusion: 7.905|total_loss: 48.157 | 61.52 Examples/sec\n",
      "INFO:tensorflow:training step 2549 | tagging_loss_video: 7.319|tagging_loss_audio: 13.814|tagging_loss_text: 20.873|tagging_loss_image: 9.312|tagging_loss_fusion: 9.055|total_loss: 60.373 | 69.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2550 |tagging_loss_video: 7.918|tagging_loss_audio: 12.206|tagging_loss_text: 15.290|tagging_loss_image: 9.966|tagging_loss_fusion: 10.650|total_loss: 56.029 | Examples/sec: 69.63\n",
      "INFO:tensorflow:GAP: 0.87 | precision@0.1: 0.72 | precision@0.5: 0.90 |recall@0.1: 0.93 | recall@0.5: 0.75\n",
      "INFO:tensorflow:training step 2551 | tagging_loss_video: 7.733|tagging_loss_audio: 11.014|tagging_loss_text: 14.170|tagging_loss_image: 8.978|tagging_loss_fusion: 8.477|total_loss: 50.372 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 2552 | tagging_loss_video: 6.679|tagging_loss_audio: 9.551|tagging_loss_text: 17.814|tagging_loss_image: 7.853|tagging_loss_fusion: 6.617|total_loss: 48.514 | 68.48 Examples/sec\n",
      "INFO:tensorflow:training step 2553 | tagging_loss_video: 8.285|tagging_loss_audio: 13.128|tagging_loss_text: 16.430|tagging_loss_image: 9.702|tagging_loss_fusion: 8.540|total_loss: 56.085 | 72.42 Examples/sec\n",
      "INFO:tensorflow:training step 2554 | tagging_loss_video: 7.707|tagging_loss_audio: 11.730|tagging_loss_text: 14.731|tagging_loss_image: 8.863|tagging_loss_fusion: 6.707|total_loss: 49.739 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 2555 | tagging_loss_video: 6.515|tagging_loss_audio: 13.103|tagging_loss_text: 14.355|tagging_loss_image: 8.489|tagging_loss_fusion: 7.128|total_loss: 49.589 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 2556 | tagging_loss_video: 6.635|tagging_loss_audio: 10.456|tagging_loss_text: 11.905|tagging_loss_image: 7.778|tagging_loss_fusion: 5.777|total_loss: 42.550 | 62.70 Examples/sec\n",
      "INFO:tensorflow:training step 2557 | tagging_loss_video: 7.435|tagging_loss_audio: 12.566|tagging_loss_text: 15.876|tagging_loss_image: 8.995|tagging_loss_fusion: 8.834|total_loss: 53.706 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 2558 | tagging_loss_video: 6.831|tagging_loss_audio: 10.735|tagging_loss_text: 18.776|tagging_loss_image: 8.531|tagging_loss_fusion: 7.783|total_loss: 52.655 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 2559 | tagging_loss_video: 6.660|tagging_loss_audio: 9.996|tagging_loss_text: 13.065|tagging_loss_image: 7.347|tagging_loss_fusion: 5.878|total_loss: 42.947 | 62.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2560 |tagging_loss_video: 6.747|tagging_loss_audio: 10.781|tagging_loss_text: 14.883|tagging_loss_image: 7.502|tagging_loss_fusion: 6.296|total_loss: 46.210 | Examples/sec: 68.02\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2561 | tagging_loss_video: 7.161|tagging_loss_audio: 10.302|tagging_loss_text: 14.578|tagging_loss_image: 9.342|tagging_loss_fusion: 6.492|total_loss: 47.874 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 2562 | tagging_loss_video: 6.837|tagging_loss_audio: 10.469|tagging_loss_text: 15.539|tagging_loss_image: 8.119|tagging_loss_fusion: 9.232|total_loss: 50.196 | 63.66 Examples/sec\n",
      "INFO:tensorflow:training step 2563 | tagging_loss_video: 7.069|tagging_loss_audio: 11.388|tagging_loss_text: 19.331|tagging_loss_image: 9.434|tagging_loss_fusion: 7.856|total_loss: 55.079 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 2564 | tagging_loss_video: 6.958|tagging_loss_audio: 12.581|tagging_loss_text: 16.858|tagging_loss_image: 8.270|tagging_loss_fusion: 7.349|total_loss: 52.016 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 2565 | tagging_loss_video: 7.199|tagging_loss_audio: 11.570|tagging_loss_text: 14.347|tagging_loss_image: 8.291|tagging_loss_fusion: 4.874|total_loss: 46.281 | 65.26 Examples/sec\n",
      "INFO:tensorflow:training step 2566 | tagging_loss_video: 7.637|tagging_loss_audio: 9.251|tagging_loss_text: 17.684|tagging_loss_image: 8.870|tagging_loss_fusion: 10.332|total_loss: 53.775 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 2567 | tagging_loss_video: 6.891|tagging_loss_audio: 11.583|tagging_loss_text: 15.652|tagging_loss_image: 7.634|tagging_loss_fusion: 6.546|total_loss: 48.306 | 62.73 Examples/sec\n",
      "INFO:tensorflow:training step 2568 | tagging_loss_video: 6.546|tagging_loss_audio: 12.059|tagging_loss_text: 13.895|tagging_loss_image: 8.624|tagging_loss_fusion: 7.101|total_loss: 48.225 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 2569 | tagging_loss_video: 6.065|tagging_loss_audio: 10.270|tagging_loss_text: 16.054|tagging_loss_image: 7.300|tagging_loss_fusion: 7.013|total_loss: 46.703 | 71.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2570 |tagging_loss_video: 7.095|tagging_loss_audio: 11.820|tagging_loss_text: 15.064|tagging_loss_image: 8.514|tagging_loss_fusion: 7.374|total_loss: 49.867 | Examples/sec: 59.70\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.74 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2571 | tagging_loss_video: 6.446|tagging_loss_audio: 10.614|tagging_loss_text: 14.239|tagging_loss_image: 7.751|tagging_loss_fusion: 7.017|total_loss: 46.067 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 2572 | tagging_loss_video: 6.783|tagging_loss_audio: 11.884|tagging_loss_text: 14.979|tagging_loss_image: 8.817|tagging_loss_fusion: 6.628|total_loss: 49.091 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 2573 | tagging_loss_video: 7.358|tagging_loss_audio: 11.201|tagging_loss_text: 16.271|tagging_loss_image: 8.431|tagging_loss_fusion: 8.271|total_loss: 51.532 | 62.06 Examples/sec\n",
      "INFO:tensorflow:training step 2574 | tagging_loss_video: 6.260|tagging_loss_audio: 11.499|tagging_loss_text: 14.665|tagging_loss_image: 8.323|tagging_loss_fusion: 6.237|total_loss: 46.985 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 2575 | tagging_loss_video: 5.463|tagging_loss_audio: 10.031|tagging_loss_text: 13.204|tagging_loss_image: 7.901|tagging_loss_fusion: 4.998|total_loss: 41.598 | 65.13 Examples/sec\n",
      "INFO:tensorflow:training step 2576 | tagging_loss_video: 8.109|tagging_loss_audio: 10.778|tagging_loss_text: 11.921|tagging_loss_image: 8.201|tagging_loss_fusion: 9.214|total_loss: 48.223 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 2577 | tagging_loss_video: 7.926|tagging_loss_audio: 12.038|tagging_loss_text: 17.914|tagging_loss_image: 8.189|tagging_loss_fusion: 6.896|total_loss: 52.962 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 2578 | tagging_loss_video: 6.456|tagging_loss_audio: 11.033|tagging_loss_text: 13.967|tagging_loss_image: 8.078|tagging_loss_fusion: 7.240|total_loss: 46.774 | 67.66 Examples/sec\n",
      "INFO:tensorflow:training step 2579 | tagging_loss_video: 7.299|tagging_loss_audio: 11.350|tagging_loss_text: 16.837|tagging_loss_image: 8.906|tagging_loss_fusion: 8.081|total_loss: 52.473 | 70.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2580 |tagging_loss_video: 8.415|tagging_loss_audio: 10.787|tagging_loss_text: 18.385|tagging_loss_image: 7.974|tagging_loss_fusion: 7.392|total_loss: 52.953 | Examples/sec: 70.52\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.76 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2581 | tagging_loss_video: 6.573|tagging_loss_audio: 10.168|tagging_loss_text: 14.113|tagging_loss_image: 8.843|tagging_loss_fusion: 6.721|total_loss: 46.417 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 2582 | tagging_loss_video: 7.024|tagging_loss_audio: 10.294|tagging_loss_text: 17.023|tagging_loss_image: 8.960|tagging_loss_fusion: 8.164|total_loss: 51.465 | 69.92 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2583 | tagging_loss_video: 7.473|tagging_loss_audio: 10.011|tagging_loss_text: 16.069|tagging_loss_image: 8.955|tagging_loss_fusion: 6.946|total_loss: 49.452 | 67.20 Examples/sec\n",
      "INFO:tensorflow:training step 2584 | tagging_loss_video: 7.810|tagging_loss_audio: 11.341|tagging_loss_text: 13.898|tagging_loss_image: 8.192|tagging_loss_fusion: 6.666|total_loss: 47.909 | 62.88 Examples/sec\n",
      "INFO:tensorflow:training step 2585 | tagging_loss_video: 7.763|tagging_loss_audio: 11.803|tagging_loss_text: 15.237|tagging_loss_image: 8.581|tagging_loss_fusion: 7.252|total_loss: 50.636 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 2586 | tagging_loss_video: 6.629|tagging_loss_audio: 12.860|tagging_loss_text: 15.550|tagging_loss_image: 9.140|tagging_loss_fusion: 6.347|total_loss: 50.526 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 2587 | tagging_loss_video: 7.710|tagging_loss_audio: 11.923|tagging_loss_text: 17.628|tagging_loss_image: 8.879|tagging_loss_fusion: 8.201|total_loss: 54.342 | 63.39 Examples/sec\n",
      "INFO:tensorflow:training step 2588 | tagging_loss_video: 6.656|tagging_loss_audio: 10.443|tagging_loss_text: 14.781|tagging_loss_image: 8.156|tagging_loss_fusion: 5.556|total_loss: 45.592 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 2589 | tagging_loss_video: 6.773|tagging_loss_audio: 11.094|tagging_loss_text: 18.627|tagging_loss_image: 8.617|tagging_loss_fusion: 6.684|total_loss: 51.795 | 71.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2590 |tagging_loss_video: 6.856|tagging_loss_audio: 10.487|tagging_loss_text: 14.177|tagging_loss_image: 8.228|tagging_loss_fusion: 6.329|total_loss: 46.077 | Examples/sec: 67.63\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.74 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2591 | tagging_loss_video: 7.064|tagging_loss_audio: 12.163|tagging_loss_text: 15.376|tagging_loss_image: 8.787|tagging_loss_fusion: 6.638|total_loss: 50.027 | 67.12 Examples/sec\n",
      "INFO:tensorflow:training step 2592 | tagging_loss_video: 6.568|tagging_loss_audio: 10.036|tagging_loss_text: 13.078|tagging_loss_image: 7.442|tagging_loss_fusion: 6.962|total_loss: 44.086 | 72.15 Examples/sec\n",
      "INFO:tensorflow:training step 2593 | tagging_loss_video: 7.059|tagging_loss_audio: 10.899|tagging_loss_text: 15.074|tagging_loss_image: 8.847|tagging_loss_fusion: 11.505|total_loss: 53.384 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 2594 | tagging_loss_video: 7.794|tagging_loss_audio: 11.022|tagging_loss_text: 13.377|tagging_loss_image: 8.938|tagging_loss_fusion: 7.366|total_loss: 48.497 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 2595 | tagging_loss_video: 7.694|tagging_loss_audio: 11.161|tagging_loss_text: 15.243|tagging_loss_image: 9.245|tagging_loss_fusion: 7.663|total_loss: 51.006 | 59.92 Examples/sec\n",
      "INFO:tensorflow:training step 2596 | tagging_loss_video: 7.953|tagging_loss_audio: 10.562|tagging_loss_text: 18.305|tagging_loss_image: 8.587|tagging_loss_fusion: 7.048|total_loss: 52.455 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 2597 | tagging_loss_video: 6.994|tagging_loss_audio: 13.214|tagging_loss_text: 19.050|tagging_loss_image: 9.892|tagging_loss_fusion: 12.311|total_loss: 61.462 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 2598 | tagging_loss_video: 7.174|tagging_loss_audio: 10.615|tagging_loss_text: 15.771|tagging_loss_image: 7.525|tagging_loss_fusion: 5.376|total_loss: 46.461 | 64.71 Examples/sec\n",
      "INFO:tensorflow:training step 2599 | tagging_loss_video: 6.936|tagging_loss_audio: 8.181|tagging_loss_text: 15.746|tagging_loss_image: 8.035|tagging_loss_fusion: 8.039|total_loss: 46.938 | 71.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2600 |tagging_loss_video: 7.597|tagging_loss_audio: 10.953|tagging_loss_text: 14.156|tagging_loss_image: 7.693|tagging_loss_fusion: 5.122|total_loss: 45.521 | Examples/sec: 70.95\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 2601 | tagging_loss_video: 6.583|tagging_loss_audio: 8.900|tagging_loss_text: 12.904|tagging_loss_image: 8.343|tagging_loss_fusion: 7.087|total_loss: 43.816 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 2602 | tagging_loss_video: 5.728|tagging_loss_audio: 11.150|tagging_loss_text: 15.546|tagging_loss_image: 8.689|tagging_loss_fusion: 4.942|total_loss: 46.054 | 66.25 Examples/sec\n",
      "INFO:tensorflow:training step 2603 | tagging_loss_video: 7.127|tagging_loss_audio: 11.969|tagging_loss_text: 15.127|tagging_loss_image: 8.311|tagging_loss_fusion: 6.507|total_loss: 49.041 | 72.14 Examples/sec\n",
      "INFO:tensorflow:training step 2604 | tagging_loss_video: 6.909|tagging_loss_audio: 11.265|tagging_loss_text: 14.655|tagging_loss_image: 8.913|tagging_loss_fusion: 5.848|total_loss: 47.589 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 2605 | tagging_loss_video: 6.034|tagging_loss_audio: 9.527|tagging_loss_text: 15.403|tagging_loss_image: 7.180|tagging_loss_fusion: 6.375|total_loss: 44.519 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 2606 | tagging_loss_video: 7.903|tagging_loss_audio: 10.548|tagging_loss_text: 21.462|tagging_loss_image: 10.639|tagging_loss_fusion: 9.386|total_loss: 59.938 | 61.50 Examples/sec\n",
      "INFO:tensorflow:training step 2607 | tagging_loss_video: 6.523|tagging_loss_audio: 8.897|tagging_loss_text: 15.792|tagging_loss_image: 7.734|tagging_loss_fusion: 5.582|total_loss: 44.527 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 2608 | tagging_loss_video: 5.332|tagging_loss_audio: 9.697|tagging_loss_text: 16.154|tagging_loss_image: 7.364|tagging_loss_fusion: 4.368|total_loss: 42.915 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 2609 | tagging_loss_video: 6.031|tagging_loss_audio: 9.801|tagging_loss_text: 14.881|tagging_loss_image: 6.916|tagging_loss_fusion: 5.138|total_loss: 42.766 | 62.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2610 |tagging_loss_video: 7.360|tagging_loss_audio: 11.255|tagging_loss_text: 13.942|tagging_loss_image: 8.359|tagging_loss_fusion: 6.201|total_loss: 47.117 | Examples/sec: 71.97\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 2611 | tagging_loss_video: 7.068|tagging_loss_audio: 13.540|tagging_loss_text: 17.536|tagging_loss_image: 8.877|tagging_loss_fusion: 6.758|total_loss: 53.779 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 2612 | tagging_loss_video: 6.297|tagging_loss_audio: 10.813|tagging_loss_text: 12.874|tagging_loss_image: 7.681|tagging_loss_fusion: 6.905|total_loss: 44.570 | 64.44 Examples/sec\n",
      "INFO:tensorflow:training step 2613 | tagging_loss_video: 6.122|tagging_loss_audio: 12.247|tagging_loss_text: 16.841|tagging_loss_image: 8.146|tagging_loss_fusion: 7.215|total_loss: 50.572 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 2614 | tagging_loss_video: 6.999|tagging_loss_audio: 10.781|tagging_loss_text: 15.901|tagging_loss_image: 8.118|tagging_loss_fusion: 5.388|total_loss: 47.186 | 67.42 Examples/sec\n",
      "INFO:tensorflow:training step 2615 | tagging_loss_video: 6.015|tagging_loss_audio: 10.563|tagging_loss_text: 13.717|tagging_loss_image: 7.304|tagging_loss_fusion: 6.269|total_loss: 43.868 | 65.44 Examples/sec\n",
      "INFO:tensorflow:training step 2616 | tagging_loss_video: 6.909|tagging_loss_audio: 10.959|tagging_loss_text: 11.139|tagging_loss_image: 7.610|tagging_loss_fusion: 6.391|total_loss: 43.009 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 2617 | tagging_loss_video: 7.754|tagging_loss_audio: 10.294|tagging_loss_text: 12.570|tagging_loss_image: 8.615|tagging_loss_fusion: 7.287|total_loss: 46.520 | 64.86 Examples/sec\n",
      "INFO:tensorflow:training step 2618 | tagging_loss_video: 6.911|tagging_loss_audio: 10.573|tagging_loss_text: 11.366|tagging_loss_image: 8.768|tagging_loss_fusion: 7.188|total_loss: 44.806 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 2619 | tagging_loss_video: 6.621|tagging_loss_audio: 10.382|tagging_loss_text: 12.768|tagging_loss_image: 7.702|tagging_loss_fusion: 6.207|total_loss: 43.680 | 67.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2620 |tagging_loss_video: 7.429|tagging_loss_audio: 11.594|tagging_loss_text: 13.933|tagging_loss_image: 7.495|tagging_loss_fusion: 7.315|total_loss: 47.767 | Examples/sec: 72.21\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2621 | tagging_loss_video: 7.317|tagging_loss_audio: 11.983|tagging_loss_text: 16.162|tagging_loss_image: 8.264|tagging_loss_fusion: 8.046|total_loss: 51.773 | 62.40 Examples/sec\n",
      "INFO:tensorflow:training step 2622 | tagging_loss_video: 7.175|tagging_loss_audio: 10.317|tagging_loss_text: 12.927|tagging_loss_image: 9.006|tagging_loss_fusion: 6.802|total_loss: 46.228 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 2623 | tagging_loss_video: 7.806|tagging_loss_audio: 11.684|tagging_loss_text: 12.878|tagging_loss_image: 9.583|tagging_loss_fusion: 8.569|total_loss: 50.520 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 2624 | tagging_loss_video: 6.980|tagging_loss_audio: 10.971|tagging_loss_text: 14.627|tagging_loss_image: 8.347|tagging_loss_fusion: 6.124|total_loss: 47.049 | 65.34 Examples/sec\n",
      "INFO:tensorflow:training step 2625 | tagging_loss_video: 5.345|tagging_loss_audio: 8.591|tagging_loss_text: 14.911|tagging_loss_image: 6.315|tagging_loss_fusion: 4.431|total_loss: 39.592 | 66.46 Examples/sec\n",
      "INFO:tensorflow:training step 2626 | tagging_loss_video: 7.060|tagging_loss_audio: 11.468|tagging_loss_text: 13.376|tagging_loss_image: 8.130|tagging_loss_fusion: 6.935|total_loss: 46.969 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 2627 | tagging_loss_video: 6.458|tagging_loss_audio: 10.488|tagging_loss_text: 12.906|tagging_loss_image: 7.337|tagging_loss_fusion: 7.571|total_loss: 44.760 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 2628 | tagging_loss_video: 6.053|tagging_loss_audio: 11.305|tagging_loss_text: 17.228|tagging_loss_image: 8.305|tagging_loss_fusion: 5.066|total_loss: 47.957 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 2629 | tagging_loss_video: 6.578|tagging_loss_audio: 9.981|tagging_loss_text: 14.588|tagging_loss_image: 8.223|tagging_loss_fusion: 6.907|total_loss: 46.277 | 65.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2630 |tagging_loss_video: 6.360|tagging_loss_audio: 8.826|tagging_loss_text: 14.160|tagging_loss_image: 7.466|tagging_loss_fusion: 5.505|total_loss: 42.316 | Examples/sec: 69.18\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.76 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2631 | tagging_loss_video: 7.955|tagging_loss_audio: 11.623|tagging_loss_text: 16.785|tagging_loss_image: 9.049|tagging_loss_fusion: 7.885|total_loss: 53.296 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 2632 | tagging_loss_video: 7.447|tagging_loss_audio: 12.102|tagging_loss_text: 16.316|tagging_loss_image: 8.697|tagging_loss_fusion: 9.029|total_loss: 53.591 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 2633 | tagging_loss_video: 7.209|tagging_loss_audio: 11.702|tagging_loss_text: 14.075|tagging_loss_image: 8.623|tagging_loss_fusion: 9.665|total_loss: 51.275 | 67.27 Examples/sec\n",
      "INFO:tensorflow:training step 2634 | tagging_loss_video: 8.318|tagging_loss_audio: 11.516|tagging_loss_text: 17.117|tagging_loss_image: 9.089|tagging_loss_fusion: 6.635|total_loss: 52.675 | 69.29 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 2634.\n",
      "INFO:tensorflow:training step 2635 | tagging_loss_video: 6.366|tagging_loss_audio: 10.717|tagging_loss_text: 15.309|tagging_loss_image: 6.969|tagging_loss_fusion: 6.505|total_loss: 45.867 | 47.25 Examples/sec\n",
      "INFO:tensorflow:training step 2636 | tagging_loss_video: 6.480|tagging_loss_audio: 9.070|tagging_loss_text: 15.005|tagging_loss_image: 6.676|tagging_loss_fusion: 7.091|total_loss: 44.322 | 60.55 Examples/sec\n",
      "INFO:tensorflow:training step 2637 | tagging_loss_video: 7.446|tagging_loss_audio: 10.522|tagging_loss_text: 13.000|tagging_loss_image: 7.329|tagging_loss_fusion: 5.745|total_loss: 44.042 | 63.37 Examples/sec\n",
      "INFO:tensorflow:training step 2638 | tagging_loss_video: 7.482|tagging_loss_audio: 11.777|tagging_loss_text: 17.882|tagging_loss_image: 8.351|tagging_loss_fusion: 8.044|total_loss: 53.537 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 2639 | tagging_loss_video: 7.213|tagging_loss_audio: 10.601|tagging_loss_text: 14.907|tagging_loss_image: 8.029|tagging_loss_fusion: 7.840|total_loss: 48.591 | 71.11 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2640 |tagging_loss_video: 6.832|tagging_loss_audio: 10.878|tagging_loss_text: 14.720|tagging_loss_image: 8.908|tagging_loss_fusion: 7.040|total_loss: 48.378 | Examples/sec: 63.78\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 2641 | tagging_loss_video: 7.237|tagging_loss_audio: 12.500|tagging_loss_text: 14.254|tagging_loss_image: 8.202|tagging_loss_fusion: 7.331|total_loss: 49.524 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 2642 | tagging_loss_video: 6.590|tagging_loss_audio: 12.508|tagging_loss_text: 18.515|tagging_loss_image: 9.268|tagging_loss_fusion: 7.075|total_loss: 53.956 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 2643 | tagging_loss_video: 7.068|tagging_loss_audio: 12.376|tagging_loss_text: 17.250|tagging_loss_image: 9.777|tagging_loss_fusion: 9.335|total_loss: 55.805 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 2644 | tagging_loss_video: 7.349|tagging_loss_audio: 12.217|tagging_loss_text: 15.339|tagging_loss_image: 9.234|tagging_loss_fusion: 7.990|total_loss: 52.130 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 2645 | tagging_loss_video: 7.690|tagging_loss_audio: 10.671|tagging_loss_text: 15.705|tagging_loss_image: 8.616|tagging_loss_fusion: 7.670|total_loss: 50.352 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 2646 | tagging_loss_video: 7.779|tagging_loss_audio: 12.254|tagging_loss_text: 17.323|tagging_loss_image: 9.073|tagging_loss_fusion: 10.229|total_loss: 56.659 | 72.60 Examples/sec\n",
      "INFO:tensorflow:training step 2647 | tagging_loss_video: 6.609|tagging_loss_audio: 9.078|tagging_loss_text: 12.580|tagging_loss_image: 8.238|tagging_loss_fusion: 6.483|total_loss: 42.988 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 2648 | tagging_loss_video: 6.888|tagging_loss_audio: 13.555|tagging_loss_text: 14.429|tagging_loss_image: 9.740|tagging_loss_fusion: 6.345|total_loss: 50.957 | 65.58 Examples/sec\n",
      "INFO:tensorflow:training step 2649 | tagging_loss_video: 7.573|tagging_loss_audio: 11.166|tagging_loss_text: 18.630|tagging_loss_image: 10.099|tagging_loss_fusion: 8.988|total_loss: 56.457 | 69.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2650 |tagging_loss_video: 7.044|tagging_loss_audio: 11.192|tagging_loss_text: 13.103|tagging_loss_image: 8.167|tagging_loss_fusion: 5.943|total_loss: 45.449 | Examples/sec: 70.64\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2651 | tagging_loss_video: 6.851|tagging_loss_audio: 11.580|tagging_loss_text: 16.531|tagging_loss_image: 7.925|tagging_loss_fusion: 7.566|total_loss: 50.453 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 2652 | tagging_loss_video: 7.815|tagging_loss_audio: 12.392|tagging_loss_text: 15.963|tagging_loss_image: 9.010|tagging_loss_fusion: 7.341|total_loss: 52.521 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 2653 | tagging_loss_video: 7.600|tagging_loss_audio: 11.144|tagging_loss_text: 15.210|tagging_loss_image: 9.077|tagging_loss_fusion: 7.201|total_loss: 50.233 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 2654 | tagging_loss_video: 7.638|tagging_loss_audio: 10.045|tagging_loss_text: 16.915|tagging_loss_image: 8.763|tagging_loss_fusion: 8.277|total_loss: 51.638 | 65.20 Examples/sec\n",
      "INFO:tensorflow:training step 2655 | tagging_loss_video: 7.757|tagging_loss_audio: 13.137|tagging_loss_text: 15.028|tagging_loss_image: 10.553|tagging_loss_fusion: 8.496|total_loss: 54.971 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 2656 | tagging_loss_video: 7.153|tagging_loss_audio: 12.632|tagging_loss_text: 12.556|tagging_loss_image: 8.217|tagging_loss_fusion: 7.402|total_loss: 47.960 | 63.75 Examples/sec\n",
      "INFO:tensorflow:training step 2657 | tagging_loss_video: 7.047|tagging_loss_audio: 11.949|tagging_loss_text: 14.791|tagging_loss_image: 8.000|tagging_loss_fusion: 7.061|total_loss: 48.848 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2658 | tagging_loss_video: 6.828|tagging_loss_audio: 11.194|tagging_loss_text: 14.895|tagging_loss_image: 7.813|tagging_loss_fusion: 7.944|total_loss: 48.674 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 2659 | tagging_loss_video: 7.068|tagging_loss_audio: 13.752|tagging_loss_text: 14.467|tagging_loss_image: 8.959|tagging_loss_fusion: 7.051|total_loss: 51.297 | 60.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2660 |tagging_loss_video: 8.541|tagging_loss_audio: 11.729|tagging_loss_text: 17.293|tagging_loss_image: 7.972|tagging_loss_fusion: 10.929|total_loss: 56.464 | Examples/sec: 71.64\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.93 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2661 | tagging_loss_video: 7.829|tagging_loss_audio: 11.991|tagging_loss_text: 14.545|tagging_loss_image: 8.585|tagging_loss_fusion: 9.119|total_loss: 52.068 | 68.39 Examples/sec\n",
      "INFO:tensorflow:training step 2662 | tagging_loss_video: 7.405|tagging_loss_audio: 12.123|tagging_loss_text: 14.012|tagging_loss_image: 9.425|tagging_loss_fusion: 9.081|total_loss: 52.046 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 2663 | tagging_loss_video: 8.909|tagging_loss_audio: 13.180|tagging_loss_text: 16.030|tagging_loss_image: 9.221|tagging_loss_fusion: 10.408|total_loss: 57.749 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 2664 | tagging_loss_video: 7.252|tagging_loss_audio: 10.016|tagging_loss_text: 19.402|tagging_loss_image: 8.226|tagging_loss_fusion: 6.066|total_loss: 50.963 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 2665 | tagging_loss_video: 7.210|tagging_loss_audio: 11.351|tagging_loss_text: 13.340|tagging_loss_image: 7.125|tagging_loss_fusion: 8.481|total_loss: 47.507 | 64.92 Examples/sec\n",
      "INFO:tensorflow:training step 2666 | tagging_loss_video: 6.638|tagging_loss_audio: 11.241|tagging_loss_text: 17.416|tagging_loss_image: 9.122|tagging_loss_fusion: 7.549|total_loss: 51.966 | 66.42 Examples/sec\n",
      "INFO:tensorflow:training step 2667 | tagging_loss_video: 6.317|tagging_loss_audio: 12.375|tagging_loss_text: 15.105|tagging_loss_image: 8.600|tagging_loss_fusion: 6.146|total_loss: 48.543 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 2668 | tagging_loss_video: 6.959|tagging_loss_audio: 11.481|tagging_loss_text: 17.342|tagging_loss_image: 7.907|tagging_loss_fusion: 6.427|total_loss: 50.116 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 2669 | tagging_loss_video: 8.243|tagging_loss_audio: 12.909|tagging_loss_text: 18.559|tagging_loss_image: 9.669|tagging_loss_fusion: 8.601|total_loss: 57.981 | 70.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2670 |tagging_loss_video: 8.667|tagging_loss_audio: 13.198|tagging_loss_text: 17.017|tagging_loss_image: 9.670|tagging_loss_fusion: 9.604|total_loss: 58.156 | Examples/sec: 59.72\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.69 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2671 | tagging_loss_video: 7.231|tagging_loss_audio: 8.710|tagging_loss_text: 13.184|tagging_loss_image: 8.069|tagging_loss_fusion: 7.162|total_loss: 44.357 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 2672 | tagging_loss_video: 7.301|tagging_loss_audio: 10.910|tagging_loss_text: 14.327|tagging_loss_image: 8.762|tagging_loss_fusion: 7.340|total_loss: 48.640 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 2673 | tagging_loss_video: 7.052|tagging_loss_audio: 10.312|tagging_loss_text: 14.255|tagging_loss_image: 7.540|tagging_loss_fusion: 7.918|total_loss: 47.077 | 65.83 Examples/sec\n",
      "INFO:tensorflow:training step 2674 | tagging_loss_video: 5.378|tagging_loss_audio: 12.410|tagging_loss_text: 18.313|tagging_loss_image: 8.815|tagging_loss_fusion: 5.706|total_loss: 50.622 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 2675 | tagging_loss_video: 7.957|tagging_loss_audio: 11.671|tagging_loss_text: 12.303|tagging_loss_image: 8.476|tagging_loss_fusion: 10.045|total_loss: 50.452 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 2676 | tagging_loss_video: 6.487|tagging_loss_audio: 11.304|tagging_loss_text: 17.868|tagging_loss_image: 8.184|tagging_loss_fusion: 7.661|total_loss: 51.503 | 64.75 Examples/sec\n",
      "INFO:tensorflow:training step 2677 | tagging_loss_video: 6.866|tagging_loss_audio: 9.743|tagging_loss_text: 15.342|tagging_loss_image: 7.971|tagging_loss_fusion: 7.998|total_loss: 47.919 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 2678 | tagging_loss_video: 7.259|tagging_loss_audio: 10.548|tagging_loss_text: 16.266|tagging_loss_image: 8.034|tagging_loss_fusion: 5.642|total_loss: 47.751 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 2679 | tagging_loss_video: 5.717|tagging_loss_audio: 10.181|tagging_loss_text: 15.192|tagging_loss_image: 7.638|tagging_loss_fusion: 5.139|total_loss: 43.866 | 61.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2680 |tagging_loss_video: 6.877|tagging_loss_audio: 8.874|tagging_loss_text: 8.814|tagging_loss_image: 7.220|tagging_loss_fusion: 7.481|total_loss: 39.266 | Examples/sec: 69.11\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.70 | precision@0.5: 0.89 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2681 | tagging_loss_video: 5.517|tagging_loss_audio: 10.169|tagging_loss_text: 16.807|tagging_loss_image: 7.708|tagging_loss_fusion: 3.977|total_loss: 44.177 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 2682 | tagging_loss_video: 8.101|tagging_loss_audio: 9.169|tagging_loss_text: 18.103|tagging_loss_image: 7.988|tagging_loss_fusion: 7.801|total_loss: 51.162 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 2683 | tagging_loss_video: 7.432|tagging_loss_audio: 11.127|tagging_loss_text: 15.865|tagging_loss_image: 7.910|tagging_loss_fusion: 7.453|total_loss: 49.787 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 2684 | tagging_loss_video: 6.943|tagging_loss_audio: 10.492|tagging_loss_text: 12.962|tagging_loss_image: 7.900|tagging_loss_fusion: 7.275|total_loss: 45.571 | 61.60 Examples/sec\n",
      "INFO:tensorflow:training step 2685 | tagging_loss_video: 6.595|tagging_loss_audio: 10.574|tagging_loss_text: 12.041|tagging_loss_image: 7.073|tagging_loss_fusion: 8.401|total_loss: 44.684 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 2686 | tagging_loss_video: 6.399|tagging_loss_audio: 10.068|tagging_loss_text: 13.300|tagging_loss_image: 8.471|tagging_loss_fusion: 7.978|total_loss: 46.216 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 2687 | tagging_loss_video: 7.225|tagging_loss_audio: 11.198|tagging_loss_text: 14.872|tagging_loss_image: 8.030|tagging_loss_fusion: 9.007|total_loss: 50.331 | 65.73 Examples/sec\n",
      "INFO:tensorflow:training step 2688 | tagging_loss_video: 8.209|tagging_loss_audio: 11.197|tagging_loss_text: 12.574|tagging_loss_image: 8.062|tagging_loss_fusion: 9.356|total_loss: 49.399 | 68.48 Examples/sec\n",
      "INFO:tensorflow:training step 2689 | tagging_loss_video: 7.641|tagging_loss_audio: 12.935|tagging_loss_text: 21.393|tagging_loss_image: 9.875|tagging_loss_fusion: 9.486|total_loss: 61.329 | 72.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2690 |tagging_loss_video: 7.274|tagging_loss_audio: 13.573|tagging_loss_text: 15.487|tagging_loss_image: 10.328|tagging_loss_fusion: 7.023|total_loss: 53.685 | Examples/sec: 60.47\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2691 | tagging_loss_video: 8.049|tagging_loss_audio: 14.014|tagging_loss_text: 14.456|tagging_loss_image: 8.340|tagging_loss_fusion: 7.960|total_loss: 52.818 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 2692 | tagging_loss_video: 8.596|tagging_loss_audio: 11.036|tagging_loss_text: 13.559|tagging_loss_image: 9.141|tagging_loss_fusion: 8.283|total_loss: 50.615 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 2693 | tagging_loss_video: 6.810|tagging_loss_audio: 9.424|tagging_loss_text: 15.380|tagging_loss_image: 7.648|tagging_loss_fusion: 6.023|total_loss: 45.285 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 2694 | tagging_loss_video: 6.956|tagging_loss_audio: 11.407|tagging_loss_text: 15.753|tagging_loss_image: 8.580|tagging_loss_fusion: 6.664|total_loss: 49.359 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 2695 | tagging_loss_video: 6.617|tagging_loss_audio: 9.983|tagging_loss_text: 13.885|tagging_loss_image: 8.121|tagging_loss_fusion: 6.335|total_loss: 44.940 | 65.14 Examples/sec\n",
      "INFO:tensorflow:training step 2696 | tagging_loss_video: 7.234|tagging_loss_audio: 10.341|tagging_loss_text: 17.247|tagging_loss_image: 7.898|tagging_loss_fusion: 6.695|total_loss: 49.416 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 2697 | tagging_loss_video: 7.112|tagging_loss_audio: 10.644|tagging_loss_text: 15.305|tagging_loss_image: 8.145|tagging_loss_fusion: 8.444|total_loss: 49.649 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 2698 | tagging_loss_video: 6.712|tagging_loss_audio: 9.631|tagging_loss_text: 18.052|tagging_loss_image: 7.100|tagging_loss_fusion: 6.329|total_loss: 47.824 | 60.07 Examples/sec\n",
      "INFO:tensorflow:training step 2699 | tagging_loss_video: 7.011|tagging_loss_audio: 11.507|tagging_loss_text: 14.251|tagging_loss_image: 8.502|tagging_loss_fusion: 6.192|total_loss: 47.462 | 70.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2700 |tagging_loss_video: 6.636|tagging_loss_audio: 11.544|tagging_loss_text: 17.336|tagging_loss_image: 8.229|tagging_loss_fusion: 6.486|total_loss: 50.232 | Examples/sec: 69.76\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2701 | tagging_loss_video: 5.946|tagging_loss_audio: 9.568|tagging_loss_text: 14.224|tagging_loss_image: 8.101|tagging_loss_fusion: 5.786|total_loss: 43.626 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 2702 | tagging_loss_video: 7.781|tagging_loss_audio: 13.237|tagging_loss_text: 17.192|tagging_loss_image: 8.123|tagging_loss_fusion: 8.186|total_loss: 54.520 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 2703 | tagging_loss_video: 6.789|tagging_loss_audio: 11.517|tagging_loss_text: 15.449|tagging_loss_image: 8.001|tagging_loss_fusion: 7.010|total_loss: 48.767 | 67.05 Examples/sec\n",
      "INFO:tensorflow:training step 2704 | tagging_loss_video: 6.214|tagging_loss_audio: 11.469|tagging_loss_text: 16.438|tagging_loss_image: 7.521|tagging_loss_fusion: 5.230|total_loss: 46.873 | 64.82 Examples/sec\n",
      "INFO:tensorflow:training step 2705 | tagging_loss_video: 7.272|tagging_loss_audio: 10.501|tagging_loss_text: 17.724|tagging_loss_image: 8.428|tagging_loss_fusion: 7.631|total_loss: 51.554 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 2706 | tagging_loss_video: 6.459|tagging_loss_audio: 10.860|tagging_loss_text: 12.889|tagging_loss_image: 8.085|tagging_loss_fusion: 6.325|total_loss: 44.617 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 2707 | tagging_loss_video: 6.252|tagging_loss_audio: 10.917|tagging_loss_text: 13.841|tagging_loss_image: 8.648|tagging_loss_fusion: 5.982|total_loss: 45.640 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 2708 | tagging_loss_video: 7.269|tagging_loss_audio: 10.379|tagging_loss_text: 13.336|tagging_loss_image: 7.198|tagging_loss_fusion: 6.203|total_loss: 44.385 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 2709 | tagging_loss_video: 6.804|tagging_loss_audio: 9.580|tagging_loss_text: 17.952|tagging_loss_image: 8.600|tagging_loss_fusion: 9.633|total_loss: 52.569 | 62.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2710 |tagging_loss_video: 6.393|tagging_loss_audio: 10.387|tagging_loss_text: 16.771|tagging_loss_image: 7.005|tagging_loss_fusion: 6.273|total_loss: 46.829 | Examples/sec: 68.37\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2711 | tagging_loss_video: 6.490|tagging_loss_audio: 10.052|tagging_loss_text: 16.377|tagging_loss_image: 7.624|tagging_loss_fusion: 5.529|total_loss: 46.072 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 2712 | tagging_loss_video: 6.964|tagging_loss_audio: 10.123|tagging_loss_text: 14.339|tagging_loss_image: 8.164|tagging_loss_fusion: 6.946|total_loss: 46.536 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 2713 | tagging_loss_video: 5.842|tagging_loss_audio: 12.376|tagging_loss_text: 18.058|tagging_loss_image: 7.955|tagging_loss_fusion: 5.821|total_loss: 50.052 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 2714 | tagging_loss_video: 6.478|tagging_loss_audio: 10.123|tagging_loss_text: 16.542|tagging_loss_image: 8.603|tagging_loss_fusion: 6.699|total_loss: 48.446 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 2715 | tagging_loss_video: 7.234|tagging_loss_audio: 11.181|tagging_loss_text: 12.531|tagging_loss_image: 8.535|tagging_loss_fusion: 8.310|total_loss: 47.791 | 65.62 Examples/sec\n",
      "INFO:tensorflow:training step 2716 | tagging_loss_video: 6.888|tagging_loss_audio: 11.881|tagging_loss_text: 20.833|tagging_loss_image: 8.531|tagging_loss_fusion: 6.675|total_loss: 54.808 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 2717 | tagging_loss_video: 7.213|tagging_loss_audio: 9.442|tagging_loss_text: 12.648|tagging_loss_image: 7.697|tagging_loss_fusion: 7.381|total_loss: 44.380 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 2718 | tagging_loss_video: 7.543|tagging_loss_audio: 11.753|tagging_loss_text: 14.774|tagging_loss_image: 7.871|tagging_loss_fusion: 9.422|total_loss: 51.364 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 2719 | tagging_loss_video: 7.583|tagging_loss_audio: 11.540|tagging_loss_text: 14.121|tagging_loss_image: 9.561|tagging_loss_fusion: 7.518|total_loss: 50.322 | 70.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2720 |tagging_loss_video: 7.105|tagging_loss_audio: 9.189|tagging_loss_text: 16.850|tagging_loss_image: 8.546|tagging_loss_fusion: 6.676|total_loss: 48.366 | Examples/sec: 61.47\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.75 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exitsINFO:tensorflow:training step 2721 | tagging_loss_video: 6.888|tagging_loss_audio: 10.366|tagging_loss_text: 13.443|tagging_loss_image: 8.745|tagging_loss_fusion: 8.208|total_loss: 47.649 | 68.75 Examples/sec\n",
      "\n",
      "INFO:tensorflow:training step 2722 | tagging_loss_video: 6.793|tagging_loss_audio: 11.581|tagging_loss_text: 14.006|tagging_loss_image: 8.952|tagging_loss_fusion: 7.306|total_loss: 48.638 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 2723 | tagging_loss_video: 7.063|tagging_loss_audio: 10.070|tagging_loss_text: 16.629|tagging_loss_image: 8.325|tagging_loss_fusion: 8.947|total_loss: 51.034 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 2724 | tagging_loss_video: 7.358|tagging_loss_audio: 9.782|tagging_loss_text: 15.255|tagging_loss_image: 7.966|tagging_loss_fusion: 8.863|total_loss: 49.224 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 2725 | tagging_loss_video: 6.104|tagging_loss_audio: 9.389|tagging_loss_text: 12.181|tagging_loss_image: 8.381|tagging_loss_fusion: 7.309|total_loss: 43.362 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 2726 | tagging_loss_video: 7.577|tagging_loss_audio: 13.041|tagging_loss_text: 16.184|tagging_loss_image: 8.788|tagging_loss_fusion: 6.457|total_loss: 52.047 | 47.66 Examples/sec\n",
      "INFO:tensorflow:training step 2727 | tagging_loss_video: 6.518|tagging_loss_audio: 10.279|tagging_loss_text: 12.305|tagging_loss_image: 6.711|tagging_loss_fusion: 5.556|total_loss: 41.368 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 2728 | tagging_loss_video: 6.789|tagging_loss_audio: 10.699|tagging_loss_text: 12.386|tagging_loss_image: 8.386|tagging_loss_fusion: 7.840|total_loss: 46.100 | 65.72 Examples/sec\n",
      "INFO:tensorflow:training step 2729 | tagging_loss_video: 6.776|tagging_loss_audio: 9.859|tagging_loss_text: 13.437|tagging_loss_image: 8.258|tagging_loss_fusion: 7.124|total_loss: 45.453 | 62.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2730 |tagging_loss_video: 6.280|tagging_loss_audio: 10.578|tagging_loss_text: 13.971|tagging_loss_image: 8.278|tagging_loss_fusion: 5.670|total_loss: 44.777 | Examples/sec: 69.15\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2731 | tagging_loss_video: 6.742|tagging_loss_audio: 10.231|tagging_loss_text: 13.697|tagging_loss_image: 7.489|tagging_loss_fusion: 6.499|total_loss: 44.657 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 2732 | tagging_loss_video: 7.290|tagging_loss_audio: 9.675|tagging_loss_text: 16.398|tagging_loss_image: 8.398|tagging_loss_fusion: 9.579|total_loss: 51.339 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 2733 | tagging_loss_video: 7.121|tagging_loss_audio: 11.182|tagging_loss_text: 17.875|tagging_loss_image: 8.884|tagging_loss_fusion: 6.723|total_loss: 51.784 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 2734 | tagging_loss_video: 7.196|tagging_loss_audio: 11.491|tagging_loss_text: 20.396|tagging_loss_image: 9.662|tagging_loss_fusion: 7.202|total_loss: 55.947 | 58.45 Examples/sec\n",
      "INFO:tensorflow:training step 2735 | tagging_loss_video: 7.217|tagging_loss_audio: 10.215|tagging_loss_text: 16.144|tagging_loss_image: 8.474|tagging_loss_fusion: 9.836|total_loss: 51.886 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 2736 | tagging_loss_video: 8.450|tagging_loss_audio: 11.139|tagging_loss_text: 19.044|tagging_loss_image: 8.575|tagging_loss_fusion: 8.672|total_loss: 55.880 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 2737 | tagging_loss_video: 5.560|tagging_loss_audio: 9.132|tagging_loss_text: 12.561|tagging_loss_image: 7.700|tagging_loss_fusion: 4.345|total_loss: 39.297 | 61.54 Examples/sec\n",
      "INFO:tensorflow:training step 2738 | tagging_loss_video: 5.995|tagging_loss_audio: 9.424|tagging_loss_text: 17.041|tagging_loss_image: 7.525|tagging_loss_fusion: 6.979|total_loss: 46.964 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 2739 | tagging_loss_video: 6.627|tagging_loss_audio: 12.956|tagging_loss_text: 16.836|tagging_loss_image: 7.896|tagging_loss_fusion: 7.642|total_loss: 51.957 | 70.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2740 |tagging_loss_video: 6.964|tagging_loss_audio: 10.331|tagging_loss_text: 13.487|tagging_loss_image: 7.940|tagging_loss_fusion: 6.123|total_loss: 44.846 | Examples/sec: 63.92\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.71 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 2741 | tagging_loss_video: 7.593|tagging_loss_audio: 10.952|tagging_loss_text: 14.781|tagging_loss_image: 8.316|tagging_loss_fusion: 6.895|total_loss: 48.537 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 2742 | tagging_loss_video: 7.345|tagging_loss_audio: 9.901|tagging_loss_text: 15.799|tagging_loss_image: 8.265|tagging_loss_fusion: 7.732|total_loss: 49.042 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 2743 | tagging_loss_video: 6.165|tagging_loss_audio: 10.129|tagging_loss_text: 13.994|tagging_loss_image: 8.425|tagging_loss_fusion: 7.047|total_loss: 45.759 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 2744 | tagging_loss_video: 6.891|tagging_loss_audio: 10.401|tagging_loss_text: 15.305|tagging_loss_image: 8.427|tagging_loss_fusion: 5.217|total_loss: 46.241 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 2745 | tagging_loss_video: 7.708|tagging_loss_audio: 11.618|tagging_loss_text: 16.246|tagging_loss_image: 9.138|tagging_loss_fusion: 6.513|total_loss: 51.223 | 64.83 Examples/sec\n",
      "INFO:tensorflow:training step 2746 | tagging_loss_video: 6.093|tagging_loss_audio: 9.396|tagging_loss_text: 15.106|tagging_loss_image: 7.272|tagging_loss_fusion: 5.833|total_loss: 43.699 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 2747 | tagging_loss_video: 6.104|tagging_loss_audio: 9.374|tagging_loss_text: 14.888|tagging_loss_image: 7.007|tagging_loss_fusion: 4.973|total_loss: 42.345 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 2748 | tagging_loss_video: 5.831|tagging_loss_audio: 9.783|tagging_loss_text: 16.031|tagging_loss_image: 6.878|tagging_loss_fusion: 5.837|total_loss: 44.360 | 61.28 Examples/sec\n",
      "INFO:tensorflow:training step 2749 | tagging_loss_video: 7.076|tagging_loss_audio: 10.585|tagging_loss_text: 15.324|tagging_loss_image: 7.836|tagging_loss_fusion: 8.283|total_loss: 49.104 | 72.37 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2750 |tagging_loss_video: 7.053|tagging_loss_audio: 11.468|tagging_loss_text: 16.163|tagging_loss_image: 8.127|tagging_loss_fusion: 7.115|total_loss: 49.926 | Examples/sec: 70.58\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2751 | tagging_loss_video: 6.939|tagging_loss_audio: 10.085|tagging_loss_text: 16.073|tagging_loss_image: 7.660|tagging_loss_fusion: 6.374|total_loss: 47.131 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 2752 | tagging_loss_video: 6.453|tagging_loss_audio: 12.375|tagging_loss_text: 18.202|tagging_loss_image: 8.506|tagging_loss_fusion: 5.671|total_loss: 51.208 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 2753 | tagging_loss_video: 5.344|tagging_loss_audio: 11.013|tagging_loss_text: 15.231|tagging_loss_image: 8.592|tagging_loss_fusion: 5.180|total_loss: 45.361 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 2754 | tagging_loss_video: 6.974|tagging_loss_audio: 10.499|tagging_loss_text: 15.612|tagging_loss_image: 7.870|tagging_loss_fusion: 7.547|total_loss: 48.502 | 63.12 Examples/sec\n",
      "INFO:tensorflow:training step 2755 | tagging_loss_video: 6.567|tagging_loss_audio: 10.575|tagging_loss_text: 13.690|tagging_loss_image: 7.112|tagging_loss_fusion: 6.950|total_loss: 44.894 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 2756 | tagging_loss_video: 7.238|tagging_loss_audio: 9.768|tagging_loss_text: 15.245|tagging_loss_image: 8.374|tagging_loss_fusion: 7.142|total_loss: 47.768 | 65.49 Examples/sec\n",
      "INFO:tensorflow:training step 2757 | tagging_loss_video: 6.452|tagging_loss_audio: 10.396|tagging_loss_text: 16.009|tagging_loss_image: 8.448|tagging_loss_fusion: 7.178|total_loss: 48.483 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 2758 | tagging_loss_video: 7.382|tagging_loss_audio: 10.371|tagging_loss_text: 14.121|tagging_loss_image: 7.592|tagging_loss_fusion: 6.518|total_loss: 45.984 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 2759 | tagging_loss_video: 5.566|tagging_loss_audio: 10.860|tagging_loss_text: 15.349|tagging_loss_image: 7.803|tagging_loss_fusion: 5.228|total_loss: 44.806 | 65.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2760 |tagging_loss_video: 7.308|tagging_loss_audio: 10.521|tagging_loss_text: 11.963|tagging_loss_image: 8.855|tagging_loss_fusion: 8.546|total_loss: 47.193 | Examples/sec: 61.02\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.73 | precision@0.5: 0.90 |recall@0.1: 0.94 | recall@0.5: 0.81\n",
      "INFO:tensorflow:training step 2761 | tagging_loss_video: 6.406|tagging_loss_audio: 11.632|tagging_loss_text: 16.952|tagging_loss_image: 7.915|tagging_loss_fusion: 5.434|total_loss: 48.340 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 2762 | tagging_loss_video: 7.790|tagging_loss_audio: 12.186|tagging_loss_text: 17.209|tagging_loss_image: 8.726|tagging_loss_fusion: 7.750|total_loss: 53.661 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 2763 | tagging_loss_video: 6.039|tagging_loss_audio: 10.337|tagging_loss_text: 12.081|tagging_loss_image: 7.353|tagging_loss_fusion: 6.552|total_loss: 42.362 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 2764 | tagging_loss_video: 5.804|tagging_loss_audio: 8.295|tagging_loss_text: 15.470|tagging_loss_image: 6.461|tagging_loss_fusion: 5.528|total_loss: 41.559 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 2765 | tagging_loss_video: 6.886|tagging_loss_audio: 10.392|tagging_loss_text: 15.998|tagging_loss_image: 7.665|tagging_loss_fusion: 6.781|total_loss: 47.722 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 2766 | tagging_loss_video: 6.556|tagging_loss_audio: 10.034|tagging_loss_text: 16.814|tagging_loss_image: 6.825|tagging_loss_fusion: 5.721|total_loss: 45.951 | 61.55 Examples/sec\n",
      "INFO:tensorflow:training step 2767 | tagging_loss_video: 7.155|tagging_loss_audio: 12.162|tagging_loss_text: 15.151|tagging_loss_image: 7.344|tagging_loss_fusion: 6.607|total_loss: 48.419 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 2768 | tagging_loss_video: 6.007|tagging_loss_audio: 11.152|tagging_loss_text: 13.226|tagging_loss_image: 7.341|tagging_loss_fusion: 4.880|total_loss: 42.606 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 2769 | tagging_loss_video: 6.778|tagging_loss_audio: 11.113|tagging_loss_text: 12.218|tagging_loss_image: 7.567|tagging_loss_fusion: 7.752|total_loss: 45.428 | 67.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2770 |tagging_loss_video: 6.496|tagging_loss_audio: 11.034|tagging_loss_text: 14.641|tagging_loss_image: 7.677|tagging_loss_fusion: 7.005|total_loss: 46.852 | Examples/sec: 69.31\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2771 | tagging_loss_video: 7.150|tagging_loss_audio: 11.697|tagging_loss_text: 12.927|tagging_loss_image: 8.047|tagging_loss_fusion: 8.522|total_loss: 48.342 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 2772 | tagging_loss_video: 6.745|tagging_loss_audio: 11.287|tagging_loss_text: 15.483|tagging_loss_image: 8.804|tagging_loss_fusion: 6.637|total_loss: 48.956 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 2773 | tagging_loss_video: 7.225|tagging_loss_audio: 10.841|tagging_loss_text: 16.379|tagging_loss_image: 8.974|tagging_loss_fusion: 8.204|total_loss: 51.624 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 2774 | tagging_loss_video: 7.358|tagging_loss_audio: 10.691|tagging_loss_text: 13.793|tagging_loss_image: 7.887|tagging_loss_fusion: 9.086|total_loss: 48.815 | 63.30 Examples/sec\n",
      "INFO:tensorflow:training step 2775 | tagging_loss_video: 6.254|tagging_loss_audio: 9.096|tagging_loss_text: 13.976|tagging_loss_image: 7.468|tagging_loss_fusion: 5.294|total_loss: 42.087 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 2776 | tagging_loss_video: 6.516|tagging_loss_audio: 10.656|tagging_loss_text: 11.832|tagging_loss_image: 7.234|tagging_loss_fusion: 7.728|total_loss: 43.965 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 2777 | tagging_loss_video: 6.217|tagging_loss_audio: 10.565|tagging_loss_text: 14.298|tagging_loss_image: 6.990|tagging_loss_fusion: 7.169|total_loss: 45.239 | 65.54 Examples/sec\n",
      "INFO:tensorflow:training step 2778 | tagging_loss_video: 6.817|tagging_loss_audio: 8.338|tagging_loss_text: 16.444|tagging_loss_image: 7.472|tagging_loss_fusion: 7.842|total_loss: 46.913 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 2779 | tagging_loss_video: 7.077|tagging_loss_audio: 10.262|tagging_loss_text: 14.916|tagging_loss_image: 7.796|tagging_loss_fusion: 6.343|total_loss: 46.393 | 71.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2780 |tagging_loss_video: 7.999|tagging_loss_audio: 11.319|tagging_loss_text: 13.311|tagging_loss_image: 8.129|tagging_loss_fusion: 7.396|total_loss: 48.155 | Examples/sec: 65.91\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.78 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2781 | tagging_loss_video: 6.032|tagging_loss_audio: 12.039|tagging_loss_text: 14.895|tagging_loss_image: 7.693|tagging_loss_fusion: 6.236|total_loss: 46.896 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 2782 | tagging_loss_video: 7.579|tagging_loss_audio: 11.733|tagging_loss_text: 15.044|tagging_loss_image: 8.547|tagging_loss_fusion: 8.289|total_loss: 51.192 | 67.68 Examples/sec\n",
      "INFO:tensorflow:training step 2783 | tagging_loss_video: 7.438|tagging_loss_audio: 12.833|tagging_loss_text: 15.103|tagging_loss_image: 9.335|tagging_loss_fusion: 7.867|total_loss: 52.576 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 2784 | tagging_loss_video: 7.406|tagging_loss_audio: 11.915|tagging_loss_text: 18.179|tagging_loss_image: 8.545|tagging_loss_fusion: 9.117|total_loss: 55.161 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 2785 | tagging_loss_video: 6.997|tagging_loss_audio: 11.457|tagging_loss_text: 15.046|tagging_loss_image: 7.437|tagging_loss_fusion: 7.337|total_loss: 48.273 | 61.37 Examples/sec\n",
      "INFO:tensorflow:training step 2786 | tagging_loss_video: 7.457|tagging_loss_audio: 11.423|tagging_loss_text: 17.311|tagging_loss_image: 8.912|tagging_loss_fusion: 5.810|total_loss: 50.914 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 2787 | tagging_loss_video: 6.836|tagging_loss_audio: 10.862|tagging_loss_text: 15.211|tagging_loss_image: 7.945|tagging_loss_fusion: 6.810|total_loss: 47.664 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 2788 | tagging_loss_video: 7.562|tagging_loss_audio: 11.154|tagging_loss_text: 15.805|tagging_loss_image: 8.616|tagging_loss_fusion: 11.131|total_loss: 54.269 | 64.49 Examples/sec\n",
      "INFO:tensorflow:training step 2789 | tagging_loss_video: 7.419|tagging_loss_audio: 14.402|tagging_loss_text: 15.599|tagging_loss_image: 9.562|tagging_loss_fusion: 9.341|total_loss: 56.324 | 69.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2790 |tagging_loss_video: 6.846|tagging_loss_audio: 10.589|tagging_loss_text: 17.539|tagging_loss_image: 7.860|tagging_loss_fusion: 6.490|total_loss: 49.324 | Examples/sec: 67.63\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2791 | tagging_loss_video: 6.750|tagging_loss_audio: 11.780|tagging_loss_text: 17.313|tagging_loss_image: 8.145|tagging_loss_fusion: 6.284|total_loss: 50.273 | 65.26 Examples/sec\n",
      "INFO:tensorflow:training step 2792 | tagging_loss_video: 7.490|tagging_loss_audio: 10.454|tagging_loss_text: 17.300|tagging_loss_image: 8.513|tagging_loss_fusion: 7.099|total_loss: 50.857 | 66.87 Examples/sec\n",
      "INFO:tensorflow:training step 2793 | tagging_loss_video: 7.038|tagging_loss_audio: 12.305|tagging_loss_text: 16.757|tagging_loss_image: 8.506|tagging_loss_fusion: 6.084|total_loss: 50.690 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 2794 | tagging_loss_video: 8.118|tagging_loss_audio: 11.234|tagging_loss_text: 16.188|tagging_loss_image: 8.624|tagging_loss_fusion: 8.206|total_loss: 52.369 | 66.03 Examples/sec\n",
      "INFO:tensorflow:training step 2795 | tagging_loss_video: 7.434|tagging_loss_audio: 11.229|tagging_loss_text: 16.518|tagging_loss_image: 9.342|tagging_loss_fusion: 6.406|total_loss: 50.929 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 2796 | tagging_loss_video: 6.898|tagging_loss_audio: 9.942|tagging_loss_text: 13.856|tagging_loss_image: 7.739|tagging_loss_fusion: 8.065|total_loss: 46.501 | 71.86 Examples/sec\n",
      "INFO:tensorflow:training step 2797 | tagging_loss_video: 6.624|tagging_loss_audio: 11.884|tagging_loss_text: 14.887|tagging_loss_image: 8.176|tagging_loss_fusion: 7.329|total_loss: 48.900 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 2798 | tagging_loss_video: 6.054|tagging_loss_audio: 9.963|tagging_loss_text: 15.480|tagging_loss_image: 8.046|tagging_loss_fusion: 5.556|total_loss: 45.099 | 66.83 Examples/sec\n",
      "INFO:tensorflow:training step 2799 | tagging_loss_video: 7.817|tagging_loss_audio: 12.270|tagging_loss_text: 16.729|tagging_loss_image: 8.697|tagging_loss_fusion: 8.278|total_loss: 53.791 | 65.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2800 |tagging_loss_video: 6.490|tagging_loss_audio: 12.946|tagging_loss_text: 14.458|tagging_loss_image: 9.290|tagging_loss_fusion: 5.401|total_loss: 48.585 | Examples/sec: 70.49\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2801 | tagging_loss_video: 7.470|tagging_loss_audio: 12.718|tagging_loss_text: 13.849|tagging_loss_image: 8.423|tagging_loss_fusion: 8.281|total_loss: 50.741 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 2802 | tagging_loss_video: 7.039|tagging_loss_audio: 11.562|tagging_loss_text: 14.686|tagging_loss_image: 8.184|tagging_loss_fusion: 8.240|total_loss: 49.711 | 62.24 Examples/sec\n",
      "INFO:tensorflow:training step 2803 | tagging_loss_video: 7.496|tagging_loss_audio: 10.372|tagging_loss_text: 17.591|tagging_loss_image: 9.739|tagging_loss_fusion: 6.306|total_loss: 51.505 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 2804 | tagging_loss_video: 7.290|tagging_loss_audio: 10.330|tagging_loss_text: 15.258|tagging_loss_image: 8.127|tagging_loss_fusion: 7.603|total_loss: 48.607 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 2805 | tagging_loss_video: 6.501|tagging_loss_audio: 8.476|tagging_loss_text: 14.059|tagging_loss_image: 8.026|tagging_loss_fusion: 6.681|total_loss: 43.744 | 65.11 Examples/sec\n",
      "INFO:tensorflow:training step 2806 | tagging_loss_video: 7.192|tagging_loss_audio: 11.443|tagging_loss_text: 12.082|tagging_loss_image: 8.970|tagging_loss_fusion: 7.317|total_loss: 47.004 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2807 | tagging_loss_video: 7.472|tagging_loss_audio: 10.882|tagging_loss_text: 13.724|tagging_loss_image: 7.879|tagging_loss_fusion: 9.258|total_loss: 49.215 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 2808 | tagging_loss_video: 7.498|tagging_loss_audio: 9.016|tagging_loss_text: 13.359|tagging_loss_image: 7.990|tagging_loss_fusion: 8.501|total_loss: 46.364 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 2809 | tagging_loss_video: 7.108|tagging_loss_audio: 13.939|tagging_loss_text: 15.593|tagging_loss_image: 9.343|tagging_loss_fusion: 8.138|total_loss: 54.120 | 68.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2810 |tagging_loss_video: 7.884|tagging_loss_audio: 9.982|tagging_loss_text: 19.310|tagging_loss_image: 8.576|tagging_loss_fusion: 7.121|total_loss: 52.873 | Examples/sec: 64.16\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.74 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2811 | tagging_loss_video: 6.368|tagging_loss_audio: 11.330|tagging_loss_text: 14.490|tagging_loss_image: 8.222|tagging_loss_fusion: 6.725|total_loss: 47.136 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 2812 | tagging_loss_video: 6.499|tagging_loss_audio: 10.496|tagging_loss_text: 17.815|tagging_loss_image: 7.742|tagging_loss_fusion: 5.977|total_loss: 48.529 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 2813 | tagging_loss_video: 5.584|tagging_loss_audio: 10.476|tagging_loss_text: 18.762|tagging_loss_image: 7.692|tagging_loss_fusion: 4.190|total_loss: 46.704 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 2814 | tagging_loss_video: 7.236|tagging_loss_audio: 13.077|tagging_loss_text: 14.614|tagging_loss_image: 8.918|tagging_loss_fusion: 11.145|total_loss: 54.990 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 2815 | tagging_loss_video: 6.495|tagging_loss_audio: 10.719|tagging_loss_text: 17.035|tagging_loss_image: 7.779|tagging_loss_fusion: 10.689|total_loss: 52.717 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 2816 | tagging_loss_video: 7.352|tagging_loss_audio: 10.646|tagging_loss_text: 15.195|tagging_loss_image: 7.204|tagging_loss_fusion: 8.525|total_loss: 48.922 | 62.82 Examples/sec\n",
      "INFO:tensorflow:training step 2817 | tagging_loss_video: 6.878|tagging_loss_audio: 12.132|tagging_loss_text: 14.626|tagging_loss_image: 8.510|tagging_loss_fusion: 7.095|total_loss: 49.241 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 2818 | tagging_loss_video: 7.025|tagging_loss_audio: 12.764|tagging_loss_text: 16.926|tagging_loss_image: 8.178|tagging_loss_fusion: 5.982|total_loss: 50.874 | 66.49 Examples/sec\n",
      "INFO:tensorflow:training step 2819 | tagging_loss_video: 6.733|tagging_loss_audio: 9.936|tagging_loss_text: 17.204|tagging_loss_image: 7.942|tagging_loss_fusion: 6.050|total_loss: 47.867 | 64.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2820 |tagging_loss_video: 5.865|tagging_loss_audio: 10.614|tagging_loss_text: 11.957|tagging_loss_image: 6.921|tagging_loss_fusion: 5.048|total_loss: 40.406 | Examples/sec: 68.74\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2821 | tagging_loss_video: 7.176|tagging_loss_audio: 11.309|tagging_loss_text: 14.077|tagging_loss_image: 7.820|tagging_loss_fusion: 7.899|total_loss: 48.282 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2822 | tagging_loss_video: 6.880|tagging_loss_audio: 10.429|tagging_loss_text: 12.723|tagging_loss_image: 8.040|tagging_loss_fusion: 6.290|total_loss: 44.362 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 2823 | tagging_loss_video: 6.857|tagging_loss_audio: 10.185|tagging_loss_text: 15.286|tagging_loss_image: 7.651|tagging_loss_fusion: 6.587|total_loss: 46.566 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2824 | tagging_loss_video: 6.423|tagging_loss_audio: 10.726|tagging_loss_text: 15.486|tagging_loss_image: 7.747|tagging_loss_fusion: 6.062|total_loss: 46.445 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2825 | tagging_loss_video: 6.857|tagging_loss_audio: 10.354|tagging_loss_text: 18.038|tagging_loss_image: 7.722|tagging_loss_fusion: 7.160|total_loss: 50.131 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 2826 | tagging_loss_video: 7.125|tagging_loss_audio: 9.828|tagging_loss_text: 11.289|tagging_loss_image: 7.502|tagging_loss_fusion: 6.215|total_loss: 41.959 | 66.81 Examples/sec\n",
      "INFO:tensorflow:training step 2827 | tagging_loss_video: 6.546|tagging_loss_audio: 11.333|tagging_loss_text: 13.551|tagging_loss_image: 9.129|tagging_loss_fusion: 5.555|total_loss: 46.115 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 2828 | tagging_loss_video: 7.476|tagging_loss_audio: 11.339|tagging_loss_text: 12.538|tagging_loss_image: 8.185|tagging_loss_fusion: 10.785|total_loss: 50.323 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2829 | tagging_loss_video: 7.084|tagging_loss_audio: 11.819|tagging_loss_text: 16.125|tagging_loss_image: 9.386|tagging_loss_fusion: 6.472|total_loss: 50.886 | 63.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2830 |tagging_loss_video: 7.977|tagging_loss_audio: 11.496|tagging_loss_text: 12.128|tagging_loss_image: 9.124|tagging_loss_fusion: 7.324|total_loss: 48.049 | Examples/sec: 64.62\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2831 | tagging_loss_video: 6.101|tagging_loss_audio: 11.555|tagging_loss_text: 15.786|tagging_loss_image: 8.319|tagging_loss_fusion: 5.391|total_loss: 47.152 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 2832 | tagging_loss_video: 7.675|tagging_loss_audio: 12.643|tagging_loss_text: 13.573|tagging_loss_image: 9.286|tagging_loss_fusion: 8.938|total_loss: 52.115 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 2833 | tagging_loss_video: 6.485|tagging_loss_audio: 9.466|tagging_loss_text: 13.431|tagging_loss_image: 7.419|tagging_loss_fusion: 5.682|total_loss: 42.483 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 2834 | tagging_loss_video: 7.218|tagging_loss_audio: 11.725|tagging_loss_text: 15.856|tagging_loss_image: 7.865|tagging_loss_fusion: 7.684|total_loss: 50.347 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 2835 | tagging_loss_video: 6.443|tagging_loss_audio: 10.888|tagging_loss_text: 16.498|tagging_loss_image: 7.563|tagging_loss_fusion: 6.697|total_loss: 48.090 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 2836 | tagging_loss_video: 6.808|tagging_loss_audio: 10.612|tagging_loss_text: 15.877|tagging_loss_image: 8.202|tagging_loss_fusion: 8.203|total_loss: 49.702 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 2837 | tagging_loss_video: 6.870|tagging_loss_audio: 11.870|tagging_loss_text: 14.601|tagging_loss_image: 7.839|tagging_loss_fusion: 5.808|total_loss: 46.987 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 2838 | tagging_loss_video: 6.524|tagging_loss_audio: 10.136|tagging_loss_text: 12.439|tagging_loss_image: 6.894|tagging_loss_fusion: 8.441|total_loss: 44.435 | 63.22 Examples/sec\n",
      "INFO:tensorflow:training step 2839 | tagging_loss_video: 6.959|tagging_loss_audio: 8.491|tagging_loss_text: 16.958|tagging_loss_image: 7.207|tagging_loss_fusion: 6.118|total_loss: 45.732 | 69.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2840 |tagging_loss_video: 6.814|tagging_loss_audio: 11.668|tagging_loss_text: 14.538|tagging_loss_image: 7.775|tagging_loss_fusion: 5.698|total_loss: 46.493 | Examples/sec: 70.92\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2841 | tagging_loss_video: 6.479|tagging_loss_audio: 10.354|tagging_loss_text: 16.899|tagging_loss_image: 7.951|tagging_loss_fusion: 5.772|total_loss: 47.455 | 66.72 Examples/sec\n",
      "INFO:tensorflow:training step 2842 | tagging_loss_video: 7.258|tagging_loss_audio: 12.607|tagging_loss_text: 17.157|tagging_loss_image: 8.558|tagging_loss_fusion: 6.052|total_loss: 51.631 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 2843 | tagging_loss_video: 6.515|tagging_loss_audio: 12.000|tagging_loss_text: 14.781|tagging_loss_image: 7.407|tagging_loss_fusion: 5.715|total_loss: 46.419 | 65.67 Examples/sec\n",
      "INFO:tensorflow:training step 2844 | tagging_loss_video: 6.682|tagging_loss_audio: 10.909|tagging_loss_text: 12.316|tagging_loss_image: 8.459|tagging_loss_fusion: 6.020|total_loss: 44.386 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 2845 | tagging_loss_video: 8.956|tagging_loss_audio: 10.842|tagging_loss_text: 19.160|tagging_loss_image: 8.389|tagging_loss_fusion: 8.915|total_loss: 56.262 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 2846 | tagging_loss_video: 6.945|tagging_loss_audio: 11.953|tagging_loss_text: 15.632|tagging_loss_image: 7.397|tagging_loss_fusion: 7.495|total_loss: 49.422 | 72.08 Examples/sec\n",
      "INFO:tensorflow:training step 2847 | tagging_loss_video: 6.294|tagging_loss_audio: 10.172|tagging_loss_text: 16.643|tagging_loss_image: 8.179|tagging_loss_fusion: 7.087|total_loss: 48.375 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 2848 | tagging_loss_video: 6.555|tagging_loss_audio: 9.215|tagging_loss_text: 14.044|tagging_loss_image: 7.107|tagging_loss_fusion: 7.852|total_loss: 44.773 | 72.28 Examples/sec\n",
      "INFO:tensorflow:training step 2849 | tagging_loss_video: 5.295|tagging_loss_audio: 10.877|tagging_loss_text: 18.538|tagging_loss_image: 8.058|tagging_loss_fusion: 4.224|total_loss: 46.992 | 59.33 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2850 |tagging_loss_video: 5.951|tagging_loss_audio: 11.432|tagging_loss_text: 14.805|tagging_loss_image: 7.424|tagging_loss_fusion: 5.792|total_loss: 45.404 | Examples/sec: 71.05\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2851 | tagging_loss_video: 6.218|tagging_loss_audio: 9.902|tagging_loss_text: 12.327|tagging_loss_image: 7.223|tagging_loss_fusion: 4.412|total_loss: 40.081 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 2852 | tagging_loss_video: 7.032|tagging_loss_audio: 9.751|tagging_loss_text: 15.055|tagging_loss_image: 7.386|tagging_loss_fusion: 7.580|total_loss: 46.804 | 64.50 Examples/sec\n",
      "INFO:tensorflow:training step 2853 | tagging_loss_video: 7.246|tagging_loss_audio: 9.351|tagging_loss_text: 14.615|tagging_loss_image: 7.692|tagging_loss_fusion: 7.890|total_loss: 46.794 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 2854 | tagging_loss_video: 5.761|tagging_loss_audio: 11.427|tagging_loss_text: 16.544|tagging_loss_image: 7.988|tagging_loss_fusion: 5.095|total_loss: 46.814 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 2855 | tagging_loss_video: 6.148|tagging_loss_audio: 10.258|tagging_loss_text: 13.998|tagging_loss_image: 8.624|tagging_loss_fusion: 5.095|total_loss: 44.123 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 2856 | tagging_loss_video: 7.390|tagging_loss_audio: 13.595|tagging_loss_text: 17.070|tagging_loss_image: 8.750|tagging_loss_fusion: 6.580|total_loss: 53.385 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 2857 | tagging_loss_video: 6.682|tagging_loss_audio: 10.404|tagging_loss_text: 14.671|tagging_loss_image: 8.138|tagging_loss_fusion: 5.674|total_loss: 45.569 | 63.32 Examples/sec\n",
      "INFO:tensorflow:training step 2858 | tagging_loss_video: 6.915|tagging_loss_audio: 10.121|tagging_loss_text: 16.154|tagging_loss_image: 9.023|tagging_loss_fusion: 6.813|total_loss: 49.027 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 2859 | tagging_loss_video: 8.782|tagging_loss_audio: 10.397|tagging_loss_text: 14.161|tagging_loss_image: 8.574|tagging_loss_fusion: 9.267|total_loss: 51.180 | 68.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2860 |tagging_loss_video: 7.020|tagging_loss_audio: 9.041|tagging_loss_text: 15.524|tagging_loss_image: 8.277|tagging_loss_fusion: 6.651|total_loss: 46.514 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.75 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 2861 | tagging_loss_video: 7.823|tagging_loss_audio: 10.022|tagging_loss_text: 16.262|tagging_loss_image: 8.444|tagging_loss_fusion: 7.506|total_loss: 50.057 | 67.27 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 2862 | tagging_loss_video: 7.237|tagging_loss_audio: 10.873|tagging_loss_text: 16.778|tagging_loss_image: 8.284|tagging_loss_fusion: 7.334|total_loss: 50.506 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 2863 | tagging_loss_video: 8.121|tagging_loss_audio: 10.882|tagging_loss_text: 18.013|tagging_loss_image: 7.883|tagging_loss_fusion: 7.051|total_loss: 51.949 | 61.97 Examples/sec\n",
      "INFO:tensorflow:training step 2864 | tagging_loss_video: 6.589|tagging_loss_audio: 9.594|tagging_loss_text: 15.311|tagging_loss_image: 7.641|tagging_loss_fusion: 7.596|total_loss: 46.730 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2865 | tagging_loss_video: 6.750|tagging_loss_audio: 10.064|tagging_loss_text: 13.535|tagging_loss_image: 8.271|tagging_loss_fusion: 6.191|total_loss: 44.812 | 70.68 Examples/sec\n",
      "INFO:tensorflow:training step 2866 | tagging_loss_video: 7.498|tagging_loss_audio: 11.884|tagging_loss_text: 13.892|tagging_loss_image: 8.969|tagging_loss_fusion: 8.657|total_loss: 50.900 | 63.23 Examples/sec\n",
      "INFO:tensorflow:training step 2867 | tagging_loss_video: 7.656|tagging_loss_audio: 9.353|tagging_loss_text: 14.904|tagging_loss_image: 6.627|tagging_loss_fusion: 6.181|total_loss: 44.721 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 2868 | tagging_loss_video: 7.060|tagging_loss_audio: 11.061|tagging_loss_text: 13.087|tagging_loss_image: 7.865|tagging_loss_fusion: 8.545|total_loss: 47.619 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 2869 | tagging_loss_video: 7.374|tagging_loss_audio: 10.118|tagging_loss_text: 12.788|tagging_loss_image: 7.828|tagging_loss_fusion: 8.404|total_loss: 46.512 | 61.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2870 |tagging_loss_video: 7.396|tagging_loss_audio: 9.807|tagging_loss_text: 16.776|tagging_loss_image: 8.244|tagging_loss_fusion: 7.416|total_loss: 49.639 | Examples/sec: 71.54\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.74 | precision@0.5: 0.88 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2871 | tagging_loss_video: 5.901|tagging_loss_audio: 10.060|tagging_loss_text: 13.075|tagging_loss_image: 7.130|tagging_loss_fusion: 5.683|total_loss: 41.849 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 2872 | tagging_loss_video: 7.310|tagging_loss_audio: 11.234|tagging_loss_text: 15.024|tagging_loss_image: 9.329|tagging_loss_fusion: 7.163|total_loss: 50.060 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 2873 | tagging_loss_video: 7.358|tagging_loss_audio: 11.312|tagging_loss_text: 18.572|tagging_loss_image: 9.201|tagging_loss_fusion: 9.319|total_loss: 55.762 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 2874 | tagging_loss_video: 7.871|tagging_loss_audio: 12.708|tagging_loss_text: 13.159|tagging_loss_image: 9.186|tagging_loss_fusion: 7.910|total_loss: 50.833 | 65.31 Examples/sec\n",
      "INFO:tensorflow:training step 2875 | tagging_loss_video: 6.604|tagging_loss_audio: 11.319|tagging_loss_text: 14.076|tagging_loss_image: 7.343|tagging_loss_fusion: 6.227|total_loss: 45.570 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 2876 | tagging_loss_video: 8.135|tagging_loss_audio: 11.620|tagging_loss_text: 15.239|tagging_loss_image: 9.696|tagging_loss_fusion: 6.045|total_loss: 50.736 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 2877 | tagging_loss_video: 6.208|tagging_loss_audio: 9.313|tagging_loss_text: 13.845|tagging_loss_image: 7.184|tagging_loss_fusion: 5.838|total_loss: 42.387 | 63.01 Examples/sec\n",
      "INFO:tensorflow:training step 2878 | tagging_loss_video: 6.640|tagging_loss_audio: 10.685|tagging_loss_text: 15.763|tagging_loss_image: 7.359|tagging_loss_fusion: 4.677|total_loss: 45.124 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 2879 | tagging_loss_video: 6.091|tagging_loss_audio: 10.845|tagging_loss_text: 12.785|tagging_loss_image: 8.229|tagging_loss_fusion: 5.953|total_loss: 43.902 | 68.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2880 |tagging_loss_video: 5.866|tagging_loss_audio: 10.979|tagging_loss_text: 13.459|tagging_loss_image: 6.720|tagging_loss_fusion: 5.375|total_loss: 42.398 | Examples/sec: 71.23\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.75 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2881 | tagging_loss_video: 7.450|tagging_loss_audio: 10.547|tagging_loss_text: 16.166|tagging_loss_image: 8.015|tagging_loss_fusion: 7.809|total_loss: 49.986 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 2882 | tagging_loss_video: 7.052|tagging_loss_audio: 11.507|tagging_loss_text: 19.141|tagging_loss_image: 8.189|tagging_loss_fusion: 7.138|total_loss: 53.028 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 2883 | tagging_loss_video: 6.392|tagging_loss_audio: 10.526|tagging_loss_text: 16.592|tagging_loss_image: 7.612|tagging_loss_fusion: 5.262|total_loss: 46.385 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 2884 | tagging_loss_video: 5.692|tagging_loss_audio: 10.857|tagging_loss_text: 15.335|tagging_loss_image: 6.741|tagging_loss_fusion: 5.102|total_loss: 43.727 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 2885 | tagging_loss_video: 7.829|tagging_loss_audio: 10.687|tagging_loss_text: 15.503|tagging_loss_image: 8.905|tagging_loss_fusion: 7.456|total_loss: 50.380 | 62.59 Examples/sec\n",
      "INFO:tensorflow:training step 2886 | tagging_loss_video: 5.645|tagging_loss_audio: 10.239|tagging_loss_text: 13.062|tagging_loss_image: 6.706|tagging_loss_fusion: 6.786|total_loss: 42.437 | 55.82 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 2886.\n",
      "INFO:tensorflow:training step 2887 | tagging_loss_video: 5.828|tagging_loss_audio: 8.866|tagging_loss_text: 15.416|tagging_loss_image: 7.584|tagging_loss_fusion: 5.681|total_loss: 43.376 | 50.65 Examples/sec\n",
      "INFO:tensorflow:training step 2888 | tagging_loss_video: 7.348|tagging_loss_audio: 11.293|tagging_loss_text: 14.582|tagging_loss_image: 8.186|tagging_loss_fusion: 7.175|total_loss: 48.584 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 2889 | tagging_loss_video: 6.351|tagging_loss_audio: 10.150|tagging_loss_text: 15.509|tagging_loss_image: 7.577|tagging_loss_fusion: 7.463|total_loss: 47.049 | 66.54 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2890 |tagging_loss_video: 5.685|tagging_loss_audio: 9.439|tagging_loss_text: 14.855|tagging_loss_image: 7.216|tagging_loss_fusion: 5.299|total_loss: 42.494 | Examples/sec: 63.34\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.99 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2891 | tagging_loss_video: 6.564|tagging_loss_audio: 11.800|tagging_loss_text: 14.123|tagging_loss_image: 8.339|tagging_loss_fusion: 5.492|total_loss: 46.319 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2892 | tagging_loss_video: 7.289|tagging_loss_audio: 10.935|tagging_loss_text: 15.869|tagging_loss_image: 8.408|tagging_loss_fusion: 8.354|total_loss: 50.856 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 2893 | tagging_loss_video: 5.831|tagging_loss_audio: 11.111|tagging_loss_text: 14.346|tagging_loss_image: 6.992|tagging_loss_fusion: 5.173|total_loss: 43.454 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 2894 | tagging_loss_video: 6.897|tagging_loss_audio: 11.523|tagging_loss_text: 15.025|tagging_loss_image: 6.562|tagging_loss_fusion: 8.282|total_loss: 48.289 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 2895 | tagging_loss_video: 6.644|tagging_loss_audio: 9.333|tagging_loss_text: 14.616|tagging_loss_image: 8.155|tagging_loss_fusion: 7.638|total_loss: 46.386 | 66.38 Examples/sec\n",
      "INFO:tensorflow:training step 2896 | tagging_loss_video: 6.394|tagging_loss_audio: 11.492|tagging_loss_text: 15.995|tagging_loss_image: 8.502|tagging_loss_fusion: 6.663|total_loss: 49.045 | 72.20 Examples/sec\n",
      "INFO:tensorflow:training step 2897 | tagging_loss_video: 5.950|tagging_loss_audio: 10.289|tagging_loss_text: 13.406|tagging_loss_image: 7.789|tagging_loss_fusion: 6.635|total_loss: 44.069 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 2898 | tagging_loss_video: 7.171|tagging_loss_audio: 9.805|tagging_loss_text: 17.440|tagging_loss_image: 8.263|tagging_loss_fusion: 6.714|total_loss: 49.394 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 2899 | tagging_loss_video: 7.358|tagging_loss_audio: 11.198|tagging_loss_text: 15.482|tagging_loss_image: 8.576|tagging_loss_fusion: 6.736|total_loss: 49.349 | 62.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2900 |tagging_loss_video: 6.458|tagging_loss_audio: 9.275|tagging_loss_text: 14.264|tagging_loss_image: 8.024|tagging_loss_fusion: 6.114|total_loss: 44.135 | Examples/sec: 69.37\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.77 | precision@0.5: 0.94 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 2901 | tagging_loss_video: 8.006|tagging_loss_audio: 9.888|tagging_loss_text: 18.548|tagging_loss_image: 8.189|tagging_loss_fusion: 7.181|total_loss: 51.813 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 2902 | tagging_loss_video: 6.709|tagging_loss_audio: 9.582|tagging_loss_text: 14.612|tagging_loss_image: 6.573|tagging_loss_fusion: 6.208|total_loss: 43.685 | 60.76 Examples/sec\n",
      "INFO:tensorflow:training step 2903 | tagging_loss_video: 4.910|tagging_loss_audio: 9.311|tagging_loss_text: 12.506|tagging_loss_image: 6.415|tagging_loss_fusion: 3.894|total_loss: 37.036 | 71.63 Examples/sec\n",
      "INFO:tensorflow:training step 2904 | tagging_loss_video: 7.036|tagging_loss_audio: 10.396|tagging_loss_text: 15.285|tagging_loss_image: 7.205|tagging_loss_fusion: 6.055|total_loss: 45.976 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 2905 | tagging_loss_video: 5.468|tagging_loss_audio: 9.791|tagging_loss_text: 15.462|tagging_loss_image: 6.777|tagging_loss_fusion: 5.117|total_loss: 42.616 | 63.97 Examples/sec\n",
      "INFO:tensorflow:training step 2906 | tagging_loss_video: 6.803|tagging_loss_audio: 11.136|tagging_loss_text: 19.258|tagging_loss_image: 7.896|tagging_loss_fusion: 7.028|total_loss: 52.119 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 2907 | tagging_loss_video: 5.961|tagging_loss_audio: 10.684|tagging_loss_text: 13.247|tagging_loss_image: 6.987|tagging_loss_fusion: 5.341|total_loss: 42.220 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 2908 | tagging_loss_video: 5.859|tagging_loss_audio: 9.933|tagging_loss_text: 12.883|tagging_loss_image: 7.792|tagging_loss_fusion: 5.933|total_loss: 42.399 | 65.35 Examples/sec\n",
      "INFO:tensorflow:training step 2909 | tagging_loss_video: 5.953|tagging_loss_audio: 12.160|tagging_loss_text: 19.474|tagging_loss_image: 8.193|tagging_loss_fusion: 6.084|total_loss: 51.863 | 69.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2910 |tagging_loss_video: 6.062|tagging_loss_audio: 10.235|tagging_loss_text: 12.939|tagging_loss_image: 8.032|tagging_loss_fusion: 5.924|total_loss: 43.191 | Examples/sec: 67.33\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2911 | tagging_loss_video: 6.470|tagging_loss_audio: 11.655|tagging_loss_text: 17.532|tagging_loss_image: 8.255|tagging_loss_fusion: 8.571|total_loss: 52.483 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 2912 | tagging_loss_video: 6.452|tagging_loss_audio: 11.732|tagging_loss_text: 19.374|tagging_loss_image: 8.166|tagging_loss_fusion: 4.475|total_loss: 50.199 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 2913 | tagging_loss_video: 7.266|tagging_loss_audio: 9.028|tagging_loss_text: 15.190|tagging_loss_image: 7.856|tagging_loss_fusion: 6.666|total_loss: 46.005 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 2914 | tagging_loss_video: 6.290|tagging_loss_audio: 9.852|tagging_loss_text: 13.939|tagging_loss_image: 7.159|tagging_loss_fusion: 6.904|total_loss: 44.144 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 2915 | tagging_loss_video: 6.503|tagging_loss_audio: 9.647|tagging_loss_text: 9.982|tagging_loss_image: 6.702|tagging_loss_fusion: 5.519|total_loss: 38.354 | 67.72 Examples/sec\n",
      "INFO:tensorflow:training step 2916 | tagging_loss_video: 6.035|tagging_loss_audio: 11.446|tagging_loss_text: 13.679|tagging_loss_image: 6.326|tagging_loss_fusion: 5.718|total_loss: 43.203 | 64.94 Examples/sec\n",
      "INFO:tensorflow:training step 2917 | tagging_loss_video: 5.985|tagging_loss_audio: 11.186|tagging_loss_text: 17.845|tagging_loss_image: 8.492|tagging_loss_fusion: 4.639|total_loss: 48.147 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 2918 | tagging_loss_video: 6.306|tagging_loss_audio: 10.594|tagging_loss_text: 16.220|tagging_loss_image: 7.230|tagging_loss_fusion: 6.285|total_loss: 46.636 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 2919 | tagging_loss_video: 7.862|tagging_loss_audio: 10.493|tagging_loss_text: 15.413|tagging_loss_image: 8.468|tagging_loss_fusion: 8.209|total_loss: 50.445 | 64.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2920 |tagging_loss_video: 5.997|tagging_loss_audio: 10.824|tagging_loss_text: 11.047|tagging_loss_image: 7.547|tagging_loss_fusion: 6.490|total_loss: 41.904 | Examples/sec: 70.97\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2921 | tagging_loss_video: 7.122|tagging_loss_audio: 11.373|tagging_loss_text: 16.365|tagging_loss_image: 8.269|tagging_loss_fusion: 10.050|total_loss: 53.179 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 2922 | tagging_loss_video: 6.518|tagging_loss_audio: 12.797|tagging_loss_text: 18.718|tagging_loss_image: 8.961|tagging_loss_fusion: 6.367|total_loss: 53.361 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 2923 | tagging_loss_video: 7.184|tagging_loss_audio: 11.800|tagging_loss_text: 14.004|tagging_loss_image: 8.665|tagging_loss_fusion: 5.928|total_loss: 47.581 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 2924 | tagging_loss_video: 6.125|tagging_loss_audio: 10.701|tagging_loss_text: 15.159|tagging_loss_image: 9.311|tagging_loss_fusion: 6.311|total_loss: 47.606 | 62.15 Examples/sec\n",
      "INFO:tensorflow:training step 2925 | tagging_loss_video: 7.307|tagging_loss_audio: 10.337|tagging_loss_text: 16.680|tagging_loss_image: 8.695|tagging_loss_fusion: 8.231|total_loss: 51.250 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 2926 | tagging_loss_video: 7.204|tagging_loss_audio: 10.195|tagging_loss_text: 11.916|tagging_loss_image: 7.747|tagging_loss_fusion: 7.007|total_loss: 44.070 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 2927 | tagging_loss_video: 6.659|tagging_loss_audio: 10.637|tagging_loss_text: 19.259|tagging_loss_image: 9.404|tagging_loss_fusion: 7.130|total_loss: 53.089 | 63.00 Examples/sec\n",
      "INFO:tensorflow:training step 2928 | tagging_loss_video: 7.442|tagging_loss_audio: 11.425|tagging_loss_text: 18.635|tagging_loss_image: 8.872|tagging_loss_fusion: 6.985|total_loss: 53.359 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 2929 | tagging_loss_video: 6.471|tagging_loss_audio: 10.453|tagging_loss_text: 17.017|tagging_loss_image: 8.166|tagging_loss_fusion: 5.390|total_loss: 47.497 | 70.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2930 |tagging_loss_video: 7.064|tagging_loss_audio: 10.728|tagging_loss_text: 16.666|tagging_loss_image: 7.449|tagging_loss_fusion: 7.160|total_loss: 49.066 | Examples/sec: 61.03\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.75 | precision@0.5: 0.89 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2931 | tagging_loss_video: 6.916|tagging_loss_audio: 9.791|tagging_loss_text: 13.865|tagging_loss_image: 8.183|tagging_loss_fusion: 7.787|total_loss: 46.541 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 2932 | tagging_loss_video: 7.099|tagging_loss_audio: 12.395|tagging_loss_text: 13.798|tagging_loss_image: 8.539|tagging_loss_fusion: 6.838|total_loss: 48.669 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 2933 | tagging_loss_video: 7.030|tagging_loss_audio: 11.549|tagging_loss_text: 14.094|tagging_loss_image: 8.702|tagging_loss_fusion: 5.139|total_loss: 46.514 | 66.29 Examples/sec\n",
      "INFO:tensorflow:training step 2934 | tagging_loss_video: 7.277|tagging_loss_audio: 12.438|tagging_loss_text: 14.999|tagging_loss_image: 9.803|tagging_loss_fusion: 8.601|total_loss: 53.118 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 2935 | tagging_loss_video: 6.988|tagging_loss_audio: 9.028|tagging_loss_text: 18.499|tagging_loss_image: 7.346|tagging_loss_fusion: 5.236|total_loss: 47.098 | 65.37 Examples/sec\n",
      "INFO:tensorflow:training step 2936 | tagging_loss_video: 6.159|tagging_loss_audio: 11.865|tagging_loss_text: 17.702|tagging_loss_image: 7.964|tagging_loss_fusion: 6.128|total_loss: 49.818 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 2937 | tagging_loss_video: 6.685|tagging_loss_audio: 8.895|tagging_loss_text: 12.996|tagging_loss_image: 7.869|tagging_loss_fusion: 8.042|total_loss: 44.487 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 2938 | tagging_loss_video: 7.066|tagging_loss_audio: 12.820|tagging_loss_text: 18.804|tagging_loss_image: 8.878|tagging_loss_fusion: 7.338|total_loss: 54.906 | 60.06 Examples/sec\n",
      "INFO:tensorflow:training step 2939 | tagging_loss_video: 7.282|tagging_loss_audio: 13.310|tagging_loss_text: 18.072|tagging_loss_image: 10.029|tagging_loss_fusion: 7.977|total_loss: 56.671 | 69.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2940 |tagging_loss_video: 6.876|tagging_loss_audio: 10.560|tagging_loss_text: 18.149|tagging_loss_image: 8.189|tagging_loss_fusion: 8.052|total_loss: 51.826 | Examples/sec: 69.59\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 2941 | tagging_loss_video: 6.447|tagging_loss_audio: 11.124|tagging_loss_text: 18.854|tagging_loss_image: 7.886|tagging_loss_fusion: 6.122|total_loss: 50.433 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 2942 | tagging_loss_video: 7.867|tagging_loss_audio: 13.343|tagging_loss_text: 17.587|tagging_loss_image: 9.400|tagging_loss_fusion: 10.435|total_loss: 58.632 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 2943 | tagging_loss_video: 6.590|tagging_loss_audio: 10.912|tagging_loss_text: 11.789|tagging_loss_image: 6.852|tagging_loss_fusion: 6.515|total_loss: 42.659 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 2944 | tagging_loss_video: 6.308|tagging_loss_audio: 10.765|tagging_loss_text: 14.703|tagging_loss_image: 7.312|tagging_loss_fusion: 7.013|total_loss: 46.101 | 64.49 Examples/sec\n",
      "INFO:tensorflow:training step 2945 | tagging_loss_video: 7.243|tagging_loss_audio: 11.873|tagging_loss_text: 15.436|tagging_loss_image: 8.466|tagging_loss_fusion: 5.997|total_loss: 49.016 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 2946 | tagging_loss_video: 5.321|tagging_loss_audio: 10.741|tagging_loss_text: 13.768|tagging_loss_image: 8.386|tagging_loss_fusion: 4.384|total_loss: 42.599 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2947 | tagging_loss_video: 6.486|tagging_loss_audio: 10.997|tagging_loss_text: 16.715|tagging_loss_image: 7.428|tagging_loss_fusion: 6.664|total_loss: 48.291 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 2948 | tagging_loss_video: 7.011|tagging_loss_audio: 12.189|tagging_loss_text: 14.963|tagging_loss_image: 8.772|tagging_loss_fusion: 8.252|total_loss: 51.187 | 67.88 Examples/sec\n",
      "INFO:tensorflow:training step 2949 | tagging_loss_video: 7.910|tagging_loss_audio: 12.832|tagging_loss_text: 17.033|tagging_loss_image: 8.365|tagging_loss_fusion: 7.396|total_loss: 53.536 | 69.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2950 |tagging_loss_video: 6.677|tagging_loss_audio: 11.135|tagging_loss_text: 15.771|tagging_loss_image: 7.286|tagging_loss_fusion: 7.414|total_loss: 48.284 | Examples/sec: 69.36\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.74 | precision@0.5: 0.88 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2951 | tagging_loss_video: 7.303|tagging_loss_audio: 11.825|tagging_loss_text: 14.359|tagging_loss_image: 7.270|tagging_loss_fusion: 6.225|total_loss: 46.982 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 2952 | tagging_loss_video: 6.838|tagging_loss_audio: 11.433|tagging_loss_text: 13.582|tagging_loss_image: 8.207|tagging_loss_fusion: 6.604|total_loss: 46.664 | 64.13 Examples/sec\n",
      "INFO:tensorflow:training step 2953 | tagging_loss_video: 8.337|tagging_loss_audio: 10.890|tagging_loss_text: 15.342|tagging_loss_image: 8.394|tagging_loss_fusion: 8.106|total_loss: 51.069 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 2954 | tagging_loss_video: 6.913|tagging_loss_audio: 11.710|tagging_loss_text: 16.387|tagging_loss_image: 7.577|tagging_loss_fusion: 6.148|total_loss: 48.734 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 2955 | tagging_loss_video: 5.790|tagging_loss_audio: 10.140|tagging_loss_text: 14.571|tagging_loss_image: 7.776|tagging_loss_fusion: 5.018|total_loss: 43.295 | 60.38 Examples/sec\n",
      "INFO:tensorflow:training step 2956 | tagging_loss_video: 5.985|tagging_loss_audio: 8.817|tagging_loss_text: 15.346|tagging_loss_image: 7.546|tagging_loss_fusion: 5.247|total_loss: 42.941 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 2957 | tagging_loss_video: 6.830|tagging_loss_audio: 9.369|tagging_loss_text: 16.960|tagging_loss_image: 7.428|tagging_loss_fusion: 7.928|total_loss: 48.516 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 2958 | tagging_loss_video: 6.063|tagging_loss_audio: 9.681|tagging_loss_text: 12.533|tagging_loss_image: 7.617|tagging_loss_fusion: 5.801|total_loss: 41.695 | 64.12 Examples/sec\n",
      "INFO:tensorflow:training step 2959 | tagging_loss_video: 6.335|tagging_loss_audio: 10.443|tagging_loss_text: 13.875|tagging_loss_image: 6.841|tagging_loss_fusion: 6.027|total_loss: 43.522 | 69.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2960 |tagging_loss_video: 6.335|tagging_loss_audio: 10.455|tagging_loss_text: 18.860|tagging_loss_image: 7.615|tagging_loss_fusion: 5.603|total_loss: 48.868 | Examples/sec: 67.90\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.74 | precision@0.5: 0.89 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 2961 | tagging_loss_video: 6.534|tagging_loss_audio: 11.576|tagging_loss_text: 16.062|tagging_loss_image: 7.402|tagging_loss_fusion: 7.313|total_loss: 48.887 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 2962 | tagging_loss_video: 6.255|tagging_loss_audio: 11.668|tagging_loss_text: 16.610|tagging_loss_image: 7.740|tagging_loss_fusion: 7.268|total_loss: 49.542 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 2963 | tagging_loss_video: 6.967|tagging_loss_audio: 10.813|tagging_loss_text: 13.577|tagging_loss_image: 7.505|tagging_loss_fusion: 7.497|total_loss: 46.360 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 2964 | tagging_loss_video: 6.371|tagging_loss_audio: 10.045|tagging_loss_text: 16.028|tagging_loss_image: 7.184|tagging_loss_fusion: 6.762|total_loss: 46.390 | 71.94 Examples/sec\n",
      "INFO:tensorflow:training step 2965 | tagging_loss_video: 6.090|tagging_loss_audio: 11.068|tagging_loss_text: 17.285|tagging_loss_image: 8.191|tagging_loss_fusion: 6.881|total_loss: 49.515 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 2966 | tagging_loss_video: 6.942|tagging_loss_audio: 10.494|tagging_loss_text: 15.553|tagging_loss_image: 7.843|tagging_loss_fusion: 7.028|total_loss: 47.860 | 71.63 Examples/sec\n",
      "INFO:tensorflow:training step 2967 | tagging_loss_video: 7.032|tagging_loss_audio: 12.023|tagging_loss_text: 14.611|tagging_loss_image: 8.432|tagging_loss_fusion: 5.711|total_loss: 47.810 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 2968 | tagging_loss_video: 7.542|tagging_loss_audio: 12.993|tagging_loss_text: 11.468|tagging_loss_image: 9.304|tagging_loss_fusion: 6.896|total_loss: 48.202 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 2969 | tagging_loss_video: 6.813|tagging_loss_audio: 13.603|tagging_loss_text: 14.778|tagging_loss_image: 8.315|tagging_loss_fusion: 6.572|total_loss: 50.080 | 64.53 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2970 |tagging_loss_video: 6.170|tagging_loss_audio: 11.766|tagging_loss_text: 15.846|tagging_loss_image: 7.897|tagging_loss_fusion: 8.782|total_loss: 50.461 | Examples/sec: 67.22\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.73 | precision@0.5: 0.87 |recall@0.1: 0.95 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 2971 | tagging_loss_video: 8.605|tagging_loss_audio: 11.933|tagging_loss_text: 12.109|tagging_loss_image: 9.087|tagging_loss_fusion: 8.126|total_loss: 49.861 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 2972 | tagging_loss_video: 6.904|tagging_loss_audio: 10.204|tagging_loss_text: 11.351|tagging_loss_image: 7.245|tagging_loss_fusion: 7.513|total_loss: 43.217 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 2973 | tagging_loss_video: 7.439|tagging_loss_audio: 11.829|tagging_loss_text: 15.106|tagging_loss_image: 8.164|tagging_loss_fusion: 6.776|total_loss: 49.315 | 71.84 Examples/sec\n",
      "INFO:tensorflow:training step 2974 | tagging_loss_video: 6.674|tagging_loss_audio: 10.014|tagging_loss_text: 16.619|tagging_loss_image: 7.647|tagging_loss_fusion: 5.096|total_loss: 46.050 | 60.62 Examples/sec\n",
      "INFO:tensorflow:training step 2975 | tagging_loss_video: 7.323|tagging_loss_audio: 12.414|tagging_loss_text: 16.569|tagging_loss_image: 8.356|tagging_loss_fusion: 6.840|total_loss: 51.502 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 2976 | tagging_loss_video: 7.083|tagging_loss_audio: 10.590|tagging_loss_text: 14.591|tagging_loss_image: 7.644|tagging_loss_fusion: 6.028|total_loss: 45.937 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 2977 | tagging_loss_video: 5.067|tagging_loss_audio: 9.799|tagging_loss_text: 14.894|tagging_loss_image: 6.409|tagging_loss_fusion: 4.079|total_loss: 40.248 | 62.74 Examples/sec\n",
      "INFO:tensorflow:training step 2978 | tagging_loss_video: 6.711|tagging_loss_audio: 10.144|tagging_loss_text: 14.906|tagging_loss_image: 7.198|tagging_loss_fusion: 5.774|total_loss: 44.733 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 2979 | tagging_loss_video: 7.195|tagging_loss_audio: 10.656|tagging_loss_text: 15.929|tagging_loss_image: 7.783|tagging_loss_fusion: 5.757|total_loss: 47.320 | 71.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2980 |tagging_loss_video: 6.608|tagging_loss_audio: 9.623|tagging_loss_text: 15.551|tagging_loss_image: 8.291|tagging_loss_fusion: 8.467|total_loss: 48.540 | Examples/sec: 59.07\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.69 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 2981 | tagging_loss_video: 6.670|tagging_loss_audio: 12.076|tagging_loss_text: 17.121|tagging_loss_image: 8.266|tagging_loss_fusion: 7.037|total_loss: 51.170 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 2982 | tagging_loss_video: 5.825|tagging_loss_audio: 10.305|tagging_loss_text: 14.088|tagging_loss_image: 7.790|tagging_loss_fusion: 5.419|total_loss: 43.426 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 2983 | tagging_loss_video: 7.139|tagging_loss_audio: 10.485|tagging_loss_text: 15.003|tagging_loss_image: 8.154|tagging_loss_fusion: 7.193|total_loss: 47.975 | 64.12 Examples/sec\n",
      "INFO:tensorflow:training step 2984 | tagging_loss_video: 6.908|tagging_loss_audio: 11.777|tagging_loss_text: 12.753|tagging_loss_image: 7.583|tagging_loss_fusion: 8.524|total_loss: 47.545 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 2985 | tagging_loss_video: 6.822|tagging_loss_audio: 10.584|tagging_loss_text: 15.641|tagging_loss_image: 8.019|tagging_loss_fusion: 8.034|total_loss: 49.099 | 64.24 Examples/sec\n",
      "INFO:tensorflow:training step 2986 | tagging_loss_video: 6.837|tagging_loss_audio: 12.088|tagging_loss_text: 16.616|tagging_loss_image: 7.966|tagging_loss_fusion: 6.806|total_loss: 50.313 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 2987 | tagging_loss_video: 6.004|tagging_loss_audio: 10.769|tagging_loss_text: 15.375|tagging_loss_image: 7.298|tagging_loss_fusion: 5.827|total_loss: 45.273 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 2988 | tagging_loss_video: 7.326|tagging_loss_audio: 9.250|tagging_loss_text: 17.143|tagging_loss_image: 8.428|tagging_loss_fusion: 6.185|total_loss: 48.332 | 64.85 Examples/sec\n",
      "INFO:tensorflow:training step 2989 | tagging_loss_video: 5.878|tagging_loss_audio: 10.642|tagging_loss_text: 18.210|tagging_loss_image: 6.333|tagging_loss_fusion: 4.190|total_loss: 45.253 | 67.86 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 2990 |tagging_loss_video: 6.131|tagging_loss_audio: 9.784|tagging_loss_text: 14.302|tagging_loss_image: 7.476|tagging_loss_fusion: 6.373|total_loss: 44.066 | Examples/sec: 71.71\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 2991 | tagging_loss_video: 6.477|tagging_loss_audio: 10.237|tagging_loss_text: 15.588|tagging_loss_image: 7.642|tagging_loss_fusion: 8.127|total_loss: 48.071 | 63.25 Examples/sec\n",
      "INFO:tensorflow:training step 2992 | tagging_loss_video: 5.769|tagging_loss_audio: 10.737|tagging_loss_text: 16.724|tagging_loss_image: 7.932|tagging_loss_fusion: 6.001|total_loss: 47.163 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 2993 | tagging_loss_video: 6.635|tagging_loss_audio: 11.255|tagging_loss_text: 14.489|tagging_loss_image: 7.687|tagging_loss_fusion: 6.627|total_loss: 46.693 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 2994 | tagging_loss_video: 7.047|tagging_loss_audio: 10.368|tagging_loss_text: 18.773|tagging_loss_image: 7.957|tagging_loss_fusion: 4.440|total_loss: 48.585 | 65.03 Examples/sec\n",
      "INFO:tensorflow:training step 2995 | tagging_loss_video: 6.893|tagging_loss_audio: 11.194|tagging_loss_text: 17.173|tagging_loss_image: 7.886|tagging_loss_fusion: 6.710|total_loss: 49.857 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 2996 | tagging_loss_video: 7.361|tagging_loss_audio: 10.900|tagging_loss_text: 15.986|tagging_loss_image: 7.443|tagging_loss_fusion: 6.932|total_loss: 48.622 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 2997 | tagging_loss_video: 7.258|tagging_loss_audio: 10.668|tagging_loss_text: 18.852|tagging_loss_image: 8.579|tagging_loss_fusion: 6.754|total_loss: 52.111 | 67.01 Examples/sec\n",
      "INFO:tensorflow:training step 2998 | tagging_loss_video: 6.616|tagging_loss_audio: 11.592|tagging_loss_text: 16.991|tagging_loss_image: 8.343|tagging_loss_fusion: 6.468|total_loss: 50.009 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 2999 | tagging_loss_video: 5.876|tagging_loss_audio: 10.040|tagging_loss_text: 14.793|tagging_loss_image: 7.861|tagging_loss_fusion: 5.473|total_loss: 44.044 | 63.08 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3000 |tagging_loss_video: 6.151|tagging_loss_audio: 10.622|tagging_loss_text: 19.127|tagging_loss_image: 9.363|tagging_loss_fusion: 5.638|total_loss: 50.901 | Examples/sec: 71.06\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exitsINFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.99 | recall@0.5: 0.89\n",
      "\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.719|loss: 29.216|GAP: 0.719|examples_per_second: 92.910\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.735|loss: 28.643|GAP: 0.741|examples_per_second: 97.521\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 0.969|perr: 0.683|loss: 35.948|GAP: 0.684|examples_per_second: 88.607\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.748|loss: 27.801|GAP: 0.739|examples_per_second: 99.209\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.742|loss: 27.932|GAP: 0.737|examples_per_second: 88.015\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 0.969|perr: 0.686|loss: 30.697|GAP: 0.692|examples_per_second: 99.153\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.721|loss: 32.805|GAP: 0.725|examples_per_second: 91.792\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.728|loss: 29.596|GAP: 0.733|examples_per_second: 98.312\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 1.000|perr: 0.701|loss: 32.575|GAP: 0.708|examples_per_second: 92.520\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.672|loss: 31.089|GAP: 0.682|examples_per_second: 96.107\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.744|loss: 25.889|GAP: 0.751|examples_per_second: 83.068\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 1.000|perr: 0.744|loss: 28.502|GAP: 0.757|examples_per_second: 95.067\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.721|loss: 30.636|GAP: 0.734|examples_per_second: 92.501\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.722|loss: 30.687|GAP: 0.728|examples_per_second: 101.480\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.711|loss: 29.096|GAP: 0.715|examples_per_second: 90.183\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 3000 | MAP: 0.277 | GAP: 0.707 | p@0.1: 0.647 | p@0.5:0.823 | r@0.1:0.745 | r@0.5: 0.571 | Avg_Loss: 24.223750\n",
      "INFO:tensorflow:epoch/eval number 3000 | MAP: 0.246 | GAP: 0.661 | p@0.1: 0.495 | p@0.5:0.759 | r@0.1:0.835 | r@0.5: 0.566 | Avg_Loss: 20.679615\n",
      "INFO:tensorflow:epoch/eval number 3000 | MAP: 0.112 | GAP: 0.575 | p@0.1: 0.335 | p@0.5:0.844 | r@0.1:0.878 | r@0.5: 0.365 | Avg_Loss: 22.068062\n",
      "INFO:tensorflow:epoch/eval number 3000 | MAP: 0.260 | GAP: 0.635 | p@0.1: 0.589 | p@0.5:0.714 | r@0.1:0.731 | r@0.5: 0.600 | Avg_Loss: 31.698582\n",
      "INFO:tensorflow:epoch/eval number 3000 | MAP: 0.302 | GAP: 0.723 | p@0.1: 0.720 | p@0.5:0.822 | r@0.1:0.710 | r@0.5: 0.595 | Avg_Loss: 30.073969\n",
      "INFO:tensorflow:validation score on val799 is : 0.7228\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-3000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_3000_0.7228/saved_model.pb\n",
      "INFO:tensorflow:training step 3001 | tagging_loss_video: 6.294|tagging_loss_audio: 9.018|tagging_loss_text: 13.784|tagging_loss_image: 6.847|tagging_loss_fusion: 6.128|total_loss: 42.070 | 71.92 Examples/sec\n",
      "INFO:tensorflow:training step 3002 | tagging_loss_video: 6.023|tagging_loss_audio: 9.993|tagging_loss_text: 13.329|tagging_loss_image: 6.987|tagging_loss_fusion: 8.194|total_loss: 44.526 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 3003 | tagging_loss_video: 6.942|tagging_loss_audio: 10.871|tagging_loss_text: 16.887|tagging_loss_image: 7.403|tagging_loss_fusion: 7.043|total_loss: 49.146 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 3004 | tagging_loss_video: 4.414|tagging_loss_audio: 9.124|tagging_loss_text: 10.377|tagging_loss_image: 6.384|tagging_loss_fusion: 3.935|total_loss: 34.235 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 3005 | tagging_loss_video: 6.884|tagging_loss_audio: 10.480|tagging_loss_text: 11.941|tagging_loss_image: 8.345|tagging_loss_fusion: 7.152|total_loss: 44.803 | 72.21 Examples/sec\n",
      "INFO:tensorflow:training step 3006 | tagging_loss_video: 6.629|tagging_loss_audio: 10.122|tagging_loss_text: 13.790|tagging_loss_image: 7.839|tagging_loss_fusion: 7.560|total_loss: 45.940 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 3007 | tagging_loss_video: 6.010|tagging_loss_audio: 9.879|tagging_loss_text: 14.679|tagging_loss_image: 7.456|tagging_loss_fusion: 6.561|total_loss: 44.584 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 3008 | tagging_loss_video: 5.352|tagging_loss_audio: 9.110|tagging_loss_text: 15.270|tagging_loss_image: 7.113|tagging_loss_fusion: 6.184|total_loss: 43.029 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 3009 | tagging_loss_video: 5.995|tagging_loss_audio: 11.223|tagging_loss_text: 16.341|tagging_loss_image: 8.218|tagging_loss_fusion: 5.855|total_loss: 47.631 | 71.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3010 |tagging_loss_video: 5.801|tagging_loss_audio: 10.165|tagging_loss_text: 13.745|tagging_loss_image: 7.989|tagging_loss_fusion: 6.076|total_loss: 43.775 | Examples/sec: 68.92\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3011 | tagging_loss_video: 5.595|tagging_loss_audio: 9.496|tagging_loss_text: 14.807|tagging_loss_image: 6.981|tagging_loss_fusion: 6.151|total_loss: 43.031 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3012 | tagging_loss_video: 6.309|tagging_loss_audio: 9.143|tagging_loss_text: 11.709|tagging_loss_image: 6.699|tagging_loss_fusion: 5.806|total_loss: 39.666 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 3013 | tagging_loss_video: 7.356|tagging_loss_audio: 10.923|tagging_loss_text: 20.174|tagging_loss_image: 7.681|tagging_loss_fusion: 7.705|total_loss: 53.840 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 3014 | tagging_loss_video: 6.040|tagging_loss_audio: 13.240|tagging_loss_text: 14.595|tagging_loss_image: 8.380|tagging_loss_fusion: 4.826|total_loss: 47.082 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 3015 | tagging_loss_video: 6.757|tagging_loss_audio: 9.759|tagging_loss_text: 12.889|tagging_loss_image: 7.499|tagging_loss_fusion: 6.423|total_loss: 43.327 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 3016 | tagging_loss_video: 6.433|tagging_loss_audio: 10.776|tagging_loss_text: 16.191|tagging_loss_image: 7.938|tagging_loss_fusion: 6.884|total_loss: 48.223 | 65.76 Examples/sec\n",
      "INFO:tensorflow:training step 3017 | tagging_loss_video: 6.760|tagging_loss_audio: 9.713|tagging_loss_text: 16.497|tagging_loss_image: 8.079|tagging_loss_fusion: 7.163|total_loss: 48.212 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 3018 | tagging_loss_video: 6.048|tagging_loss_audio: 10.322|tagging_loss_text: 13.711|tagging_loss_image: 6.978|tagging_loss_fusion: 5.965|total_loss: 43.024 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 3019 | tagging_loss_video: 5.411|tagging_loss_audio: 10.108|tagging_loss_text: 11.084|tagging_loss_image: 7.345|tagging_loss_fusion: 4.614|total_loss: 38.562 | 69.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3020 |tagging_loss_video: 6.564|tagging_loss_audio: 9.853|tagging_loss_text: 17.939|tagging_loss_image: 7.440|tagging_loss_fusion: 6.632|total_loss: 48.427 | Examples/sec: 71.17\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.78 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3021 | tagging_loss_video: 7.265|tagging_loss_audio: 10.248|tagging_loss_text: 16.013|tagging_loss_image: 8.432|tagging_loss_fusion: 6.182|total_loss: 48.141 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 3022 | tagging_loss_video: 5.959|tagging_loss_audio: 10.104|tagging_loss_text: 14.070|tagging_loss_image: 7.567|tagging_loss_fusion: 5.739|total_loss: 43.440 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 3023 | tagging_loss_video: 6.055|tagging_loss_audio: 9.174|tagging_loss_text: 14.706|tagging_loss_image: 6.943|tagging_loss_fusion: 5.844|total_loss: 42.722 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 3024 | tagging_loss_video: 7.592|tagging_loss_audio: 11.511|tagging_loss_text: 17.188|tagging_loss_image: 10.044|tagging_loss_fusion: 7.484|total_loss: 53.819 | 68.07 Examples/sec\n",
      "INFO:tensorflow:training step 3025 | tagging_loss_video: 7.929|tagging_loss_audio: 10.646|tagging_loss_text: 12.884|tagging_loss_image: 8.639|tagging_loss_fusion: 10.348|total_loss: 50.446 | 71.68 Examples/sec\n",
      "INFO:tensorflow:training step 3026 | tagging_loss_video: 6.614|tagging_loss_audio: 12.121|tagging_loss_text: 16.747|tagging_loss_image: 8.166|tagging_loss_fusion: 7.671|total_loss: 51.320 | 67.03 Examples/sec\n",
      "INFO:tensorflow:training step 3027 | tagging_loss_video: 6.525|tagging_loss_audio: 11.296|tagging_loss_text: 18.176|tagging_loss_image: 9.175|tagging_loss_fusion: 6.386|total_loss: 51.558 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 3028 | tagging_loss_video: 8.032|tagging_loss_audio: 12.138|tagging_loss_text: 14.153|tagging_loss_image: 8.057|tagging_loss_fusion: 8.988|total_loss: 51.368 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 3029 | tagging_loss_video: 8.185|tagging_loss_audio: 9.975|tagging_loss_text: 18.547|tagging_loss_image: 8.105|tagging_loss_fusion: 9.493|total_loss: 54.305 | 70.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3030 |tagging_loss_video: 7.041|tagging_loss_audio: 11.310|tagging_loss_text: 11.754|tagging_loss_image: 8.168|tagging_loss_fusion: 7.399|total_loss: 45.672 | Examples/sec: 68.62\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.71 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 3031 | tagging_loss_video: 6.359|tagging_loss_audio: 10.421|tagging_loss_text: 15.059|tagging_loss_image: 8.077|tagging_loss_fusion: 4.923|total_loss: 44.839 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 3032 | tagging_loss_video: 6.429|tagging_loss_audio: 8.367|tagging_loss_text: 14.229|tagging_loss_image: 7.301|tagging_loss_fusion: 6.375|total_loss: 42.700 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 3033 | tagging_loss_video: 6.709|tagging_loss_audio: 9.251|tagging_loss_text: 14.020|tagging_loss_image: 7.304|tagging_loss_fusion: 7.535|total_loss: 44.819 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 3034 | tagging_loss_video: 7.175|tagging_loss_audio: 12.149|tagging_loss_text: 17.828|tagging_loss_image: 9.001|tagging_loss_fusion: 9.662|total_loss: 55.815 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 3035 | tagging_loss_video: 6.623|tagging_loss_audio: 12.441|tagging_loss_text: 15.896|tagging_loss_image: 8.468|tagging_loss_fusion: 6.827|total_loss: 50.256 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 3036 | tagging_loss_video: 6.572|tagging_loss_audio: 10.065|tagging_loss_text: 10.614|tagging_loss_image: 9.065|tagging_loss_fusion: 7.738|total_loss: 44.055 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 3037 | tagging_loss_video: 6.441|tagging_loss_audio: 9.852|tagging_loss_text: 12.391|tagging_loss_image: 7.717|tagging_loss_fusion: 6.366|total_loss: 42.766 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 3038 | tagging_loss_video: 5.967|tagging_loss_audio: 10.692|tagging_loss_text: 14.593|tagging_loss_image: 7.443|tagging_loss_fusion: 6.171|total_loss: 44.866 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 3039 | tagging_loss_video: 7.856|tagging_loss_audio: 13.278|tagging_loss_text: 17.200|tagging_loss_image: 8.890|tagging_loss_fusion: 10.264|total_loss: 57.489 | 72.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3040 |tagging_loss_video: 6.866|tagging_loss_audio: 11.329|tagging_loss_text: 15.535|tagging_loss_image: 9.838|tagging_loss_fusion: 7.702|total_loss: 51.269 | Examples/sec: 70.86\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3041 | tagging_loss_video: 7.567|tagging_loss_audio: 11.951|tagging_loss_text: 20.007|tagging_loss_image: 8.564|tagging_loss_fusion: 8.248|total_loss: 56.338 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 3042 | tagging_loss_video: 7.175|tagging_loss_audio: 12.279|tagging_loss_text: 19.097|tagging_loss_image: 8.923|tagging_loss_fusion: 5.846|total_loss: 53.320 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 3043 | tagging_loss_video: 6.957|tagging_loss_audio: 11.262|tagging_loss_text: 16.233|tagging_loss_image: 10.211|tagging_loss_fusion: 7.188|total_loss: 51.852 | 67.88 Examples/sec\n",
      "INFO:tensorflow:training step 3044 | tagging_loss_video: 6.184|tagging_loss_audio: 10.045|tagging_loss_text: 14.869|tagging_loss_image: 8.743|tagging_loss_fusion: 7.201|total_loss: 47.042 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 3045 | tagging_loss_video: 6.768|tagging_loss_audio: 9.622|tagging_loss_text: 13.204|tagging_loss_image: 7.209|tagging_loss_fusion: 7.156|total_loss: 43.960 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 3046 | tagging_loss_video: 6.848|tagging_loss_audio: 10.141|tagging_loss_text: 13.571|tagging_loss_image: 7.463|tagging_loss_fusion: 7.404|total_loss: 45.427 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 3047 | tagging_loss_video: 7.195|tagging_loss_audio: 10.508|tagging_loss_text: 12.763|tagging_loss_image: 8.074|tagging_loss_fusion: 7.017|total_loss: 45.557 | 72.38 Examples/sec\n",
      "INFO:tensorflow:training step 3048 | tagging_loss_video: 6.174|tagging_loss_audio: 11.647|tagging_loss_text: 14.614|tagging_loss_image: 8.381|tagging_loss_fusion: 5.834|total_loss: 46.650 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 3049 | tagging_loss_video: 7.379|tagging_loss_audio: 9.059|tagging_loss_text: 13.439|tagging_loss_image: 7.811|tagging_loss_fusion: 7.713|total_loss: 45.402 | 70.34 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3050 |tagging_loss_video: 6.505|tagging_loss_audio: 12.888|tagging_loss_text: 15.262|tagging_loss_image: 8.949|tagging_loss_fusion: 5.760|total_loss: 49.363 | Examples/sec: 68.31\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3051 | tagging_loss_video: 7.894|tagging_loss_audio: 11.350|tagging_loss_text: 12.969|tagging_loss_image: 9.711|tagging_loss_fusion: 6.776|total_loss: 48.700 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 3052 | tagging_loss_video: 6.409|tagging_loss_audio: 11.098|tagging_loss_text: 12.843|tagging_loss_image: 7.815|tagging_loss_fusion: 6.906|total_loss: 45.071 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 3053 | tagging_loss_video: 6.419|tagging_loss_audio: 10.148|tagging_loss_text: 12.676|tagging_loss_image: 8.563|tagging_loss_fusion: 7.926|total_loss: 45.731 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 3054 | tagging_loss_video: 8.480|tagging_loss_audio: 14.025|tagging_loss_text: 16.664|tagging_loss_image: 9.029|tagging_loss_fusion: 8.645|total_loss: 56.843 | 71.68 Examples/sec\n",
      "INFO:tensorflow:training step 3055 | tagging_loss_video: 6.879|tagging_loss_audio: 10.830|tagging_loss_text: 11.576|tagging_loss_image: 8.050|tagging_loss_fusion: 8.855|total_loss: 46.190 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 3056 | tagging_loss_video: 7.657|tagging_loss_audio: 10.495|tagging_loss_text: 12.839|tagging_loss_image: 9.129|tagging_loss_fusion: 7.452|total_loss: 47.573 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 3057 | tagging_loss_video: 7.882|tagging_loss_audio: 12.278|tagging_loss_text: 15.984|tagging_loss_image: 9.366|tagging_loss_fusion: 7.773|total_loss: 53.282 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 3058 | tagging_loss_video: 6.982|tagging_loss_audio: 10.998|tagging_loss_text: 15.909|tagging_loss_image: 8.137|tagging_loss_fusion: 9.498|total_loss: 51.523 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 3059 | tagging_loss_video: 7.018|tagging_loss_audio: 9.899|tagging_loss_text: 15.779|tagging_loss_image: 7.657|tagging_loss_fusion: 7.182|total_loss: 47.535 | 68.50 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3060 |tagging_loss_video: 6.890|tagging_loss_audio: 9.988|tagging_loss_text: 17.655|tagging_loss_image: 8.357|tagging_loss_fusion: 6.912|total_loss: 49.803 | Examples/sec: 72.22\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.76 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3061 | tagging_loss_video: 6.942|tagging_loss_audio: 9.792|tagging_loss_text: 15.243|tagging_loss_image: 8.307|tagging_loss_fusion: 6.657|total_loss: 46.941 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3062 | tagging_loss_video: 7.071|tagging_loss_audio: 11.451|tagging_loss_text: 14.186|tagging_loss_image: 7.458|tagging_loss_fusion: 6.875|total_loss: 47.041 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 3063 | tagging_loss_video: 7.209|tagging_loss_audio: 11.147|tagging_loss_text: 12.024|tagging_loss_image: 8.365|tagging_loss_fusion: 7.685|total_loss: 46.431 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 3064 | tagging_loss_video: 7.964|tagging_loss_audio: 11.842|tagging_loss_text: 16.014|tagging_loss_image: 9.745|tagging_loss_fusion: 9.854|total_loss: 55.420 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 3065 | tagging_loss_video: 7.592|tagging_loss_audio: 10.672|tagging_loss_text: 17.984|tagging_loss_image: 9.496|tagging_loss_fusion: 7.890|total_loss: 53.633 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 3066 | tagging_loss_video: 7.639|tagging_loss_audio: 13.250|tagging_loss_text: 13.868|tagging_loss_image: 8.910|tagging_loss_fusion: 8.398|total_loss: 52.066 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 3067 | tagging_loss_video: 6.431|tagging_loss_audio: 10.399|tagging_loss_text: 12.813|tagging_loss_image: 6.985|tagging_loss_fusion: 5.899|total_loss: 42.527 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 3068 | tagging_loss_video: 8.152|tagging_loss_audio: 11.614|tagging_loss_text: 14.724|tagging_loss_image: 8.418|tagging_loss_fusion: 6.366|total_loss: 49.274 | 72.00 Examples/sec\n",
      "INFO:tensorflow:training step 3069 | tagging_loss_video: 8.569|tagging_loss_audio: 10.959|tagging_loss_text: 15.338|tagging_loss_image: 8.347|tagging_loss_fusion: 9.712|total_loss: 52.926 | 68.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3070 |tagging_loss_video: 7.524|tagging_loss_audio: 10.752|tagging_loss_text: 16.657|tagging_loss_image: 8.818|tagging_loss_fusion: 7.875|total_loss: 51.626 | Examples/sec: 71.13\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 3071 | tagging_loss_video: 7.174|tagging_loss_audio: 11.355|tagging_loss_text: 16.357|tagging_loss_image: 9.241|tagging_loss_fusion: 8.703|total_loss: 52.829 | 72.54 Examples/sec\n",
      "INFO:tensorflow:training step 3072 | tagging_loss_video: 9.236|tagging_loss_audio: 11.144|tagging_loss_text: 14.999|tagging_loss_image: 9.645|tagging_loss_fusion: 10.976|total_loss: 56.000 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 3073 | tagging_loss_video: 7.956|tagging_loss_audio: 9.519|tagging_loss_text: 17.405|tagging_loss_image: 8.097|tagging_loss_fusion: 6.800|total_loss: 49.778 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 3074 | tagging_loss_video: 7.654|tagging_loss_audio: 12.030|tagging_loss_text: 15.647|tagging_loss_image: 8.742|tagging_loss_fusion: 7.328|total_loss: 51.400 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 3075 | tagging_loss_video: 6.321|tagging_loss_audio: 10.126|tagging_loss_text: 14.822|tagging_loss_image: 7.552|tagging_loss_fusion: 6.470|total_loss: 45.290 | 72.05 Examples/sec\n",
      "INFO:tensorflow:training step 3076 | tagging_loss_video: 6.665|tagging_loss_audio: 9.724|tagging_loss_text: 13.697|tagging_loss_image: 8.402|tagging_loss_fusion: 6.969|total_loss: 45.457 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 3077 | tagging_loss_video: 7.685|tagging_loss_audio: 12.062|tagging_loss_text: 18.362|tagging_loss_image: 8.158|tagging_loss_fusion: 9.025|total_loss: 55.292 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 3078 | tagging_loss_video: 7.374|tagging_loss_audio: 10.289|tagging_loss_text: 17.342|tagging_loss_image: 9.052|tagging_loss_fusion: 8.744|total_loss: 52.802 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 3079 | tagging_loss_video: 7.368|tagging_loss_audio: 12.286|tagging_loss_text: 14.107|tagging_loss_image: 9.512|tagging_loss_fusion: 6.582|total_loss: 49.855 | 69.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3080 |tagging_loss_video: 7.562|tagging_loss_audio: 13.515|tagging_loss_text: 19.109|tagging_loss_image: 8.221|tagging_loss_fusion: 9.379|total_loss: 57.787 | Examples/sec: 72.16\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.72 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 3081 | tagging_loss_video: 6.770|tagging_loss_audio: 11.222|tagging_loss_text: 15.139|tagging_loss_image: 8.210|tagging_loss_fusion: 6.831|total_loss: 48.172 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 3082 | tagging_loss_video: 7.811|tagging_loss_audio: 10.408|tagging_loss_text: 14.421|tagging_loss_image: 8.348|tagging_loss_fusion: 7.970|total_loss: 48.958 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 3083 | tagging_loss_video: 7.583|tagging_loss_audio: 11.872|tagging_loss_text: 15.947|tagging_loss_image: 7.462|tagging_loss_fusion: 9.073|total_loss: 51.937 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 3084 | tagging_loss_video: 7.845|tagging_loss_audio: 11.080|tagging_loss_text: 15.238|tagging_loss_image: 7.268|tagging_loss_fusion: 9.292|total_loss: 50.722 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 3085 | tagging_loss_video: 7.813|tagging_loss_audio: 11.827|tagging_loss_text: 13.553|tagging_loss_image: 8.230|tagging_loss_fusion: 7.419|total_loss: 48.844 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 3086 | tagging_loss_video: 6.846|tagging_loss_audio: 12.484|tagging_loss_text: 14.539|tagging_loss_image: 8.469|tagging_loss_fusion: 7.241|total_loss: 49.578 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 3087 | tagging_loss_video: 7.403|tagging_loss_audio: 11.334|tagging_loss_text: 17.341|tagging_loss_image: 7.871|tagging_loss_fusion: 10.324|total_loss: 54.274 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 3088 | tagging_loss_video: 7.776|tagging_loss_audio: 11.095|tagging_loss_text: 15.494|tagging_loss_image: 8.317|tagging_loss_fusion: 9.078|total_loss: 51.759 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 3089 | tagging_loss_video: 8.232|tagging_loss_audio: 10.880|tagging_loss_text: 13.235|tagging_loss_image: 9.280|tagging_loss_fusion: 10.628|total_loss: 52.254 | 70.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3090 |tagging_loss_video: 6.741|tagging_loss_audio: 11.597|tagging_loss_text: 15.387|tagging_loss_image: 8.055|tagging_loss_fusion: 7.229|total_loss: 49.010 | Examples/sec: 69.11\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.74 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 3091 | tagging_loss_video: 7.621|tagging_loss_audio: 11.919|tagging_loss_text: 18.230|tagging_loss_image: 6.828|tagging_loss_fusion: 10.618|total_loss: 55.216 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 3092 | tagging_loss_video: 6.379|tagging_loss_audio: 11.128|tagging_loss_text: 14.438|tagging_loss_image: 7.639|tagging_loss_fusion: 6.511|total_loss: 46.095 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 3093 | tagging_loss_video: 6.753|tagging_loss_audio: 11.494|tagging_loss_text: 15.097|tagging_loss_image: 8.162|tagging_loss_fusion: 8.781|total_loss: 50.287 | 69.45 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 3093.\n",
      "INFO:tensorflow:training step 3094 | tagging_loss_video: 7.881|tagging_loss_audio: 10.553|tagging_loss_text: 13.938|tagging_loss_image: 7.556|tagging_loss_fusion: 8.192|total_loss: 48.120 | 48.18 Examples/sec\n",
      "INFO:tensorflow:training step 3095 | tagging_loss_video: 7.663|tagging_loss_audio: 12.318|tagging_loss_text: 13.521|tagging_loss_image: 9.398|tagging_loss_fusion: 9.230|total_loss: 52.131 | 60.86 Examples/sec\n",
      "INFO:tensorflow:training step 3096 | tagging_loss_video: 7.336|tagging_loss_audio: 10.717|tagging_loss_text: 13.327|tagging_loss_image: 8.038|tagging_loss_fusion: 10.423|total_loss: 49.841 | 70.71 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3097 | tagging_loss_video: 8.015|tagging_loss_audio: 10.896|tagging_loss_text: 14.054|tagging_loss_image: 9.266|tagging_loss_fusion: 10.149|total_loss: 52.380 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 3098 | tagging_loss_video: 6.447|tagging_loss_audio: 10.389|tagging_loss_text: 16.359|tagging_loss_image: 6.994|tagging_loss_fusion: 7.562|total_loss: 47.752 | 62.63 Examples/sec\n",
      "INFO:tensorflow:training step 3099 | tagging_loss_video: 6.356|tagging_loss_audio: 9.979|tagging_loss_text: 13.407|tagging_loss_image: 7.942|tagging_loss_fusion: 6.479|total_loss: 44.163 | 68.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3100 |tagging_loss_video: 6.958|tagging_loss_audio: 10.456|tagging_loss_text: 14.567|tagging_loss_image: 9.440|tagging_loss_fusion: 7.152|total_loss: 48.571 | Examples/sec: 68.97\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.73 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3101 | tagging_loss_video: 7.130|tagging_loss_audio: 12.808|tagging_loss_text: 17.951|tagging_loss_image: 8.623|tagging_loss_fusion: 7.859|total_loss: 54.370 | 67.09 Examples/sec\n",
      "INFO:tensorflow:training step 3102 | tagging_loss_video: 7.406|tagging_loss_audio: 10.582|tagging_loss_text: 12.977|tagging_loss_image: 8.063|tagging_loss_fusion: 8.202|total_loss: 47.230 | 66.92 Examples/sec\n",
      "INFO:tensorflow:training step 3103 | tagging_loss_video: 7.756|tagging_loss_audio: 13.659|tagging_loss_text: 17.295|tagging_loss_image: 9.551|tagging_loss_fusion: 10.857|total_loss: 59.118 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 3104 | tagging_loss_video: 7.871|tagging_loss_audio: 10.866|tagging_loss_text: 16.676|tagging_loss_image: 8.832|tagging_loss_fusion: 6.546|total_loss: 50.791 | 63.05 Examples/sec\n",
      "INFO:tensorflow:training step 3105 | tagging_loss_video: 7.647|tagging_loss_audio: 12.363|tagging_loss_text: 15.232|tagging_loss_image: 8.626|tagging_loss_fusion: 8.735|total_loss: 52.603 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 3106 | tagging_loss_video: 7.235|tagging_loss_audio: 13.979|tagging_loss_text: 17.267|tagging_loss_image: 8.322|tagging_loss_fusion: 7.001|total_loss: 53.803 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 3107 | tagging_loss_video: 8.284|tagging_loss_audio: 14.822|tagging_loss_text: 16.265|tagging_loss_image: 8.733|tagging_loss_fusion: 8.413|total_loss: 56.516 | 64.35 Examples/sec\n",
      "INFO:tensorflow:training step 3108 | tagging_loss_video: 7.182|tagging_loss_audio: 9.887|tagging_loss_text: 13.520|tagging_loss_image: 8.319|tagging_loss_fusion: 8.152|total_loss: 47.060 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 3109 | tagging_loss_video: 7.441|tagging_loss_audio: 11.598|tagging_loss_text: 15.835|tagging_loss_image: 8.405|tagging_loss_fusion: 8.649|total_loss: 51.929 | 68.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3110 |tagging_loss_video: 6.464|tagging_loss_audio: 10.450|tagging_loss_text: 15.841|tagging_loss_image: 7.726|tagging_loss_fusion: 7.644|total_loss: 48.125 | Examples/sec: 67.27\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.72 | precision@0.5: 0.89 |recall@0.1: 0.95 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3111 | tagging_loss_video: 7.826|tagging_loss_audio: 11.556|tagging_loss_text: 14.549|tagging_loss_image: 8.541|tagging_loss_fusion: 6.721|total_loss: 49.193 | 68.32 Examples/sec\n",
      "INFO:tensorflow:training step 3112 | tagging_loss_video: 8.048|tagging_loss_audio: 12.535|tagging_loss_text: 19.500|tagging_loss_image: 9.001|tagging_loss_fusion: 6.991|total_loss: 56.075 | 63.35 Examples/sec\n",
      "INFO:tensorflow:training step 3113 | tagging_loss_video: 7.816|tagging_loss_audio: 12.798|tagging_loss_text: 18.311|tagging_loss_image: 9.798|tagging_loss_fusion: 10.216|total_loss: 58.939 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 3114 | tagging_loss_video: 6.668|tagging_loss_audio: 10.962|tagging_loss_text: 15.159|tagging_loss_image: 8.029|tagging_loss_fusion: 6.220|total_loss: 47.038 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 3115 | tagging_loss_video: 8.185|tagging_loss_audio: 11.394|tagging_loss_text: 14.808|tagging_loss_image: 8.481|tagging_loss_fusion: 11.291|total_loss: 54.159 | 63.82 Examples/sec\n",
      "INFO:tensorflow:training step 3116 | tagging_loss_video: 6.305|tagging_loss_audio: 10.340|tagging_loss_text: 13.767|tagging_loss_image: 7.604|tagging_loss_fusion: 8.332|total_loss: 46.347 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 3117 | tagging_loss_video: 8.199|tagging_loss_audio: 10.056|tagging_loss_text: 19.277|tagging_loss_image: 9.425|tagging_loss_fusion: 8.698|total_loss: 55.655 | 71.94 Examples/sec\n",
      "INFO:tensorflow:training step 3118 | tagging_loss_video: 7.024|tagging_loss_audio: 11.327|tagging_loss_text: 12.720|tagging_loss_image: 7.770|tagging_loss_fusion: 6.834|total_loss: 45.675 | 61.33 Examples/sec\n",
      "INFO:tensorflow:training step 3119 | tagging_loss_video: 6.923|tagging_loss_audio: 9.973|tagging_loss_text: 14.451|tagging_loss_image: 8.223|tagging_loss_fusion: 6.943|total_loss: 46.514 | 67.11 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3120 |tagging_loss_video: 8.889|tagging_loss_audio: 12.709|tagging_loss_text: 16.714|tagging_loss_image: 9.451|tagging_loss_fusion: 11.067|total_loss: 58.830 | Examples/sec: 71.19\n",
      "INFO:tensorflow:GAP: 0.88 | precision@0.1: 0.71 | precision@0.5: 0.92 |recall@0.1: 0.90 | recall@0.5: 0.78\n",
      "INFO:tensorflow:training step 3121 | tagging_loss_video: 6.988|tagging_loss_audio: 10.349|tagging_loss_text: 15.667|tagging_loss_image: 6.902|tagging_loss_fusion: 5.099|total_loss: 45.006 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 3122 | tagging_loss_video: 8.370|tagging_loss_audio: 13.447|tagging_loss_text: 15.496|tagging_loss_image: 10.728|tagging_loss_fusion: 8.785|total_loss: 56.826 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 3123 | tagging_loss_video: 7.039|tagging_loss_audio: 11.329|tagging_loss_text: 18.374|tagging_loss_image: 7.973|tagging_loss_fusion: 8.786|total_loss: 53.500 | 66.62 Examples/sec\n",
      "INFO:tensorflow:training step 3124 | tagging_loss_video: 8.569|tagging_loss_audio: 11.842|tagging_loss_text: 17.573|tagging_loss_image: 8.952|tagging_loss_fusion: 7.876|total_loss: 54.812 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 3125 | tagging_loss_video: 7.712|tagging_loss_audio: 11.484|tagging_loss_text: 17.851|tagging_loss_image: 9.231|tagging_loss_fusion: 7.583|total_loss: 53.861 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 3126 | tagging_loss_video: 6.204|tagging_loss_audio: 11.684|tagging_loss_text: 14.272|tagging_loss_image: 7.793|tagging_loss_fusion: 6.848|total_loss: 46.801 | 65.20 Examples/sec\n",
      "INFO:tensorflow:training step 3127 | tagging_loss_video: 6.999|tagging_loss_audio: 10.597|tagging_loss_text: 17.300|tagging_loss_image: 8.620|tagging_loss_fusion: 7.403|total_loss: 50.919 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 3128 | tagging_loss_video: 7.971|tagging_loss_audio: 11.969|tagging_loss_text: 16.030|tagging_loss_image: 8.490|tagging_loss_fusion: 10.807|total_loss: 55.267 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 3129 | tagging_loss_video: 7.297|tagging_loss_audio: 12.369|tagging_loss_text: 16.359|tagging_loss_image: 8.620|tagging_loss_fusion: 8.748|total_loss: 53.394 | 61.08 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3130 |tagging_loss_video: 7.268|tagging_loss_audio: 10.726|tagging_loss_text: 14.145|tagging_loss_image: 8.582|tagging_loss_fusion: 6.562|total_loss: 47.283 | Examples/sec: 71.61\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.71 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3131 | tagging_loss_video: 7.281|tagging_loss_audio: 10.526|tagging_loss_text: 13.231|tagging_loss_image: 7.531|tagging_loss_fusion: 6.226|total_loss: 44.795 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 3132 | tagging_loss_video: 7.478|tagging_loss_audio: 11.808|tagging_loss_text: 17.053|tagging_loss_image: 8.422|tagging_loss_fusion: 9.306|total_loss: 54.067 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 3133 | tagging_loss_video: 7.871|tagging_loss_audio: 10.996|tagging_loss_text: 19.058|tagging_loss_image: 8.358|tagging_loss_fusion: 9.517|total_loss: 55.800 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 3134 | tagging_loss_video: 7.020|tagging_loss_audio: 11.100|tagging_loss_text: 11.990|tagging_loss_image: 7.665|tagging_loss_fusion: 7.057|total_loss: 44.832 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 3135 | tagging_loss_video: 7.711|tagging_loss_audio: 11.995|tagging_loss_text: 13.007|tagging_loss_image: 7.721|tagging_loss_fusion: 7.818|total_loss: 48.252 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 3136 | tagging_loss_video: 7.981|tagging_loss_audio: 14.061|tagging_loss_text: 15.814|tagging_loss_image: 8.531|tagging_loss_fusion: 6.800|total_loss: 53.188 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 3137 | tagging_loss_video: 7.460|tagging_loss_audio: 12.184|tagging_loss_text: 18.751|tagging_loss_image: 8.234|tagging_loss_fusion: 7.747|total_loss: 54.376 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 3138 | tagging_loss_video: 6.572|tagging_loss_audio: 10.260|tagging_loss_text: 15.826|tagging_loss_image: 8.162|tagging_loss_fusion: 8.248|total_loss: 49.069 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 3139 | tagging_loss_video: 7.610|tagging_loss_audio: 10.077|tagging_loss_text: 13.862|tagging_loss_image: 8.320|tagging_loss_fusion: 8.050|total_loss: 47.918 | 69.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3140 |tagging_loss_video: 7.414|tagging_loss_audio: 12.285|tagging_loss_text: 13.560|tagging_loss_image: 8.588|tagging_loss_fusion: 6.174|total_loss: 48.022 | Examples/sec: 65.67\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.76 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3141 | tagging_loss_video: 7.149|tagging_loss_audio: 11.028|tagging_loss_text: 16.192|tagging_loss_image: 8.533|tagging_loss_fusion: 7.422|total_loss: 50.324 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 3142 | tagging_loss_video: 7.236|tagging_loss_audio: 12.472|tagging_loss_text: 13.527|tagging_loss_image: 9.351|tagging_loss_fusion: 7.162|total_loss: 49.748 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 3143 | tagging_loss_video: 6.359|tagging_loss_audio: 13.615|tagging_loss_text: 18.574|tagging_loss_image: 8.768|tagging_loss_fusion: 6.159|total_loss: 53.475 | 65.77 Examples/sec\n",
      "INFO:tensorflow:training step 3144 | tagging_loss_video: 8.337|tagging_loss_audio: 10.932|tagging_loss_text: 15.190|tagging_loss_image: 9.117|tagging_loss_fusion: 7.723|total_loss: 51.299 | 68.25 Examples/sec\n",
      "INFO:tensorflow:training step 3145 | tagging_loss_video: 7.602|tagging_loss_audio: 10.153|tagging_loss_text: 15.365|tagging_loss_image: 8.396|tagging_loss_fusion: 7.505|total_loss: 49.021 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3146 | tagging_loss_video: 7.186|tagging_loss_audio: 12.146|tagging_loss_text: 13.527|tagging_loss_image: 7.757|tagging_loss_fusion: 6.412|total_loss: 47.028 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 3147 | tagging_loss_video: 7.538|tagging_loss_audio: 13.452|tagging_loss_text: 17.666|tagging_loss_image: 8.017|tagging_loss_fusion: 7.768|total_loss: 54.440 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 3148 | tagging_loss_video: 8.145|tagging_loss_audio: 12.370|tagging_loss_text: 16.747|tagging_loss_image: 10.565|tagging_loss_fusion: 10.463|total_loss: 58.290 | 62.87 Examples/sec\n",
      "INFO:tensorflow:training step 3149 | tagging_loss_video: 7.942|tagging_loss_audio: 10.285|tagging_loss_text: 12.241|tagging_loss_image: 8.178|tagging_loss_fusion: 11.181|total_loss: 49.827 | 69.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3150 |tagging_loss_video: 8.098|tagging_loss_audio: 14.335|tagging_loss_text: 18.212|tagging_loss_image: 9.644|tagging_loss_fusion: 6.653|total_loss: 56.941 | Examples/sec: 68.78\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3151 | tagging_loss_video: 6.597|tagging_loss_audio: 11.543|tagging_loss_text: 16.292|tagging_loss_image: 8.718|tagging_loss_fusion: 7.821|total_loss: 50.973 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 3152 | tagging_loss_video: 7.754|tagging_loss_audio: 11.540|tagging_loss_text: 16.089|tagging_loss_image: 7.508|tagging_loss_fusion: 6.838|total_loss: 49.729 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 3153 | tagging_loss_video: 6.942|tagging_loss_audio: 12.294|tagging_loss_text: 14.904|tagging_loss_image: 8.972|tagging_loss_fusion: 7.351|total_loss: 50.463 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 3154 | tagging_loss_video: 7.075|tagging_loss_audio: 12.476|tagging_loss_text: 12.709|tagging_loss_image: 8.840|tagging_loss_fusion: 6.224|total_loss: 47.324 | 60.87 Examples/sec\n",
      "INFO:tensorflow:training step 3155 | tagging_loss_video: 7.301|tagging_loss_audio: 11.573|tagging_loss_text: 14.014|tagging_loss_image: 8.883|tagging_loss_fusion: 6.306|total_loss: 48.076 | 69.43 Examples/sec\n",
      "INFO:tensorflow:training step 3156 | tagging_loss_video: 8.151|tagging_loss_audio: 12.052|tagging_loss_text: 12.352|tagging_loss_image: 8.577|tagging_loss_fusion: 8.121|total_loss: 49.253 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 3157 | tagging_loss_video: 7.039|tagging_loss_audio: 9.717|tagging_loss_text: 15.472|tagging_loss_image: 7.954|tagging_loss_fusion: 8.071|total_loss: 48.251 | 63.04 Examples/sec\n",
      "INFO:tensorflow:training step 3158 | tagging_loss_video: 6.599|tagging_loss_audio: 12.348|tagging_loss_text: 16.138|tagging_loss_image: 7.934|tagging_loss_fusion: 6.644|total_loss: 49.663 | 67.61 Examples/sec\n",
      "INFO:tensorflow:training step 3159 | tagging_loss_video: 7.327|tagging_loss_audio: 12.104|tagging_loss_text: 17.913|tagging_loss_image: 9.005|tagging_loss_fusion: 7.039|total_loss: 53.387 | 70.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3160 |tagging_loss_video: 7.284|tagging_loss_audio: 11.411|tagging_loss_text: 16.883|tagging_loss_image: 9.333|tagging_loss_fusion: 7.750|total_loss: 52.661 | Examples/sec: 70.48\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.69 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3161 | tagging_loss_video: 7.551|tagging_loss_audio: 11.709|tagging_loss_text: 16.501|tagging_loss_image: 9.847|tagging_loss_fusion: 8.463|total_loss: 54.071 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 3162 | tagging_loss_video: 7.153|tagging_loss_audio: 11.193|tagging_loss_text: 13.257|tagging_loss_image: 7.646|tagging_loss_fusion: 6.863|total_loss: 46.111 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 3163 | tagging_loss_video: 8.026|tagging_loss_audio: 10.084|tagging_loss_text: 16.122|tagging_loss_image: 8.075|tagging_loss_fusion: 8.759|total_loss: 51.066 | 61.41 Examples/sec\n",
      "INFO:tensorflow:training step 3164 | tagging_loss_video: 6.203|tagging_loss_audio: 8.946|tagging_loss_text: 15.497|tagging_loss_image: 7.537|tagging_loss_fusion: 6.075|total_loss: 44.257 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 3165 | tagging_loss_video: 6.422|tagging_loss_audio: 11.331|tagging_loss_text: 17.171|tagging_loss_image: 8.284|tagging_loss_fusion: 6.057|total_loss: 49.265 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 3166 | tagging_loss_video: 5.815|tagging_loss_audio: 11.996|tagging_loss_text: 18.503|tagging_loss_image: 7.320|tagging_loss_fusion: 5.967|total_loss: 49.600 | 63.83 Examples/sec\n",
      "INFO:tensorflow:training step 3167 | tagging_loss_video: 5.250|tagging_loss_audio: 8.728|tagging_loss_text: 12.530|tagging_loss_image: 6.589|tagging_loss_fusion: 3.369|total_loss: 36.465 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 3168 | tagging_loss_video: 6.895|tagging_loss_audio: 10.142|tagging_loss_text: 17.398|tagging_loss_image: 7.444|tagging_loss_fusion: 6.248|total_loss: 48.128 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 3169 | tagging_loss_video: 5.322|tagging_loss_audio: 10.288|tagging_loss_text: 10.996|tagging_loss_image: 7.193|tagging_loss_fusion: 5.123|total_loss: 38.921 | 64.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3170 |tagging_loss_video: 7.303|tagging_loss_audio: 9.999|tagging_loss_text: 15.214|tagging_loss_image: 7.238|tagging_loss_fusion: 5.990|total_loss: 45.743 | Examples/sec: 65.38\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.96 |recall@0.1: 0.95 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3171 | tagging_loss_video: 5.786|tagging_loss_audio: 10.667|tagging_loss_text: 14.457|tagging_loss_image: 6.593|tagging_loss_fusion: 5.783|total_loss: 43.286 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 3172 | tagging_loss_video: 6.044|tagging_loss_audio: 11.421|tagging_loss_text: 11.493|tagging_loss_image: 6.839|tagging_loss_fusion: 6.986|total_loss: 42.782 | 66.54 Examples/sec\n",
      "INFO:tensorflow:training step 3173 | tagging_loss_video: 6.265|tagging_loss_audio: 11.718|tagging_loss_text: 14.623|tagging_loss_image: 7.780|tagging_loss_fusion: 6.593|total_loss: 46.978 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 3174 | tagging_loss_video: 5.965|tagging_loss_audio: 11.236|tagging_loss_text: 13.577|tagging_loss_image: 7.616|tagging_loss_fusion: 5.515|total_loss: 43.909 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 3175 | tagging_loss_video: 6.340|tagging_loss_audio: 11.547|tagging_loss_text: 14.094|tagging_loss_image: 7.944|tagging_loss_fusion: 5.938|total_loss: 45.863 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3176 | tagging_loss_video: 5.812|tagging_loss_audio: 11.475|tagging_loss_text: 13.424|tagging_loss_image: 7.960|tagging_loss_fusion: 4.967|total_loss: 43.639 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 3177 | tagging_loss_video: 7.647|tagging_loss_audio: 10.383|tagging_loss_text: 15.840|tagging_loss_image: 8.431|tagging_loss_fusion: 9.855|total_loss: 52.155 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 3178 | tagging_loss_video: 5.308|tagging_loss_audio: 9.840|tagging_loss_text: 13.362|tagging_loss_image: 7.005|tagging_loss_fusion: 5.188|total_loss: 40.703 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 3179 | tagging_loss_video: 5.047|tagging_loss_audio: 10.263|tagging_loss_text: 15.635|tagging_loss_image: 6.215|tagging_loss_fusion: 3.898|total_loss: 41.057 | 70.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3180 |tagging_loss_video: 6.031|tagging_loss_audio: 8.304|tagging_loss_text: 12.988|tagging_loss_image: 5.784|tagging_loss_fusion: 5.640|total_loss: 38.747 | Examples/sec: 65.31\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3181 | tagging_loss_video: 7.333|tagging_loss_audio: 9.917|tagging_loss_text: 17.826|tagging_loss_image: 7.679|tagging_loss_fusion: 7.486|total_loss: 50.240 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3182 | tagging_loss_video: 6.526|tagging_loss_audio: 9.626|tagging_loss_text: 12.174|tagging_loss_image: 8.007|tagging_loss_fusion: 6.623|total_loss: 42.956 | 66.43 Examples/sec\n",
      "INFO:tensorflow:training step 3183 | tagging_loss_video: 6.571|tagging_loss_audio: 10.404|tagging_loss_text: 13.252|tagging_loss_image: 8.374|tagging_loss_fusion: 8.276|total_loss: 46.877 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 3184 | tagging_loss_video: 5.332|tagging_loss_audio: 10.079|tagging_loss_text: 11.635|tagging_loss_image: 7.458|tagging_loss_fusion: 4.931|total_loss: 39.436 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 3185 | tagging_loss_video: 7.967|tagging_loss_audio: 9.970|tagging_loss_text: 13.516|tagging_loss_image: 7.725|tagging_loss_fusion: 8.274|total_loss: 47.451 | 65.43 Examples/sec\n",
      "INFO:tensorflow:training step 3186 | tagging_loss_video: 6.702|tagging_loss_audio: 10.809|tagging_loss_text: 15.765|tagging_loss_image: 8.203|tagging_loss_fusion: 6.159|total_loss: 47.638 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 3187 | tagging_loss_video: 6.443|tagging_loss_audio: 11.518|tagging_loss_text: 17.442|tagging_loss_image: 7.556|tagging_loss_fusion: 5.728|total_loss: 48.687 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 3188 | tagging_loss_video: 6.785|tagging_loss_audio: 8.484|tagging_loss_text: 15.697|tagging_loss_image: 7.618|tagging_loss_fusion: 7.763|total_loss: 46.345 | 60.25 Examples/sec\n",
      "INFO:tensorflow:training step 3189 | tagging_loss_video: 6.028|tagging_loss_audio: 9.927|tagging_loss_text: 18.029|tagging_loss_image: 7.768|tagging_loss_fusion: 6.660|total_loss: 48.412 | 71.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3190 |tagging_loss_video: 5.207|tagging_loss_audio: 10.517|tagging_loss_text: 13.253|tagging_loss_image: 7.618|tagging_loss_fusion: 5.033|total_loss: 41.628 | Examples/sec: 68.33\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 3191 | tagging_loss_video: 7.313|tagging_loss_audio: 12.814|tagging_loss_text: 14.466|tagging_loss_image: 7.439|tagging_loss_fusion: 6.463|total_loss: 48.495 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 3192 | tagging_loss_video: 7.009|tagging_loss_audio: 11.426|tagging_loss_text: 19.396|tagging_loss_image: 8.582|tagging_loss_fusion: 6.453|total_loss: 52.866 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3193 | tagging_loss_video: 6.146|tagging_loss_audio: 11.245|tagging_loss_text: 17.692|tagging_loss_image: 7.613|tagging_loss_fusion: 5.590|total_loss: 48.287 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 3194 | tagging_loss_video: 7.194|tagging_loss_audio: 11.370|tagging_loss_text: 13.813|tagging_loss_image: 7.430|tagging_loss_fusion: 7.411|total_loss: 47.218 | 65.17 Examples/sec\n",
      "INFO:tensorflow:training step 3195 | tagging_loss_video: 6.213|tagging_loss_audio: 10.825|tagging_loss_text: 10.384|tagging_loss_image: 7.566|tagging_loss_fusion: 6.634|total_loss: 41.622 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 3196 | tagging_loss_video: 7.847|tagging_loss_audio: 11.438|tagging_loss_text: 18.829|tagging_loss_image: 7.474|tagging_loss_fusion: 7.066|total_loss: 52.655 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 3197 | tagging_loss_video: 7.141|tagging_loss_audio: 11.181|tagging_loss_text: 19.042|tagging_loss_image: 8.810|tagging_loss_fusion: 5.861|total_loss: 52.035 | 63.64 Examples/sec\n",
      "INFO:tensorflow:training step 3198 | tagging_loss_video: 7.180|tagging_loss_audio: 12.119|tagging_loss_text: 18.696|tagging_loss_image: 9.011|tagging_loss_fusion: 6.849|total_loss: 53.854 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 3199 | tagging_loss_video: 7.039|tagging_loss_audio: 11.747|tagging_loss_text: 14.517|tagging_loss_image: 8.245|tagging_loss_fusion: 6.719|total_loss: 48.267 | 71.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3200 |tagging_loss_video: 6.166|tagging_loss_audio: 11.038|tagging_loss_text: 18.367|tagging_loss_image: 8.294|tagging_loss_fusion: 5.755|total_loss: 49.621 | Examples/sec: 67.83\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.76 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 3201 | tagging_loss_video: 6.236|tagging_loss_audio: 9.615|tagging_loss_text: 13.029|tagging_loss_image: 7.781|tagging_loss_fusion: 6.724|total_loss: 43.386 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 3202 | tagging_loss_video: 6.950|tagging_loss_audio: 11.619|tagging_loss_text: 15.194|tagging_loss_image: 8.732|tagging_loss_fusion: 6.290|total_loss: 48.785 | 61.17 Examples/sec\n",
      "INFO:tensorflow:training step 3203 | tagging_loss_video: 6.947|tagging_loss_audio: 10.851|tagging_loss_text: 18.056|tagging_loss_image: 8.889|tagging_loss_fusion: 5.949|total_loss: 50.692 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3204 | tagging_loss_video: 7.199|tagging_loss_audio: 11.639|tagging_loss_text: 16.733|tagging_loss_image: 8.077|tagging_loss_fusion: 6.092|total_loss: 49.739 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 3205 | tagging_loss_video: 6.094|tagging_loss_audio: 12.418|tagging_loss_text: 17.465|tagging_loss_image: 8.440|tagging_loss_fusion: 6.408|total_loss: 50.824 | 62.14 Examples/sec\n",
      "INFO:tensorflow:training step 3206 | tagging_loss_video: 7.432|tagging_loss_audio: 12.115|tagging_loss_text: 16.817|tagging_loss_image: 8.134|tagging_loss_fusion: 6.624|total_loss: 51.121 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 3207 | tagging_loss_video: 6.918|tagging_loss_audio: 9.689|tagging_loss_text: 10.381|tagging_loss_image: 7.651|tagging_loss_fusion: 6.330|total_loss: 40.968 | 65.93 Examples/sec\n",
      "INFO:tensorflow:training step 3208 | tagging_loss_video: 6.272|tagging_loss_audio: 9.313|tagging_loss_text: 16.736|tagging_loss_image: 6.812|tagging_loss_fusion: 4.522|total_loss: 43.656 | 65.49 Examples/sec\n",
      "INFO:tensorflow:training step 3209 | tagging_loss_video: 6.618|tagging_loss_audio: 11.388|tagging_loss_text: 14.132|tagging_loss_image: 7.529|tagging_loss_fusion: 6.821|total_loss: 46.488 | 70.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3210 |tagging_loss_video: 6.971|tagging_loss_audio: 10.659|tagging_loss_text: 14.388|tagging_loss_image: 7.967|tagging_loss_fusion: 6.837|total_loss: 46.823 | Examples/sec: 71.21\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3211 | tagging_loss_video: 6.757|tagging_loss_audio: 9.232|tagging_loss_text: 13.380|tagging_loss_image: 6.958|tagging_loss_fusion: 6.177|total_loss: 42.503 | 66.35 Examples/sec\n",
      "INFO:tensorflow:training step 3212 | tagging_loss_video: 7.567|tagging_loss_audio: 12.588|tagging_loss_text: 14.834|tagging_loss_image: 9.490|tagging_loss_fusion: 6.980|total_loss: 51.461 | 66.56 Examples/sec\n",
      "INFO:tensorflow:training step 3213 | tagging_loss_video: 8.682|tagging_loss_audio: 12.117|tagging_loss_text: 14.852|tagging_loss_image: 8.220|tagging_loss_fusion: 7.806|total_loss: 51.678 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 3214 | tagging_loss_video: 6.759|tagging_loss_audio: 10.019|tagging_loss_text: 13.814|tagging_loss_image: 7.271|tagging_loss_fusion: 6.988|total_loss: 44.851 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 3215 | tagging_loss_video: 6.861|tagging_loss_audio: 10.250|tagging_loss_text: 14.336|tagging_loss_image: 6.893|tagging_loss_fusion: 7.469|total_loss: 45.809 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 3216 | tagging_loss_video: 5.679|tagging_loss_audio: 9.970|tagging_loss_text: 15.489|tagging_loss_image: 6.597|tagging_loss_fusion: 5.005|total_loss: 42.740 | 64.87 Examples/sec\n",
      "INFO:tensorflow:training step 3217 | tagging_loss_video: 7.065|tagging_loss_audio: 11.964|tagging_loss_text: 16.845|tagging_loss_image: 8.667|tagging_loss_fusion: 8.394|total_loss: 52.935 | 68.85 Examples/sec\n",
      "INFO:tensorflow:training step 3218 | tagging_loss_video: 7.129|tagging_loss_audio: 9.776|tagging_loss_text: 13.671|tagging_loss_image: 6.992|tagging_loss_fusion: 7.365|total_loss: 44.932 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 3219 | tagging_loss_video: 6.173|tagging_loss_audio: 9.149|tagging_loss_text: 15.862|tagging_loss_image: 7.488|tagging_loss_fusion: 5.083|total_loss: 43.754 | 61.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3220 |tagging_loss_video: 5.769|tagging_loss_audio: 11.199|tagging_loss_text: 11.992|tagging_loss_image: 6.741|tagging_loss_fusion: 5.520|total_loss: 41.222 | Examples/sec: 69.32\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3221 | tagging_loss_video: 6.804|tagging_loss_audio: 11.168|tagging_loss_text: 14.246|tagging_loss_image: 6.845|tagging_loss_fusion: 5.725|total_loss: 44.788 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 3222 | tagging_loss_video: 5.387|tagging_loss_audio: 10.347|tagging_loss_text: 16.552|tagging_loss_image: 6.822|tagging_loss_fusion: 4.896|total_loss: 44.004 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 3223 | tagging_loss_video: 5.811|tagging_loss_audio: 8.830|tagging_loss_text: 16.344|tagging_loss_image: 6.620|tagging_loss_fusion: 6.515|total_loss: 44.121 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 3224 | tagging_loss_video: 5.425|tagging_loss_audio: 10.875|tagging_loss_text: 19.583|tagging_loss_image: 7.147|tagging_loss_fusion: 5.545|total_loss: 48.574 | 65.75 Examples/sec\n",
      "INFO:tensorflow:training step 3225 | tagging_loss_video: 7.301|tagging_loss_audio: 10.643|tagging_loss_text: 10.713|tagging_loss_image: 7.522|tagging_loss_fusion: 8.546|total_loss: 44.725 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 3226 | tagging_loss_video: 6.386|tagging_loss_audio: 8.631|tagging_loss_text: 12.599|tagging_loss_image: 7.531|tagging_loss_fusion: 7.513|total_loss: 42.660 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 3227 | tagging_loss_video: 6.969|tagging_loss_audio: 10.233|tagging_loss_text: 12.922|tagging_loss_image: 7.331|tagging_loss_fusion: 6.236|total_loss: 43.691 | 61.34 Examples/sec\n",
      "INFO:tensorflow:training step 3228 | tagging_loss_video: 6.598|tagging_loss_audio: 10.456|tagging_loss_text: 16.020|tagging_loss_image: 7.022|tagging_loss_fusion: 5.836|total_loss: 45.932 | 66.90 Examples/sec\n",
      "INFO:tensorflow:training step 3229 | tagging_loss_video: 5.508|tagging_loss_audio: 11.044|tagging_loss_text: 16.479|tagging_loss_image: 7.483|tagging_loss_fusion: 5.010|total_loss: 45.525 | 69.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3230 |tagging_loss_video: 7.080|tagging_loss_audio: 10.262|tagging_loss_text: 17.598|tagging_loss_image: 7.072|tagging_loss_fusion: 7.171|total_loss: 49.183 | Examples/sec: 63.23\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.75 | precision@0.5: 0.94 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 3231 | tagging_loss_video: 7.667|tagging_loss_audio: 11.830|tagging_loss_text: 15.398|tagging_loss_image: 8.448|tagging_loss_fusion: 7.806|total_loss: 51.150 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 3232 | tagging_loss_video: 6.755|tagging_loss_audio: 11.010|tagging_loss_text: 16.851|tagging_loss_image: 9.633|tagging_loss_fusion: 5.690|total_loss: 49.938 | 70.06 Examples/sec\n",
      "INFO:tensorflow:training step 3233 | tagging_loss_video: 6.159|tagging_loss_audio: 11.042|tagging_loss_text: 16.156|tagging_loss_image: 8.046|tagging_loss_fusion: 6.497|total_loss: 47.900 | 71.87 Examples/sec\n",
      "INFO:tensorflow:training step 3234 | tagging_loss_video: 6.828|tagging_loss_audio: 11.035|tagging_loss_text: 17.228|tagging_loss_image: 7.837|tagging_loss_fusion: 5.407|total_loss: 48.335 | 66.82 Examples/sec\n",
      "INFO:tensorflow:training step 3235 | tagging_loss_video: 7.493|tagging_loss_audio: 11.272|tagging_loss_text: 16.451|tagging_loss_image: 8.294|tagging_loss_fusion: 9.244|total_loss: 52.754 | 72.30 Examples/sec\n",
      "INFO:tensorflow:training step 3236 | tagging_loss_video: 6.074|tagging_loss_audio: 10.703|tagging_loss_text: 14.739|tagging_loss_image: 6.928|tagging_loss_fusion: 4.859|total_loss: 43.303 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 3237 | tagging_loss_video: 7.075|tagging_loss_audio: 11.971|tagging_loss_text: 19.416|tagging_loss_image: 7.451|tagging_loss_fusion: 7.030|total_loss: 52.943 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 3238 | tagging_loss_video: 6.011|tagging_loss_audio: 9.789|tagging_loss_text: 15.188|tagging_loss_image: 6.962|tagging_loss_fusion: 4.619|total_loss: 42.569 | 60.74 Examples/sec\n",
      "INFO:tensorflow:training step 3239 | tagging_loss_video: 7.024|tagging_loss_audio: 10.439|tagging_loss_text: 17.905|tagging_loss_image: 7.627|tagging_loss_fusion: 7.256|total_loss: 50.251 | 71.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3240 |tagging_loss_video: 6.630|tagging_loss_audio: 10.009|tagging_loss_text: 19.491|tagging_loss_image: 7.072|tagging_loss_fusion: 6.754|total_loss: 49.956 | Examples/sec: 70.05\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.74 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3241 | tagging_loss_video: 5.888|tagging_loss_audio: 9.127|tagging_loss_text: 14.874|tagging_loss_image: 6.928|tagging_loss_fusion: 8.632|total_loss: 45.449 | 66.65 Examples/sec\n",
      "INFO:tensorflow:training step 3242 | tagging_loss_video: 5.785|tagging_loss_audio: 9.869|tagging_loss_text: 13.522|tagging_loss_image: 6.375|tagging_loss_fusion: 3.635|total_loss: 39.187 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 3243 | tagging_loss_video: 6.898|tagging_loss_audio: 9.844|tagging_loss_text: 13.184|tagging_loss_image: 7.344|tagging_loss_fusion: 8.586|total_loss: 45.856 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 3244 | tagging_loss_video: 6.220|tagging_loss_audio: 11.693|tagging_loss_text: 17.531|tagging_loss_image: 7.332|tagging_loss_fusion: 9.105|total_loss: 51.881 | 64.82 Examples/sec\n",
      "INFO:tensorflow:training step 3245 | tagging_loss_video: 7.124|tagging_loss_audio: 10.761|tagging_loss_text: 17.062|tagging_loss_image: 9.242|tagging_loss_fusion: 7.177|total_loss: 51.367 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 3246 | tagging_loss_video: 6.394|tagging_loss_audio: 11.708|tagging_loss_text: 12.809|tagging_loss_image: 7.031|tagging_loss_fusion: 5.372|total_loss: 43.314 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 3247 | tagging_loss_video: 6.219|tagging_loss_audio: 10.783|tagging_loss_text: 17.753|tagging_loss_image: 7.101|tagging_loss_fusion: 5.900|total_loss: 47.756 | 63.23 Examples/sec\n",
      "INFO:tensorflow:training step 3248 | tagging_loss_video: 6.997|tagging_loss_audio: 9.843|tagging_loss_text: 14.171|tagging_loss_image: 7.774|tagging_loss_fusion: 9.575|total_loss: 48.359 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 3249 | tagging_loss_video: 6.291|tagging_loss_audio: 10.050|tagging_loss_text: 13.603|tagging_loss_image: 7.467|tagging_loss_fusion: 5.011|total_loss: 42.422 | 70.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3250 |tagging_loss_video: 6.630|tagging_loss_audio: 10.967|tagging_loss_text: 17.321|tagging_loss_image: 7.180|tagging_loss_fusion: 5.959|total_loss: 48.057 | Examples/sec: 70.39\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.76 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3251 | tagging_loss_video: 5.950|tagging_loss_audio: 9.902|tagging_loss_text: 14.747|tagging_loss_image: 6.081|tagging_loss_fusion: 5.534|total_loss: 42.215 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 3252 | tagging_loss_video: 7.349|tagging_loss_audio: 11.001|tagging_loss_text: 14.372|tagging_loss_image: 7.780|tagging_loss_fusion: 6.596|total_loss: 47.099 | 64.71 Examples/sec\n",
      "INFO:tensorflow:training step 3253 | tagging_loss_video: 5.912|tagging_loss_audio: 10.142|tagging_loss_text: 14.853|tagging_loss_image: 6.082|tagging_loss_fusion: 4.866|total_loss: 41.856 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 3254 | tagging_loss_video: 6.962|tagging_loss_audio: 10.963|tagging_loss_text: 14.420|tagging_loss_image: 7.005|tagging_loss_fusion: 8.478|total_loss: 47.829 | 66.51 Examples/sec\n",
      "INFO:tensorflow:training step 3255 | tagging_loss_video: 6.140|tagging_loss_audio: 9.684|tagging_loss_text: 14.297|tagging_loss_image: 7.799|tagging_loss_fusion: 5.618|total_loss: 43.538 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 3256 | tagging_loss_video: 5.912|tagging_loss_audio: 9.600|tagging_loss_text: 15.322|tagging_loss_image: 7.626|tagging_loss_fusion: 4.872|total_loss: 43.332 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 3257 | tagging_loss_video: 6.106|tagging_loss_audio: 9.034|tagging_loss_text: 13.807|tagging_loss_image: 7.111|tagging_loss_fusion: 5.945|total_loss: 42.003 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 3258 | tagging_loss_video: 6.641|tagging_loss_audio: 9.829|tagging_loss_text: 13.268|tagging_loss_image: 7.661|tagging_loss_fusion: 5.482|total_loss: 42.880 | 64.17 Examples/sec\n",
      "INFO:tensorflow:training step 3259 | tagging_loss_video: 7.550|tagging_loss_audio: 10.586|tagging_loss_text: 16.297|tagging_loss_image: 7.627|tagging_loss_fusion: 7.543|total_loss: 49.602 | 69.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3260 |tagging_loss_video: 6.147|tagging_loss_audio: 10.623|tagging_loss_text: 16.599|tagging_loss_image: 7.527|tagging_loss_fusion: 5.505|total_loss: 46.401 | Examples/sec: 67.24\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3261 | tagging_loss_video: 6.424|tagging_loss_audio: 10.227|tagging_loss_text: 14.784|tagging_loss_image: 7.484|tagging_loss_fusion: 4.926|total_loss: 43.845 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 3262 | tagging_loss_video: 6.835|tagging_loss_audio: 9.981|tagging_loss_text: 16.961|tagging_loss_image: 7.821|tagging_loss_fusion: 7.512|total_loss: 49.110 | 68.14 Examples/sec\n",
      "INFO:tensorflow:training step 3263 | tagging_loss_video: 6.929|tagging_loss_audio: 10.547|tagging_loss_text: 18.213|tagging_loss_image: 8.029|tagging_loss_fusion: 8.029|total_loss: 51.748 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 3264 | tagging_loss_video: 6.036|tagging_loss_audio: 9.290|tagging_loss_text: 15.546|tagging_loss_image: 8.188|tagging_loss_fusion: 4.930|total_loss: 43.990 | 67.53 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3265 | tagging_loss_video: 7.285|tagging_loss_audio: 10.393|tagging_loss_text: 16.040|tagging_loss_image: 7.939|tagging_loss_fusion: 7.978|total_loss: 49.635 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 3266 | tagging_loss_video: 6.242|tagging_loss_audio: 10.765|tagging_loss_text: 11.763|tagging_loss_image: 7.503|tagging_loss_fusion: 5.182|total_loss: 41.455 | 62.36 Examples/sec\n",
      "INFO:tensorflow:training step 3267 | tagging_loss_video: 7.299|tagging_loss_audio: 10.697|tagging_loss_text: 13.196|tagging_loss_image: 7.862|tagging_loss_fusion: 8.870|total_loss: 47.922 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3268 | tagging_loss_video: 6.882|tagging_loss_audio: 10.133|tagging_loss_text: 17.450|tagging_loss_image: 7.869|tagging_loss_fusion: 7.842|total_loss: 50.176 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 3269 | tagging_loss_video: 7.769|tagging_loss_audio: 11.039|tagging_loss_text: 18.378|tagging_loss_image: 9.435|tagging_loss_fusion: 7.950|total_loss: 54.571 | 64.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3270 |tagging_loss_video: 6.698|tagging_loss_audio: 10.522|tagging_loss_text: 14.885|tagging_loss_image: 7.062|tagging_loss_fusion: 6.718|total_loss: 45.884 | Examples/sec: 69.18\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.74 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3271 | tagging_loss_video: 7.057|tagging_loss_audio: 11.135|tagging_loss_text: 13.783|tagging_loss_image: 8.100|tagging_loss_fusion: 7.975|total_loss: 48.051 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 3272 | tagging_loss_video: 7.332|tagging_loss_audio: 8.563|tagging_loss_text: 13.485|tagging_loss_image: 7.019|tagging_loss_fusion: 6.583|total_loss: 42.981 | 64.79 Examples/sec\n",
      "INFO:tensorflow:training step 3273 | tagging_loss_video: 7.266|tagging_loss_audio: 11.259|tagging_loss_text: 15.388|tagging_loss_image: 8.315|tagging_loss_fusion: 6.736|total_loss: 48.963 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 3274 | tagging_loss_video: 6.890|tagging_loss_audio: 8.001|tagging_loss_text: 16.984|tagging_loss_image: 6.748|tagging_loss_fusion: 4.875|total_loss: 43.498 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 3275 | tagging_loss_video: 7.085|tagging_loss_audio: 10.216|tagging_loss_text: 17.770|tagging_loss_image: 8.561|tagging_loss_fusion: 7.796|total_loss: 51.428 | 65.89 Examples/sec\n",
      "INFO:tensorflow:training step 3276 | tagging_loss_video: 7.503|tagging_loss_audio: 10.663|tagging_loss_text: 17.109|tagging_loss_image: 8.318|tagging_loss_fusion: 9.299|total_loss: 52.893 | 67.10 Examples/sec\n",
      "INFO:tensorflow:training step 3277 | tagging_loss_video: 6.851|tagging_loss_audio: 10.553|tagging_loss_text: 16.689|tagging_loss_image: 8.585|tagging_loss_fusion: 8.895|total_loss: 51.573 | 64.86 Examples/sec\n",
      "INFO:tensorflow:training step 3278 | tagging_loss_video: 6.892|tagging_loss_audio: 10.749|tagging_loss_text: 13.379|tagging_loss_image: 8.031|tagging_loss_fusion: 8.694|total_loss: 47.745 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 3279 | tagging_loss_video: 8.280|tagging_loss_audio: 12.435|tagging_loss_text: 19.064|tagging_loss_image: 9.349|tagging_loss_fusion: 9.959|total_loss: 59.087 | 70.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3280 |tagging_loss_video: 6.602|tagging_loss_audio: 8.544|tagging_loss_text: 13.837|tagging_loss_image: 6.301|tagging_loss_fusion: 5.279|total_loss: 40.563 | Examples/sec: 62.23\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.78 | precision@0.5: 0.90 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 3281 | tagging_loss_video: 6.124|tagging_loss_audio: 9.100|tagging_loss_text: 16.955|tagging_loss_image: 6.315|tagging_loss_fusion: 6.870|total_loss: 45.364 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 3282 | tagging_loss_video: 6.874|tagging_loss_audio: 9.975|tagging_loss_text: 15.460|tagging_loss_image: 7.578|tagging_loss_fusion: 5.765|total_loss: 45.653 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 3283 | tagging_loss_video: 6.322|tagging_loss_audio: 8.974|tagging_loss_text: 13.407|tagging_loss_image: 6.101|tagging_loss_fusion: 5.291|total_loss: 40.096 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 3284 | tagging_loss_video: 6.209|tagging_loss_audio: 8.844|tagging_loss_text: 16.883|tagging_loss_image: 7.911|tagging_loss_fusion: 5.392|total_loss: 45.240 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 3285 | tagging_loss_video: 6.261|tagging_loss_audio: 9.480|tagging_loss_text: 13.179|tagging_loss_image: 7.204|tagging_loss_fusion: 6.003|total_loss: 42.127 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 3286 | tagging_loss_video: 5.960|tagging_loss_audio: 10.233|tagging_loss_text: 12.648|tagging_loss_image: 6.754|tagging_loss_fusion: 5.521|total_loss: 41.116 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 3287 | tagging_loss_video: 6.211|tagging_loss_audio: 9.088|tagging_loss_text: 12.148|tagging_loss_image: 6.623|tagging_loss_fusion: 5.510|total_loss: 39.579 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 3288 | tagging_loss_video: 6.363|tagging_loss_audio: 10.679|tagging_loss_text: 17.059|tagging_loss_image: 7.626|tagging_loss_fusion: 5.877|total_loss: 47.603 | 63.36 Examples/sec\n",
      "INFO:tensorflow:training step 3289 | tagging_loss_video: 6.466|tagging_loss_audio: 9.260|tagging_loss_text: 16.440|tagging_loss_image: 6.464|tagging_loss_fusion: 5.264|total_loss: 43.893 | 69.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3290 |tagging_loss_video: 6.193|tagging_loss_audio: 10.898|tagging_loss_text: 10.545|tagging_loss_image: 6.317|tagging_loss_fusion: 6.734|total_loss: 40.687 | Examples/sec: 70.38\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3291 | tagging_loss_video: 5.914|tagging_loss_audio: 8.529|tagging_loss_text: 13.603|tagging_loss_image: 6.766|tagging_loss_fusion: 5.607|total_loss: 40.420 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 3292 | tagging_loss_video: 6.367|tagging_loss_audio: 9.340|tagging_loss_text: 17.331|tagging_loss_image: 6.844|tagging_loss_fusion: 8.555|total_loss: 48.437 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 3293 | tagging_loss_video: 7.941|tagging_loss_audio: 10.015|tagging_loss_text: 17.572|tagging_loss_image: 7.025|tagging_loss_fusion: 9.511|total_loss: 52.063 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 3294 | tagging_loss_video: 5.956|tagging_loss_audio: 10.009|tagging_loss_text: 14.804|tagging_loss_image: 7.044|tagging_loss_fusion: 6.138|total_loss: 43.949 | 61.77 Examples/sec\n",
      "INFO:tensorflow:training step 3295 | tagging_loss_video: 6.876|tagging_loss_audio: 10.029|tagging_loss_text: 14.098|tagging_loss_image: 7.351|tagging_loss_fusion: 8.063|total_loss: 46.417 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 3296 | tagging_loss_video: 6.682|tagging_loss_audio: 9.406|tagging_loss_text: 14.513|tagging_loss_image: 7.433|tagging_loss_fusion: 6.889|total_loss: 44.923 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 3297 | tagging_loss_video: 6.012|tagging_loss_audio: 9.181|tagging_loss_text: 13.044|tagging_loss_image: 7.063|tagging_loss_fusion: 8.350|total_loss: 43.650 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 3298 | tagging_loss_video: 6.580|tagging_loss_audio: 10.378|tagging_loss_text: 13.031|tagging_loss_image: 7.597|tagging_loss_fusion: 7.847|total_loss: 45.432 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 3299 | tagging_loss_video: 6.434|tagging_loss_audio: 9.432|tagging_loss_text: 13.856|tagging_loss_image: 7.162|tagging_loss_fusion: 5.988|total_loss: 42.871 | 71.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3300 |tagging_loss_video: 6.423|tagging_loss_audio: 10.332|tagging_loss_text: 12.682|tagging_loss_image: 7.258|tagging_loss_fusion: 5.493|total_loss: 42.189 | Examples/sec: 71.22\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.90 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3301 | tagging_loss_video: 6.252|tagging_loss_audio: 10.110|tagging_loss_text: 15.409|tagging_loss_image: 6.699|tagging_loss_fusion: 5.149|total_loss: 43.618 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 3302 | tagging_loss_video: 6.671|tagging_loss_audio: 10.068|tagging_loss_text: 14.675|tagging_loss_image: 7.086|tagging_loss_fusion: 4.279|total_loss: 42.780 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 3303 | tagging_loss_video: 6.816|tagging_loss_audio: 10.808|tagging_loss_text: 16.210|tagging_loss_image: 7.336|tagging_loss_fusion: 6.936|total_loss: 48.105 | 60.03 Examples/sec\n",
      "INFO:tensorflow:training step 3304 | tagging_loss_video: 4.992|tagging_loss_audio: 9.164|tagging_loss_text: 18.351|tagging_loss_image: 7.530|tagging_loss_fusion: 4.451|total_loss: 44.487 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 3305 | tagging_loss_video: 6.866|tagging_loss_audio: 12.020|tagging_loss_text: 15.020|tagging_loss_image: 9.645|tagging_loss_fusion: 9.036|total_loss: 52.587 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 3306 | tagging_loss_video: 6.457|tagging_loss_audio: 9.659|tagging_loss_text: 13.304|tagging_loss_image: 6.371|tagging_loss_fusion: 5.516|total_loss: 41.307 | 61.49 Examples/sec\n",
      "INFO:tensorflow:training step 3307 | tagging_loss_video: 5.157|tagging_loss_audio: 8.929|tagging_loss_text: 15.467|tagging_loss_image: 5.877|tagging_loss_fusion: 4.543|total_loss: 39.972 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 3308 | tagging_loss_video: 5.332|tagging_loss_audio: 10.688|tagging_loss_text: 15.336|tagging_loss_image: 7.026|tagging_loss_fusion: 4.550|total_loss: 42.932 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 3309 | tagging_loss_video: 5.716|tagging_loss_audio: 9.469|tagging_loss_text: 14.170|tagging_loss_image: 6.988|tagging_loss_fusion: 5.871|total_loss: 42.214 | 68.32 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3310 |tagging_loss_video: 6.681|tagging_loss_audio: 10.907|tagging_loss_text: 14.504|tagging_loss_image: 7.531|tagging_loss_fusion: 7.733|total_loss: 47.356 | Examples/sec: 70.80\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.77 | precision@0.5: 0.90 |recall@0.1: 0.93 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 3311 | tagging_loss_video: 5.223|tagging_loss_audio: 9.261|tagging_loss_text: 15.194|tagging_loss_image: 8.306|tagging_loss_fusion: 5.469|total_loss: 43.453 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 3312 | tagging_loss_video: 5.812|tagging_loss_audio: 9.630|tagging_loss_text: 16.754|tagging_loss_image: 6.905|tagging_loss_fusion: 6.262|total_loss: 45.363 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 3313 | tagging_loss_video: 6.622|tagging_loss_audio: 10.972|tagging_loss_text: 13.945|tagging_loss_image: 8.457|tagging_loss_fusion: 7.532|total_loss: 47.528 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 3314 | tagging_loss_video: 7.226|tagging_loss_audio: 11.437|tagging_loss_text: 15.632|tagging_loss_image: 7.583|tagging_loss_fusion: 8.939|total_loss: 50.818 | 65.05 Examples/sec\n",
      "INFO:tensorflow:training step 3315 | tagging_loss_video: 7.415|tagging_loss_audio: 9.654|tagging_loss_text: 14.080|tagging_loss_image: 7.361|tagging_loss_fusion: 6.476|total_loss: 44.986 | 67.12 Examples/sec\n",
      "INFO:tensorflow:training step 3316 | tagging_loss_video: 7.008|tagging_loss_audio: 10.780|tagging_loss_text: 20.862|tagging_loss_image: 8.080|tagging_loss_fusion: 8.249|total_loss: 54.979 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 3317 | tagging_loss_video: 6.664|tagging_loss_audio: 9.264|tagging_loss_text: 13.770|tagging_loss_image: 7.568|tagging_loss_fusion: 4.574|total_loss: 41.839 | 66.86 Examples/sec\n",
      "INFO:tensorflow:training step 3318 | tagging_loss_video: 5.815|tagging_loss_audio: 8.940|tagging_loss_text: 16.508|tagging_loss_image: 6.634|tagging_loss_fusion: 5.058|total_loss: 42.955 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 3319 | tagging_loss_video: 5.662|tagging_loss_audio: 8.564|tagging_loss_text: 13.740|tagging_loss_image: 6.687|tagging_loss_fusion: 5.144|total_loss: 39.797 | 71.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3320 |tagging_loss_video: 6.924|tagging_loss_audio: 8.552|tagging_loss_text: 14.953|tagging_loss_image: 6.797|tagging_loss_fusion: 7.122|total_loss: 44.347 | Examples/sec: 61.67\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.72 | precision@0.5: 0.87 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3321 | tagging_loss_video: 6.605|tagging_loss_audio: 11.630|tagging_loss_text: 17.133|tagging_loss_image: 7.404|tagging_loss_fusion: 7.266|total_loss: 50.038 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 3322 | tagging_loss_video: 6.509|tagging_loss_audio: 9.196|tagging_loss_text: 14.175|tagging_loss_image: 6.699|tagging_loss_fusion: 7.635|total_loss: 44.214 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 3323 | tagging_loss_video: 7.258|tagging_loss_audio: 10.442|tagging_loss_text: 15.443|tagging_loss_image: 7.733|tagging_loss_fusion: 6.521|total_loss: 47.398 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 3324 | tagging_loss_video: 6.974|tagging_loss_audio: 11.433|tagging_loss_text: 12.297|tagging_loss_image: 7.326|tagging_loss_fusion: 6.939|total_loss: 44.969 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 3325 | tagging_loss_video: 6.914|tagging_loss_audio: 10.622|tagging_loss_text: 15.642|tagging_loss_image: 7.578|tagging_loss_fusion: 5.875|total_loss: 46.631 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 3326 | tagging_loss_video: 6.650|tagging_loss_audio: 10.938|tagging_loss_text: 17.988|tagging_loss_image: 8.198|tagging_loss_fusion: 7.997|total_loss: 51.772 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 3327 | tagging_loss_video: 6.851|tagging_loss_audio: 11.026|tagging_loss_text: 13.978|tagging_loss_image: 7.644|tagging_loss_fusion: 4.727|total_loss: 44.226 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 3328 | tagging_loss_video: 5.362|tagging_loss_audio: 11.284|tagging_loss_text: 16.356|tagging_loss_image: 7.667|tagging_loss_fusion: 5.716|total_loss: 46.384 | 62.60 Examples/sec\n",
      "INFO:tensorflow:training step 3329 | tagging_loss_video: 6.564|tagging_loss_audio: 9.899|tagging_loss_text: 15.196|tagging_loss_image: 8.210|tagging_loss_fusion: 6.583|total_loss: 46.451 | 71.03 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3330 |tagging_loss_video: 6.223|tagging_loss_audio: 9.990|tagging_loss_text: 15.223|tagging_loss_image: 6.318|tagging_loss_fusion: 5.439|total_loss: 43.193 | Examples/sec: 69.95\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3331 | tagging_loss_video: 7.203|tagging_loss_audio: 9.642|tagging_loss_text: 14.470|tagging_loss_image: 7.337|tagging_loss_fusion: 6.661|total_loss: 45.313 | 62.30 Examples/sec\n",
      "INFO:tensorflow:training step 3332 | tagging_loss_video: 6.621|tagging_loss_audio: 10.167|tagging_loss_text: 16.412|tagging_loss_image: 8.453|tagging_loss_fusion: 6.988|total_loss: 48.641 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 3333 | tagging_loss_video: 6.171|tagging_loss_audio: 9.988|tagging_loss_text: 15.734|tagging_loss_image: 7.336|tagging_loss_fusion: 6.791|total_loss: 46.019 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 3334 | tagging_loss_video: 7.706|tagging_loss_audio: 8.424|tagging_loss_text: 15.191|tagging_loss_image: 8.079|tagging_loss_fusion: 7.603|total_loss: 47.003 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 3335 | tagging_loss_video: 7.339|tagging_loss_audio: 10.198|tagging_loss_text: 15.233|tagging_loss_image: 7.801|tagging_loss_fusion: 6.715|total_loss: 47.287 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 3336 | tagging_loss_video: 6.851|tagging_loss_audio: 11.188|tagging_loss_text: 18.839|tagging_loss_image: 8.469|tagging_loss_fusion: 7.440|total_loss: 52.787 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 3337 | tagging_loss_video: 7.135|tagging_loss_audio: 10.290|tagging_loss_text: 16.909|tagging_loss_image: 7.599|tagging_loss_fusion: 8.460|total_loss: 50.393 | 63.64 Examples/sec\n",
      "INFO:tensorflow:training step 3338 | tagging_loss_video: 7.275|tagging_loss_audio: 11.516|tagging_loss_text: 16.476|tagging_loss_image: 8.610|tagging_loss_fusion: 7.539|total_loss: 51.417 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 3339 | tagging_loss_video: 5.042|tagging_loss_audio: 11.122|tagging_loss_text: 15.163|tagging_loss_image: 7.349|tagging_loss_fusion: 4.134|total_loss: 42.811 | 68.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3340 |tagging_loss_video: 6.058|tagging_loss_audio: 10.695|tagging_loss_text: 16.226|tagging_loss_image: 7.822|tagging_loss_fusion: 6.034|total_loss: 46.835 | Examples/sec: 71.76\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.77 | precision@0.5: 0.90 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3341 | tagging_loss_video: 6.613|tagging_loss_audio: 9.406|tagging_loss_text: 13.646|tagging_loss_image: 6.615|tagging_loss_fusion: 5.238|total_loss: 41.518 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3342 | tagging_loss_video: 7.306|tagging_loss_audio: 9.210|tagging_loss_text: 13.038|tagging_loss_image: 8.164|tagging_loss_fusion: 6.631|total_loss: 44.349 | 62.13 Examples/sec\n",
      "INFO:tensorflow:training step 3343 | tagging_loss_video: 7.451|tagging_loss_audio: 12.668|tagging_loss_text: 13.705|tagging_loss_image: 8.389|tagging_loss_fusion: 6.857|total_loss: 49.069 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 3344 | tagging_loss_video: 7.252|tagging_loss_audio: 10.476|tagging_loss_text: 16.770|tagging_loss_image: 7.956|tagging_loss_fusion: 7.601|total_loss: 50.055 | 70.78 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 3345.\n",
      "INFO:tensorflow:training step 3345 | tagging_loss_video: 6.831|tagging_loss_audio: 11.970|tagging_loss_text: 18.242|tagging_loss_image: 7.541|tagging_loss_fusion: 6.961|total_loss: 51.545 | 49.00 Examples/sec\n",
      "INFO:tensorflow:training step 3346 | tagging_loss_video: 6.810|tagging_loss_audio: 9.046|tagging_loss_text: 15.960|tagging_loss_image: 7.839|tagging_loss_fusion: 5.238|total_loss: 44.894 | 64.39 Examples/sec\n",
      "INFO:tensorflow:training step 3347 | tagging_loss_video: 6.504|tagging_loss_audio: 9.557|tagging_loss_text: 12.680|tagging_loss_image: 6.433|tagging_loss_fusion: 5.855|total_loss: 41.028 | 65.22 Examples/sec\n",
      "INFO:tensorflow:training step 3348 | tagging_loss_video: 6.194|tagging_loss_audio: 9.516|tagging_loss_text: 14.783|tagging_loss_image: 8.588|tagging_loss_fusion: 6.425|total_loss: 45.505 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 3349 | tagging_loss_video: 7.562|tagging_loss_audio: 10.696|tagging_loss_text: 19.232|tagging_loss_image: 7.935|tagging_loss_fusion: 8.177|total_loss: 53.602 | 72.08 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3350 |tagging_loss_video: 6.982|tagging_loss_audio: 11.567|tagging_loss_text: 16.658|tagging_loss_image: 6.851|tagging_loss_fusion: 6.485|total_loss: 48.544 | Examples/sec: 70.28\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 3351 | tagging_loss_video: 7.655|tagging_loss_audio: 12.448|tagging_loss_text: 20.849|tagging_loss_image: 9.490|tagging_loss_fusion: 7.789|total_loss: 58.231 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 3352 | tagging_loss_video: 7.260|tagging_loss_audio: 10.113|tagging_loss_text: 17.751|tagging_loss_image: 8.451|tagging_loss_fusion: 8.209|total_loss: 51.783 | 63.58 Examples/sec\n",
      "INFO:tensorflow:training step 3353 | tagging_loss_video: 6.996|tagging_loss_audio: 10.017|tagging_loss_text: 15.729|tagging_loss_image: 7.342|tagging_loss_fusion: 6.664|total_loss: 46.747 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 3354 | tagging_loss_video: 5.815|tagging_loss_audio: 10.238|tagging_loss_text: 12.300|tagging_loss_image: 7.585|tagging_loss_fusion: 5.567|total_loss: 41.506 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 3355 | tagging_loss_video: 6.033|tagging_loss_audio: 9.648|tagging_loss_text: 10.940|tagging_loss_image: 7.392|tagging_loss_fusion: 6.244|total_loss: 40.258 | 61.30 Examples/sec\n",
      "INFO:tensorflow:training step 3356 | tagging_loss_video: 6.225|tagging_loss_audio: 11.741|tagging_loss_text: 17.651|tagging_loss_image: 7.618|tagging_loss_fusion: 4.685|total_loss: 47.920 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 3357 | tagging_loss_video: 6.634|tagging_loss_audio: 10.402|tagging_loss_text: 13.508|tagging_loss_image: 6.808|tagging_loss_fusion: 5.572|total_loss: 42.924 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 3358 | tagging_loss_video: 5.854|tagging_loss_audio: 9.679|tagging_loss_text: 15.175|tagging_loss_image: 6.923|tagging_loss_fusion: 6.883|total_loss: 44.514 | 67.69 Examples/sec\n",
      "INFO:tensorflow:training step 3359 | tagging_loss_video: 6.553|tagging_loss_audio: 9.273|tagging_loss_text: 14.576|tagging_loss_image: 6.772|tagging_loss_fusion: 6.405|total_loss: 43.580 | 69.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3360 |tagging_loss_video: 6.835|tagging_loss_audio: 10.685|tagging_loss_text: 12.847|tagging_loss_image: 7.232|tagging_loss_fusion: 5.857|total_loss: 43.457 | Examples/sec: 65.90\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.78 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3361 | tagging_loss_video: 5.684|tagging_loss_audio: 7.585|tagging_loss_text: 12.547|tagging_loss_image: 7.211|tagging_loss_fusion: 4.554|total_loss: 37.581 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 3362 | tagging_loss_video: 5.370|tagging_loss_audio: 10.750|tagging_loss_text: 14.562|tagging_loss_image: 6.483|tagging_loss_fusion: 4.654|total_loss: 41.819 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 3363 | tagging_loss_video: 6.012|tagging_loss_audio: 9.183|tagging_loss_text: 13.964|tagging_loss_image: 6.837|tagging_loss_fusion: 5.205|total_loss: 41.200 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 3364 | tagging_loss_video: 6.205|tagging_loss_audio: 11.022|tagging_loss_text: 14.746|tagging_loss_image: 7.286|tagging_loss_fusion: 5.021|total_loss: 44.281 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 3365 | tagging_loss_video: 5.792|tagging_loss_audio: 11.059|tagging_loss_text: 17.175|tagging_loss_image: 7.177|tagging_loss_fusion: 4.950|total_loss: 46.153 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 3366 | tagging_loss_video: 7.177|tagging_loss_audio: 10.882|tagging_loss_text: 14.917|tagging_loss_image: 7.067|tagging_loss_fusion: 6.739|total_loss: 46.781 | 61.23 Examples/sec\n",
      "INFO:tensorflow:training step 3367 | tagging_loss_video: 5.992|tagging_loss_audio: 9.769|tagging_loss_text: 14.716|tagging_loss_image: 6.700|tagging_loss_fusion: 4.547|total_loss: 41.724 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 3368 | tagging_loss_video: 6.068|tagging_loss_audio: 10.097|tagging_loss_text: 14.632|tagging_loss_image: 7.031|tagging_loss_fusion: 6.199|total_loss: 44.027 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 3369 | tagging_loss_video: 7.036|tagging_loss_audio: 10.532|tagging_loss_text: 16.324|tagging_loss_image: 7.729|tagging_loss_fusion: 9.187|total_loss: 50.808 | 61.34 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3370 |tagging_loss_video: 7.155|tagging_loss_audio: 11.498|tagging_loss_text: 16.068|tagging_loss_image: 7.626|tagging_loss_fusion: 7.393|total_loss: 49.740 | Examples/sec: 67.94\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.78 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3371 | tagging_loss_video: 7.814|tagging_loss_audio: 12.284|tagging_loss_text: 15.997|tagging_loss_image: 8.580|tagging_loss_fusion: 7.656|total_loss: 52.332 | 70.06 Examples/sec\n",
      "INFO:tensorflow:training step 3372 | tagging_loss_video: 5.356|tagging_loss_audio: 11.259|tagging_loss_text: 14.849|tagging_loss_image: 7.616|tagging_loss_fusion: 4.668|total_loss: 43.748 | 63.60 Examples/sec\n",
      "INFO:tensorflow:training step 3373 | tagging_loss_video: 6.529|tagging_loss_audio: 10.490|tagging_loss_text: 16.370|tagging_loss_image: 6.995|tagging_loss_fusion: 6.898|total_loss: 47.283 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 3374 | tagging_loss_video: 7.519|tagging_loss_audio: 12.184|tagging_loss_text: 14.247|tagging_loss_image: 8.768|tagging_loss_fusion: 6.935|total_loss: 49.653 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 3375 | tagging_loss_video: 6.379|tagging_loss_audio: 9.360|tagging_loss_text: 14.024|tagging_loss_image: 6.634|tagging_loss_fusion: 5.547|total_loss: 41.943 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 3376 | tagging_loss_video: 6.766|tagging_loss_audio: 11.621|tagging_loss_text: 16.566|tagging_loss_image: 7.656|tagging_loss_fusion: 6.918|total_loss: 49.528 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 3377 | tagging_loss_video: 6.269|tagging_loss_audio: 9.727|tagging_loss_text: 16.522|tagging_loss_image: 7.268|tagging_loss_fusion: 6.054|total_loss: 45.839 | 61.60 Examples/sec\n",
      "INFO:tensorflow:training step 3378 | tagging_loss_video: 6.666|tagging_loss_audio: 11.497|tagging_loss_text: 16.502|tagging_loss_image: 7.003|tagging_loss_fusion: 7.693|total_loss: 49.361 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 3379 | tagging_loss_video: 7.677|tagging_loss_audio: 10.432|tagging_loss_text: 14.760|tagging_loss_image: 7.264|tagging_loss_fusion: 6.554|total_loss: 46.688 | 69.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3380 |tagging_loss_video: 5.950|tagging_loss_audio: 9.489|tagging_loss_text: 10.402|tagging_loss_image: 6.338|tagging_loss_fusion: 5.976|total_loss: 38.155 | Examples/sec: 64.95\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.76 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3381 | tagging_loss_video: 5.175|tagging_loss_audio: 9.816|tagging_loss_text: 14.924|tagging_loss_image: 6.955|tagging_loss_fusion: 4.194|total_loss: 41.065 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 3382 | tagging_loss_video: 6.787|tagging_loss_audio: 11.041|tagging_loss_text: 13.853|tagging_loss_image: 7.506|tagging_loss_fusion: 7.494|total_loss: 46.681 | 66.50 Examples/sec\n",
      "INFO:tensorflow:training step 3383 | tagging_loss_video: 6.941|tagging_loss_audio: 10.231|tagging_loss_text: 13.549|tagging_loss_image: 6.843|tagging_loss_fusion: 6.449|total_loss: 44.013 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 3384 | tagging_loss_video: 6.902|tagging_loss_audio: 10.403|tagging_loss_text: 16.360|tagging_loss_image: 7.755|tagging_loss_fusion: 7.235|total_loss: 48.655 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 3385 | tagging_loss_video: 7.320|tagging_loss_audio: 10.624|tagging_loss_text: 11.337|tagging_loss_image: 6.830|tagging_loss_fusion: 7.403|total_loss: 43.513 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 3386 | tagging_loss_video: 6.027|tagging_loss_audio: 10.277|tagging_loss_text: 19.075|tagging_loss_image: 7.054|tagging_loss_fusion: 5.124|total_loss: 47.557 | 65.67 Examples/sec\n",
      "INFO:tensorflow:training step 3387 | tagging_loss_video: 6.963|tagging_loss_audio: 10.840|tagging_loss_text: 18.546|tagging_loss_image: 7.520|tagging_loss_fusion: 7.204|total_loss: 51.073 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 3388 | tagging_loss_video: 6.239|tagging_loss_audio: 10.365|tagging_loss_text: 14.099|tagging_loss_image: 7.127|tagging_loss_fusion: 5.964|total_loss: 43.793 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 3389 | tagging_loss_video: 5.255|tagging_loss_audio: 9.955|tagging_loss_text: 14.588|tagging_loss_image: 7.637|tagging_loss_fusion: 4.023|total_loss: 41.458 | 68.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3390 |tagging_loss_video: 6.545|tagging_loss_audio: 10.398|tagging_loss_text: 12.047|tagging_loss_image: 6.719|tagging_loss_fusion: 6.380|total_loss: 42.090 | Examples/sec: 65.66\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.74 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3391 | tagging_loss_video: 6.416|tagging_loss_audio: 10.707|tagging_loss_text: 17.784|tagging_loss_image: 7.085|tagging_loss_fusion: 7.662|total_loss: 49.654 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 3392 | tagging_loss_video: 5.511|tagging_loss_audio: 9.342|tagging_loss_text: 16.046|tagging_loss_image: 6.358|tagging_loss_fusion: 3.868|total_loss: 41.125 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3393 | tagging_loss_video: 5.406|tagging_loss_audio: 9.219|tagging_loss_text: 17.052|tagging_loss_image: 6.833|tagging_loss_fusion: 5.298|total_loss: 43.808 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 3394 | tagging_loss_video: 7.315|tagging_loss_audio: 9.754|tagging_loss_text: 17.566|tagging_loss_image: 7.006|tagging_loss_fusion: 7.213|total_loss: 48.854 | 64.06 Examples/sec\n",
      "INFO:tensorflow:training step 3395 | tagging_loss_video: 6.748|tagging_loss_audio: 9.047|tagging_loss_text: 14.588|tagging_loss_image: 6.298|tagging_loss_fusion: 7.990|total_loss: 44.672 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 3396 | tagging_loss_video: 6.690|tagging_loss_audio: 9.718|tagging_loss_text: 13.788|tagging_loss_image: 6.958|tagging_loss_fusion: 5.450|total_loss: 42.604 | 66.15 Examples/sec\n",
      "INFO:tensorflow:training step 3397 | tagging_loss_video: 6.203|tagging_loss_audio: 8.912|tagging_loss_text: 17.494|tagging_loss_image: 6.722|tagging_loss_fusion: 5.463|total_loss: 44.795 | 60.33 Examples/sec\n",
      "INFO:tensorflow:training step 3398 | tagging_loss_video: 6.491|tagging_loss_audio: 10.706|tagging_loss_text: 20.709|tagging_loss_image: 7.368|tagging_loss_fusion: 5.777|total_loss: 51.052 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 3399 | tagging_loss_video: 6.280|tagging_loss_audio: 7.611|tagging_loss_text: 13.946|tagging_loss_image: 7.355|tagging_loss_fusion: 6.573|total_loss: 41.764 | 71.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3400 |tagging_loss_video: 6.557|tagging_loss_audio: 10.621|tagging_loss_text: 17.656|tagging_loss_image: 8.193|tagging_loss_fusion: 6.357|total_loss: 49.385 | Examples/sec: 67.85\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3401 | tagging_loss_video: 5.836|tagging_loss_audio: 9.938|tagging_loss_text: 17.635|tagging_loss_image: 6.803|tagging_loss_fusion: 4.292|total_loss: 44.504 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 3402 | tagging_loss_video: 5.937|tagging_loss_audio: 10.019|tagging_loss_text: 12.827|tagging_loss_image: 7.082|tagging_loss_fusion: 5.603|total_loss: 41.469 | 61.97 Examples/sec\n",
      "INFO:tensorflow:training step 3403 | tagging_loss_video: 6.613|tagging_loss_audio: 9.365|tagging_loss_text: 14.864|tagging_loss_image: 7.235|tagging_loss_fusion: 5.369|total_loss: 43.446 | 71.68 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3404 | tagging_loss_video: 6.774|tagging_loss_audio: 9.704|tagging_loss_text: 14.617|tagging_loss_image: 8.004|tagging_loss_fusion: 5.377|total_loss: 44.477 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 3405 | tagging_loss_video: 6.191|tagging_loss_audio: 9.397|tagging_loss_text: 14.563|tagging_loss_image: 7.125|tagging_loss_fusion: 7.401|total_loss: 44.677 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 3406 | tagging_loss_video: 6.690|tagging_loss_audio: 8.970|tagging_loss_text: 14.628|tagging_loss_image: 7.506|tagging_loss_fusion: 6.494|total_loss: 44.288 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 3407 | tagging_loss_video: 5.886|tagging_loss_audio: 10.110|tagging_loss_text: 16.708|tagging_loss_image: 7.335|tagging_loss_fusion: 4.980|total_loss: 45.019 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 3408 | tagging_loss_video: 6.330|tagging_loss_audio: 10.571|tagging_loss_text: 16.799|tagging_loss_image: 8.526|tagging_loss_fusion: 5.319|total_loss: 47.545 | 62.30 Examples/sec\n",
      "INFO:tensorflow:training step 3409 | tagging_loss_video: 6.849|tagging_loss_audio: 10.027|tagging_loss_text: 15.398|tagging_loss_image: 6.738|tagging_loss_fusion: 7.007|total_loss: 46.019 | 68.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3410 |tagging_loss_video: 6.903|tagging_loss_audio: 10.006|tagging_loss_text: 10.994|tagging_loss_image: 8.364|tagging_loss_fusion: 5.941|total_loss: 42.208 | Examples/sec: 69.74\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3411 | tagging_loss_video: 6.280|tagging_loss_audio: 10.432|tagging_loss_text: 14.770|tagging_loss_image: 7.165|tagging_loss_fusion: 4.452|total_loss: 43.099 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 3412 | tagging_loss_video: 6.644|tagging_loss_audio: 9.270|tagging_loss_text: 15.459|tagging_loss_image: 7.391|tagging_loss_fusion: 6.077|total_loss: 44.841 | 66.61 Examples/sec\n",
      "INFO:tensorflow:training step 3413 | tagging_loss_video: 6.122|tagging_loss_audio: 9.766|tagging_loss_text: 15.636|tagging_loss_image: 7.046|tagging_loss_fusion: 5.245|total_loss: 43.815 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 3414 | tagging_loss_video: 6.550|tagging_loss_audio: 9.890|tagging_loss_text: 16.331|tagging_loss_image: 7.080|tagging_loss_fusion: 6.526|total_loss: 46.377 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 3415 | tagging_loss_video: 6.479|tagging_loss_audio: 10.613|tagging_loss_text: 14.098|tagging_loss_image: 7.770|tagging_loss_fusion: 5.404|total_loss: 44.365 | 72.05 Examples/sec\n",
      "INFO:tensorflow:training step 3416 | tagging_loss_video: 6.145|tagging_loss_audio: 10.821|tagging_loss_text: 15.226|tagging_loss_image: 8.200|tagging_loss_fusion: 5.938|total_loss: 46.330 | 59.86 Examples/sec\n",
      "INFO:tensorflow:training step 3417 | tagging_loss_video: 6.512|tagging_loss_audio: 11.342|tagging_loss_text: 13.397|tagging_loss_image: 7.537|tagging_loss_fusion: 5.322|total_loss: 44.111 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 3418 | tagging_loss_video: 7.275|tagging_loss_audio: 11.244|tagging_loss_text: 19.056|tagging_loss_image: 8.265|tagging_loss_fusion: 6.855|total_loss: 52.696 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 3419 | tagging_loss_video: 6.271|tagging_loss_audio: 8.064|tagging_loss_text: 17.028|tagging_loss_image: 6.235|tagging_loss_fusion: 5.153|total_loss: 42.751 | 63.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3420 |tagging_loss_video: 5.638|tagging_loss_audio: 8.184|tagging_loss_text: 13.744|tagging_loss_image: 6.255|tagging_loss_fusion: 5.106|total_loss: 38.926 | Examples/sec: 63.31\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3421 | tagging_loss_video: 6.733|tagging_loss_audio: 8.997|tagging_loss_text: 14.801|tagging_loss_image: 6.618|tagging_loss_fusion: 5.766|total_loss: 42.915 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 3422 | tagging_loss_video: 5.513|tagging_loss_audio: 9.373|tagging_loss_text: 13.994|tagging_loss_image: 6.366|tagging_loss_fusion: 5.287|total_loss: 40.532 | 65.03 Examples/sec\n",
      "INFO:tensorflow:training step 3423 | tagging_loss_video: 6.602|tagging_loss_audio: 9.787|tagging_loss_text: 16.834|tagging_loss_image: 7.100|tagging_loss_fusion: 7.438|total_loss: 47.761 | 68.20 Examples/sec\n",
      "INFO:tensorflow:training step 3424 | tagging_loss_video: 6.391|tagging_loss_audio: 9.163|tagging_loss_text: 14.478|tagging_loss_image: 7.061|tagging_loss_fusion: 6.665|total_loss: 43.758 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 3425 | tagging_loss_video: 7.241|tagging_loss_audio: 9.106|tagging_loss_text: 13.259|tagging_loss_image: 7.268|tagging_loss_fusion: 6.588|total_loss: 43.462 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 3426 | tagging_loss_video: 5.955|tagging_loss_audio: 8.375|tagging_loss_text: 13.971|tagging_loss_image: 6.326|tagging_loss_fusion: 5.729|total_loss: 40.356 | 65.86 Examples/sec\n",
      "INFO:tensorflow:training step 3427 | tagging_loss_video: 6.421|tagging_loss_audio: 9.538|tagging_loss_text: 17.668|tagging_loss_image: 6.701|tagging_loss_fusion: 4.941|total_loss: 45.268 | 60.92 Examples/sec\n",
      "INFO:tensorflow:training step 3428 | tagging_loss_video: 7.069|tagging_loss_audio: 8.200|tagging_loss_text: 12.306|tagging_loss_image: 7.897|tagging_loss_fusion: 5.645|total_loss: 41.118 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 3429 | tagging_loss_video: 4.689|tagging_loss_audio: 8.686|tagging_loss_text: 9.903|tagging_loss_image: 6.939|tagging_loss_fusion: 5.183|total_loss: 35.400 | 69.75 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3430 |tagging_loss_video: 6.136|tagging_loss_audio: 8.369|tagging_loss_text: 14.244|tagging_loss_image: 6.851|tagging_loss_fusion: 5.908|total_loss: 41.508 | Examples/sec: 64.65\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.74 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3431 | tagging_loss_video: 5.915|tagging_loss_audio: 8.324|tagging_loss_text: 15.184|tagging_loss_image: 6.086|tagging_loss_fusion: 4.004|total_loss: 39.513 | 66.58 Examples/sec\n",
      "INFO:tensorflow:training step 3432 | tagging_loss_video: 5.772|tagging_loss_audio: 10.269|tagging_loss_text: 17.442|tagging_loss_image: 7.467|tagging_loss_fusion: 4.553|total_loss: 45.503 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 3433 | tagging_loss_video: 5.998|tagging_loss_audio: 9.719|tagging_loss_text: 14.814|tagging_loss_image: 6.724|tagging_loss_fusion: 7.968|total_loss: 45.223 | 62.00 Examples/sec\n",
      "INFO:tensorflow:training step 3434 | tagging_loss_video: 5.773|tagging_loss_audio: 8.992|tagging_loss_text: 14.087|tagging_loss_image: 6.520|tagging_loss_fusion: 5.370|total_loss: 40.741 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 3435 | tagging_loss_video: 5.150|tagging_loss_audio: 9.438|tagging_loss_text: 17.237|tagging_loss_image: 5.976|tagging_loss_fusion: 3.916|total_loss: 41.718 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 3436 | tagging_loss_video: 6.449|tagging_loss_audio: 8.596|tagging_loss_text: 11.727|tagging_loss_image: 5.795|tagging_loss_fusion: 7.048|total_loss: 39.615 | 65.60 Examples/sec\n",
      "INFO:tensorflow:training step 3437 | tagging_loss_video: 6.179|tagging_loss_audio: 7.712|tagging_loss_text: 14.952|tagging_loss_image: 6.270|tagging_loss_fusion: 5.255|total_loss: 40.369 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 3438 | tagging_loss_video: 5.647|tagging_loss_audio: 8.870|tagging_loss_text: 15.843|tagging_loss_image: 7.599|tagging_loss_fusion: 4.806|total_loss: 42.765 | 63.19 Examples/sec\n",
      "INFO:tensorflow:training step 3439 | tagging_loss_video: 6.129|tagging_loss_audio: 11.203|tagging_loss_text: 11.275|tagging_loss_image: 7.798|tagging_loss_fusion: 5.902|total_loss: 42.306 | 71.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3440 |tagging_loss_video: 4.078|tagging_loss_audio: 8.751|tagging_loss_text: 13.414|tagging_loss_image: 6.895|tagging_loss_fusion: 3.526|total_loss: 36.663 | Examples/sec: 68.98\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 3441 | tagging_loss_video: 5.665|tagging_loss_audio: 10.344|tagging_loss_text: 13.897|tagging_loss_image: 7.397|tagging_loss_fusion: 4.897|total_loss: 42.199 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 3442 | tagging_loss_video: 6.611|tagging_loss_audio: 9.097|tagging_loss_text: 14.749|tagging_loss_image: 8.067|tagging_loss_fusion: 5.695|total_loss: 44.219 | 63.43 Examples/sec\n",
      "INFO:tensorflow:training step 3443 | tagging_loss_video: 5.921|tagging_loss_audio: 9.847|tagging_loss_text: 18.869|tagging_loss_image: 6.267|tagging_loss_fusion: 6.267|total_loss: 47.172 | 67.81 Examples/sec\n",
      "INFO:tensorflow:training step 3444 | tagging_loss_video: 6.714|tagging_loss_audio: 9.056|tagging_loss_text: 15.031|tagging_loss_image: 6.667|tagging_loss_fusion: 6.558|total_loss: 44.026 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 3445 | tagging_loss_video: 5.556|tagging_loss_audio: 7.694|tagging_loss_text: 14.566|tagging_loss_image: 5.910|tagging_loss_fusion: 3.856|total_loss: 37.582 | 61.14 Examples/sec\n",
      "INFO:tensorflow:training step 3446 | tagging_loss_video: 5.937|tagging_loss_audio: 8.444|tagging_loss_text: 11.857|tagging_loss_image: 6.449|tagging_loss_fusion: 6.869|total_loss: 39.556 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 3447 | tagging_loss_video: 5.152|tagging_loss_audio: 9.803|tagging_loss_text: 12.669|tagging_loss_image: 7.236|tagging_loss_fusion: 4.938|total_loss: 39.798 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 3448 | tagging_loss_video: 5.517|tagging_loss_audio: 10.187|tagging_loss_text: 15.451|tagging_loss_image: 5.854|tagging_loss_fusion: 7.094|total_loss: 44.104 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 3449 | tagging_loss_video: 6.331|tagging_loss_audio: 9.491|tagging_loss_text: 13.800|tagging_loss_image: 6.989|tagging_loss_fusion: 7.022|total_loss: 43.633 | 69.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3450 |tagging_loss_video: 5.839|tagging_loss_audio: 9.386|tagging_loss_text: 17.772|tagging_loss_image: 6.390|tagging_loss_fusion: 7.910|total_loss: 47.297 | Examples/sec: 66.79\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.74 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.79\n",
      "INFO:tensorflow:training step 3451 | tagging_loss_video: 5.981|tagging_loss_audio: 9.608|tagging_loss_text: 15.364|tagging_loss_image: 5.893|tagging_loss_fusion: 5.737|total_loss: 42.584 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 3452 | tagging_loss_video: 5.314|tagging_loss_audio: 9.751|tagging_loss_text: 15.974|tagging_loss_image: 7.301|tagging_loss_fusion: 4.048|total_loss: 42.389 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 3453 | tagging_loss_video: 4.198|tagging_loss_audio: 9.856|tagging_loss_text: 13.569|tagging_loss_image: 7.660|tagging_loss_fusion: 3.575|total_loss: 38.859 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 3454 | tagging_loss_video: 6.683|tagging_loss_audio: 9.259|tagging_loss_text: 14.670|tagging_loss_image: 6.859|tagging_loss_fusion: 7.848|total_loss: 45.318 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 3455 | tagging_loss_video: 7.266|tagging_loss_audio: 10.474|tagging_loss_text: 17.740|tagging_loss_image: 7.101|tagging_loss_fusion: 6.586|total_loss: 49.167 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 3456 | tagging_loss_video: 4.971|tagging_loss_audio: 10.043|tagging_loss_text: 13.701|tagging_loss_image: 6.802|tagging_loss_fusion: 3.108|total_loss: 38.625 | 61.70 Examples/sec\n",
      "INFO:tensorflow:training step 3457 | tagging_loss_video: 5.417|tagging_loss_audio: 8.766|tagging_loss_text: 18.427|tagging_loss_image: 6.639|tagging_loss_fusion: 5.682|total_loss: 44.931 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3458 | tagging_loss_video: 5.824|tagging_loss_audio: 8.718|tagging_loss_text: 11.879|tagging_loss_image: 6.091|tagging_loss_fusion: 6.003|total_loss: 38.515 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 3459 | tagging_loss_video: 5.864|tagging_loss_audio: 9.213|tagging_loss_text: 14.273|tagging_loss_image: 6.274|tagging_loss_fusion: 5.632|total_loss: 41.256 | 71.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3460 |tagging_loss_video: 5.474|tagging_loss_audio: 9.484|tagging_loss_text: 14.225|tagging_loss_image: 7.167|tagging_loss_fusion: 4.450|total_loss: 40.800 | Examples/sec: 68.78\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3461 | tagging_loss_video: 5.917|tagging_loss_audio: 8.858|tagging_loss_text: 16.202|tagging_loss_image: 6.787|tagging_loss_fusion: 8.107|total_loss: 45.871 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 3462 | tagging_loss_video: 5.782|tagging_loss_audio: 9.225|tagging_loss_text: 15.339|tagging_loss_image: 7.061|tagging_loss_fusion: 4.648|total_loss: 42.054 | 65.90 Examples/sec\n",
      "INFO:tensorflow:training step 3463 | tagging_loss_video: 5.777|tagging_loss_audio: 8.428|tagging_loss_text: 12.857|tagging_loss_image: 6.697|tagging_loss_fusion: 5.037|total_loss: 38.796 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 3464 | tagging_loss_video: 6.643|tagging_loss_audio: 9.634|tagging_loss_text: 14.216|tagging_loss_image: 6.880|tagging_loss_fusion: 6.514|total_loss: 43.887 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 3465 | tagging_loss_video: 6.632|tagging_loss_audio: 8.905|tagging_loss_text: 17.896|tagging_loss_image: 7.134|tagging_loss_fusion: 5.899|total_loss: 46.466 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 3466 | tagging_loss_video: 6.740|tagging_loss_audio: 10.474|tagging_loss_text: 16.009|tagging_loss_image: 6.765|tagging_loss_fusion: 5.932|total_loss: 45.920 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 3467 | tagging_loss_video: 5.288|tagging_loss_audio: 9.622|tagging_loss_text: 16.324|tagging_loss_image: 6.596|tagging_loss_fusion: 4.466|total_loss: 42.296 | 63.56 Examples/sec\n",
      "INFO:tensorflow:training step 3468 | tagging_loss_video: 7.238|tagging_loss_audio: 9.865|tagging_loss_text: 18.073|tagging_loss_image: 7.166|tagging_loss_fusion: 7.993|total_loss: 50.335 | 67.61 Examples/sec\n",
      "INFO:tensorflow:training step 3469 | tagging_loss_video: 5.496|tagging_loss_audio: 9.337|tagging_loss_text: 16.459|tagging_loss_image: 6.416|tagging_loss_fusion: 4.429|total_loss: 42.138 | 70.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3470 |tagging_loss_video: 6.129|tagging_loss_audio: 9.648|tagging_loss_text: 15.822|tagging_loss_image: 7.426|tagging_loss_fusion: 4.899|total_loss: 43.923 | Examples/sec: 62.58\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3471 | tagging_loss_video: 7.464|tagging_loss_audio: 11.555|tagging_loss_text: 18.611|tagging_loss_image: 7.424|tagging_loss_fusion: 5.412|total_loss: 50.466 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 3472 | tagging_loss_video: 6.357|tagging_loss_audio: 8.989|tagging_loss_text: 14.301|tagging_loss_image: 6.793|tagging_loss_fusion: 5.275|total_loss: 41.715 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 3473 | tagging_loss_video: 6.859|tagging_loss_audio: 10.551|tagging_loss_text: 15.225|tagging_loss_image: 6.826|tagging_loss_fusion: 5.665|total_loss: 45.125 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 3474 | tagging_loss_video: 6.228|tagging_loss_audio: 8.894|tagging_loss_text: 19.395|tagging_loss_image: 7.274|tagging_loss_fusion: 5.039|total_loss: 46.831 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 3475 | tagging_loss_video: 6.835|tagging_loss_audio: 10.026|tagging_loss_text: 14.519|tagging_loss_image: 7.262|tagging_loss_fusion: 5.612|total_loss: 44.255 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 3476 | tagging_loss_video: 6.767|tagging_loss_audio: 8.585|tagging_loss_text: 16.932|tagging_loss_image: 7.250|tagging_loss_fusion: 6.062|total_loss: 45.596 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 3477 | tagging_loss_video: 7.246|tagging_loss_audio: 11.470|tagging_loss_text: 17.923|tagging_loss_image: 7.473|tagging_loss_fusion: 6.859|total_loss: 50.972 | 67.83 Examples/sec\n",
      "INFO:tensorflow:training step 3478 | tagging_loss_video: 5.920|tagging_loss_audio: 10.372|tagging_loss_text: 15.112|tagging_loss_image: 6.217|tagging_loss_fusion: 4.154|total_loss: 41.775 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 3479 | tagging_loss_video: 6.705|tagging_loss_audio: 10.489|tagging_loss_text: 16.909|tagging_loss_image: 7.673|tagging_loss_fusion: 7.969|total_loss: 49.745 | 68.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3480 |tagging_loss_video: 6.295|tagging_loss_audio: 11.091|tagging_loss_text: 14.238|tagging_loss_image: 6.012|tagging_loss_fusion: 4.685|total_loss: 42.321 | Examples/sec: 70.29\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3481 | tagging_loss_video: 6.821|tagging_loss_audio: 10.465|tagging_loss_text: 15.187|tagging_loss_image: 7.666|tagging_loss_fusion: 7.023|total_loss: 47.163 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 3482 | tagging_loss_video: 6.547|tagging_loss_audio: 10.701|tagging_loss_text: 15.175|tagging_loss_image: 7.655|tagging_loss_fusion: 7.750|total_loss: 47.828 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 3483 | tagging_loss_video: 5.602|tagging_loss_audio: 10.001|tagging_loss_text: 18.208|tagging_loss_image: 6.947|tagging_loss_fusion: 5.508|total_loss: 46.266 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 3484 | tagging_loss_video: 6.623|tagging_loss_audio: 9.965|tagging_loss_text: 16.785|tagging_loss_image: 6.584|tagging_loss_fusion: 4.779|total_loss: 44.735 | 59.83 Examples/sec\n",
      "INFO:tensorflow:training step 3485 | tagging_loss_video: 7.361|tagging_loss_audio: 11.946|tagging_loss_text: 18.375|tagging_loss_image: 8.540|tagging_loss_fusion: 6.103|total_loss: 52.326 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 3486 | tagging_loss_video: 7.098|tagging_loss_audio: 9.688|tagging_loss_text: 17.306|tagging_loss_image: 6.557|tagging_loss_fusion: 5.966|total_loss: 46.615 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 3487 | tagging_loss_video: 6.597|tagging_loss_audio: 9.115|tagging_loss_text: 12.664|tagging_loss_image: 6.659|tagging_loss_fusion: 6.421|total_loss: 41.456 | 63.54 Examples/sec\n",
      "INFO:tensorflow:training step 3488 | tagging_loss_video: 5.834|tagging_loss_audio: 9.826|tagging_loss_text: 14.756|tagging_loss_image: 7.620|tagging_loss_fusion: 5.189|total_loss: 43.224 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 3489 | tagging_loss_video: 6.061|tagging_loss_audio: 9.576|tagging_loss_text: 13.050|tagging_loss_image: 7.015|tagging_loss_fusion: 6.527|total_loss: 42.229 | 69.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3490 |tagging_loss_video: 6.286|tagging_loss_audio: 10.843|tagging_loss_text: 10.660|tagging_loss_image: 6.446|tagging_loss_fusion: 5.781|total_loss: 40.016 | Examples/sec: 68.75\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3491 | tagging_loss_video: 7.768|tagging_loss_audio: 12.129|tagging_loss_text: 18.559|tagging_loss_image: 7.324|tagging_loss_fusion: 5.749|total_loss: 51.529 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 3492 | tagging_loss_video: 6.528|tagging_loss_audio: 10.918|tagging_loss_text: 16.995|tagging_loss_image: 7.025|tagging_loss_fusion: 4.272|total_loss: 45.738 | 61.70 Examples/sec\n",
      "INFO:tensorflow:training step 3493 | tagging_loss_video: 6.143|tagging_loss_audio: 10.549|tagging_loss_text: 15.067|tagging_loss_image: 6.459|tagging_loss_fusion: 4.986|total_loss: 43.204 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 3494 | tagging_loss_video: 6.359|tagging_loss_audio: 9.576|tagging_loss_text: 20.479|tagging_loss_image: 6.607|tagging_loss_fusion: 6.829|total_loss: 49.850 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 3495 | tagging_loss_video: 6.300|tagging_loss_audio: 9.730|tagging_loss_text: 17.981|tagging_loss_image: 5.576|tagging_loss_fusion: 4.884|total_loss: 44.472 | 62.50 Examples/sec\n",
      "INFO:tensorflow:training step 3496 | tagging_loss_video: 6.557|tagging_loss_audio: 11.224|tagging_loss_text: 17.643|tagging_loss_image: 7.538|tagging_loss_fusion: 6.158|total_loss: 49.120 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 3497 | tagging_loss_video: 6.812|tagging_loss_audio: 9.801|tagging_loss_text: 16.231|tagging_loss_image: 6.571|tagging_loss_fusion: 6.041|total_loss: 45.456 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 3498 | tagging_loss_video: 6.235|tagging_loss_audio: 9.981|tagging_loss_text: 13.210|tagging_loss_image: 6.640|tagging_loss_fusion: 6.298|total_loss: 42.365 | 65.47 Examples/sec\n",
      "INFO:tensorflow:training step 3499 | tagging_loss_video: 6.245|tagging_loss_audio: 9.342|tagging_loss_text: 16.556|tagging_loss_image: 5.889|tagging_loss_fusion: 4.462|total_loss: 42.493 | 67.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3500 |tagging_loss_video: 6.436|tagging_loss_audio: 8.844|tagging_loss_text: 15.491|tagging_loss_image: 6.994|tagging_loss_fusion: 6.435|total_loss: 44.200 | Examples/sec: 70.55\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.75 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3501 | tagging_loss_video: 5.623|tagging_loss_audio: 9.439|tagging_loss_text: 15.813|tagging_loss_image: 6.603|tagging_loss_fusion: 4.588|total_loss: 42.067 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 3502 | tagging_loss_video: 4.616|tagging_loss_audio: 10.217|tagging_loss_text: 16.466|tagging_loss_image: 5.892|tagging_loss_fusion: 4.406|total_loss: 41.597 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 3503 | tagging_loss_video: 6.762|tagging_loss_audio: 8.675|tagging_loss_text: 13.975|tagging_loss_image: 7.135|tagging_loss_fusion: 6.727|total_loss: 43.274 | 65.75 Examples/sec\n",
      "INFO:tensorflow:training step 3504 | tagging_loss_video: 6.440|tagging_loss_audio: 8.904|tagging_loss_text: 13.362|tagging_loss_image: 5.266|tagging_loss_fusion: 5.073|total_loss: 39.045 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 3505 | tagging_loss_video: 6.109|tagging_loss_audio: 10.174|tagging_loss_text: 13.877|tagging_loss_image: 6.769|tagging_loss_fusion: 7.044|total_loss: 43.973 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3506 | tagging_loss_video: 6.441|tagging_loss_audio: 10.678|tagging_loss_text: 16.174|tagging_loss_image: 6.701|tagging_loss_fusion: 6.686|total_loss: 46.680 | 62.08 Examples/sec\n",
      "INFO:tensorflow:training step 3507 | tagging_loss_video: 6.494|tagging_loss_audio: 9.466|tagging_loss_text: 12.680|tagging_loss_image: 7.040|tagging_loss_fusion: 7.247|total_loss: 42.927 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 3508 | tagging_loss_video: 7.379|tagging_loss_audio: 9.617|tagging_loss_text: 15.919|tagging_loss_image: 6.863|tagging_loss_fusion: 8.520|total_loss: 48.298 | 71.75 Examples/sec\n",
      "INFO:tensorflow:training step 3509 | tagging_loss_video: 6.423|tagging_loss_audio: 9.340|tagging_loss_text: 14.895|tagging_loss_image: 7.231|tagging_loss_fusion: 6.452|total_loss: 44.341 | 62.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3510 |tagging_loss_video: 6.285|tagging_loss_audio: 11.158|tagging_loss_text: 14.615|tagging_loss_image: 7.404|tagging_loss_fusion: 5.814|total_loss: 45.275 | Examples/sec: 68.08\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3511 | tagging_loss_video: 6.685|tagging_loss_audio: 10.662|tagging_loss_text: 16.720|tagging_loss_image: 7.636|tagging_loss_fusion: 7.047|total_loss: 48.750 | 71.44 Examples/sec\n",
      "INFO:tensorflow:training step 3512 | tagging_loss_video: 6.020|tagging_loss_audio: 8.802|tagging_loss_text: 16.175|tagging_loss_image: 7.559|tagging_loss_fusion: 5.880|total_loss: 44.436 | 64.52 Examples/sec\n",
      "INFO:tensorflow:training step 3513 | tagging_loss_video: 5.991|tagging_loss_audio: 10.342|tagging_loss_text: 14.357|tagging_loss_image: 6.814|tagging_loss_fusion: 5.287|total_loss: 42.791 | 67.13 Examples/sec\n",
      "INFO:tensorflow:training step 3514 | tagging_loss_video: 6.263|tagging_loss_audio: 11.001|tagging_loss_text: 16.398|tagging_loss_image: 7.552|tagging_loss_fusion: 4.046|total_loss: 45.259 | 71.81 Examples/sec\n",
      "INFO:tensorflow:training step 3515 | tagging_loss_video: 5.711|tagging_loss_audio: 9.732|tagging_loss_text: 15.356|tagging_loss_image: 5.779|tagging_loss_fusion: 4.069|total_loss: 40.647 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 3516 | tagging_loss_video: 6.468|tagging_loss_audio: 10.574|tagging_loss_text: 15.082|tagging_loss_image: 7.313|tagging_loss_fusion: 5.658|total_loss: 45.095 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 3517 | tagging_loss_video: 5.174|tagging_loss_audio: 9.714|tagging_loss_text: 14.549|tagging_loss_image: 6.525|tagging_loss_fusion: 4.647|total_loss: 40.610 | 59.11 Examples/sec\n",
      "INFO:tensorflow:training step 3518 | tagging_loss_video: 6.128|tagging_loss_audio: 11.185|tagging_loss_text: 12.330|tagging_loss_image: 7.049|tagging_loss_fusion: 5.188|total_loss: 41.880 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 3519 | tagging_loss_video: 6.419|tagging_loss_audio: 9.370|tagging_loss_text: 16.079|tagging_loss_image: 6.985|tagging_loss_fusion: 5.200|total_loss: 44.054 | 68.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3520 |tagging_loss_video: 5.855|tagging_loss_audio: 7.959|tagging_loss_text: 14.728|tagging_loss_image: 5.394|tagging_loss_fusion: 5.193|total_loss: 39.128 | Examples/sec: 67.17\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.79 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 3521 | tagging_loss_video: 6.188|tagging_loss_audio: 8.615|tagging_loss_text: 16.241|tagging_loss_image: 5.900|tagging_loss_fusion: 5.113|total_loss: 42.057 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 3522 | tagging_loss_video: 4.800|tagging_loss_audio: 9.269|tagging_loss_text: 19.332|tagging_loss_image: 6.434|tagging_loss_fusion: 3.204|total_loss: 43.039 | 67.83 Examples/sec\n",
      "INFO:tensorflow:training step 3523 | tagging_loss_video: 6.147|tagging_loss_audio: 10.234|tagging_loss_text: 16.224|tagging_loss_image: 6.470|tagging_loss_fusion: 5.007|total_loss: 44.082 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 3524 | tagging_loss_video: 7.668|tagging_loss_audio: 9.936|tagging_loss_text: 15.519|tagging_loss_image: 7.259|tagging_loss_fusion: 5.829|total_loss: 46.211 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 3525 | tagging_loss_video: 5.760|tagging_loss_audio: 8.570|tagging_loss_text: 16.719|tagging_loss_image: 5.881|tagging_loss_fusion: 5.555|total_loss: 42.485 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 3526 | tagging_loss_video: 6.274|tagging_loss_audio: 9.687|tagging_loss_text: 13.619|tagging_loss_image: 5.295|tagging_loss_fusion: 7.770|total_loss: 42.644 | 64.66 Examples/sec\n",
      "INFO:tensorflow:training step 3527 | tagging_loss_video: 7.085|tagging_loss_audio: 9.749|tagging_loss_text: 15.684|tagging_loss_image: 7.065|tagging_loss_fusion: 5.578|total_loss: 45.160 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 3528 | tagging_loss_video: 6.362|tagging_loss_audio: 10.812|tagging_loss_text: 13.417|tagging_loss_image: 6.923|tagging_loss_fusion: 7.500|total_loss: 45.014 | 65.88 Examples/sec\n",
      "INFO:tensorflow:training step 3529 | tagging_loss_video: 6.299|tagging_loss_audio: 10.227|tagging_loss_text: 13.872|tagging_loss_image: 6.768|tagging_loss_fusion: 5.271|total_loss: 42.436 | 70.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3530 |tagging_loss_video: 5.997|tagging_loss_audio: 8.524|tagging_loss_text: 15.387|tagging_loss_image: 6.140|tagging_loss_fusion: 7.133|total_loss: 43.181 | Examples/sec: 70.37\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.72 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 3531 | tagging_loss_video: 6.874|tagging_loss_audio: 9.577|tagging_loss_text: 14.996|tagging_loss_image: 6.233|tagging_loss_fusion: 6.038|total_loss: 43.718 | 63.49 Examples/sec\n",
      "INFO:tensorflow:training step 3532 | tagging_loss_video: 4.551|tagging_loss_audio: 9.272|tagging_loss_text: 12.061|tagging_loss_image: 5.682|tagging_loss_fusion: 3.199|total_loss: 34.766 | 66.81 Examples/sec\n",
      "INFO:tensorflow:training step 3533 | tagging_loss_video: 5.041|tagging_loss_audio: 9.609|tagging_loss_text: 16.309|tagging_loss_image: 6.136|tagging_loss_fusion: 3.514|total_loss: 40.610 | 71.67 Examples/sec\n",
      "INFO:tensorflow:training step 3534 | tagging_loss_video: 6.485|tagging_loss_audio: 8.981|tagging_loss_text: 13.664|tagging_loss_image: 6.962|tagging_loss_fusion: 6.241|total_loss: 42.332 | 63.83 Examples/sec\n",
      "INFO:tensorflow:training step 3535 | tagging_loss_video: 5.546|tagging_loss_audio: 9.588|tagging_loss_text: 14.597|tagging_loss_image: 6.095|tagging_loss_fusion: 4.902|total_loss: 40.728 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 3536 | tagging_loss_video: 5.222|tagging_loss_audio: 9.946|tagging_loss_text: 13.780|tagging_loss_image: 7.073|tagging_loss_fusion: 5.821|total_loss: 41.842 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 3537 | tagging_loss_video: 6.405|tagging_loss_audio: 9.126|tagging_loss_text: 15.358|tagging_loss_image: 6.686|tagging_loss_fusion: 6.686|total_loss: 44.262 | 63.32 Examples/sec\n",
      "INFO:tensorflow:training step 3538 | tagging_loss_video: 7.130|tagging_loss_audio: 9.708|tagging_loss_text: 19.237|tagging_loss_image: 7.417|tagging_loss_fusion: 6.837|total_loss: 50.329 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 3539 | tagging_loss_video: 6.005|tagging_loss_audio: 9.205|tagging_loss_text: 13.272|tagging_loss_image: 6.366|tagging_loss_fusion: 4.704|total_loss: 39.552 | 66.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3540 |tagging_loss_video: 6.228|tagging_loss_audio: 9.600|tagging_loss_text: 17.605|tagging_loss_image: 6.821|tagging_loss_fusion: 7.181|total_loss: 47.436 | Examples/sec: 71.05\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.73 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3541 | tagging_loss_video: 6.770|tagging_loss_audio: 10.487|tagging_loss_text: 14.086|tagging_loss_image: 6.797|tagging_loss_fusion: 7.271|total_loss: 45.411 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 3542 | tagging_loss_video: 5.499|tagging_loss_audio: 9.196|tagging_loss_text: 12.822|tagging_loss_image: 7.099|tagging_loss_fusion: 4.844|total_loss: 39.460 | 64.46 Examples/sec\n",
      "INFO:tensorflow:training step 3543 | tagging_loss_video: 6.734|tagging_loss_audio: 9.089|tagging_loss_text: 16.969|tagging_loss_image: 7.283|tagging_loss_fusion: 7.650|total_loss: 47.725 | 69.86 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3544 | tagging_loss_video: 6.268|tagging_loss_audio: 9.230|tagging_loss_text: 14.626|tagging_loss_image: 6.907|tagging_loss_fusion: 4.585|total_loss: 41.617 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 3545 | tagging_loss_video: 6.554|tagging_loss_audio: 9.703|tagging_loss_text: 16.611|tagging_loss_image: 6.598|tagging_loss_fusion: 6.212|total_loss: 45.678 | 63.81 Examples/sec\n",
      "INFO:tensorflow:training step 3546 | tagging_loss_video: 6.122|tagging_loss_audio: 8.643|tagging_loss_text: 13.914|tagging_loss_image: 6.356|tagging_loss_fusion: 5.236|total_loss: 40.271 | 67.46 Examples/sec\n",
      "INFO:tensorflow:training step 3547 | tagging_loss_video: 6.552|tagging_loss_audio: 9.483|tagging_loss_text: 13.406|tagging_loss_image: 7.563|tagging_loss_fusion: 6.946|total_loss: 43.950 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 3548 | tagging_loss_video: 6.437|tagging_loss_audio: 10.086|tagging_loss_text: 17.508|tagging_loss_image: 7.263|tagging_loss_fusion: 5.663|total_loss: 46.958 | 66.06 Examples/sec\n",
      "INFO:tensorflow:training step 3549 | tagging_loss_video: 5.656|tagging_loss_audio: 10.286|tagging_loss_text: 14.751|tagging_loss_image: 6.242|tagging_loss_fusion: 4.531|total_loss: 41.465 | 67.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3550 |tagging_loss_video: 7.007|tagging_loss_audio: 9.428|tagging_loss_text: 17.839|tagging_loss_image: 7.413|tagging_loss_fusion: 7.106|total_loss: 48.794 | Examples/sec: 70.81\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.76 | precision@0.5: 0.94 |recall@0.1: 0.94 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 3551 | tagging_loss_video: 6.689|tagging_loss_audio: 8.756|tagging_loss_text: 15.451|tagging_loss_image: 6.971|tagging_loss_fusion: 6.551|total_loss: 44.418 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 3552 | tagging_loss_video: 5.413|tagging_loss_audio: 9.955|tagging_loss_text: 16.772|tagging_loss_image: 7.162|tagging_loss_fusion: 4.937|total_loss: 44.239 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 3553 | tagging_loss_video: 6.437|tagging_loss_audio: 9.080|tagging_loss_text: 14.300|tagging_loss_image: 6.649|tagging_loss_fusion: 6.679|total_loss: 43.145 | 72.64 Examples/sec\n",
      "INFO:tensorflow:training step 3554 | tagging_loss_video: 6.723|tagging_loss_audio: 10.201|tagging_loss_text: 14.951|tagging_loss_image: 6.540|tagging_loss_fusion: 5.928|total_loss: 44.343 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 3555 | tagging_loss_video: 5.675|tagging_loss_audio: 8.933|tagging_loss_text: 17.062|tagging_loss_image: 8.050|tagging_loss_fusion: 4.293|total_loss: 44.014 | 64.20 Examples/sec\n",
      "INFO:tensorflow:training step 3556 | tagging_loss_video: 7.019|tagging_loss_audio: 9.900|tagging_loss_text: 15.235|tagging_loss_image: 7.603|tagging_loss_fusion: 8.930|total_loss: 48.688 | 64.18 Examples/sec\n",
      "INFO:tensorflow:training step 3557 | tagging_loss_video: 5.206|tagging_loss_audio: 9.747|tagging_loss_text: 15.509|tagging_loss_image: 5.926|tagging_loss_fusion: 3.908|total_loss: 40.295 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 3558 | tagging_loss_video: 6.150|tagging_loss_audio: 10.416|tagging_loss_text: 19.027|tagging_loss_image: 6.971|tagging_loss_fusion: 5.110|total_loss: 47.674 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 3559 | tagging_loss_video: 5.872|tagging_loss_audio: 7.851|tagging_loss_text: 14.463|tagging_loss_image: 6.289|tagging_loss_fusion: 5.023|total_loss: 39.498 | 59.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3560 |tagging_loss_video: 5.369|tagging_loss_audio: 8.551|tagging_loss_text: 13.164|tagging_loss_image: 6.433|tagging_loss_fusion: 4.413|total_loss: 37.931 | Examples/sec: 68.18\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3561 | tagging_loss_video: 6.377|tagging_loss_audio: 9.312|tagging_loss_text: 16.119|tagging_loss_image: 6.497|tagging_loss_fusion: 5.995|total_loss: 44.300 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 3562 | tagging_loss_video: 6.185|tagging_loss_audio: 8.475|tagging_loss_text: 13.384|tagging_loss_image: 6.799|tagging_loss_fusion: 7.285|total_loss: 42.127 | 65.37 Examples/sec\n",
      "INFO:tensorflow:training step 3563 | tagging_loss_video: 6.348|tagging_loss_audio: 9.239|tagging_loss_text: 16.096|tagging_loss_image: 6.774|tagging_loss_fusion: 4.995|total_loss: 43.453 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 3564 | tagging_loss_video: 6.201|tagging_loss_audio: 8.905|tagging_loss_text: 13.150|tagging_loss_image: 6.273|tagging_loss_fusion: 4.649|total_loss: 39.178 | 64.87 Examples/sec\n",
      "INFO:tensorflow:training step 3565 | tagging_loss_video: 6.727|tagging_loss_audio: 8.396|tagging_loss_text: 17.271|tagging_loss_image: 6.415|tagging_loss_fusion: 6.149|total_loss: 44.959 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 3566 | tagging_loss_video: 5.249|tagging_loss_audio: 8.824|tagging_loss_text: 15.905|tagging_loss_image: 6.523|tagging_loss_fusion: 4.464|total_loss: 40.964 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 3567 | tagging_loss_video: 5.489|tagging_loss_audio: 10.015|tagging_loss_text: 16.945|tagging_loss_image: 8.202|tagging_loss_fusion: 4.891|total_loss: 45.541 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 3568 | tagging_loss_video: 6.144|tagging_loss_audio: 8.559|tagging_loss_text: 12.296|tagging_loss_image: 6.996|tagging_loss_fusion: 6.093|total_loss: 40.089 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 3569 | tagging_loss_video: 4.332|tagging_loss_audio: 9.130|tagging_loss_text: 13.645|tagging_loss_image: 6.057|tagging_loss_fusion: 3.460|total_loss: 36.624 | 69.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3570 |tagging_loss_video: 4.984|tagging_loss_audio: 9.179|tagging_loss_text: 13.015|tagging_loss_image: 5.457|tagging_loss_fusion: 4.724|total_loss: 37.360 | Examples/sec: 64.89\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3571 | tagging_loss_video: 4.782|tagging_loss_audio: 9.365|tagging_loss_text: 17.945|tagging_loss_image: 6.234|tagging_loss_fusion: 3.768|total_loss: 42.093 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 3572 | tagging_loss_video: 7.220|tagging_loss_audio: 9.526|tagging_loss_text: 17.430|tagging_loss_image: 5.977|tagging_loss_fusion: 7.209|total_loss: 47.362 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 3573 | tagging_loss_video: 5.445|tagging_loss_audio: 7.428|tagging_loss_text: 12.861|tagging_loss_image: 6.393|tagging_loss_fusion: 6.357|total_loss: 38.484 | 66.02 Examples/sec\n",
      "INFO:tensorflow:training step 3574 | tagging_loss_video: 6.296|tagging_loss_audio: 9.389|tagging_loss_text: 13.393|tagging_loss_image: 5.523|tagging_loss_fusion: 6.532|total_loss: 41.133 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 3575 | tagging_loss_video: 6.983|tagging_loss_audio: 9.198|tagging_loss_text: 17.922|tagging_loss_image: 6.714|tagging_loss_fusion: 6.410|total_loss: 47.228 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 3576 | tagging_loss_video: 5.822|tagging_loss_audio: 9.063|tagging_loss_text: 12.383|tagging_loss_image: 5.193|tagging_loss_fusion: 5.203|total_loss: 37.663 | 66.69 Examples/sec\n",
      "INFO:tensorflow:training step 3577 | tagging_loss_video: 5.764|tagging_loss_audio: 8.164|tagging_loss_text: 13.704|tagging_loss_image: 6.483|tagging_loss_fusion: 5.359|total_loss: 39.474 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 3578 | tagging_loss_video: 6.089|tagging_loss_audio: 10.622|tagging_loss_text: 18.477|tagging_loss_image: 7.098|tagging_loss_fusion: 6.052|total_loss: 48.337 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 3579 | tagging_loss_video: 5.365|tagging_loss_audio: 9.192|tagging_loss_text: 13.926|tagging_loss_image: 5.395|tagging_loss_fusion: 4.725|total_loss: 38.604 | 70.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3580 |tagging_loss_video: 5.202|tagging_loss_audio: 8.033|tagging_loss_text: 15.402|tagging_loss_image: 6.018|tagging_loss_fusion: 5.843|total_loss: 40.498 | Examples/sec: 70.73\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3581 | tagging_loss_video: 6.469|tagging_loss_audio: 10.318|tagging_loss_text: 13.217|tagging_loss_image: 4.634|tagging_loss_fusion: 4.898|total_loss: 39.536 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 3582 | tagging_loss_video: 6.630|tagging_loss_audio: 10.343|tagging_loss_text: 15.466|tagging_loss_image: 7.199|tagging_loss_fusion: 5.999|total_loss: 45.636 | 64.15 Examples/sec\n",
      "INFO:tensorflow:training step 3583 | tagging_loss_video: 5.147|tagging_loss_audio: 8.815|tagging_loss_text: 13.493|tagging_loss_image: 6.555|tagging_loss_fusion: 3.772|total_loss: 37.783 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 3584 | tagging_loss_video: 6.316|tagging_loss_audio: 10.036|tagging_loss_text: 14.994|tagging_loss_image: 7.162|tagging_loss_fusion: 4.784|total_loss: 43.292 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 3585 | tagging_loss_video: 5.340|tagging_loss_audio: 8.932|tagging_loss_text: 13.957|tagging_loss_image: 5.236|tagging_loss_fusion: 3.893|total_loss: 37.357 | 59.25 Examples/sec\n",
      "INFO:tensorflow:training step 3586 | tagging_loss_video: 5.368|tagging_loss_audio: 7.475|tagging_loss_text: 13.055|tagging_loss_image: 5.240|tagging_loss_fusion: 4.714|total_loss: 35.850 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 3587 | tagging_loss_video: 6.060|tagging_loss_audio: 9.006|tagging_loss_text: 11.999|tagging_loss_image: 7.004|tagging_loss_fusion: 5.507|total_loss: 39.576 | 66.77 Examples/sec\n",
      "INFO:tensorflow:training step 3588 | tagging_loss_video: 5.546|tagging_loss_audio: 9.109|tagging_loss_text: 14.188|tagging_loss_image: 6.039|tagging_loss_fusion: 4.997|total_loss: 39.880 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 3589 | tagging_loss_video: 6.675|tagging_loss_audio: 9.008|tagging_loss_text: 15.187|tagging_loss_image: 6.727|tagging_loss_fusion: 6.279|total_loss: 43.876 | 67.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3590 |tagging_loss_video: 5.033|tagging_loss_audio: 7.912|tagging_loss_text: 14.478|tagging_loss_image: 5.791|tagging_loss_fusion: 4.605|total_loss: 37.818 | Examples/sec: 69.04\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.80 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 3591 | tagging_loss_video: 5.654|tagging_loss_audio: 8.532|tagging_loss_text: 13.447|tagging_loss_image: 5.835|tagging_loss_fusion: 5.345|total_loss: 38.812 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 3592 | tagging_loss_video: 5.163|tagging_loss_audio: 8.764|tagging_loss_text: 15.980|tagging_loss_image: 7.423|tagging_loss_fusion: 4.841|total_loss: 42.170 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3593 | tagging_loss_video: 6.054|tagging_loss_audio: 9.376|tagging_loss_text: 13.570|tagging_loss_image: 6.426|tagging_loss_fusion: 4.670|total_loss: 40.095 | 71.19 Examples/sec\n",
      "INFO:tensorflow:training step 3594 | tagging_loss_video: 5.791|tagging_loss_audio: 9.577|tagging_loss_text: 15.382|tagging_loss_image: 6.460|tagging_loss_fusion: 3.642|total_loss: 40.852 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 3595 | tagging_loss_video: 6.037|tagging_loss_audio: 10.819|tagging_loss_text: 11.838|tagging_loss_image: 7.363|tagging_loss_fusion: 5.864|total_loss: 41.921 | 66.99 Examples/sec\n",
      "INFO:tensorflow:training step 3596 | tagging_loss_video: 5.264|tagging_loss_audio: 8.605|tagging_loss_text: 14.397|tagging_loss_image: 6.491|tagging_loss_fusion: 4.628|total_loss: 39.384 | 61.54 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 3597.\n",
      "INFO:tensorflow:training step 3597 | tagging_loss_video: 6.159|tagging_loss_audio: 8.541|tagging_loss_text: 10.791|tagging_loss_image: 5.703|tagging_loss_fusion: 6.285|total_loss: 37.479 | 50.80 Examples/sec\n",
      "INFO:tensorflow:training step 3598 | tagging_loss_video: 5.727|tagging_loss_audio: 8.644|tagging_loss_text: 15.585|tagging_loss_image: 5.968|tagging_loss_fusion: 6.548|total_loss: 42.471 | 56.12 Examples/sec\n",
      "INFO:tensorflow:training step 3599 | tagging_loss_video: 6.033|tagging_loss_audio: 8.906|tagging_loss_text: 14.981|tagging_loss_image: 5.328|tagging_loss_fusion: 4.415|total_loss: 39.664 | 68.50 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3600 |tagging_loss_video: 5.642|tagging_loss_audio: 9.131|tagging_loss_text: 14.210|tagging_loss_image: 6.455|tagging_loss_fusion: 5.576|total_loss: 41.014 | Examples/sec: 71.15\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3601 | tagging_loss_video: 6.177|tagging_loss_audio: 9.679|tagging_loss_text: 14.658|tagging_loss_image: 5.759|tagging_loss_fusion: 4.974|total_loss: 41.246 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 3602 | tagging_loss_video: 5.235|tagging_loss_audio: 7.813|tagging_loss_text: 12.865|tagging_loss_image: 5.521|tagging_loss_fusion: 5.204|total_loss: 36.638 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 3603 | tagging_loss_video: 6.860|tagging_loss_audio: 10.227|tagging_loss_text: 16.337|tagging_loss_image: 6.866|tagging_loss_fusion: 7.926|total_loss: 48.216 | 65.08 Examples/sec\n",
      "INFO:tensorflow:training step 3604 | tagging_loss_video: 6.021|tagging_loss_audio: 8.860|tagging_loss_text: 15.032|tagging_loss_image: 7.254|tagging_loss_fusion: 4.684|total_loss: 41.852 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 3605 | tagging_loss_video: 6.263|tagging_loss_audio: 8.882|tagging_loss_text: 16.711|tagging_loss_image: 6.557|tagging_loss_fusion: 4.946|total_loss: 43.359 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 3606 | tagging_loss_video: 7.788|tagging_loss_audio: 10.990|tagging_loss_text: 17.692|tagging_loss_image: 6.529|tagging_loss_fusion: 5.545|total_loss: 48.544 | 66.39 Examples/sec\n",
      "INFO:tensorflow:training step 3607 | tagging_loss_video: 7.204|tagging_loss_audio: 10.938|tagging_loss_text: 16.609|tagging_loss_image: 5.728|tagging_loss_fusion: 5.699|total_loss: 46.178 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 3608 | tagging_loss_video: 5.932|tagging_loss_audio: 9.180|tagging_loss_text: 13.170|tagging_loss_image: 5.882|tagging_loss_fusion: 7.626|total_loss: 41.790 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 3609 | tagging_loss_video: 6.490|tagging_loss_audio: 9.638|tagging_loss_text: 15.821|tagging_loss_image: 6.668|tagging_loss_fusion: 6.423|total_loss: 45.040 | 60.95 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3610 |tagging_loss_video: 6.146|tagging_loss_audio: 9.852|tagging_loss_text: 17.851|tagging_loss_image: 6.479|tagging_loss_fusion: 5.914|total_loss: 46.242 | Examples/sec: 71.44\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3611 | tagging_loss_video: 5.891|tagging_loss_audio: 8.636|tagging_loss_text: 12.994|tagging_loss_image: 6.485|tagging_loss_fusion: 7.017|total_loss: 41.024 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 3612 | tagging_loss_video: 6.651|tagging_loss_audio: 10.020|tagging_loss_text: 15.965|tagging_loss_image: 7.247|tagging_loss_fusion: 5.647|total_loss: 45.530 | 61.65 Examples/sec\n",
      "INFO:tensorflow:training step 3613 | tagging_loss_video: 6.730|tagging_loss_audio: 8.588|tagging_loss_text: 15.213|tagging_loss_image: 6.455|tagging_loss_fusion: 6.915|total_loss: 43.902 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 3614 | tagging_loss_video: 5.174|tagging_loss_audio: 10.030|tagging_loss_text: 18.842|tagging_loss_image: 7.395|tagging_loss_fusion: 3.521|total_loss: 44.962 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3615 | tagging_loss_video: 6.682|tagging_loss_audio: 8.614|tagging_loss_text: 16.184|tagging_loss_image: 7.136|tagging_loss_fusion: 5.651|total_loss: 44.267 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 3616 | tagging_loss_video: 7.362|tagging_loss_audio: 11.751|tagging_loss_text: 15.655|tagging_loss_image: 7.190|tagging_loss_fusion: 7.793|total_loss: 49.751 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 3617 | tagging_loss_video: 5.952|tagging_loss_audio: 11.241|tagging_loss_text: 13.120|tagging_loss_image: 6.571|tagging_loss_fusion: 5.820|total_loss: 42.704 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 3618 | tagging_loss_video: 6.059|tagging_loss_audio: 10.351|tagging_loss_text: 15.475|tagging_loss_image: 6.666|tagging_loss_fusion: 5.037|total_loss: 43.588 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 3619 | tagging_loss_video: 5.394|tagging_loss_audio: 9.582|tagging_loss_text: 14.821|tagging_loss_image: 6.146|tagging_loss_fusion: 5.852|total_loss: 41.795 | 68.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3620 |tagging_loss_video: 7.091|tagging_loss_audio: 11.330|tagging_loss_text: 15.913|tagging_loss_image: 6.100|tagging_loss_fusion: 7.611|total_loss: 48.044 | Examples/sec: 60.99\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 3621 | tagging_loss_video: 6.865|tagging_loss_audio: 11.150|tagging_loss_text: 18.023|tagging_loss_image: 8.137|tagging_loss_fusion: 6.545|total_loss: 50.720 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 3622 | tagging_loss_video: 6.514|tagging_loss_audio: 9.683|tagging_loss_text: 16.775|tagging_loss_image: 7.472|tagging_loss_fusion: 6.892|total_loss: 47.336 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 3623 | tagging_loss_video: 6.613|tagging_loss_audio: 11.748|tagging_loss_text: 17.426|tagging_loss_image: 7.333|tagging_loss_fusion: 5.635|total_loss: 48.756 | 64.27 Examples/sec\n",
      "INFO:tensorflow:training step 3624 | tagging_loss_video: 6.695|tagging_loss_audio: 11.941|tagging_loss_text: 18.338|tagging_loss_image: 7.852|tagging_loss_fusion: 7.148|total_loss: 51.974 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 3625 | tagging_loss_video: 6.740|tagging_loss_audio: 9.229|tagging_loss_text: 15.895|tagging_loss_image: 6.441|tagging_loss_fusion: 6.627|total_loss: 44.932 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 3626 | tagging_loss_video: 6.591|tagging_loss_audio: 10.313|tagging_loss_text: 13.324|tagging_loss_image: 6.356|tagging_loss_fusion: 6.182|total_loss: 42.765 | 64.52 Examples/sec\n",
      "INFO:tensorflow:training step 3627 | tagging_loss_video: 5.332|tagging_loss_audio: 10.574|tagging_loss_text: 14.769|tagging_loss_image: 7.452|tagging_loss_fusion: 5.143|total_loss: 43.269 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 3628 | tagging_loss_video: 5.501|tagging_loss_audio: 10.384|tagging_loss_text: 14.423|tagging_loss_image: 6.904|tagging_loss_fusion: 4.739|total_loss: 41.950 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 3629 | tagging_loss_video: 6.469|tagging_loss_audio: 10.465|tagging_loss_text: 17.953|tagging_loss_image: 5.822|tagging_loss_fusion: 5.721|total_loss: 46.430 | 67.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3630 |tagging_loss_video: 5.752|tagging_loss_audio: 11.274|tagging_loss_text: 17.817|tagging_loss_image: 7.015|tagging_loss_fusion: 5.256|total_loss: 47.113 | Examples/sec: 69.98\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.87 | precision@0.5: 0.94 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3631 | tagging_loss_video: 6.215|tagging_loss_audio: 10.970|tagging_loss_text: 16.269|tagging_loss_image: 5.821|tagging_loss_fusion: 5.245|total_loss: 44.520 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 3632 | tagging_loss_video: 5.778|tagging_loss_audio: 9.740|tagging_loss_text: 15.719|tagging_loss_image: 6.415|tagging_loss_fusion: 4.937|total_loss: 42.589 | 66.30 Examples/sec\n",
      "INFO:tensorflow:training step 3633 | tagging_loss_video: 4.397|tagging_loss_audio: 10.667|tagging_loss_text: 19.093|tagging_loss_image: 6.645|tagging_loss_fusion: 3.075|total_loss: 43.877 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 3634 | tagging_loss_video: 5.877|tagging_loss_audio: 9.459|tagging_loss_text: 13.513|tagging_loss_image: 6.444|tagging_loss_fusion: 5.372|total_loss: 40.665 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 3635 | tagging_loss_video: 6.878|tagging_loss_audio: 11.684|tagging_loss_text: 15.419|tagging_loss_image: 7.417|tagging_loss_fusion: 6.671|total_loss: 48.070 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 3636 | tagging_loss_video: 5.343|tagging_loss_audio: 9.191|tagging_loss_text: 12.841|tagging_loss_image: 5.828|tagging_loss_fusion: 3.759|total_loss: 36.962 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 3637 | tagging_loss_video: 6.183|tagging_loss_audio: 8.734|tagging_loss_text: 15.862|tagging_loss_image: 6.367|tagging_loss_fusion: 5.575|total_loss: 42.720 | 64.64 Examples/sec\n",
      "INFO:tensorflow:training step 3638 | tagging_loss_video: 6.279|tagging_loss_audio: 8.542|tagging_loss_text: 15.261|tagging_loss_image: 6.154|tagging_loss_fusion: 4.858|total_loss: 41.095 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 3639 | tagging_loss_video: 6.210|tagging_loss_audio: 9.375|tagging_loss_text: 14.186|tagging_loss_image: 6.415|tagging_loss_fusion: 5.885|total_loss: 42.071 | 70.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3640 |tagging_loss_video: 5.382|tagging_loss_audio: 7.869|tagging_loss_text: 11.881|tagging_loss_image: 5.952|tagging_loss_fusion: 3.843|total_loss: 34.926 | Examples/sec: 67.24\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3641 | tagging_loss_video: 5.376|tagging_loss_audio: 8.446|tagging_loss_text: 11.413|tagging_loss_image: 5.793|tagging_loss_fusion: 4.088|total_loss: 35.117 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 3642 | tagging_loss_video: 6.315|tagging_loss_audio: 8.965|tagging_loss_text: 15.337|tagging_loss_image: 6.086|tagging_loss_fusion: 5.769|total_loss: 42.472 | 72.06 Examples/sec\n",
      "INFO:tensorflow:training step 3643 | tagging_loss_video: 6.092|tagging_loss_audio: 9.356|tagging_loss_text: 15.385|tagging_loss_image: 6.618|tagging_loss_fusion: 4.647|total_loss: 42.098 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 3644 | tagging_loss_video: 5.371|tagging_loss_audio: 10.596|tagging_loss_text: 14.547|tagging_loss_image: 5.688|tagging_loss_fusion: 4.223|total_loss: 40.426 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 3645 | tagging_loss_video: 6.240|tagging_loss_audio: 9.156|tagging_loss_text: 11.638|tagging_loss_image: 5.512|tagging_loss_fusion: 5.987|total_loss: 38.533 | 61.20 Examples/sec\n",
      "INFO:tensorflow:training step 3646 | tagging_loss_video: 6.329|tagging_loss_audio: 9.709|tagging_loss_text: 16.040|tagging_loss_image: 6.290|tagging_loss_fusion: 5.328|total_loss: 43.695 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 3647 | tagging_loss_video: 5.783|tagging_loss_audio: 9.564|tagging_loss_text: 16.606|tagging_loss_image: 6.138|tagging_loss_fusion: 5.006|total_loss: 43.096 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 3648 | tagging_loss_video: 5.911|tagging_loss_audio: 10.470|tagging_loss_text: 14.871|tagging_loss_image: 7.381|tagging_loss_fusion: 6.350|total_loss: 44.984 | 66.26 Examples/sec\n",
      "INFO:tensorflow:training step 3649 | tagging_loss_video: 6.962|tagging_loss_audio: 9.787|tagging_loss_text: 16.028|tagging_loss_image: 6.911|tagging_loss_fusion: 5.416|total_loss: 45.104 | 70.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3650 |tagging_loss_video: 5.282|tagging_loss_audio: 11.502|tagging_loss_text: 14.408|tagging_loss_image: 7.368|tagging_loss_fusion: 4.209|total_loss: 42.770 | Examples/sec: 67.09\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3651 | tagging_loss_video: 6.028|tagging_loss_audio: 10.511|tagging_loss_text: 16.156|tagging_loss_image: 7.613|tagging_loss_fusion: 6.869|total_loss: 47.177 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 3652 | tagging_loss_video: 6.426|tagging_loss_audio: 10.062|tagging_loss_text: 17.115|tagging_loss_image: 6.526|tagging_loss_fusion: 6.078|total_loss: 46.207 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 3653 | tagging_loss_video: 6.847|tagging_loss_audio: 10.740|tagging_loss_text: 15.678|tagging_loss_image: 7.169|tagging_loss_fusion: 7.431|total_loss: 47.866 | 72.25 Examples/sec\n",
      "INFO:tensorflow:training step 3654 | tagging_loss_video: 5.785|tagging_loss_audio: 9.447|tagging_loss_text: 14.679|tagging_loss_image: 6.718|tagging_loss_fusion: 4.631|total_loss: 41.260 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 3655 | tagging_loss_video: 7.659|tagging_loss_audio: 11.690|tagging_loss_text: 17.214|tagging_loss_image: 5.917|tagging_loss_fusion: 4.995|total_loss: 47.475 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 3656 | tagging_loss_video: 4.544|tagging_loss_audio: 9.075|tagging_loss_text: 12.542|tagging_loss_image: 6.215|tagging_loss_fusion: 3.880|total_loss: 36.255 | 59.30 Examples/sec\n",
      "INFO:tensorflow:training step 3657 | tagging_loss_video: 6.324|tagging_loss_audio: 10.466|tagging_loss_text: 18.526|tagging_loss_image: 5.913|tagging_loss_fusion: 5.384|total_loss: 46.613 | 72.16 Examples/sec\n",
      "INFO:tensorflow:training step 3658 | tagging_loss_video: 6.380|tagging_loss_audio: 9.862|tagging_loss_text: 15.351|tagging_loss_image: 6.532|tagging_loss_fusion: 4.414|total_loss: 42.540 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 3659 | tagging_loss_video: 4.584|tagging_loss_audio: 8.572|tagging_loss_text: 13.488|tagging_loss_image: 4.422|tagging_loss_fusion: 2.780|total_loss: 33.846 | 69.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3660 |tagging_loss_video: 6.452|tagging_loss_audio: 8.129|tagging_loss_text: 14.195|tagging_loss_image: 6.371|tagging_loss_fusion: 5.868|total_loss: 41.017 | Examples/sec: 70.21\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3661 | tagging_loss_video: 5.420|tagging_loss_audio: 10.882|tagging_loss_text: 13.824|tagging_loss_image: 6.208|tagging_loss_fusion: 3.685|total_loss: 40.019 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 3662 | tagging_loss_video: 6.718|tagging_loss_audio: 9.298|tagging_loss_text: 12.150|tagging_loss_image: 5.957|tagging_loss_fusion: 5.623|total_loss: 39.747 | 62.00 Examples/sec\n",
      "INFO:tensorflow:training step 3663 | tagging_loss_video: 7.121|tagging_loss_audio: 10.169|tagging_loss_text: 18.485|tagging_loss_image: 7.452|tagging_loss_fusion: 6.684|total_loss: 49.911 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 3664 | tagging_loss_video: 5.816|tagging_loss_audio: 9.190|tagging_loss_text: 8.684|tagging_loss_image: 6.456|tagging_loss_fusion: 5.592|total_loss: 35.738 | 67.01 Examples/sec\n",
      "INFO:tensorflow:training step 3665 | tagging_loss_video: 6.776|tagging_loss_audio: 9.327|tagging_loss_text: 15.649|tagging_loss_image: 6.094|tagging_loss_fusion: 6.158|total_loss: 44.004 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 3666 | tagging_loss_video: 6.256|tagging_loss_audio: 10.192|tagging_loss_text: 16.362|tagging_loss_image: 7.107|tagging_loss_fusion: 5.698|total_loss: 45.615 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 3667 | tagging_loss_video: 6.529|tagging_loss_audio: 9.918|tagging_loss_text: 14.095|tagging_loss_image: 6.327|tagging_loss_fusion: 6.235|total_loss: 43.103 | 66.09 Examples/sec\n",
      "INFO:tensorflow:training step 3668 | tagging_loss_video: 5.420|tagging_loss_audio: 9.147|tagging_loss_text: 15.928|tagging_loss_image: 5.748|tagging_loss_fusion: 3.594|total_loss: 39.836 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 3669 | tagging_loss_video: 6.328|tagging_loss_audio: 8.660|tagging_loss_text: 15.391|tagging_loss_image: 5.811|tagging_loss_fusion: 5.723|total_loss: 41.913 | 69.15 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3670 |tagging_loss_video: 5.122|tagging_loss_audio: 9.631|tagging_loss_text: 14.999|tagging_loss_image: 7.217|tagging_loss_fusion: 3.936|total_loss: 40.906 | Examples/sec: 61.20\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.84 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3671 | tagging_loss_video: 5.614|tagging_loss_audio: 10.200|tagging_loss_text: 12.060|tagging_loss_image: 5.715|tagging_loss_fusion: 4.425|total_loss: 38.014 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 3672 | tagging_loss_video: 6.131|tagging_loss_audio: 8.595|tagging_loss_text: 17.021|tagging_loss_image: 5.863|tagging_loss_fusion: 5.753|total_loss: 43.363 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3673 | tagging_loss_video: 6.205|tagging_loss_audio: 8.260|tagging_loss_text: 13.017|tagging_loss_image: 5.837|tagging_loss_fusion: 4.503|total_loss: 37.821 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 3674 | tagging_loss_video: 6.252|tagging_loss_audio: 8.647|tagging_loss_text: 17.358|tagging_loss_image: 5.859|tagging_loss_fusion: 6.576|total_loss: 44.692 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 3675 | tagging_loss_video: 5.940|tagging_loss_audio: 8.629|tagging_loss_text: 18.617|tagging_loss_image: 6.988|tagging_loss_fusion: 5.037|total_loss: 45.212 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 3676 | tagging_loss_video: 4.971|tagging_loss_audio: 9.133|tagging_loss_text: 14.663|tagging_loss_image: 6.955|tagging_loss_fusion: 3.904|total_loss: 39.625 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 3677 | tagging_loss_video: 6.960|tagging_loss_audio: 10.434|tagging_loss_text: 17.018|tagging_loss_image: 7.409|tagging_loss_fusion: 6.520|total_loss: 48.341 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 3678 | tagging_loss_video: 4.391|tagging_loss_audio: 9.662|tagging_loss_text: 15.928|tagging_loss_image: 5.794|tagging_loss_fusion: 3.686|total_loss: 39.461 | 66.77 Examples/sec\n",
      "INFO:tensorflow:training step 3679 | tagging_loss_video: 6.552|tagging_loss_audio: 9.193|tagging_loss_text: 16.889|tagging_loss_image: 6.558|tagging_loss_fusion: 7.169|total_loss: 46.361 | 70.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3680 |tagging_loss_video: 6.895|tagging_loss_audio: 9.791|tagging_loss_text: 14.791|tagging_loss_image: 7.336|tagging_loss_fusion: 7.421|total_loss: 46.235 | Examples/sec: 70.68\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.79 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 3681 | tagging_loss_video: 5.470|tagging_loss_audio: 9.850|tagging_loss_text: 14.797|tagging_loss_image: 6.813|tagging_loss_fusion: 4.935|total_loss: 41.866 | 64.96 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3682 | tagging_loss_video: 6.727|tagging_loss_audio: 9.709|tagging_loss_text: 16.950|tagging_loss_image: 6.756|tagging_loss_fusion: 7.404|total_loss: 47.545 | 67.19 Examples/sec\n",
      "INFO:tensorflow:training step 3683 | tagging_loss_video: 6.393|tagging_loss_audio: 9.156|tagging_loss_text: 18.814|tagging_loss_image: 5.904|tagging_loss_fusion: 4.746|total_loss: 45.012 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 3684 | tagging_loss_video: 5.725|tagging_loss_audio: 10.768|tagging_loss_text: 16.611|tagging_loss_image: 6.696|tagging_loss_fusion: 4.699|total_loss: 44.499 | 71.64 Examples/sec\n",
      "INFO:tensorflow:training step 3685 | tagging_loss_video: 6.277|tagging_loss_audio: 8.817|tagging_loss_text: 18.071|tagging_loss_image: 6.068|tagging_loss_fusion: 4.549|total_loss: 43.782 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 3686 | tagging_loss_video: 5.607|tagging_loss_audio: 9.374|tagging_loss_text: 15.418|tagging_loss_image: 7.098|tagging_loss_fusion: 5.816|total_loss: 43.314 | 72.64 Examples/sec\n",
      "INFO:tensorflow:training step 3687 | tagging_loss_video: 6.521|tagging_loss_audio: 10.300|tagging_loss_text: 16.831|tagging_loss_image: 8.164|tagging_loss_fusion: 6.178|total_loss: 47.994 | 60.25 Examples/sec\n",
      "INFO:tensorflow:training step 3688 | tagging_loss_video: 6.373|tagging_loss_audio: 8.640|tagging_loss_text: 13.468|tagging_loss_image: 5.398|tagging_loss_fusion: 5.908|total_loss: 39.787 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 3689 | tagging_loss_video: 6.570|tagging_loss_audio: 9.194|tagging_loss_text: 15.071|tagging_loss_image: 6.151|tagging_loss_fusion: 5.577|total_loss: 42.563 | 67.49 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3690 |tagging_loss_video: 7.012|tagging_loss_audio: 9.847|tagging_loss_text: 16.114|tagging_loss_image: 6.382|tagging_loss_fusion: 5.801|total_loss: 45.155 | Examples/sec: 65.49\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3691 | tagging_loss_video: 5.582|tagging_loss_audio: 9.974|tagging_loss_text: 15.378|tagging_loss_image: 7.052|tagging_loss_fusion: 4.867|total_loss: 42.853 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 3692 | tagging_loss_video: 4.919|tagging_loss_audio: 9.418|tagging_loss_text: 17.544|tagging_loss_image: 5.734|tagging_loss_fusion: 4.577|total_loss: 42.191 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 3693 | tagging_loss_video: 6.293|tagging_loss_audio: 10.059|tagging_loss_text: 12.943|tagging_loss_image: 6.337|tagging_loss_fusion: 5.637|total_loss: 41.269 | 66.45 Examples/sec\n",
      "INFO:tensorflow:training step 3694 | tagging_loss_video: 5.874|tagging_loss_audio: 10.132|tagging_loss_text: 15.560|tagging_loss_image: 7.714|tagging_loss_fusion: 4.315|total_loss: 43.595 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 3695 | tagging_loss_video: 6.516|tagging_loss_audio: 10.450|tagging_loss_text: 18.101|tagging_loss_image: 7.390|tagging_loss_fusion: 7.257|total_loss: 49.715 | 65.65 Examples/sec\n",
      "INFO:tensorflow:training step 3696 | tagging_loss_video: 5.482|tagging_loss_audio: 10.823|tagging_loss_text: 12.668|tagging_loss_image: 6.852|tagging_loss_fusion: 3.897|total_loss: 39.723 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 3697 | tagging_loss_video: 5.122|tagging_loss_audio: 10.869|tagging_loss_text: 15.999|tagging_loss_image: 7.561|tagging_loss_fusion: 4.718|total_loss: 44.269 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 3698 | tagging_loss_video: 6.330|tagging_loss_audio: 7.582|tagging_loss_text: 16.341|tagging_loss_image: 5.957|tagging_loss_fusion: 4.741|total_loss: 40.951 | 61.11 Examples/sec\n",
      "INFO:tensorflow:training step 3699 | tagging_loss_video: 6.045|tagging_loss_audio: 8.751|tagging_loss_text: 14.410|tagging_loss_image: 5.187|tagging_loss_fusion: 4.492|total_loss: 38.884 | 72.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3700 |tagging_loss_video: 5.245|tagging_loss_audio: 8.829|tagging_loss_text: 15.456|tagging_loss_image: 5.765|tagging_loss_fusion: 3.989|total_loss: 39.283 | Examples/sec: 67.03\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 3701 | tagging_loss_video: 6.042|tagging_loss_audio: 8.514|tagging_loss_text: 13.981|tagging_loss_image: 5.316|tagging_loss_fusion: 4.270|total_loss: 38.122 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 3702 | tagging_loss_video: 4.554|tagging_loss_audio: 8.937|tagging_loss_text: 14.738|tagging_loss_image: 6.351|tagging_loss_fusion: 3.845|total_loss: 38.425 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 3703 | tagging_loss_video: 5.655|tagging_loss_audio: 9.255|tagging_loss_text: 13.802|tagging_loss_image: 6.702|tagging_loss_fusion: 5.608|total_loss: 41.021 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 3704 | tagging_loss_video: 5.918|tagging_loss_audio: 8.405|tagging_loss_text: 13.949|tagging_loss_image: 6.434|tagging_loss_fusion: 5.870|total_loss: 40.576 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 3705 | tagging_loss_video: 4.885|tagging_loss_audio: 8.323|tagging_loss_text: 15.943|tagging_loss_image: 5.556|tagging_loss_fusion: 3.415|total_loss: 38.123 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 3706 | tagging_loss_video: 7.453|tagging_loss_audio: 9.887|tagging_loss_text: 13.242|tagging_loss_image: 5.808|tagging_loss_fusion: 5.869|total_loss: 42.259 | 60.08 Examples/sec\n",
      "INFO:tensorflow:training step 3707 | tagging_loss_video: 5.071|tagging_loss_audio: 8.687|tagging_loss_text: 19.146|tagging_loss_image: 6.545|tagging_loss_fusion: 4.253|total_loss: 43.702 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 3708 | tagging_loss_video: 5.545|tagging_loss_audio: 8.892|tagging_loss_text: 15.493|tagging_loss_image: 5.086|tagging_loss_fusion: 4.526|total_loss: 39.541 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 3709 | tagging_loss_video: 5.404|tagging_loss_audio: 8.202|tagging_loss_text: 16.095|tagging_loss_image: 5.102|tagging_loss_fusion: 4.141|total_loss: 38.944 | 70.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3710 |tagging_loss_video: 5.431|tagging_loss_audio: 9.069|tagging_loss_text: 17.999|tagging_loss_image: 6.107|tagging_loss_fusion: 2.715|total_loss: 41.321 | Examples/sec: 69.68\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 3711 | tagging_loss_video: 5.590|tagging_loss_audio: 8.271|tagging_loss_text: 16.714|tagging_loss_image: 6.593|tagging_loss_fusion: 4.584|total_loss: 41.752 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 3712 | tagging_loss_video: 3.981|tagging_loss_audio: 8.732|tagging_loss_text: 14.156|tagging_loss_image: 5.850|tagging_loss_fusion: 3.088|total_loss: 35.806 | 65.25 Examples/sec\n",
      "INFO:tensorflow:training step 3713 | tagging_loss_video: 6.616|tagging_loss_audio: 8.153|tagging_loss_text: 16.098|tagging_loss_image: 5.780|tagging_loss_fusion: 6.214|total_loss: 42.862 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 3714 | tagging_loss_video: 4.838|tagging_loss_audio: 8.028|tagging_loss_text: 17.243|tagging_loss_image: 6.083|tagging_loss_fusion: 3.467|total_loss: 39.660 | 71.97 Examples/sec\n",
      "INFO:tensorflow:training step 3715 | tagging_loss_video: 6.105|tagging_loss_audio: 9.185|tagging_loss_text: 15.650|tagging_loss_image: 5.824|tagging_loss_fusion: 4.397|total_loss: 41.162 | 61.92 Examples/sec\n",
      "INFO:tensorflow:training step 3716 | tagging_loss_video: 5.559|tagging_loss_audio: 6.361|tagging_loss_text: 12.413|tagging_loss_image: 6.045|tagging_loss_fusion: 3.879|total_loss: 34.257 | 68.32 Examples/sec\n",
      "INFO:tensorflow:training step 3717 | tagging_loss_video: 6.227|tagging_loss_audio: 9.689|tagging_loss_text: 13.203|tagging_loss_image: 6.339|tagging_loss_fusion: 6.056|total_loss: 41.515 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 3718 | tagging_loss_video: 6.021|tagging_loss_audio: 9.045|tagging_loss_text: 14.606|tagging_loss_image: 5.650|tagging_loss_fusion: 4.428|total_loss: 39.750 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 3719 | tagging_loss_video: 5.698|tagging_loss_audio: 9.091|tagging_loss_text: 14.051|tagging_loss_image: 5.915|tagging_loss_fusion: 4.990|total_loss: 39.745 | 69.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3720 |tagging_loss_video: 5.982|tagging_loss_audio: 10.582|tagging_loss_text: 14.605|tagging_loss_image: 5.915|tagging_loss_fusion: 5.156|total_loss: 42.241 | Examples/sec: 68.29\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3721 | tagging_loss_video: 6.317|tagging_loss_audio: 8.703|tagging_loss_text: 16.169|tagging_loss_image: 6.651|tagging_loss_fusion: 6.334|total_loss: 44.174 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 3722 | tagging_loss_video: 5.023|tagging_loss_audio: 9.361|tagging_loss_text: 16.819|tagging_loss_image: 5.296|tagging_loss_fusion: 4.216|total_loss: 40.716 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 3723 | tagging_loss_video: 6.436|tagging_loss_audio: 8.850|tagging_loss_text: 16.440|tagging_loss_image: 7.015|tagging_loss_fusion: 5.399|total_loss: 44.139 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 3724 | tagging_loss_video: 4.604|tagging_loss_audio: 8.564|tagging_loss_text: 10.801|tagging_loss_image: 6.079|tagging_loss_fusion: 3.082|total_loss: 33.130 | 60.97 Examples/sec\n",
      "INFO:tensorflow:training step 3725 | tagging_loss_video: 4.707|tagging_loss_audio: 7.255|tagging_loss_text: 11.856|tagging_loss_image: 5.222|tagging_loss_fusion: 3.798|total_loss: 32.838 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 3726 | tagging_loss_video: 5.254|tagging_loss_audio: 8.769|tagging_loss_text: 15.338|tagging_loss_image: 6.527|tagging_loss_fusion: 5.226|total_loss: 41.113 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 3727 | tagging_loss_video: 5.600|tagging_loss_audio: 8.698|tagging_loss_text: 16.739|tagging_loss_image: 5.559|tagging_loss_fusion: 5.837|total_loss: 42.433 | 63.51 Examples/sec\n",
      "INFO:tensorflow:training step 3728 | tagging_loss_video: 6.599|tagging_loss_audio: 9.893|tagging_loss_text: 15.170|tagging_loss_image: 6.032|tagging_loss_fusion: 5.045|total_loss: 42.739 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 3729 | tagging_loss_video: 5.474|tagging_loss_audio: 8.666|tagging_loss_text: 16.472|tagging_loss_image: 5.991|tagging_loss_fusion: 4.464|total_loss: 41.067 | 71.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3730 |tagging_loss_video: 5.365|tagging_loss_audio: 7.782|tagging_loss_text: 16.011|tagging_loss_image: 5.849|tagging_loss_fusion: 3.716|total_loss: 38.723 | Examples/sec: 68.04\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 3731 | tagging_loss_video: 6.004|tagging_loss_audio: 9.005|tagging_loss_text: 15.230|tagging_loss_image: 6.448|tagging_loss_fusion: 5.468|total_loss: 42.155 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 3732 | tagging_loss_video: 5.978|tagging_loss_audio: 8.906|tagging_loss_text: 14.928|tagging_loss_image: 6.833|tagging_loss_fusion: 7.628|total_loss: 44.272 | 71.99 Examples/sec\n",
      "INFO:tensorflow:training step 3733 | tagging_loss_video: 6.258|tagging_loss_audio: 8.441|tagging_loss_text: 14.682|tagging_loss_image: 6.037|tagging_loss_fusion: 6.275|total_loss: 41.694 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 3734 | tagging_loss_video: 6.287|tagging_loss_audio: 9.686|tagging_loss_text: 19.280|tagging_loss_image: 7.180|tagging_loss_fusion: 5.000|total_loss: 47.433 | 67.34 Examples/sec\n",
      "INFO:tensorflow:training step 3735 | tagging_loss_video: 6.820|tagging_loss_audio: 8.974|tagging_loss_text: 13.696|tagging_loss_image: 6.276|tagging_loss_fusion: 5.408|total_loss: 41.174 | 62.85 Examples/sec\n",
      "INFO:tensorflow:training step 3736 | tagging_loss_video: 5.794|tagging_loss_audio: 9.220|tagging_loss_text: 12.083|tagging_loss_image: 5.330|tagging_loss_fusion: 4.323|total_loss: 36.750 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 3737 | tagging_loss_video: 5.769|tagging_loss_audio: 8.873|tagging_loss_text: 12.469|tagging_loss_image: 6.362|tagging_loss_fusion: 4.146|total_loss: 37.619 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 3738 | tagging_loss_video: 5.609|tagging_loss_audio: 9.298|tagging_loss_text: 13.627|tagging_loss_image: 5.920|tagging_loss_fusion: 4.971|total_loss: 39.425 | 61.59 Examples/sec\n",
      "INFO:tensorflow:training step 3739 | tagging_loss_video: 6.839|tagging_loss_audio: 9.836|tagging_loss_text: 12.124|tagging_loss_image: 6.534|tagging_loss_fusion: 5.607|total_loss: 40.941 | 70.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3740 |tagging_loss_video: 6.127|tagging_loss_audio: 8.599|tagging_loss_text: 16.897|tagging_loss_image: 6.430|tagging_loss_fusion: 7.011|total_loss: 45.064 | Examples/sec: 70.08\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.78 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3741 | tagging_loss_video: 6.377|tagging_loss_audio: 9.052|tagging_loss_text: 18.100|tagging_loss_image: 6.031|tagging_loss_fusion: 5.112|total_loss: 44.672 | 65.94 Examples/sec\n",
      "INFO:tensorflow:training step 3742 | tagging_loss_video: 5.088|tagging_loss_audio: 8.364|tagging_loss_text: 14.809|tagging_loss_image: 5.100|tagging_loss_fusion: 3.654|total_loss: 37.015 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 3743 | tagging_loss_video: 7.616|tagging_loss_audio: 9.701|tagging_loss_text: 15.660|tagging_loss_image: 6.594|tagging_loss_fusion: 8.264|total_loss: 47.836 | 66.12 Examples/sec\n",
      "INFO:tensorflow:training step 3744 | tagging_loss_video: 5.945|tagging_loss_audio: 9.369|tagging_loss_text: 15.758|tagging_loss_image: 6.940|tagging_loss_fusion: 4.677|total_loss: 42.690 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3745 | tagging_loss_video: 5.916|tagging_loss_audio: 9.688|tagging_loss_text: 14.650|tagging_loss_image: 6.096|tagging_loss_fusion: 4.524|total_loss: 40.874 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 3746 | tagging_loss_video: 6.379|tagging_loss_audio: 10.694|tagging_loss_text: 13.605|tagging_loss_image: 5.716|tagging_loss_fusion: 4.776|total_loss: 41.169 | 59.94 Examples/sec\n",
      "INFO:tensorflow:training step 3747 | tagging_loss_video: 5.423|tagging_loss_audio: 10.427|tagging_loss_text: 15.909|tagging_loss_image: 6.574|tagging_loss_fusion: 4.636|total_loss: 42.970 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 3748 | tagging_loss_video: 5.679|tagging_loss_audio: 10.588|tagging_loss_text: 15.837|tagging_loss_image: 5.139|tagging_loss_fusion: 4.130|total_loss: 41.374 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 3749 | tagging_loss_video: 6.136|tagging_loss_audio: 10.221|tagging_loss_text: 13.745|tagging_loss_image: 6.642|tagging_loss_fusion: 5.170|total_loss: 41.914 | 69.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3750 |tagging_loss_video: 5.652|tagging_loss_audio: 11.487|tagging_loss_text: 15.609|tagging_loss_image: 7.158|tagging_loss_fusion: 6.067|total_loss: 45.973 | Examples/sec: 67.72\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3751 | tagging_loss_video: 5.332|tagging_loss_audio: 8.709|tagging_loss_text: 14.295|tagging_loss_image: 5.908|tagging_loss_fusion: 3.893|total_loss: 38.137 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 3752 | tagging_loss_video: 6.571|tagging_loss_audio: 9.020|tagging_loss_text: 15.265|tagging_loss_image: 6.530|tagging_loss_fusion: 4.500|total_loss: 41.887 | 66.54 Examples/sec\n",
      "INFO:tensorflow:training step 3753 | tagging_loss_video: 6.604|tagging_loss_audio: 9.621|tagging_loss_text: 14.519|tagging_loss_image: 6.763|tagging_loss_fusion: 5.182|total_loss: 42.689 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 3754 | tagging_loss_video: 6.259|tagging_loss_audio: 10.960|tagging_loss_text: 18.832|tagging_loss_image: 6.954|tagging_loss_fusion: 4.607|total_loss: 47.612 | 65.53 Examples/sec\n",
      "INFO:tensorflow:training step 3755 | tagging_loss_video: 5.969|tagging_loss_audio: 9.849|tagging_loss_text: 14.776|tagging_loss_image: 6.745|tagging_loss_fusion: 6.164|total_loss: 43.504 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 3756 | tagging_loss_video: 6.637|tagging_loss_audio: 10.275|tagging_loss_text: 11.928|tagging_loss_image: 7.413|tagging_loss_fusion: 6.330|total_loss: 42.583 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 3757 | tagging_loss_video: 5.925|tagging_loss_audio: 9.558|tagging_loss_text: 15.107|tagging_loss_image: 6.102|tagging_loss_fusion: 4.478|total_loss: 41.170 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 3758 | tagging_loss_video: 5.695|tagging_loss_audio: 10.763|tagging_loss_text: 13.401|tagging_loss_image: 6.928|tagging_loss_fusion: 5.319|total_loss: 42.107 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 3759 | tagging_loss_video: 5.505|tagging_loss_audio: 9.055|tagging_loss_text: 14.206|tagging_loss_image: 5.506|tagging_loss_fusion: 5.655|total_loss: 39.928 | 70.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3760 |tagging_loss_video: 6.145|tagging_loss_audio: 10.298|tagging_loss_text: 13.736|tagging_loss_image: 6.742|tagging_loss_fusion: 4.495|total_loss: 41.415 | Examples/sec: 62.08\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3761 | tagging_loss_video: 5.737|tagging_loss_audio: 9.774|tagging_loss_text: 15.142|tagging_loss_image: 7.252|tagging_loss_fusion: 4.438|total_loss: 42.344 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 3762 | tagging_loss_video: 6.112|tagging_loss_audio: 10.921|tagging_loss_text: 20.383|tagging_loss_image: 6.876|tagging_loss_fusion: 5.025|total_loss: 49.318 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 3763 | tagging_loss_video: 6.263|tagging_loss_audio: 10.052|tagging_loss_text: 13.956|tagging_loss_image: 7.253|tagging_loss_fusion: 5.899|total_loss: 43.424 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 3764 | tagging_loss_video: 7.868|tagging_loss_audio: 12.655|tagging_loss_text: 13.729|tagging_loss_image: 6.916|tagging_loss_fusion: 7.146|total_loss: 48.314 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 3765 | tagging_loss_video: 5.881|tagging_loss_audio: 9.855|tagging_loss_text: 18.688|tagging_loss_image: 5.303|tagging_loss_fusion: 5.009|total_loss: 44.736 | 71.64 Examples/sec\n",
      "INFO:tensorflow:training step 3766 | tagging_loss_video: 5.613|tagging_loss_audio: 10.432|tagging_loss_text: 17.991|tagging_loss_image: 5.857|tagging_loss_fusion: 3.849|total_loss: 43.743 | 62.23 Examples/sec\n",
      "INFO:tensorflow:training step 3767 | tagging_loss_video: 6.035|tagging_loss_audio: 9.014|tagging_loss_text: 17.454|tagging_loss_image: 6.994|tagging_loss_fusion: 5.473|total_loss: 44.969 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 3768 | tagging_loss_video: 5.667|tagging_loss_audio: 9.729|tagging_loss_text: 16.463|tagging_loss_image: 7.004|tagging_loss_fusion: 5.073|total_loss: 43.936 | 65.24 Examples/sec\n",
      "INFO:tensorflow:training step 3769 | tagging_loss_video: 6.138|tagging_loss_audio: 10.084|tagging_loss_text: 15.303|tagging_loss_image: 5.683|tagging_loss_fusion: 4.756|total_loss: 41.965 | 70.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3770 |tagging_loss_video: 7.843|tagging_loss_audio: 10.662|tagging_loss_text: 16.324|tagging_loss_image: 7.518|tagging_loss_fusion: 6.684|total_loss: 49.031 | Examples/sec: 67.32\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3771 | tagging_loss_video: 6.956|tagging_loss_audio: 10.105|tagging_loss_text: 19.948|tagging_loss_image: 7.772|tagging_loss_fusion: 6.231|total_loss: 51.012 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 3772 | tagging_loss_video: 5.663|tagging_loss_audio: 10.063|tagging_loss_text: 13.137|tagging_loss_image: 6.384|tagging_loss_fusion: 3.098|total_loss: 38.344 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 3773 | tagging_loss_video: 5.898|tagging_loss_audio: 8.923|tagging_loss_text: 17.019|tagging_loss_image: 7.065|tagging_loss_fusion: 6.305|total_loss: 45.210 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 3774 | tagging_loss_video: 5.533|tagging_loss_audio: 8.853|tagging_loss_text: 13.507|tagging_loss_image: 6.532|tagging_loss_fusion: 3.661|total_loss: 38.086 | 61.60 Examples/sec\n",
      "INFO:tensorflow:training step 3775 | tagging_loss_video: 6.213|tagging_loss_audio: 10.506|tagging_loss_text: 18.367|tagging_loss_image: 7.120|tagging_loss_fusion: 5.371|total_loss: 47.578 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 3776 | tagging_loss_video: 6.366|tagging_loss_audio: 9.426|tagging_loss_text: 16.935|tagging_loss_image: 6.207|tagging_loss_fusion: 6.311|total_loss: 45.244 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 3777 | tagging_loss_video: 6.627|tagging_loss_audio: 9.508|tagging_loss_text: 13.857|tagging_loss_image: 6.120|tagging_loss_fusion: 6.447|total_loss: 42.559 | 66.57 Examples/sec\n",
      "INFO:tensorflow:training step 3778 | tagging_loss_video: 5.843|tagging_loss_audio: 9.143|tagging_loss_text: 13.285|tagging_loss_image: 6.322|tagging_loss_fusion: 5.293|total_loss: 39.887 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 3779 | tagging_loss_video: 6.556|tagging_loss_audio: 9.817|tagging_loss_text: 15.533|tagging_loss_image: 5.825|tagging_loss_fusion: 5.138|total_loss: 42.870 | 71.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3780 |tagging_loss_video: 5.471|tagging_loss_audio: 9.365|tagging_loss_text: 13.192|tagging_loss_image: 6.180|tagging_loss_fusion: 4.306|total_loss: 38.512 | Examples/sec: 68.20\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3781 | tagging_loss_video: 5.316|tagging_loss_audio: 9.867|tagging_loss_text: 14.547|tagging_loss_image: 5.471|tagging_loss_fusion: 4.300|total_loss: 39.501 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 3782 | tagging_loss_video: 6.346|tagging_loss_audio: 9.720|tagging_loss_text: 13.263|tagging_loss_image: 6.787|tagging_loss_fusion: 6.620|total_loss: 42.735 | 72.20 Examples/sec\n",
      "INFO:tensorflow:training step 3783 | tagging_loss_video: 6.180|tagging_loss_audio: 10.561|tagging_loss_text: 14.059|tagging_loss_image: 6.743|tagging_loss_fusion: 4.566|total_loss: 42.108 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 3784 | tagging_loss_video: 6.455|tagging_loss_audio: 9.399|tagging_loss_text: 13.212|tagging_loss_image: 5.906|tagging_loss_fusion: 4.599|total_loss: 39.570 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 3785 | tagging_loss_video: 5.618|tagging_loss_audio: 9.806|tagging_loss_text: 13.582|tagging_loss_image: 6.193|tagging_loss_fusion: 5.690|total_loss: 40.889 | 60.62 Examples/sec\n",
      "INFO:tensorflow:training step 3786 | tagging_loss_video: 5.510|tagging_loss_audio: 9.685|tagging_loss_text: 16.050|tagging_loss_image: 6.453|tagging_loss_fusion: 4.229|total_loss: 41.927 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 3787 | tagging_loss_video: 6.085|tagging_loss_audio: 9.989|tagging_loss_text: 15.262|tagging_loss_image: 5.955|tagging_loss_fusion: 4.623|total_loss: 41.914 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 3788 | tagging_loss_video: 6.568|tagging_loss_audio: 10.012|tagging_loss_text: 16.215|tagging_loss_image: 5.432|tagging_loss_fusion: 5.895|total_loss: 44.121 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 3789 | tagging_loss_video: 6.213|tagging_loss_audio: 11.543|tagging_loss_text: 15.332|tagging_loss_image: 7.208|tagging_loss_fusion: 6.170|total_loss: 46.466 | 67.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3790 |tagging_loss_video: 6.541|tagging_loss_audio: 10.042|tagging_loss_text: 18.251|tagging_loss_image: 7.632|tagging_loss_fusion: 7.363|total_loss: 49.829 | Examples/sec: 69.95\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 3791 | tagging_loss_video: 6.337|tagging_loss_audio: 10.038|tagging_loss_text: 14.779|tagging_loss_image: 6.975|tagging_loss_fusion: 5.983|total_loss: 44.112 | 66.12 Examples/sec\n",
      "INFO:tensorflow:training step 3792 | tagging_loss_video: 6.408|tagging_loss_audio: 9.421|tagging_loss_text: 15.077|tagging_loss_image: 6.838|tagging_loss_fusion: 5.180|total_loss: 42.924 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 3793 | tagging_loss_video: 5.956|tagging_loss_audio: 11.046|tagging_loss_text: 14.264|tagging_loss_image: 6.715|tagging_loss_fusion: 3.880|total_loss: 41.861 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 3794 | tagging_loss_video: 6.661|tagging_loss_audio: 9.661|tagging_loss_text: 15.367|tagging_loss_image: 5.783|tagging_loss_fusion: 5.513|total_loss: 42.984 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 3795 | tagging_loss_video: 5.786|tagging_loss_audio: 9.670|tagging_loss_text: 15.058|tagging_loss_image: 6.525|tagging_loss_fusion: 5.280|total_loss: 42.319 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 3796 | tagging_loss_video: 4.368|tagging_loss_audio: 9.209|tagging_loss_text: 15.184|tagging_loss_image: 5.915|tagging_loss_fusion: 2.894|total_loss: 37.570 | 60.69 Examples/sec\n",
      "INFO:tensorflow:training step 3797 | tagging_loss_video: 6.492|tagging_loss_audio: 10.261|tagging_loss_text: 15.088|tagging_loss_image: 6.094|tagging_loss_fusion: 6.834|total_loss: 44.770 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 3798 | tagging_loss_video: 6.094|tagging_loss_audio: 9.389|tagging_loss_text: 13.956|tagging_loss_image: 6.237|tagging_loss_fusion: 4.749|total_loss: 40.425 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 3799 | tagging_loss_video: 4.822|tagging_loss_audio: 8.070|tagging_loss_text: 12.288|tagging_loss_image: 5.099|tagging_loss_fusion: 4.891|total_loss: 35.169 | 65.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3800 |tagging_loss_video: 6.580|tagging_loss_audio: 8.570|tagging_loss_text: 14.209|tagging_loss_image: 6.044|tagging_loss_fusion: 6.550|total_loss: 41.954 | Examples/sec: 69.21\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3801 | tagging_loss_video: 5.929|tagging_loss_audio: 9.013|tagging_loss_text: 16.604|tagging_loss_image: 6.565|tagging_loss_fusion: 4.600|total_loss: 42.711 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 3802 | tagging_loss_video: 6.129|tagging_loss_audio: 9.449|tagging_loss_text: 16.210|tagging_loss_image: 6.097|tagging_loss_fusion: 4.790|total_loss: 42.675 | 64.45 Examples/sec\n",
      "INFO:tensorflow:training step 3803 | tagging_loss_video: 7.452|tagging_loss_audio: 9.568|tagging_loss_text: 17.761|tagging_loss_image: 7.026|tagging_loss_fusion: 8.921|total_loss: 50.728 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 3804 | tagging_loss_video: 5.521|tagging_loss_audio: 8.939|tagging_loss_text: 15.334|tagging_loss_image: 6.654|tagging_loss_fusion: 6.262|total_loss: 42.710 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 3805 | tagging_loss_video: 5.923|tagging_loss_audio: 9.735|tagging_loss_text: 14.950|tagging_loss_image: 5.555|tagging_loss_fusion: 4.724|total_loss: 40.887 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 3806 | tagging_loss_video: 7.125|tagging_loss_audio: 9.880|tagging_loss_text: 14.947|tagging_loss_image: 6.780|tagging_loss_fusion: 5.904|total_loss: 44.636 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3807 | tagging_loss_video: 6.064|tagging_loss_audio: 10.761|tagging_loss_text: 13.430|tagging_loss_image: 6.365|tagging_loss_fusion: 4.545|total_loss: 41.164 | 65.59 Examples/sec\n",
      "INFO:tensorflow:training step 3808 | tagging_loss_video: 5.845|tagging_loss_audio: 9.265|tagging_loss_text: 13.854|tagging_loss_image: 5.357|tagging_loss_fusion: 3.491|total_loss: 37.812 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 3809 | tagging_loss_video: 5.975|tagging_loss_audio: 9.310|tagging_loss_text: 16.700|tagging_loss_image: 5.882|tagging_loss_fusion: 6.254|total_loss: 44.121 | 69.41 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3810 |tagging_loss_video: 5.665|tagging_loss_audio: 9.038|tagging_loss_text: 13.633|tagging_loss_image: 5.224|tagging_loss_fusion: 4.445|total_loss: 38.005 | Examples/sec: 65.74\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3811 | tagging_loss_video: 6.501|tagging_loss_audio: 9.752|tagging_loss_text: 12.056|tagging_loss_image: 4.333|tagging_loss_fusion: 7.332|total_loss: 39.975 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 3812 | tagging_loss_video: 6.047|tagging_loss_audio: 9.583|tagging_loss_text: 18.367|tagging_loss_image: 5.657|tagging_loss_fusion: 5.627|total_loss: 45.281 | 67.64 Examples/sec\n",
      "INFO:tensorflow:training step 3813 | tagging_loss_video: 5.516|tagging_loss_audio: 8.603|tagging_loss_text: 14.984|tagging_loss_image: 6.436|tagging_loss_fusion: 5.625|total_loss: 41.164 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 3814 | tagging_loss_video: 6.454|tagging_loss_audio: 9.325|tagging_loss_text: 13.187|tagging_loss_image: 4.795|tagging_loss_fusion: 6.563|total_loss: 40.325 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 3815 | tagging_loss_video: 5.913|tagging_loss_audio: 9.650|tagging_loss_text: 17.898|tagging_loss_image: 6.503|tagging_loss_fusion: 6.468|total_loss: 46.432 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 3816 | tagging_loss_video: 4.987|tagging_loss_audio: 9.061|tagging_loss_text: 13.938|tagging_loss_image: 6.361|tagging_loss_fusion: 4.277|total_loss: 38.624 | 63.82 Examples/sec\n",
      "INFO:tensorflow:training step 3817 | tagging_loss_video: 5.439|tagging_loss_audio: 9.694|tagging_loss_text: 19.195|tagging_loss_image: 6.561|tagging_loss_fusion: 4.166|total_loss: 45.054 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 3818 | tagging_loss_video: 5.679|tagging_loss_audio: 8.514|tagging_loss_text: 14.619|tagging_loss_image: 6.221|tagging_loss_fusion: 5.376|total_loss: 40.409 | 66.28 Examples/sec\n",
      "INFO:tensorflow:training step 3819 | tagging_loss_video: 6.016|tagging_loss_audio: 9.158|tagging_loss_text: 13.361|tagging_loss_image: 6.669|tagging_loss_fusion: 4.313|total_loss: 39.516 | 68.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3820 |tagging_loss_video: 6.639|tagging_loss_audio: 10.468|tagging_loss_text: 16.225|tagging_loss_image: 6.822|tagging_loss_fusion: 6.954|total_loss: 47.108 | Examples/sec: 71.70\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 3821 | tagging_loss_video: 5.528|tagging_loss_audio: 9.679|tagging_loss_text: 14.168|tagging_loss_image: 6.555|tagging_loss_fusion: 4.809|total_loss: 40.739 | 60.10 Examples/sec\n",
      "INFO:tensorflow:training step 3822 | tagging_loss_video: 6.766|tagging_loss_audio: 8.443|tagging_loss_text: 17.622|tagging_loss_image: 6.524|tagging_loss_fusion: 5.018|total_loss: 44.373 | 71.88 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3823 | tagging_loss_video: 7.349|tagging_loss_audio: 8.670|tagging_loss_text: 16.013|tagging_loss_image: 6.580|tagging_loss_fusion: 5.953|total_loss: 44.565 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 3824 | tagging_loss_video: 5.972|tagging_loss_audio: 10.219|tagging_loss_text: 13.138|tagging_loss_image: 5.933|tagging_loss_fusion: 4.153|total_loss: 39.414 | 64.25 Examples/sec\n",
      "INFO:tensorflow:training step 3825 | tagging_loss_video: 6.326|tagging_loss_audio: 8.796|tagging_loss_text: 16.005|tagging_loss_image: 6.606|tagging_loss_fusion: 5.226|total_loss: 42.960 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 3826 | tagging_loss_video: 6.428|tagging_loss_audio: 8.822|tagging_loss_text: 13.370|tagging_loss_image: 6.250|tagging_loss_fusion: 6.201|total_loss: 41.070 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 3827 | tagging_loss_video: 6.695|tagging_loss_audio: 10.289|tagging_loss_text: 16.849|tagging_loss_image: 7.130|tagging_loss_fusion: 5.874|total_loss: 46.838 | 61.83 Examples/sec\n",
      "INFO:tensorflow:training step 3828 | tagging_loss_video: 6.329|tagging_loss_audio: 8.980|tagging_loss_text: 15.358|tagging_loss_image: 5.502|tagging_loss_fusion: 5.608|total_loss: 41.777 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 3829 | tagging_loss_video: 5.733|tagging_loss_audio: 9.880|tagging_loss_text: 16.477|tagging_loss_image: 6.435|tagging_loss_fusion: 5.634|total_loss: 44.158 | 70.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3830 |tagging_loss_video: 6.365|tagging_loss_audio: 8.576|tagging_loss_text: 14.079|tagging_loss_image: 6.137|tagging_loss_fusion: 5.907|total_loss: 41.064 | Examples/sec: 68.83\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3831 | tagging_loss_video: 6.815|tagging_loss_audio: 10.141|tagging_loss_text: 13.280|tagging_loss_image: 6.739|tagging_loss_fusion: 7.004|total_loss: 43.979 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 3832 | tagging_loss_video: 5.099|tagging_loss_audio: 9.615|tagging_loss_text: 14.312|tagging_loss_image: 5.901|tagging_loss_fusion: 3.557|total_loss: 38.484 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 3833 | tagging_loss_video: 6.668|tagging_loss_audio: 9.767|tagging_loss_text: 14.276|tagging_loss_image: 6.660|tagging_loss_fusion: 5.634|total_loss: 43.006 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 3834 | tagging_loss_video: 5.601|tagging_loss_audio: 8.569|tagging_loss_text: 14.104|tagging_loss_image: 6.719|tagging_loss_fusion: 4.398|total_loss: 39.391 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 3835 | tagging_loss_video: 6.615|tagging_loss_audio: 9.994|tagging_loss_text: 15.215|tagging_loss_image: 7.302|tagging_loss_fusion: 5.973|total_loss: 45.100 | 61.10 Examples/sec\n",
      "INFO:tensorflow:training step 3836 | tagging_loss_video: 6.592|tagging_loss_audio: 10.579|tagging_loss_text: 13.361|tagging_loss_image: 6.322|tagging_loss_fusion: 6.225|total_loss: 43.079 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 3837 | tagging_loss_video: 5.999|tagging_loss_audio: 10.041|tagging_loss_text: 18.238|tagging_loss_image: 7.766|tagging_loss_fusion: 5.586|total_loss: 47.630 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 3838 | tagging_loss_video: 5.591|tagging_loss_audio: 8.518|tagging_loss_text: 15.072|tagging_loss_image: 5.504|tagging_loss_fusion: 4.594|total_loss: 39.279 | 64.28 Examples/sec\n",
      "INFO:tensorflow:training step 3839 | tagging_loss_video: 4.948|tagging_loss_audio: 8.173|tagging_loss_text: 15.028|tagging_loss_image: 5.198|tagging_loss_fusion: 3.217|total_loss: 36.564 | 70.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3840 |tagging_loss_video: 6.016|tagging_loss_audio: 8.807|tagging_loss_text: 15.466|tagging_loss_image: 4.223|tagging_loss_fusion: 4.786|total_loss: 39.298 | Examples/sec: 68.56\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3841 | tagging_loss_video: 4.747|tagging_loss_audio: 8.470|tagging_loss_text: 11.531|tagging_loss_image: 4.993|tagging_loss_fusion: 3.220|total_loss: 32.961 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 3842 | tagging_loss_video: 5.870|tagging_loss_audio: 8.478|tagging_loss_text: 13.318|tagging_loss_image: 4.538|tagging_loss_fusion: 4.250|total_loss: 36.453 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 3843 | tagging_loss_video: 5.274|tagging_loss_audio: 8.686|tagging_loss_text: 11.176|tagging_loss_image: 5.712|tagging_loss_fusion: 3.567|total_loss: 34.415 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 3844 | tagging_loss_video: 5.304|tagging_loss_audio: 9.020|tagging_loss_text: 14.596|tagging_loss_image: 5.909|tagging_loss_fusion: 4.233|total_loss: 39.064 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 3845 | tagging_loss_video: 5.338|tagging_loss_audio: 8.617|tagging_loss_text: 14.028|tagging_loss_image: 4.346|tagging_loss_fusion: 3.629|total_loss: 35.958 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 3846 | tagging_loss_video: 5.855|tagging_loss_audio: 10.318|tagging_loss_text: 16.976|tagging_loss_image: 6.566|tagging_loss_fusion: 4.278|total_loss: 43.994 | 60.45 Examples/sec\n",
      "INFO:tensorflow:training step 3847 | tagging_loss_video: 5.257|tagging_loss_audio: 10.848|tagging_loss_text: 16.428|tagging_loss_image: 5.723|tagging_loss_fusion: 3.399|total_loss: 41.655 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 3848 | tagging_loss_video: 4.636|tagging_loss_audio: 7.232|tagging_loss_text: 11.145|tagging_loss_image: 5.605|tagging_loss_fusion: 4.300|total_loss: 32.917 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 3849 | tagging_loss_video: 4.293|tagging_loss_audio: 8.325|tagging_loss_text: 11.155|tagging_loss_image: 5.093|tagging_loss_fusion: 2.519|total_loss: 31.386 | 62.79 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 3850.\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3850 |tagging_loss_video: 6.312|tagging_loss_audio: 8.674|tagging_loss_text: 16.570|tagging_loss_image: 6.408|tagging_loss_fusion: 6.010|total_loss: 43.974 | Examples/sec: 52.86\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3851 | tagging_loss_video: 5.815|tagging_loss_audio: 9.026|tagging_loss_text: 14.791|tagging_loss_image: 5.792|tagging_loss_fusion: 5.974|total_loss: 41.398 | 63.35 Examples/sec\n",
      "INFO:tensorflow:training step 3852 | tagging_loss_video: 6.620|tagging_loss_audio: 8.724|tagging_loss_text: 14.746|tagging_loss_image: 4.944|tagging_loss_fusion: 5.418|total_loss: 40.452 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 3853 | tagging_loss_video: 6.456|tagging_loss_audio: 9.293|tagging_loss_text: 14.457|tagging_loss_image: 7.039|tagging_loss_fusion: 7.309|total_loss: 44.553 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 3854 | tagging_loss_video: 5.761|tagging_loss_audio: 8.255|tagging_loss_text: 16.289|tagging_loss_image: 5.062|tagging_loss_fusion: 4.149|total_loss: 39.517 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 3855 | tagging_loss_video: 6.218|tagging_loss_audio: 7.621|tagging_loss_text: 14.358|tagging_loss_image: 5.455|tagging_loss_fusion: 5.243|total_loss: 38.895 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 3856 | tagging_loss_video: 6.365|tagging_loss_audio: 9.510|tagging_loss_text: 15.170|tagging_loss_image: 6.621|tagging_loss_fusion: 5.838|total_loss: 43.504 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 3857 | tagging_loss_video: 4.024|tagging_loss_audio: 8.431|tagging_loss_text: 15.250|tagging_loss_image: 6.345|tagging_loss_fusion: 2.652|total_loss: 36.701 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 3858 | tagging_loss_video: 4.962|tagging_loss_audio: 8.397|tagging_loss_text: 16.073|tagging_loss_image: 6.085|tagging_loss_fusion: 3.776|total_loss: 39.293 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 3859 | tagging_loss_video: 6.001|tagging_loss_audio: 9.703|tagging_loss_text: 16.681|tagging_loss_image: 5.339|tagging_loss_fusion: 3.438|total_loss: 41.163 | 68.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3860 |tagging_loss_video: 5.993|tagging_loss_audio: 10.208|tagging_loss_text: 14.751|tagging_loss_image: 5.346|tagging_loss_fusion: 3.762|total_loss: 40.059 | Examples/sec: 63.64\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 3861 | tagging_loss_video: 6.135|tagging_loss_audio: 8.581|tagging_loss_text: 16.854|tagging_loss_image: 5.636|tagging_loss_fusion: 6.311|total_loss: 43.517 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 3862 | tagging_loss_video: 6.020|tagging_loss_audio: 9.393|tagging_loss_text: 15.728|tagging_loss_image: 6.091|tagging_loss_fusion: 5.384|total_loss: 42.616 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 3863 | tagging_loss_video: 5.443|tagging_loss_audio: 8.290|tagging_loss_text: 12.680|tagging_loss_image: 5.759|tagging_loss_fusion: 4.925|total_loss: 37.097 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 3864 | tagging_loss_video: 5.266|tagging_loss_audio: 7.511|tagging_loss_text: 10.087|tagging_loss_image: 4.719|tagging_loss_fusion: 3.759|total_loss: 31.341 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 3865 | tagging_loss_video: 5.362|tagging_loss_audio: 7.971|tagging_loss_text: 13.326|tagging_loss_image: 6.713|tagging_loss_fusion: 5.589|total_loss: 38.960 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 3866 | tagging_loss_video: 4.643|tagging_loss_audio: 9.423|tagging_loss_text: 13.528|tagging_loss_image: 5.674|tagging_loss_fusion: 3.774|total_loss: 37.042 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 3867 | tagging_loss_video: 6.588|tagging_loss_audio: 8.360|tagging_loss_text: 15.862|tagging_loss_image: 6.327|tagging_loss_fusion: 6.796|total_loss: 43.934 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 3868 | tagging_loss_video: 6.345|tagging_loss_audio: 8.687|tagging_loss_text: 14.495|tagging_loss_image: 6.035|tagging_loss_fusion: 5.654|total_loss: 41.217 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 3869 | tagging_loss_video: 5.539|tagging_loss_audio: 7.818|tagging_loss_text: 16.022|tagging_loss_image: 6.176|tagging_loss_fusion: 5.353|total_loss: 40.908 | 66.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3870 |tagging_loss_video: 6.397|tagging_loss_audio: 8.875|tagging_loss_text: 13.867|tagging_loss_image: 6.464|tagging_loss_fusion: 5.732|total_loss: 41.336 | Examples/sec: 69.40\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 3871 | tagging_loss_video: 5.783|tagging_loss_audio: 8.212|tagging_loss_text: 16.279|tagging_loss_image: 5.859|tagging_loss_fusion: 5.423|total_loss: 41.556 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 3872 | tagging_loss_video: 5.319|tagging_loss_audio: 8.307|tagging_loss_text: 18.179|tagging_loss_image: 5.968|tagging_loss_fusion: 4.563|total_loss: 42.335 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 3873 | tagging_loss_video: 6.787|tagging_loss_audio: 10.192|tagging_loss_text: 14.083|tagging_loss_image: 7.167|tagging_loss_fusion: 6.804|total_loss: 45.033 | 71.97 Examples/sec\n",
      "INFO:tensorflow:training step 3874 | tagging_loss_video: 6.268|tagging_loss_audio: 8.695|tagging_loss_text: 16.432|tagging_loss_image: 6.243|tagging_loss_fusion: 6.417|total_loss: 44.054 | 60.33 Examples/sec\n",
      "INFO:tensorflow:training step 3875 | tagging_loss_video: 4.763|tagging_loss_audio: 8.378|tagging_loss_text: 12.706|tagging_loss_image: 5.461|tagging_loss_fusion: 4.888|total_loss: 36.196 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 3876 | tagging_loss_video: 5.987|tagging_loss_audio: 7.990|tagging_loss_text: 11.830|tagging_loss_image: 5.668|tagging_loss_fusion: 5.829|total_loss: 37.304 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 3877 | tagging_loss_video: 4.044|tagging_loss_audio: 8.804|tagging_loss_text: 14.263|tagging_loss_image: 5.592|tagging_loss_fusion: 3.694|total_loss: 36.397 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 3878 | tagging_loss_video: 6.747|tagging_loss_audio: 9.817|tagging_loss_text: 15.710|tagging_loss_image: 5.904|tagging_loss_fusion: 6.490|total_loss: 44.668 | 66.80 Examples/sec\n",
      "INFO:tensorflow:training step 3879 | tagging_loss_video: 6.564|tagging_loss_audio: 9.193|tagging_loss_text: 16.194|tagging_loss_image: 6.450|tagging_loss_fusion: 5.421|total_loss: 43.822 | 70.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3880 |tagging_loss_video: 5.581|tagging_loss_audio: 10.922|tagging_loss_text: 12.554|tagging_loss_image: 6.513|tagging_loss_fusion: 4.212|total_loss: 39.782 | Examples/sec: 64.13\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 3881 | tagging_loss_video: 5.184|tagging_loss_audio: 8.305|tagging_loss_text: 14.147|tagging_loss_image: 5.381|tagging_loss_fusion: 4.125|total_loss: 37.142 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 3882 | tagging_loss_video: 6.123|tagging_loss_audio: 9.290|tagging_loss_text: 18.493|tagging_loss_image: 6.968|tagging_loss_fusion: 5.988|total_loss: 46.861 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 3883 | tagging_loss_video: 7.039|tagging_loss_audio: 8.795|tagging_loss_text: 14.308|tagging_loss_image: 6.993|tagging_loss_fusion: 6.171|total_loss: 43.306 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 3884 | tagging_loss_video: 6.900|tagging_loss_audio: 9.359|tagging_loss_text: 15.311|tagging_loss_image: 5.922|tagging_loss_fusion: 5.094|total_loss: 42.587 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 3885 | tagging_loss_video: 6.067|tagging_loss_audio: 9.472|tagging_loss_text: 14.976|tagging_loss_image: 6.076|tagging_loss_fusion: 4.166|total_loss: 40.757 | 60.27 Examples/sec\n",
      "INFO:tensorflow:training step 3886 | tagging_loss_video: 5.111|tagging_loss_audio: 10.571|tagging_loss_text: 18.073|tagging_loss_image: 6.437|tagging_loss_fusion: 4.386|total_loss: 44.578 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 3887 | tagging_loss_video: 5.581|tagging_loss_audio: 9.481|tagging_loss_text: 15.161|tagging_loss_image: 5.171|tagging_loss_fusion: 4.941|total_loss: 40.335 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 3888 | tagging_loss_video: 4.935|tagging_loss_audio: 9.281|tagging_loss_text: 17.184|tagging_loss_image: 5.779|tagging_loss_fusion: 3.815|total_loss: 40.994 | 63.44 Examples/sec\n",
      "INFO:tensorflow:training step 3889 | tagging_loss_video: 6.071|tagging_loss_audio: 10.629|tagging_loss_text: 14.135|tagging_loss_image: 6.756|tagging_loss_fusion: 5.542|total_loss: 43.134 | 70.91 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3890 |tagging_loss_video: 4.791|tagging_loss_audio: 8.969|tagging_loss_text: 12.332|tagging_loss_image: 5.975|tagging_loss_fusion: 3.019|total_loss: 35.086 | Examples/sec: 69.67\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.86 | precision@0.5: 0.98 |recall@0.1: 1.00 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 3891 | tagging_loss_video: 6.645|tagging_loss_audio: 10.408|tagging_loss_text: 17.348|tagging_loss_image: 6.316|tagging_loss_fusion: 5.784|total_loss: 46.502 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 3892 | tagging_loss_video: 6.471|tagging_loss_audio: 10.362|tagging_loss_text: 14.533|tagging_loss_image: 6.621|tagging_loss_fusion: 6.823|total_loss: 44.809 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 3893 | tagging_loss_video: 5.654|tagging_loss_audio: 10.555|tagging_loss_text: 13.055|tagging_loss_image: 6.958|tagging_loss_fusion: 3.385|total_loss: 39.607 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 3894 | tagging_loss_video: 6.434|tagging_loss_audio: 9.767|tagging_loss_text: 16.180|tagging_loss_image: 6.315|tagging_loss_fusion: 5.573|total_loss: 44.269 | 65.25 Examples/sec\n",
      "INFO:tensorflow:training step 3895 | tagging_loss_video: 6.557|tagging_loss_audio: 11.237|tagging_loss_text: 17.117|tagging_loss_image: 7.584|tagging_loss_fusion: 6.497|total_loss: 48.993 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 3896 | tagging_loss_video: 5.641|tagging_loss_audio: 9.294|tagging_loss_text: 15.113|tagging_loss_image: 6.241|tagging_loss_fusion: 4.928|total_loss: 41.216 | 67.53 Examples/sec\n",
      "INFO:tensorflow:training step 3897 | tagging_loss_video: 6.761|tagging_loss_audio: 9.634|tagging_loss_text: 17.613|tagging_loss_image: 5.816|tagging_loss_fusion: 6.263|total_loss: 46.088 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 3898 | tagging_loss_video: 5.658|tagging_loss_audio: 8.751|tagging_loss_text: 11.755|tagging_loss_image: 5.994|tagging_loss_fusion: 4.507|total_loss: 36.665 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 3899 | tagging_loss_video: 6.542|tagging_loss_audio: 11.150|tagging_loss_text: 15.175|tagging_loss_image: 7.113|tagging_loss_fusion: 7.423|total_loss: 47.403 | 66.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3900 |tagging_loss_video: 6.267|tagging_loss_audio: 10.525|tagging_loss_text: 15.846|tagging_loss_image: 6.801|tagging_loss_fusion: 5.148|total_loss: 44.588 | Examples/sec: 69.07\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 3901 | tagging_loss_video: 5.239|tagging_loss_audio: 9.952|tagging_loss_text: 18.225|tagging_loss_image: 6.169|tagging_loss_fusion: 3.853|total_loss: 43.438 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 3902 | tagging_loss_video: 6.187|tagging_loss_audio: 9.926|tagging_loss_text: 18.119|tagging_loss_image: 6.359|tagging_loss_fusion: 4.691|total_loss: 45.283 | 64.77 Examples/sec\n",
      "INFO:tensorflow:training step 3903 | tagging_loss_video: 6.705|tagging_loss_audio: 11.530|tagging_loss_text: 16.788|tagging_loss_image: 7.738|tagging_loss_fusion: 6.399|total_loss: 49.160 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 3904 | tagging_loss_video: 6.510|tagging_loss_audio: 9.824|tagging_loss_text: 17.994|tagging_loss_image: 5.943|tagging_loss_fusion: 4.994|total_loss: 45.265 | 67.10 Examples/sec\n",
      "INFO:tensorflow:training step 3905 | tagging_loss_video: 6.543|tagging_loss_audio: 9.962|tagging_loss_text: 17.317|tagging_loss_image: 6.043|tagging_loss_fusion: 4.895|total_loss: 44.759 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 3906 | tagging_loss_video: 5.832|tagging_loss_audio: 9.555|tagging_loss_text: 16.095|tagging_loss_image: 6.058|tagging_loss_fusion: 5.163|total_loss: 42.703 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 3907 | tagging_loss_video: 6.591|tagging_loss_audio: 9.674|tagging_loss_text: 16.459|tagging_loss_image: 6.554|tagging_loss_fusion: 6.157|total_loss: 45.435 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 3908 | tagging_loss_video: 5.975|tagging_loss_audio: 10.591|tagging_loss_text: 14.650|tagging_loss_image: 5.606|tagging_loss_fusion: 5.261|total_loss: 42.082 | 65.38 Examples/sec\n",
      "INFO:tensorflow:training step 3909 | tagging_loss_video: 7.267|tagging_loss_audio: 11.157|tagging_loss_text: 11.112|tagging_loss_image: 7.823|tagging_loss_fusion: 9.572|total_loss: 46.929 | 71.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3910 |tagging_loss_video: 6.548|tagging_loss_audio: 9.603|tagging_loss_text: 16.239|tagging_loss_image: 6.495|tagging_loss_fusion: 4.838|total_loss: 43.723 | Examples/sec: 61.84\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 3911 | tagging_loss_video: 6.414|tagging_loss_audio: 7.884|tagging_loss_text: 11.145|tagging_loss_image: 6.216|tagging_loss_fusion: 6.439|total_loss: 38.098 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 3912 | tagging_loss_video: 5.510|tagging_loss_audio: 9.485|tagging_loss_text: 14.962|tagging_loss_image: 5.770|tagging_loss_fusion: 3.927|total_loss: 39.654 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 3913 | tagging_loss_video: 5.953|tagging_loss_audio: 8.750|tagging_loss_text: 12.222|tagging_loss_image: 5.630|tagging_loss_fusion: 4.744|total_loss: 37.299 | 63.77 Examples/sec\n",
      "INFO:tensorflow:training step 3914 | tagging_loss_video: 6.256|tagging_loss_audio: 10.693|tagging_loss_text: 13.954|tagging_loss_image: 6.869|tagging_loss_fusion: 4.710|total_loss: 42.481 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 3915 | tagging_loss_video: 5.343|tagging_loss_audio: 8.735|tagging_loss_text: 13.512|tagging_loss_image: 5.217|tagging_loss_fusion: 3.887|total_loss: 36.694 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 3916 | tagging_loss_video: 6.121|tagging_loss_audio: 8.597|tagging_loss_text: 14.504|tagging_loss_image: 6.372|tagging_loss_fusion: 5.498|total_loss: 41.092 | 61.24 Examples/sec\n",
      "INFO:tensorflow:training step 3917 | tagging_loss_video: 6.772|tagging_loss_audio: 9.148|tagging_loss_text: 16.595|tagging_loss_image: 5.660|tagging_loss_fusion: 5.284|total_loss: 43.459 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 3918 | tagging_loss_video: 4.657|tagging_loss_audio: 9.461|tagging_loss_text: 14.806|tagging_loss_image: 5.766|tagging_loss_fusion: 2.919|total_loss: 37.609 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 3919 | tagging_loss_video: 4.676|tagging_loss_audio: 9.272|tagging_loss_text: 15.814|tagging_loss_image: 6.134|tagging_loss_fusion: 4.411|total_loss: 40.308 | 63.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3920 |tagging_loss_video: 5.409|tagging_loss_audio: 9.133|tagging_loss_text: 14.572|tagging_loss_image: 5.450|tagging_loss_fusion: 5.378|total_loss: 39.942 | Examples/sec: 69.53\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3921 | tagging_loss_video: 6.171|tagging_loss_audio: 8.985|tagging_loss_text: 14.665|tagging_loss_image: 6.124|tagging_loss_fusion: 5.145|total_loss: 41.090 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 3922 | tagging_loss_video: 6.417|tagging_loss_audio: 9.591|tagging_loss_text: 16.051|tagging_loss_image: 6.376|tagging_loss_fusion: 5.779|total_loss: 44.214 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 3923 | tagging_loss_video: 6.145|tagging_loss_audio: 10.423|tagging_loss_text: 15.194|tagging_loss_image: 6.032|tagging_loss_fusion: 6.109|total_loss: 43.902 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 3924 | tagging_loss_video: 5.529|tagging_loss_audio: 9.306|tagging_loss_text: 16.195|tagging_loss_image: 5.378|tagging_loss_fusion: 4.521|total_loss: 40.930 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 3925 | tagging_loss_video: 4.913|tagging_loss_audio: 9.204|tagging_loss_text: 16.034|tagging_loss_image: 5.874|tagging_loss_fusion: 4.847|total_loss: 40.871 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 3926 | tagging_loss_video: 5.836|tagging_loss_audio: 9.818|tagging_loss_text: 15.927|tagging_loss_image: 5.445|tagging_loss_fusion: 4.513|total_loss: 41.539 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 3927 | tagging_loss_video: 6.290|tagging_loss_audio: 10.308|tagging_loss_text: 14.866|tagging_loss_image: 5.206|tagging_loss_fusion: 5.094|total_loss: 41.763 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 3928 | tagging_loss_video: 7.570|tagging_loss_audio: 10.101|tagging_loss_text: 16.715|tagging_loss_image: 7.431|tagging_loss_fusion: 7.187|total_loss: 49.004 | 67.68 Examples/sec\n",
      "INFO:tensorflow:training step 3929 | tagging_loss_video: 6.565|tagging_loss_audio: 10.223|tagging_loss_text: 21.266|tagging_loss_image: 6.996|tagging_loss_fusion: 5.896|total_loss: 50.946 | 69.99 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3930 |tagging_loss_video: 5.371|tagging_loss_audio: 9.099|tagging_loss_text: 14.131|tagging_loss_image: 6.923|tagging_loss_fusion: 5.100|total_loss: 40.625 | Examples/sec: 66.38\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3931 | tagging_loss_video: 4.550|tagging_loss_audio: 9.577|tagging_loss_text: 15.048|tagging_loss_image: 6.380|tagging_loss_fusion: 3.598|total_loss: 39.153 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 3932 | tagging_loss_video: 6.772|tagging_loss_audio: 10.434|tagging_loss_text: 17.101|tagging_loss_image: 6.319|tagging_loss_fusion: 4.736|total_loss: 45.362 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 3933 | tagging_loss_video: 5.987|tagging_loss_audio: 9.218|tagging_loss_text: 15.364|tagging_loss_image: 5.999|tagging_loss_fusion: 6.311|total_loss: 42.879 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 3934 | tagging_loss_video: 5.759|tagging_loss_audio: 10.739|tagging_loss_text: 17.940|tagging_loss_image: 6.374|tagging_loss_fusion: 5.322|total_loss: 46.134 | 69.03 Examples/sec\n",
      "INFO:tensorflow:training step 3935 | tagging_loss_video: 5.306|tagging_loss_audio: 8.998|tagging_loss_text: 12.526|tagging_loss_image: 5.344|tagging_loss_fusion: 4.215|total_loss: 36.389 | 66.70 Examples/sec\n",
      "INFO:tensorflow:training step 3936 | tagging_loss_video: 5.107|tagging_loss_audio: 10.165|tagging_loss_text: 16.437|tagging_loss_image: 5.198|tagging_loss_fusion: 2.899|total_loss: 39.806 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 3937 | tagging_loss_video: 5.208|tagging_loss_audio: 10.499|tagging_loss_text: 16.065|tagging_loss_image: 6.478|tagging_loss_fusion: 4.401|total_loss: 42.650 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 3938 | tagging_loss_video: 6.091|tagging_loss_audio: 7.517|tagging_loss_text: 12.890|tagging_loss_image: 3.864|tagging_loss_fusion: 4.403|total_loss: 34.765 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 3939 | tagging_loss_video: 6.009|tagging_loss_audio: 8.642|tagging_loss_text: 17.594|tagging_loss_image: 6.049|tagging_loss_fusion: 4.565|total_loss: 42.860 | 70.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3940 |tagging_loss_video: 6.229|tagging_loss_audio: 8.314|tagging_loss_text: 16.597|tagging_loss_image: 6.292|tagging_loss_fusion: 5.550|total_loss: 42.982 | Examples/sec: 72.05\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3941 | tagging_loss_video: 6.273|tagging_loss_audio: 9.004|tagging_loss_text: 13.501|tagging_loss_image: 6.811|tagging_loss_fusion: 6.265|total_loss: 41.854 | 60.44 Examples/sec\n",
      "INFO:tensorflow:training step 3942 | tagging_loss_video: 5.524|tagging_loss_audio: 10.452|tagging_loss_text: 12.582|tagging_loss_image: 6.373|tagging_loss_fusion: 3.876|total_loss: 38.808 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 3943 | tagging_loss_video: 5.801|tagging_loss_audio: 9.805|tagging_loss_text: 12.025|tagging_loss_image: 4.942|tagging_loss_fusion: 4.249|total_loss: 36.821 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 3944 | tagging_loss_video: 4.301|tagging_loss_audio: 9.332|tagging_loss_text: 15.653|tagging_loss_image: 5.386|tagging_loss_fusion: 2.626|total_loss: 37.298 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 3945 | tagging_loss_video: 7.293|tagging_loss_audio: 9.367|tagging_loss_text: 14.934|tagging_loss_image: 5.993|tagging_loss_fusion: 5.846|total_loss: 43.433 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 3946 | tagging_loss_video: 6.395|tagging_loss_audio: 7.756|tagging_loss_text: 16.807|tagging_loss_image: 6.114|tagging_loss_fusion: 5.017|total_loss: 42.089 | 67.80 Examples/sec\n",
      "INFO:tensorflow:training step 3947 | tagging_loss_video: 5.024|tagging_loss_audio: 9.583|tagging_loss_text: 13.173|tagging_loss_image: 4.817|tagging_loss_fusion: 3.073|total_loss: 35.669 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 3948 | tagging_loss_video: 6.264|tagging_loss_audio: 8.850|tagging_loss_text: 14.022|tagging_loss_image: 5.400|tagging_loss_fusion: 5.774|total_loss: 40.311 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 3949 | tagging_loss_video: 4.887|tagging_loss_audio: 9.185|tagging_loss_text: 14.988|tagging_loss_image: 6.046|tagging_loss_fusion: 4.136|total_loss: 39.242 | 63.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3950 |tagging_loss_video: 6.120|tagging_loss_audio: 8.756|tagging_loss_text: 17.422|tagging_loss_image: 5.830|tagging_loss_fusion: 5.413|total_loss: 43.541 | Examples/sec: 71.21\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 3951 | tagging_loss_video: 6.892|tagging_loss_audio: 9.474|tagging_loss_text: 15.659|tagging_loss_image: 4.844|tagging_loss_fusion: 5.990|total_loss: 42.858 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 3952 | tagging_loss_video: 6.287|tagging_loss_audio: 8.427|tagging_loss_text: 15.620|tagging_loss_image: 4.877|tagging_loss_fusion: 4.291|total_loss: 39.502 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 3953 | tagging_loss_video: 4.540|tagging_loss_audio: 8.544|tagging_loss_text: 18.721|tagging_loss_image: 5.652|tagging_loss_fusion: 2.857|total_loss: 40.314 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 3954 | tagging_loss_video: 5.442|tagging_loss_audio: 8.862|tagging_loss_text: 16.522|tagging_loss_image: 5.038|tagging_loss_fusion: 4.418|total_loss: 40.282 | 72.11 Examples/sec\n",
      "INFO:tensorflow:training step 3955 | tagging_loss_video: 5.704|tagging_loss_audio: 8.848|tagging_loss_text: 13.969|tagging_loss_image: 6.691|tagging_loss_fusion: 5.799|total_loss: 41.010 | 63.59 Examples/sec\n",
      "INFO:tensorflow:training step 3956 | tagging_loss_video: 5.246|tagging_loss_audio: 9.725|tagging_loss_text: 17.755|tagging_loss_image: 6.338|tagging_loss_fusion: 2.862|total_loss: 41.926 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 3957 | tagging_loss_video: 5.597|tagging_loss_audio: 9.000|tagging_loss_text: 14.610|tagging_loss_image: 5.787|tagging_loss_fusion: 5.081|total_loss: 40.076 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 3958 | tagging_loss_video: 4.383|tagging_loss_audio: 8.913|tagging_loss_text: 16.165|tagging_loss_image: 6.569|tagging_loss_fusion: 4.418|total_loss: 40.448 | 67.61 Examples/sec\n",
      "INFO:tensorflow:training step 3959 | tagging_loss_video: 6.970|tagging_loss_audio: 9.891|tagging_loss_text: 15.540|tagging_loss_image: 7.171|tagging_loss_fusion: 8.342|total_loss: 47.913 | 68.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3960 |tagging_loss_video: 5.049|tagging_loss_audio: 8.590|tagging_loss_text: 12.804|tagging_loss_image: 5.618|tagging_loss_fusion: 3.927|total_loss: 35.986 | Examples/sec: 61.36\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.93\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 3961 | tagging_loss_video: 6.250|tagging_loss_audio: 8.617|tagging_loss_text: 16.969|tagging_loss_image: 6.413|tagging_loss_fusion: 5.308|total_loss: 43.556 | 68.11 Examples/sec\n",
      "INFO:tensorflow:training step 3962 | tagging_loss_video: 6.263|tagging_loss_audio: 9.031|tagging_loss_text: 15.324|tagging_loss_image: 6.942|tagging_loss_fusion: 6.740|total_loss: 44.300 | 68.74 Examples/sec\n",
      "INFO:tensorflow:training step 3963 | tagging_loss_video: 6.780|tagging_loss_audio: 10.093|tagging_loss_text: 15.939|tagging_loss_image: 6.559|tagging_loss_fusion: 7.298|total_loss: 46.669 | 71.71 Examples/sec\n",
      "INFO:tensorflow:training step 3964 | tagging_loss_video: 5.965|tagging_loss_audio: 8.050|tagging_loss_text: 16.694|tagging_loss_image: 6.541|tagging_loss_fusion: 4.749|total_loss: 41.999 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 3965 | tagging_loss_video: 6.115|tagging_loss_audio: 9.259|tagging_loss_text: 16.062|tagging_loss_image: 5.893|tagging_loss_fusion: 5.494|total_loss: 42.824 | 61.86 Examples/sec\n",
      "INFO:tensorflow:training step 3966 | tagging_loss_video: 7.581|tagging_loss_audio: 9.925|tagging_loss_text: 16.819|tagging_loss_image: 6.791|tagging_loss_fusion: 9.045|total_loss: 50.161 | 66.57 Examples/sec\n",
      "INFO:tensorflow:training step 3967 | tagging_loss_video: 5.741|tagging_loss_audio: 10.825|tagging_loss_text: 10.912|tagging_loss_image: 4.668|tagging_loss_fusion: 5.855|total_loss: 38.001 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 3968 | tagging_loss_video: 6.474|tagging_loss_audio: 9.934|tagging_loss_text: 17.144|tagging_loss_image: 6.407|tagging_loss_fusion: 5.553|total_loss: 45.512 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 3969 | tagging_loss_video: 5.988|tagging_loss_audio: 9.084|tagging_loss_text: 17.432|tagging_loss_image: 6.383|tagging_loss_fusion: 5.212|total_loss: 44.099 | 66.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3970 |tagging_loss_video: 5.365|tagging_loss_audio: 8.732|tagging_loss_text: 12.566|tagging_loss_image: 6.174|tagging_loss_fusion: 4.951|total_loss: 37.788 | Examples/sec: 70.54\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 3971 | tagging_loss_video: 6.114|tagging_loss_audio: 9.322|tagging_loss_text: 16.249|tagging_loss_image: 5.422|tagging_loss_fusion: 4.965|total_loss: 42.072 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 3972 | tagging_loss_video: 6.322|tagging_loss_audio: 10.544|tagging_loss_text: 15.663|tagging_loss_image: 6.663|tagging_loss_fusion: 5.309|total_loss: 44.500 | 72.55 Examples/sec\n",
      "INFO:tensorflow:training step 3973 | tagging_loss_video: 7.267|tagging_loss_audio: 9.538|tagging_loss_text: 14.838|tagging_loss_image: 6.888|tagging_loss_fusion: 6.981|total_loss: 45.512 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 3974 | tagging_loss_video: 7.329|tagging_loss_audio: 10.012|tagging_loss_text: 13.767|tagging_loss_image: 6.198|tagging_loss_fusion: 6.413|total_loss: 43.719 | 63.36 Examples/sec\n",
      "INFO:tensorflow:training step 3975 | tagging_loss_video: 6.008|tagging_loss_audio: 8.608|tagging_loss_text: 13.355|tagging_loss_image: 6.660|tagging_loss_fusion: 5.393|total_loss: 40.023 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 3976 | tagging_loss_video: 6.985|tagging_loss_audio: 10.074|tagging_loss_text: 17.489|tagging_loss_image: 7.564|tagging_loss_fusion: 6.578|total_loss: 48.689 | 66.15 Examples/sec\n",
      "INFO:tensorflow:training step 3977 | tagging_loss_video: 5.600|tagging_loss_audio: 7.439|tagging_loss_text: 15.081|tagging_loss_image: 5.250|tagging_loss_fusion: 4.317|total_loss: 37.687 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 3978 | tagging_loss_video: 5.075|tagging_loss_audio: 7.541|tagging_loss_text: 13.154|tagging_loss_image: 3.925|tagging_loss_fusion: 3.339|total_loss: 33.034 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 3979 | tagging_loss_video: 5.271|tagging_loss_audio: 8.804|tagging_loss_text: 11.425|tagging_loss_image: 5.642|tagging_loss_fusion: 3.846|total_loss: 34.987 | 70.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3980 |tagging_loss_video: 5.363|tagging_loss_audio: 8.170|tagging_loss_text: 15.812|tagging_loss_image: 4.650|tagging_loss_fusion: 4.043|total_loss: 38.038 | Examples/sec: 62.78\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.79 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 3981 | tagging_loss_video: 6.563|tagging_loss_audio: 10.070|tagging_loss_text: 14.718|tagging_loss_image: 5.680|tagging_loss_fusion: 4.799|total_loss: 41.830 | 72.42 Examples/sec\n",
      "INFO:tensorflow:training step 3982 | tagging_loss_video: 5.085|tagging_loss_audio: 8.475|tagging_loss_text: 13.811|tagging_loss_image: 4.580|tagging_loss_fusion: 3.338|total_loss: 35.289 | 67.83 Examples/sec\n",
      "INFO:tensorflow:training step 3983 | tagging_loss_video: 6.476|tagging_loss_audio: 8.660|tagging_loss_text: 13.965|tagging_loss_image: 5.400|tagging_loss_fusion: 5.562|total_loss: 40.062 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 3984 | tagging_loss_video: 5.643|tagging_loss_audio: 9.881|tagging_loss_text: 15.927|tagging_loss_image: 4.799|tagging_loss_fusion: 4.244|total_loss: 40.494 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 3985 | tagging_loss_video: 5.999|tagging_loss_audio: 8.446|tagging_loss_text: 16.222|tagging_loss_image: 7.240|tagging_loss_fusion: 5.170|total_loss: 43.076 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 3986 | tagging_loss_video: 4.690|tagging_loss_audio: 8.907|tagging_loss_text: 17.113|tagging_loss_image: 6.231|tagging_loss_fusion: 3.815|total_loss: 40.756 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 3987 | tagging_loss_video: 6.506|tagging_loss_audio: 7.430|tagging_loss_text: 14.246|tagging_loss_image: 4.718|tagging_loss_fusion: 4.861|total_loss: 37.761 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 3988 | tagging_loss_video: 5.016|tagging_loss_audio: 8.523|tagging_loss_text: 12.414|tagging_loss_image: 6.163|tagging_loss_fusion: 4.613|total_loss: 36.729 | 66.34 Examples/sec\n",
      "INFO:tensorflow:training step 3989 | tagging_loss_video: 5.523|tagging_loss_audio: 9.883|tagging_loss_text: 15.201|tagging_loss_image: 5.962|tagging_loss_fusion: 5.029|total_loss: 41.598 | 67.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 3990 |tagging_loss_video: 6.552|tagging_loss_audio: 9.912|tagging_loss_text: 15.995|tagging_loss_image: 6.059|tagging_loss_fusion: 5.230|total_loss: 43.748 | Examples/sec: 71.26\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 3991 | tagging_loss_video: 4.902|tagging_loss_audio: 8.693|tagging_loss_text: 15.418|tagging_loss_image: 5.793|tagging_loss_fusion: 4.728|total_loss: 39.533 | 65.34 Examples/sec\n",
      "INFO:tensorflow:training step 3992 | tagging_loss_video: 4.827|tagging_loss_audio: 8.937|tagging_loss_text: 12.745|tagging_loss_image: 5.326|tagging_loss_fusion: 3.372|total_loss: 35.208 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 3993 | tagging_loss_video: 5.742|tagging_loss_audio: 8.960|tagging_loss_text: 15.837|tagging_loss_image: 6.672|tagging_loss_fusion: 5.641|total_loss: 42.851 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 3994 | tagging_loss_video: 5.870|tagging_loss_audio: 8.566|tagging_loss_text: 16.944|tagging_loss_image: 5.717|tagging_loss_fusion: 4.909|total_loss: 42.007 | 63.42 Examples/sec\n",
      "INFO:tensorflow:training step 3995 | tagging_loss_video: 5.908|tagging_loss_audio: 8.369|tagging_loss_text: 13.680|tagging_loss_image: 4.997|tagging_loss_fusion: 3.584|total_loss: 36.538 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 3996 | tagging_loss_video: 4.720|tagging_loss_audio: 9.312|tagging_loss_text: 16.496|tagging_loss_image: 4.544|tagging_loss_fusion: 2.444|total_loss: 37.516 | 63.59 Examples/sec\n",
      "INFO:tensorflow:training step 3997 | tagging_loss_video: 5.713|tagging_loss_audio: 9.404|tagging_loss_text: 13.276|tagging_loss_image: 6.366|tagging_loss_fusion: 5.583|total_loss: 40.342 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 3998 | tagging_loss_video: 4.842|tagging_loss_audio: 8.564|tagging_loss_text: 15.395|tagging_loss_image: 5.682|tagging_loss_fusion: 2.312|total_loss: 36.795 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 3999 | tagging_loss_video: 5.807|tagging_loss_audio: 9.342|tagging_loss_text: 17.368|tagging_loss_image: 6.484|tagging_loss_fusion: 5.565|total_loss: 44.565 | 67.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4000 |tagging_loss_video: 5.755|tagging_loss_audio: 8.976|tagging_loss_text: 14.738|tagging_loss_image: 4.938|tagging_loss_fusion: 4.217|total_loss: 38.625 | Examples/sec: 61.16\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.721|loss: 30.618|GAP: 0.729|examples_per_second: 87.747\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.734|loss: 27.205|GAP: 0.778|examples_per_second: 98.043\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 1.000|perr: 0.708|loss: 36.694|GAP: 0.708|examples_per_second: 100.847\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.761|loss: 29.134|GAP: 0.753|examples_per_second: 99.475\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.749|loss: 29.701|GAP: 0.752|examples_per_second: 89.752\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.694|loss: 29.758|GAP: 0.720|examples_per_second: 100.157\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.726|loss: 32.694|GAP: 0.736|examples_per_second: 96.778\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.755|loss: 28.798|GAP: 0.762|examples_per_second: 98.572\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 1.000|perr: 0.719|loss: 31.386|GAP: 0.726|examples_per_second: 96.228\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.692|loss: 32.113|GAP: 0.705|examples_per_second: 92.101\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.748|loss: 25.492|GAP: 0.776|examples_per_second: 102.565\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 1.000|perr: 0.768|loss: 27.882|GAP: 0.763|examples_per_second: 99.541\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.742|loss: 30.367|GAP: 0.741|examples_per_second: 101.687\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.745|loss: 30.036|GAP: 0.756|examples_per_second: 94.680\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.730|loss: 29.317|GAP: 0.740|examples_per_second: 95.591\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 4000 | MAP: 0.307 | GAP: 0.730 | p@0.1: 0.688 | p@0.5:0.823 | r@0.1:0.742 | r@0.5: 0.601 | Avg_Loss: 25.710237\n",
      "INFO:tensorflow:epoch/eval number 4000 | MAP: 0.267 | GAP: 0.674 | p@0.1: 0.535 | p@0.5:0.750 | r@0.1:0.823 | r@0.5: 0.591 | Avg_Loss: 21.781726\n",
      "INFO:tensorflow:epoch/eval number 4000 | MAP: 0.111 | GAP: 0.575 | p@0.1: 0.335 | p@0.5:0.784 | r@0.1:0.878 | r@0.5: 0.406 | Avg_Loss: 22.030255\n",
      "INFO:tensorflow:epoch/eval number 4000 | MAP: 0.272 | GAP: 0.661 | p@0.1: 0.618 | p@0.5:0.737 | r@0.1:0.743 | r@0.5: 0.629 | Avg_Loss: 32.100103\n",
      "INFO:tensorflow:epoch/eval number 4000 | MAP: 0.325 | GAP: 0.743 | p@0.1: 0.733 | p@0.5:0.818 | r@0.1:0.729 | r@0.5: 0.632 | Avg_Loss: 30.079608\n",
      "INFO:tensorflow:validation score on val799 is : 0.7425\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-4000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_4000_0.7425/saved_model.pb\n",
      "INFO:tensorflow:training step 4001 | tagging_loss_video: 5.440|tagging_loss_audio: 8.008|tagging_loss_text: 11.845|tagging_loss_image: 5.873|tagging_loss_fusion: 6.111|total_loss: 37.277 | 66.12 Examples/sec\n",
      "INFO:tensorflow:training step 4002 | tagging_loss_video: 5.455|tagging_loss_audio: 9.082|tagging_loss_text: 16.231|tagging_loss_image: 5.482|tagging_loss_fusion: 3.590|total_loss: 39.839 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 4003 | tagging_loss_video: 6.060|tagging_loss_audio: 9.688|tagging_loss_text: 17.120|tagging_loss_image: 6.170|tagging_loss_fusion: 5.658|total_loss: 44.696 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 4004 | tagging_loss_video: 5.851|tagging_loss_audio: 7.775|tagging_loss_text: 16.219|tagging_loss_image: 4.205|tagging_loss_fusion: 5.124|total_loss: 39.174 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 4005 | tagging_loss_video: 6.310|tagging_loss_audio: 9.422|tagging_loss_text: 14.650|tagging_loss_image: 6.962|tagging_loss_fusion: 6.713|total_loss: 44.056 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 4006 | tagging_loss_video: 5.334|tagging_loss_audio: 7.948|tagging_loss_text: 12.209|tagging_loss_image: 4.842|tagging_loss_fusion: 4.056|total_loss: 34.390 | 67.33 Examples/sec\n",
      "INFO:tensorflow:training step 4007 | tagging_loss_video: 4.841|tagging_loss_audio: 8.648|tagging_loss_text: 17.764|tagging_loss_image: 6.443|tagging_loss_fusion: 3.179|total_loss: 40.875 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 4008 | tagging_loss_video: 6.004|tagging_loss_audio: 9.024|tagging_loss_text: 14.307|tagging_loss_image: 6.632|tagging_loss_fusion: 5.997|total_loss: 41.963 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 4009 | tagging_loss_video: 5.670|tagging_loss_audio: 8.900|tagging_loss_text: 15.329|tagging_loss_image: 5.782|tagging_loss_fusion: 4.618|total_loss: 40.298 | 70.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4010 |tagging_loss_video: 6.254|tagging_loss_audio: 8.929|tagging_loss_text: 13.613|tagging_loss_image: 6.451|tagging_loss_fusion: 5.813|total_loss: 41.059 | Examples/sec: 69.30\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.76 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4011 | tagging_loss_video: 7.000|tagging_loss_audio: 10.123|tagging_loss_text: 17.340|tagging_loss_image: 6.196|tagging_loss_fusion: 6.929|total_loss: 47.588 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 4012 | tagging_loss_video: 5.379|tagging_loss_audio: 9.658|tagging_loss_text: 11.873|tagging_loss_image: 4.938|tagging_loss_fusion: 3.183|total_loss: 35.031 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 4013 | tagging_loss_video: 6.659|tagging_loss_audio: 9.934|tagging_loss_text: 16.451|tagging_loss_image: 6.238|tagging_loss_fusion: 5.010|total_loss: 44.293 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 4014 | tagging_loss_video: 7.212|tagging_loss_audio: 10.357|tagging_loss_text: 19.313|tagging_loss_image: 7.408|tagging_loss_fusion: 6.875|total_loss: 51.165 | 68.39 Examples/sec\n",
      "INFO:tensorflow:training step 4015 | tagging_loss_video: 5.673|tagging_loss_audio: 8.750|tagging_loss_text: 13.655|tagging_loss_image: 5.404|tagging_loss_fusion: 4.500|total_loss: 37.983 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 4016 | tagging_loss_video: 6.257|tagging_loss_audio: 10.736|tagging_loss_text: 16.664|tagging_loss_image: 6.390|tagging_loss_fusion: 6.333|total_loss: 46.380 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 4017 | tagging_loss_video: 7.011|tagging_loss_audio: 8.663|tagging_loss_text: 15.921|tagging_loss_image: 5.687|tagging_loss_fusion: 7.678|total_loss: 44.960 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 4018 | tagging_loss_video: 6.814|tagging_loss_audio: 9.960|tagging_loss_text: 17.393|tagging_loss_image: 6.893|tagging_loss_fusion: 6.844|total_loss: 47.905 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 4019 | tagging_loss_video: 6.469|tagging_loss_audio: 8.496|tagging_loss_text: 11.952|tagging_loss_image: 5.877|tagging_loss_fusion: 5.140|total_loss: 37.933 | 68.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4020 |tagging_loss_video: 6.906|tagging_loss_audio: 10.590|tagging_loss_text: 17.119|tagging_loss_image: 7.644|tagging_loss_fusion: 7.760|total_loss: 50.019 | Examples/sec: 69.86\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.79 | precision@0.5: 0.95 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 4021 | tagging_loss_video: 5.503|tagging_loss_audio: 9.471|tagging_loss_text: 16.408|tagging_loss_image: 6.237|tagging_loss_fusion: 5.919|total_loss: 43.537 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 4022 | tagging_loss_video: 5.616|tagging_loss_audio: 10.171|tagging_loss_text: 14.049|tagging_loss_image: 5.845|tagging_loss_fusion: 5.096|total_loss: 40.777 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 4023 | tagging_loss_video: 5.123|tagging_loss_audio: 8.104|tagging_loss_text: 12.986|tagging_loss_image: 5.383|tagging_loss_fusion: 4.784|total_loss: 36.381 | 71.10 Examples/sec\n",
      "INFO:tensorflow:training step 4024 | tagging_loss_video: 7.123|tagging_loss_audio: 11.028|tagging_loss_text: 15.178|tagging_loss_image: 6.513|tagging_loss_fusion: 7.026|total_loss: 46.868 | 67.38 Examples/sec\n",
      "INFO:tensorflow:training step 4025 | tagging_loss_video: 6.444|tagging_loss_audio: 10.117|tagging_loss_text: 17.978|tagging_loss_image: 6.787|tagging_loss_fusion: 6.678|total_loss: 48.005 | 71.80 Examples/sec\n",
      "INFO:tensorflow:training step 4026 | tagging_loss_video: 5.790|tagging_loss_audio: 9.375|tagging_loss_text: 16.713|tagging_loss_image: 5.413|tagging_loss_fusion: 3.896|total_loss: 41.188 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 4027 | tagging_loss_video: 6.780|tagging_loss_audio: 9.275|tagging_loss_text: 15.313|tagging_loss_image: 5.496|tagging_loss_fusion: 5.530|total_loss: 42.393 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 4028 | tagging_loss_video: 7.161|tagging_loss_audio: 10.981|tagging_loss_text: 19.049|tagging_loss_image: 6.482|tagging_loss_fusion: 6.419|total_loss: 50.092 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 4029 | tagging_loss_video: 5.641|tagging_loss_audio: 9.427|tagging_loss_text: 17.272|tagging_loss_image: 6.284|tagging_loss_fusion: 5.414|total_loss: 44.037 | 69.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4030 |tagging_loss_video: 5.586|tagging_loss_audio: 10.337|tagging_loss_text: 14.642|tagging_loss_image: 5.639|tagging_loss_fusion: 4.857|total_loss: 41.062 | Examples/sec: 69.70\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.77 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4031 | tagging_loss_video: 6.124|tagging_loss_audio: 10.493|tagging_loss_text: 18.797|tagging_loss_image: 6.002|tagging_loss_fusion: 6.484|total_loss: 47.900 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 4032 | tagging_loss_video: 6.044|tagging_loss_audio: 8.895|tagging_loss_text: 15.101|tagging_loss_image: 5.784|tagging_loss_fusion: 5.165|total_loss: 40.989 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 4033 | tagging_loss_video: 5.563|tagging_loss_audio: 9.317|tagging_loss_text: 14.646|tagging_loss_image: 4.509|tagging_loss_fusion: 3.241|total_loss: 37.276 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 4034 | tagging_loss_video: 7.056|tagging_loss_audio: 11.612|tagging_loss_text: 18.553|tagging_loss_image: 6.242|tagging_loss_fusion: 7.570|total_loss: 51.032 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 4035 | tagging_loss_video: 5.899|tagging_loss_audio: 10.493|tagging_loss_text: 18.474|tagging_loss_image: 6.618|tagging_loss_fusion: 4.778|total_loss: 46.262 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 4036 | tagging_loss_video: 5.857|tagging_loss_audio: 8.801|tagging_loss_text: 15.073|tagging_loss_image: 6.231|tagging_loss_fusion: 5.168|total_loss: 41.131 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 4037 | tagging_loss_video: 7.282|tagging_loss_audio: 9.534|tagging_loss_text: 14.992|tagging_loss_image: 6.102|tagging_loss_fusion: 8.014|total_loss: 45.924 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 4038 | tagging_loss_video: 5.301|tagging_loss_audio: 8.332|tagging_loss_text: 14.158|tagging_loss_image: 5.654|tagging_loss_fusion: 3.724|total_loss: 37.169 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 4039 | tagging_loss_video: 6.565|tagging_loss_audio: 10.441|tagging_loss_text: 15.407|tagging_loss_image: 6.870|tagging_loss_fusion: 5.181|total_loss: 44.464 | 71.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4040 |tagging_loss_video: 4.825|tagging_loss_audio: 10.344|tagging_loss_text: 16.914|tagging_loss_image: 6.466|tagging_loss_fusion: 3.368|total_loss: 41.917 | Examples/sec: 66.80\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4041 | tagging_loss_video: 5.928|tagging_loss_audio: 8.793|tagging_loss_text: 13.214|tagging_loss_image: 6.028|tagging_loss_fusion: 5.402|total_loss: 39.365 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 4042 | tagging_loss_video: 5.258|tagging_loss_audio: 9.802|tagging_loss_text: 12.596|tagging_loss_image: 5.484|tagging_loss_fusion: 4.046|total_loss: 37.186 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 4043 | tagging_loss_video: 5.883|tagging_loss_audio: 9.077|tagging_loss_text: 12.142|tagging_loss_image: 6.108|tagging_loss_fusion: 5.168|total_loss: 38.378 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 4044 | tagging_loss_video: 5.061|tagging_loss_audio: 8.649|tagging_loss_text: 17.162|tagging_loss_image: 5.878|tagging_loss_fusion: 3.801|total_loss: 40.550 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 4045 | tagging_loss_video: 5.539|tagging_loss_audio: 8.873|tagging_loss_text: 13.305|tagging_loss_image: 5.585|tagging_loss_fusion: 3.897|total_loss: 37.200 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 4046 | tagging_loss_video: 4.968|tagging_loss_audio: 9.208|tagging_loss_text: 16.067|tagging_loss_image: 6.880|tagging_loss_fusion: 4.085|total_loss: 41.209 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 4047 | tagging_loss_video: 5.921|tagging_loss_audio: 9.077|tagging_loss_text: 15.408|tagging_loss_image: 5.056|tagging_loss_fusion: 4.875|total_loss: 40.336 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 4048 | tagging_loss_video: 5.834|tagging_loss_audio: 11.492|tagging_loss_text: 12.557|tagging_loss_image: 5.278|tagging_loss_fusion: 4.076|total_loss: 39.237 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 4049 | tagging_loss_video: 5.335|tagging_loss_audio: 9.979|tagging_loss_text: 17.490|tagging_loss_image: 4.635|tagging_loss_fusion: 5.079|total_loss: 42.516 | 69.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4050 |tagging_loss_video: 5.183|tagging_loss_audio: 9.236|tagging_loss_text: 11.341|tagging_loss_image: 5.748|tagging_loss_fusion: 4.004|total_loss: 35.512 | Examples/sec: 72.39\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4051 | tagging_loss_video: 5.595|tagging_loss_audio: 8.706|tagging_loss_text: 17.299|tagging_loss_image: 5.022|tagging_loss_fusion: 4.716|total_loss: 41.337 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 4052 | tagging_loss_video: 6.153|tagging_loss_audio: 10.660|tagging_loss_text: 14.870|tagging_loss_image: 6.502|tagging_loss_fusion: 4.748|total_loss: 42.933 | 72.28 Examples/sec\n",
      "INFO:tensorflow:training step 4053 | tagging_loss_video: 6.340|tagging_loss_audio: 10.894|tagging_loss_text: 16.723|tagging_loss_image: 6.759|tagging_loss_fusion: 6.768|total_loss: 47.484 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 4054 | tagging_loss_video: 5.690|tagging_loss_audio: 8.971|tagging_loss_text: 16.710|tagging_loss_image: 7.025|tagging_loss_fusion: 5.389|total_loss: 43.784 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 4055 | tagging_loss_video: 5.017|tagging_loss_audio: 9.007|tagging_loss_text: 16.773|tagging_loss_image: 6.897|tagging_loss_fusion: 4.437|total_loss: 42.131 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 4056 | tagging_loss_video: 5.922|tagging_loss_audio: 9.218|tagging_loss_text: 19.193|tagging_loss_image: 6.202|tagging_loss_fusion: 4.782|total_loss: 45.317 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 4057 | tagging_loss_video: 6.784|tagging_loss_audio: 10.468|tagging_loss_text: 13.532|tagging_loss_image: 6.772|tagging_loss_fusion: 7.225|total_loss: 44.781 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 4058 | tagging_loss_video: 5.000|tagging_loss_audio: 8.948|tagging_loss_text: 14.667|tagging_loss_image: 3.731|tagging_loss_fusion: 3.430|total_loss: 35.777 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 4059 | tagging_loss_video: 6.618|tagging_loss_audio: 11.536|tagging_loss_text: 17.919|tagging_loss_image: 6.347|tagging_loss_fusion: 5.719|total_loss: 48.140 | 67.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4060 |tagging_loss_video: 5.835|tagging_loss_audio: 9.115|tagging_loss_text: 17.161|tagging_loss_image: 4.546|tagging_loss_fusion: 3.726|total_loss: 40.384 | Examples/sec: 71.06\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:Recording summary at step 4061.\n",
      "INFO:tensorflow:training step 4061 | tagging_loss_video: 5.798|tagging_loss_audio: 10.383|tagging_loss_text: 15.752|tagging_loss_image: 5.108|tagging_loss_fusion: 4.490|total_loss: 41.531 | 52.62 Examples/sec\n",
      "INFO:tensorflow:training step 4062 | tagging_loss_video: 5.682|tagging_loss_audio: 8.007|tagging_loss_text: 12.887|tagging_loss_image: 5.408|tagging_loss_fusion: 4.400|total_loss: 36.384 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 4063 | tagging_loss_video: 5.499|tagging_loss_audio: 10.486|tagging_loss_text: 17.562|tagging_loss_image: 5.449|tagging_loss_fusion: 5.426|total_loss: 44.421 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 4064 | tagging_loss_video: 5.999|tagging_loss_audio: 9.605|tagging_loss_text: 15.923|tagging_loss_image: 5.561|tagging_loss_fusion: 5.021|total_loss: 42.108 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 4065 | tagging_loss_video: 6.173|tagging_loss_audio: 8.580|tagging_loss_text: 16.211|tagging_loss_image: 5.322|tagging_loss_fusion: 6.228|total_loss: 42.515 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 4066 | tagging_loss_video: 6.387|tagging_loss_audio: 8.330|tagging_loss_text: 13.309|tagging_loss_image: 6.319|tagging_loss_fusion: 5.003|total_loss: 39.347 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 4067 | tagging_loss_video: 5.025|tagging_loss_audio: 9.557|tagging_loss_text: 14.711|tagging_loss_image: 6.555|tagging_loss_fusion: 3.686|total_loss: 39.533 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 4068 | tagging_loss_video: 5.911|tagging_loss_audio: 8.978|tagging_loss_text: 13.617|tagging_loss_image: 5.582|tagging_loss_fusion: 3.637|total_loss: 37.726 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 4069 | tagging_loss_video: 6.103|tagging_loss_audio: 9.333|tagging_loss_text: 15.642|tagging_loss_image: 5.890|tagging_loss_fusion: 4.826|total_loss: 41.794 | 70.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4070 |tagging_loss_video: 5.704|tagging_loss_audio: 8.910|tagging_loss_text: 12.786|tagging_loss_image: 5.832|tagging_loss_fusion: 3.058|total_loss: 36.290 | Examples/sec: 67.91\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 4071 | tagging_loss_video: 6.631|tagging_loss_audio: 9.340|tagging_loss_text: 14.559|tagging_loss_image: 5.974|tagging_loss_fusion: 5.361|total_loss: 41.864 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 4072 | tagging_loss_video: 5.430|tagging_loss_audio: 8.136|tagging_loss_text: 15.367|tagging_loss_image: 4.404|tagging_loss_fusion: 3.730|total_loss: 37.066 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 4073 | tagging_loss_video: 6.950|tagging_loss_audio: 8.342|tagging_loss_text: 17.740|tagging_loss_image: 5.855|tagging_loss_fusion: 6.729|total_loss: 45.615 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 4074 | tagging_loss_video: 5.795|tagging_loss_audio: 8.819|tagging_loss_text: 16.768|tagging_loss_image: 5.397|tagging_loss_fusion: 4.608|total_loss: 41.387 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 4075 | tagging_loss_video: 6.014|tagging_loss_audio: 8.158|tagging_loss_text: 14.278|tagging_loss_image: 5.112|tagging_loss_fusion: 3.669|total_loss: 37.231 | 72.25 Examples/sec\n",
      "INFO:tensorflow:training step 4076 | tagging_loss_video: 5.266|tagging_loss_audio: 8.154|tagging_loss_text: 17.559|tagging_loss_image: 5.002|tagging_loss_fusion: 3.830|total_loss: 39.811 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 4077 | tagging_loss_video: 6.020|tagging_loss_audio: 9.428|tagging_loss_text: 15.265|tagging_loss_image: 6.006|tagging_loss_fusion: 5.701|total_loss: 42.420 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 4078 | tagging_loss_video: 5.235|tagging_loss_audio: 7.895|tagging_loss_text: 15.148|tagging_loss_image: 5.819|tagging_loss_fusion: 4.997|total_loss: 39.094 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 4079 | tagging_loss_video: 6.536|tagging_loss_audio: 8.783|tagging_loss_text: 15.361|tagging_loss_image: 6.273|tagging_loss_fusion: 6.291|total_loss: 43.244 | 70.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4080 |tagging_loss_video: 5.331|tagging_loss_audio: 9.810|tagging_loss_text: 15.517|tagging_loss_image: 6.100|tagging_loss_fusion: 4.191|total_loss: 40.950 | Examples/sec: 70.69\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4081 | tagging_loss_video: 5.133|tagging_loss_audio: 9.384|tagging_loss_text: 15.924|tagging_loss_image: 6.623|tagging_loss_fusion: 5.739|total_loss: 42.802 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 4082 | tagging_loss_video: 6.660|tagging_loss_audio: 8.225|tagging_loss_text: 17.566|tagging_loss_image: 6.779|tagging_loss_fusion: 4.707|total_loss: 43.937 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 4083 | tagging_loss_video: 6.418|tagging_loss_audio: 10.505|tagging_loss_text: 18.355|tagging_loss_image: 5.214|tagging_loss_fusion: 5.682|total_loss: 46.174 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 4084 | tagging_loss_video: 4.693|tagging_loss_audio: 8.400|tagging_loss_text: 15.473|tagging_loss_image: 6.130|tagging_loss_fusion: 4.294|total_loss: 38.990 | 69.17 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4085 | tagging_loss_video: 6.862|tagging_loss_audio: 8.661|tagging_loss_text: 17.649|tagging_loss_image: 6.233|tagging_loss_fusion: 8.174|total_loss: 47.580 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 4086 | tagging_loss_video: 6.703|tagging_loss_audio: 9.696|tagging_loss_text: 12.541|tagging_loss_image: 5.118|tagging_loss_fusion: 6.476|total_loss: 40.534 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 4087 | tagging_loss_video: 7.382|tagging_loss_audio: 9.765|tagging_loss_text: 16.595|tagging_loss_image: 5.888|tagging_loss_fusion: 6.306|total_loss: 45.935 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 4088 | tagging_loss_video: 6.498|tagging_loss_audio: 8.854|tagging_loss_text: 14.607|tagging_loss_image: 6.585|tagging_loss_fusion: 9.513|total_loss: 46.057 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 4089 | tagging_loss_video: 5.937|tagging_loss_audio: 9.325|tagging_loss_text: 15.389|tagging_loss_image: 5.714|tagging_loss_fusion: 5.629|total_loss: 41.994 | 71.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4090 |tagging_loss_video: 7.858|tagging_loss_audio: 8.660|tagging_loss_text: 19.022|tagging_loss_image: 5.633|tagging_loss_fusion: 5.624|total_loss: 46.796 | Examples/sec: 65.05\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4091 | tagging_loss_video: 6.140|tagging_loss_audio: 9.433|tagging_loss_text: 13.465|tagging_loss_image: 5.378|tagging_loss_fusion: 6.325|total_loss: 40.741 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 4092 | tagging_loss_video: 5.434|tagging_loss_audio: 9.347|tagging_loss_text: 17.159|tagging_loss_image: 6.608|tagging_loss_fusion: 5.224|total_loss: 43.772 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 4093 | tagging_loss_video: 5.363|tagging_loss_audio: 9.019|tagging_loss_text: 14.089|tagging_loss_image: 6.565|tagging_loss_fusion: 3.563|total_loss: 38.598 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 4094 | tagging_loss_video: 6.194|tagging_loss_audio: 8.317|tagging_loss_text: 15.358|tagging_loss_image: 5.920|tagging_loss_fusion: 5.122|total_loss: 40.911 | 67.56 Examples/sec\n",
      "INFO:tensorflow:training step 4095 | tagging_loss_video: 5.361|tagging_loss_audio: 9.025|tagging_loss_text: 16.261|tagging_loss_image: 5.454|tagging_loss_fusion: 4.254|total_loss: 40.355 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 4096 | tagging_loss_video: 6.599|tagging_loss_audio: 10.193|tagging_loss_text: 15.640|tagging_loss_image: 6.331|tagging_loss_fusion: 6.339|total_loss: 45.103 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 4097 | tagging_loss_video: 6.218|tagging_loss_audio: 9.685|tagging_loss_text: 15.599|tagging_loss_image: 5.990|tagging_loss_fusion: 6.142|total_loss: 43.634 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 4098 | tagging_loss_video: 7.122|tagging_loss_audio: 8.841|tagging_loss_text: 16.676|tagging_loss_image: 7.013|tagging_loss_fusion: 7.425|total_loss: 47.078 | 61.52 Examples/sec\n",
      "INFO:tensorflow:training step 4099 | tagging_loss_video: 5.615|tagging_loss_audio: 10.769|tagging_loss_text: 16.193|tagging_loss_image: 6.645|tagging_loss_fusion: 4.611|total_loss: 43.834 | 68.68 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4100 |tagging_loss_video: 6.529|tagging_loss_audio: 8.749|tagging_loss_text: 20.526|tagging_loss_image: 7.386|tagging_loss_fusion: 8.218|total_loss: 51.409 | Examples/sec: 67.26\n",
      "INFO:tensorflow:GAP: 0.90 | precision@0.1: 0.80 | precision@0.5: 0.91 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 4101 | tagging_loss_video: 5.360|tagging_loss_audio: 8.307|tagging_loss_text: 15.707|tagging_loss_image: 5.775|tagging_loss_fusion: 4.489|total_loss: 39.639 | 66.96 Examples/sec\n",
      "INFO:tensorflow:training step 4102 | tagging_loss_video: 4.930|tagging_loss_audio: 7.360|tagging_loss_text: 17.532|tagging_loss_image: 5.667|tagging_loss_fusion: 4.468|total_loss: 39.957 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 4103 | tagging_loss_video: 5.908|tagging_loss_audio: 8.418|tagging_loss_text: 15.448|tagging_loss_image: 4.968|tagging_loss_fusion: 5.111|total_loss: 39.854 | 70.06 Examples/sec\n",
      "INFO:tensorflow:training step 4104 | tagging_loss_video: 5.586|tagging_loss_audio: 7.711|tagging_loss_text: 15.204|tagging_loss_image: 4.473|tagging_loss_fusion: 3.146|total_loss: 36.119 | 64.68 Examples/sec\n",
      "INFO:tensorflow:training step 4105 | tagging_loss_video: 5.431|tagging_loss_audio: 9.000|tagging_loss_text: 17.542|tagging_loss_image: 5.261|tagging_loss_fusion: 3.269|total_loss: 40.503 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 4106 | tagging_loss_video: 5.180|tagging_loss_audio: 8.655|tagging_loss_text: 15.135|tagging_loss_image: 6.297|tagging_loss_fusion: 4.507|total_loss: 39.773 | 65.58 Examples/sec\n",
      "INFO:tensorflow:training step 4107 | tagging_loss_video: 5.892|tagging_loss_audio: 8.648|tagging_loss_text: 14.623|tagging_loss_image: 6.222|tagging_loss_fusion: 6.285|total_loss: 41.670 | 71.97 Examples/sec\n",
      "INFO:tensorflow:training step 4108 | tagging_loss_video: 4.649|tagging_loss_audio: 8.863|tagging_loss_text: 13.385|tagging_loss_image: 3.914|tagging_loss_fusion: 2.790|total_loss: 33.601 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 4109 | tagging_loss_video: 6.210|tagging_loss_audio: 8.642|tagging_loss_text: 16.199|tagging_loss_image: 6.004|tagging_loss_fusion: 4.685|total_loss: 41.740 | 61.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4110 |tagging_loss_video: 5.603|tagging_loss_audio: 8.604|tagging_loss_text: 15.732|tagging_loss_image: 5.077|tagging_loss_fusion: 3.965|total_loss: 38.981 | Examples/sec: 70.47\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4111 | tagging_loss_video: 5.474|tagging_loss_audio: 8.292|tagging_loss_text: 9.302|tagging_loss_image: 5.425|tagging_loss_fusion: 5.668|total_loss: 34.161 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 4112 | tagging_loss_video: 4.700|tagging_loss_audio: 9.282|tagging_loss_text: 13.010|tagging_loss_image: 5.125|tagging_loss_fusion: 3.665|total_loss: 35.782 | 61.81 Examples/sec\n",
      "INFO:tensorflow:training step 4113 | tagging_loss_video: 5.913|tagging_loss_audio: 9.044|tagging_loss_text: 18.674|tagging_loss_image: 4.036|tagging_loss_fusion: 3.244|total_loss: 40.911 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 4114 | tagging_loss_video: 5.782|tagging_loss_audio: 9.229|tagging_loss_text: 15.237|tagging_loss_image: 6.308|tagging_loss_fusion: 4.555|total_loss: 41.111 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 4115 | tagging_loss_video: 4.433|tagging_loss_audio: 8.206|tagging_loss_text: 13.506|tagging_loss_image: 5.797|tagging_loss_fusion: 4.218|total_loss: 36.161 | 61.16 Examples/sec\n",
      "INFO:tensorflow:training step 4116 | tagging_loss_video: 6.024|tagging_loss_audio: 8.537|tagging_loss_text: 15.401|tagging_loss_image: 4.959|tagging_loss_fusion: 3.675|total_loss: 38.595 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 4117 | tagging_loss_video: 6.127|tagging_loss_audio: 7.646|tagging_loss_text: 14.463|tagging_loss_image: 5.673|tagging_loss_fusion: 4.964|total_loss: 38.872 | 71.80 Examples/sec\n",
      "INFO:tensorflow:training step 4118 | tagging_loss_video: 5.470|tagging_loss_audio: 9.317|tagging_loss_text: 12.376|tagging_loss_image: 5.093|tagging_loss_fusion: 3.480|total_loss: 35.737 | 65.02 Examples/sec\n",
      "INFO:tensorflow:training step 4119 | tagging_loss_video: 5.473|tagging_loss_audio: 8.434|tagging_loss_text: 15.002|tagging_loss_image: 4.571|tagging_loss_fusion: 4.348|total_loss: 37.829 | 68.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4120 |tagging_loss_video: 4.269|tagging_loss_audio: 7.772|tagging_loss_text: 13.854|tagging_loss_image: 6.192|tagging_loss_fusion: 2.969|total_loss: 35.056 | Examples/sec: 70.61\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4121 | tagging_loss_video: 6.473|tagging_loss_audio: 9.707|tagging_loss_text: 15.950|tagging_loss_image: 6.132|tagging_loss_fusion: 6.275|total_loss: 44.539 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 4122 | tagging_loss_video: 5.420|tagging_loss_audio: 9.193|tagging_loss_text: 16.762|tagging_loss_image: 4.771|tagging_loss_fusion: 3.095|total_loss: 39.241 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 4123 | tagging_loss_video: 6.589|tagging_loss_audio: 8.865|tagging_loss_text: 14.610|tagging_loss_image: 6.002|tagging_loss_fusion: 5.161|total_loss: 41.227 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 4124 | tagging_loss_video: 6.884|tagging_loss_audio: 9.353|tagging_loss_text: 15.472|tagging_loss_image: 5.837|tagging_loss_fusion: 6.989|total_loss: 44.535 | 60.16 Examples/sec\n",
      "INFO:tensorflow:training step 4125 | tagging_loss_video: 5.425|tagging_loss_audio: 8.588|tagging_loss_text: 16.205|tagging_loss_image: 5.960|tagging_loss_fusion: 3.967|total_loss: 40.144 | 67.20 Examples/sec\n",
      "INFO:tensorflow:training step 4126 | tagging_loss_video: 6.828|tagging_loss_audio: 11.265|tagging_loss_text: 14.974|tagging_loss_image: 5.313|tagging_loss_fusion: 4.706|total_loss: 43.085 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 4127 | tagging_loss_video: 4.373|tagging_loss_audio: 8.616|tagging_loss_text: 12.021|tagging_loss_image: 5.611|tagging_loss_fusion: 3.383|total_loss: 34.004 | 64.29 Examples/sec\n",
      "INFO:tensorflow:training step 4128 | tagging_loss_video: 4.924|tagging_loss_audio: 7.304|tagging_loss_text: 11.280|tagging_loss_image: 5.143|tagging_loss_fusion: 5.020|total_loss: 33.670 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 4129 | tagging_loss_video: 5.240|tagging_loss_audio: 9.186|tagging_loss_text: 17.364|tagging_loss_image: 5.564|tagging_loss_fusion: 4.386|total_loss: 41.739 | 70.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4130 |tagging_loss_video: 4.969|tagging_loss_audio: 9.069|tagging_loss_text: 15.493|tagging_loss_image: 5.090|tagging_loss_fusion: 4.200|total_loss: 38.821 | Examples/sec: 62.72\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4131 | tagging_loss_video: 6.534|tagging_loss_audio: 8.840|tagging_loss_text: 17.269|tagging_loss_image: 5.983|tagging_loss_fusion: 5.596|total_loss: 44.222 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 4132 | tagging_loss_video: 6.238|tagging_loss_audio: 8.494|tagging_loss_text: 17.143|tagging_loss_image: 5.614|tagging_loss_fusion: 5.134|total_loss: 42.623 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 4133 | tagging_loss_video: 5.634|tagging_loss_audio: 7.892|tagging_loss_text: 14.111|tagging_loss_image: 5.806|tagging_loss_fusion: 4.685|total_loss: 38.127 | 65.35 Examples/sec\n",
      "INFO:tensorflow:training step 4134 | tagging_loss_video: 5.411|tagging_loss_audio: 8.901|tagging_loss_text: 15.232|tagging_loss_image: 5.501|tagging_loss_fusion: 4.133|total_loss: 39.178 | 68.25 Examples/sec\n",
      "INFO:tensorflow:training step 4135 | tagging_loss_video: 5.956|tagging_loss_audio: 9.378|tagging_loss_text: 12.224|tagging_loss_image: 6.086|tagging_loss_fusion: 4.447|total_loss: 38.092 | 67.53 Examples/sec\n",
      "INFO:tensorflow:training step 4136 | tagging_loss_video: 5.285|tagging_loss_audio: 8.609|tagging_loss_text: 14.682|tagging_loss_image: 6.301|tagging_loss_fusion: 3.896|total_loss: 38.773 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 4137 | tagging_loss_video: 6.061|tagging_loss_audio: 9.604|tagging_loss_text: 18.513|tagging_loss_image: 6.540|tagging_loss_fusion: 5.268|total_loss: 45.985 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 4138 | tagging_loss_video: 4.902|tagging_loss_audio: 8.207|tagging_loss_text: 17.139|tagging_loss_image: 6.661|tagging_loss_fusion: 3.188|total_loss: 40.096 | 62.35 Examples/sec\n",
      "INFO:tensorflow:training step 4139 | tagging_loss_video: 5.837|tagging_loss_audio: 9.224|tagging_loss_text: 13.988|tagging_loss_image: 5.413|tagging_loss_fusion: 4.672|total_loss: 39.133 | 71.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4140 |tagging_loss_video: 4.854|tagging_loss_audio: 8.970|tagging_loss_text: 15.569|tagging_loss_image: 5.009|tagging_loss_fusion: 3.557|total_loss: 37.960 | Examples/sec: 66.83\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4141 | tagging_loss_video: 5.750|tagging_loss_audio: 9.101|tagging_loss_text: 14.927|tagging_loss_image: 4.167|tagging_loss_fusion: 4.748|total_loss: 38.692 | 70.56 Examples/sec\n",
      "INFO:tensorflow:training step 4142 | tagging_loss_video: 5.979|tagging_loss_audio: 9.055|tagging_loss_text: 14.973|tagging_loss_image: 6.968|tagging_loss_fusion: 5.681|total_loss: 42.656 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 4143 | tagging_loss_video: 6.191|tagging_loss_audio: 8.417|tagging_loss_text: 14.861|tagging_loss_image: 5.936|tagging_loss_fusion: 4.892|total_loss: 40.296 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 4144 | tagging_loss_video: 5.667|tagging_loss_audio: 9.373|tagging_loss_text: 16.042|tagging_loss_image: 5.526|tagging_loss_fusion: 5.982|total_loss: 42.590 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 4145 | tagging_loss_video: 5.292|tagging_loss_audio: 8.947|tagging_loss_text: 13.539|tagging_loss_image: 5.222|tagging_loss_fusion: 4.606|total_loss: 37.606 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 4146 | tagging_loss_video: 5.631|tagging_loss_audio: 10.330|tagging_loss_text: 14.915|tagging_loss_image: 5.524|tagging_loss_fusion: 5.147|total_loss: 41.547 | 72.15 Examples/sec\n",
      "INFO:tensorflow:training step 4147 | tagging_loss_video: 5.743|tagging_loss_audio: 10.119|tagging_loss_text: 16.453|tagging_loss_image: 6.638|tagging_loss_fusion: 4.132|total_loss: 43.085 | 64.52 Examples/sec\n",
      "INFO:tensorflow:training step 4148 | tagging_loss_video: 4.763|tagging_loss_audio: 8.852|tagging_loss_text: 13.259|tagging_loss_image: 5.652|tagging_loss_fusion: 3.604|total_loss: 36.131 | 71.94 Examples/sec\n",
      "INFO:tensorflow:training step 4149 | tagging_loss_video: 5.613|tagging_loss_audio: 8.859|tagging_loss_text: 15.673|tagging_loss_image: 5.632|tagging_loss_fusion: 4.517|total_loss: 40.293 | 61.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4150 |tagging_loss_video: 5.930|tagging_loss_audio: 9.639|tagging_loss_text: 15.203|tagging_loss_image: 6.266|tagging_loss_fusion: 5.147|total_loss: 42.186 | Examples/sec: 70.07\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4151 | tagging_loss_video: 5.830|tagging_loss_audio: 8.858|tagging_loss_text: 15.840|tagging_loss_image: 6.292|tagging_loss_fusion: 5.778|total_loss: 42.597 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 4152 | tagging_loss_video: 5.967|tagging_loss_audio: 9.337|tagging_loss_text: 16.510|tagging_loss_image: 6.212|tagging_loss_fusion: 5.985|total_loss: 44.011 | 61.12 Examples/sec\n",
      "INFO:tensorflow:training step 4153 | tagging_loss_video: 6.420|tagging_loss_audio: 10.836|tagging_loss_text: 19.351|tagging_loss_image: 5.723|tagging_loss_fusion: 5.762|total_loss: 48.091 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 4154 | tagging_loss_video: 5.930|tagging_loss_audio: 8.498|tagging_loss_text: 17.560|tagging_loss_image: 4.620|tagging_loss_fusion: 4.900|total_loss: 41.508 | 71.44 Examples/sec\n",
      "INFO:tensorflow:training step 4155 | tagging_loss_video: 5.607|tagging_loss_audio: 10.109|tagging_loss_text: 14.549|tagging_loss_image: 6.193|tagging_loss_fusion: 4.498|total_loss: 40.956 | 61.97 Examples/sec\n",
      "INFO:tensorflow:training step 4156 | tagging_loss_video: 5.392|tagging_loss_audio: 10.057|tagging_loss_text: 13.853|tagging_loss_image: 5.834|tagging_loss_fusion: 3.588|total_loss: 38.724 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 4157 | tagging_loss_video: 7.205|tagging_loss_audio: 9.842|tagging_loss_text: 15.980|tagging_loss_image: 5.966|tagging_loss_fusion: 6.222|total_loss: 45.215 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 4158 | tagging_loss_video: 5.623|tagging_loss_audio: 8.715|tagging_loss_text: 16.186|tagging_loss_image: 6.417|tagging_loss_fusion: 5.263|total_loss: 42.204 | 67.06 Examples/sec\n",
      "INFO:tensorflow:training step 4159 | tagging_loss_video: 5.858|tagging_loss_audio: 10.040|tagging_loss_text: 16.397|tagging_loss_image: 6.636|tagging_loss_fusion: 4.304|total_loss: 43.236 | 68.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4160 |tagging_loss_video: 5.264|tagging_loss_audio: 10.666|tagging_loss_text: 15.756|tagging_loss_image: 5.679|tagging_loss_fusion: 4.214|total_loss: 41.579 | Examples/sec: 69.54\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4161 | tagging_loss_video: 6.714|tagging_loss_audio: 9.765|tagging_loss_text: 16.910|tagging_loss_image: 6.006|tagging_loss_fusion: 6.398|total_loss: 45.794 | 68.93 Examples/sec\n",
      "INFO:tensorflow:training step 4162 | tagging_loss_video: 5.553|tagging_loss_audio: 8.734|tagging_loss_text: 12.980|tagging_loss_image: 4.488|tagging_loss_fusion: 5.181|total_loss: 36.936 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 4163 | tagging_loss_video: 6.302|tagging_loss_audio: 9.681|tagging_loss_text: 16.607|tagging_loss_image: 5.810|tagging_loss_fusion: 4.773|total_loss: 43.173 | 64.45 Examples/sec\n",
      "INFO:tensorflow:training step 4164 | tagging_loss_video: 5.746|tagging_loss_audio: 10.526|tagging_loss_text: 15.861|tagging_loss_image: 7.102|tagging_loss_fusion: 4.886|total_loss: 44.122 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 4165 | tagging_loss_video: 5.830|tagging_loss_audio: 9.142|tagging_loss_text: 14.566|tagging_loss_image: 6.506|tagging_loss_fusion: 5.844|total_loss: 41.888 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 4166 | tagging_loss_video: 5.826|tagging_loss_audio: 9.278|tagging_loss_text: 11.842|tagging_loss_image: 5.604|tagging_loss_fusion: 4.897|total_loss: 37.447 | 61.72 Examples/sec\n",
      "INFO:tensorflow:training step 4167 | tagging_loss_video: 5.684|tagging_loss_audio: 11.694|tagging_loss_text: 19.080|tagging_loss_image: 7.222|tagging_loss_fusion: 6.036|total_loss: 49.717 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 4168 | tagging_loss_video: 5.659|tagging_loss_audio: 9.646|tagging_loss_text: 17.285|tagging_loss_image: 5.646|tagging_loss_fusion: 5.000|total_loss: 43.236 | 72.32 Examples/sec\n",
      "INFO:tensorflow:training step 4169 | tagging_loss_video: 5.327|tagging_loss_audio: 9.662|tagging_loss_text: 15.312|tagging_loss_image: 5.096|tagging_loss_fusion: 3.579|total_loss: 38.976 | 66.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4170 |tagging_loss_video: 5.553|tagging_loss_audio: 10.046|tagging_loss_text: 16.095|tagging_loss_image: 6.333|tagging_loss_fusion: 3.990|total_loss: 42.017 | Examples/sec: 69.24\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4171 | tagging_loss_video: 6.592|tagging_loss_audio: 9.843|tagging_loss_text: 14.408|tagging_loss_image: 5.265|tagging_loss_fusion: 4.629|total_loss: 40.737 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 4172 | tagging_loss_video: 6.476|tagging_loss_audio: 10.167|tagging_loss_text: 15.977|tagging_loss_image: 4.543|tagging_loss_fusion: 5.490|total_loss: 42.652 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 4173 | tagging_loss_video: 6.945|tagging_loss_audio: 10.355|tagging_loss_text: 14.100|tagging_loss_image: 6.591|tagging_loss_fusion: 5.364|total_loss: 43.356 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 4174 | tagging_loss_video: 5.803|tagging_loss_audio: 9.430|tagging_loss_text: 16.982|tagging_loss_image: 6.290|tagging_loss_fusion: 3.802|total_loss: 42.307 | 59.84 Examples/sec\n",
      "INFO:tensorflow:training step 4175 | tagging_loss_video: 5.018|tagging_loss_audio: 9.020|tagging_loss_text: 13.767|tagging_loss_image: 6.267|tagging_loss_fusion: 4.290|total_loss: 38.362 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 4176 | tagging_loss_video: 5.825|tagging_loss_audio: 9.094|tagging_loss_text: 12.251|tagging_loss_image: 6.518|tagging_loss_fusion: 4.141|total_loss: 37.828 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 4177 | tagging_loss_video: 5.755|tagging_loss_audio: 8.946|tagging_loss_text: 12.208|tagging_loss_image: 5.954|tagging_loss_fusion: 5.056|total_loss: 37.920 | 62.80 Examples/sec\n",
      "INFO:tensorflow:training step 4178 | tagging_loss_video: 6.232|tagging_loss_audio: 11.599|tagging_loss_text: 19.808|tagging_loss_image: 6.743|tagging_loss_fusion: 7.050|total_loss: 51.432 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 4179 | tagging_loss_video: 4.752|tagging_loss_audio: 9.174|tagging_loss_text: 14.883|tagging_loss_image: 5.531|tagging_loss_fusion: 3.901|total_loss: 38.241 | 71.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4180 |tagging_loss_video: 5.187|tagging_loss_audio: 9.502|tagging_loss_text: 15.838|tagging_loss_image: 5.902|tagging_loss_fusion: 3.563|total_loss: 39.991 | Examples/sec: 60.66\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4181 | tagging_loss_video: 5.320|tagging_loss_audio: 8.520|tagging_loss_text: 14.601|tagging_loss_image: 5.986|tagging_loss_fusion: 4.194|total_loss: 38.622 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 4182 | tagging_loss_video: 4.425|tagging_loss_audio: 8.248|tagging_loss_text: 12.798|tagging_loss_image: 5.420|tagging_loss_fusion: 3.370|total_loss: 34.262 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 4183 | tagging_loss_video: 5.098|tagging_loss_audio: 9.290|tagging_loss_text: 15.837|tagging_loss_image: 5.230|tagging_loss_fusion: 3.775|total_loss: 39.230 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 4184 | tagging_loss_video: 5.514|tagging_loss_audio: 8.837|tagging_loss_text: 16.455|tagging_loss_image: 5.662|tagging_loss_fusion: 6.492|total_loss: 42.960 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 4185 | tagging_loss_video: 5.964|tagging_loss_audio: 10.048|tagging_loss_text: 16.052|tagging_loss_image: 6.225|tagging_loss_fusion: 4.743|total_loss: 43.033 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 4186 | tagging_loss_video: 5.828|tagging_loss_audio: 9.571|tagging_loss_text: 14.050|tagging_loss_image: 4.973|tagging_loss_fusion: 4.599|total_loss: 39.021 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 4187 | tagging_loss_video: 6.378|tagging_loss_audio: 9.588|tagging_loss_text: 15.201|tagging_loss_image: 6.173|tagging_loss_fusion: 5.483|total_loss: 42.824 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 4188 | tagging_loss_video: 6.423|tagging_loss_audio: 9.686|tagging_loss_text: 12.943|tagging_loss_image: 6.074|tagging_loss_fusion: 7.898|total_loss: 43.024 | 61.66 Examples/sec\n",
      "INFO:tensorflow:training step 4189 | tagging_loss_video: 5.205|tagging_loss_audio: 8.546|tagging_loss_text: 14.023|tagging_loss_image: 5.589|tagging_loss_fusion: 3.308|total_loss: 36.670 | 69.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4190 |tagging_loss_video: 6.233|tagging_loss_audio: 8.625|tagging_loss_text: 14.594|tagging_loss_image: 5.966|tagging_loss_fusion: 6.775|total_loss: 42.193 | Examples/sec: 70.76\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.77 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 4191 | tagging_loss_video: 6.943|tagging_loss_audio: 10.900|tagging_loss_text: 14.858|tagging_loss_image: 5.572|tagging_loss_fusion: 6.495|total_loss: 44.768 | 67.34 Examples/sec\n",
      "INFO:tensorflow:training step 4192 | tagging_loss_video: 6.898|tagging_loss_audio: 9.028|tagging_loss_text: 16.728|tagging_loss_image: 6.741|tagging_loss_fusion: 4.988|total_loss: 44.382 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 4193 | tagging_loss_video: 7.763|tagging_loss_audio: 10.143|tagging_loss_text: 15.205|tagging_loss_image: 6.692|tagging_loss_fusion: 5.070|total_loss: 44.873 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 4194 | tagging_loss_video: 7.406|tagging_loss_audio: 9.864|tagging_loss_text: 16.795|tagging_loss_image: 6.638|tagging_loss_fusion: 7.372|total_loss: 48.075 | 66.31 Examples/sec\n",
      "INFO:tensorflow:training step 4195 | tagging_loss_video: 6.518|tagging_loss_audio: 8.426|tagging_loss_text: 15.766|tagging_loss_image: 6.023|tagging_loss_fusion: 6.428|total_loss: 43.161 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 4196 | tagging_loss_video: 7.093|tagging_loss_audio: 11.211|tagging_loss_text: 14.962|tagging_loss_image: 6.227|tagging_loss_fusion: 7.232|total_loss: 46.725 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 4197 | tagging_loss_video: 6.326|tagging_loss_audio: 9.922|tagging_loss_text: 15.357|tagging_loss_image: 5.169|tagging_loss_fusion: 4.720|total_loss: 41.495 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 4198 | tagging_loss_video: 5.185|tagging_loss_audio: 9.973|tagging_loss_text: 16.492|tagging_loss_image: 5.781|tagging_loss_fusion: 3.891|total_loss: 41.322 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 4199 | tagging_loss_video: 5.267|tagging_loss_audio: 8.606|tagging_loss_text: 14.516|tagging_loss_image: 5.330|tagging_loss_fusion: 3.510|total_loss: 37.230 | 65.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4200 |tagging_loss_video: 6.091|tagging_loss_audio: 10.321|tagging_loss_text: 18.510|tagging_loss_image: 5.910|tagging_loss_fusion: 5.062|total_loss: 45.894 | Examples/sec: 68.82\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4201 | tagging_loss_video: 6.302|tagging_loss_audio: 10.960|tagging_loss_text: 14.654|tagging_loss_image: 4.568|tagging_loss_fusion: 5.414|total_loss: 41.898 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 4202 | tagging_loss_video: 5.026|tagging_loss_audio: 7.833|tagging_loss_text: 13.501|tagging_loss_image: 4.708|tagging_loss_fusion: 3.424|total_loss: 34.491 | 62.75 Examples/sec\n",
      "INFO:tensorflow:training step 4203 | tagging_loss_video: 5.868|tagging_loss_audio: 9.009|tagging_loss_text: 16.236|tagging_loss_image: 5.283|tagging_loss_fusion: 4.382|total_loss: 40.778 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 4204 | tagging_loss_video: 5.328|tagging_loss_audio: 10.804|tagging_loss_text: 15.888|tagging_loss_image: 5.950|tagging_loss_fusion: 4.561|total_loss: 42.531 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 4205 | tagging_loss_video: 6.231|tagging_loss_audio: 9.368|tagging_loss_text: 15.528|tagging_loss_image: 5.902|tagging_loss_fusion: 6.219|total_loss: 43.248 | 66.19 Examples/sec\n",
      "INFO:tensorflow:training step 4206 | tagging_loss_video: 5.768|tagging_loss_audio: 9.964|tagging_loss_text: 15.532|tagging_loss_image: 5.784|tagging_loss_fusion: 4.341|total_loss: 41.389 | 66.03 Examples/sec\n",
      "INFO:tensorflow:training step 4207 | tagging_loss_video: 4.086|tagging_loss_audio: 10.161|tagging_loss_text: 18.021|tagging_loss_image: 6.058|tagging_loss_fusion: 2.966|total_loss: 41.292 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 4208 | tagging_loss_video: 5.932|tagging_loss_audio: 9.217|tagging_loss_text: 15.638|tagging_loss_image: 5.314|tagging_loss_fusion: 4.250|total_loss: 40.351 | 65.91 Examples/sec\n",
      "INFO:tensorflow:training step 4209 | tagging_loss_video: 6.473|tagging_loss_audio: 8.985|tagging_loss_text: 16.337|tagging_loss_image: 6.211|tagging_loss_fusion: 5.886|total_loss: 43.891 | 69.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4210 |tagging_loss_video: 6.018|tagging_loss_audio: 9.543|tagging_loss_text: 18.838|tagging_loss_image: 5.898|tagging_loss_fusion: 4.919|total_loss: 45.215 | Examples/sec: 66.95\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4211 | tagging_loss_video: 6.380|tagging_loss_audio: 7.651|tagging_loss_text: 13.865|tagging_loss_image: 5.375|tagging_loss_fusion: 5.671|total_loss: 38.942 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 4212 | tagging_loss_video: 6.262|tagging_loss_audio: 8.458|tagging_loss_text: 14.034|tagging_loss_image: 4.980|tagging_loss_fusion: 5.285|total_loss: 39.018 | 70.68 Examples/sec\n",
      "INFO:tensorflow:training step 4213 | tagging_loss_video: 6.727|tagging_loss_audio: 8.433|tagging_loss_text: 14.310|tagging_loss_image: 5.425|tagging_loss_fusion: 7.145|total_loss: 42.040 | 65.85 Examples/sec\n",
      "INFO:tensorflow:training step 4214 | tagging_loss_video: 4.994|tagging_loss_audio: 8.531|tagging_loss_text: 16.752|tagging_loss_image: 4.017|tagging_loss_fusion: 2.968|total_loss: 37.261 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 4215 | tagging_loss_video: 6.120|tagging_loss_audio: 10.252|tagging_loss_text: 16.305|tagging_loss_image: 5.116|tagging_loss_fusion: 3.952|total_loss: 41.746 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 4216 | tagging_loss_video: 5.131|tagging_loss_audio: 8.872|tagging_loss_text: 16.265|tagging_loss_image: 5.957|tagging_loss_fusion: 3.973|total_loss: 40.198 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 4217 | tagging_loss_video: 6.291|tagging_loss_audio: 9.095|tagging_loss_text: 14.558|tagging_loss_image: 5.936|tagging_loss_fusion: 6.464|total_loss: 42.345 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 4218 | tagging_loss_video: 5.769|tagging_loss_audio: 9.367|tagging_loss_text: 15.825|tagging_loss_image: 6.469|tagging_loss_fusion: 5.442|total_loss: 42.872 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 4219 | tagging_loss_video: 6.095|tagging_loss_audio: 8.728|tagging_loss_text: 11.880|tagging_loss_image: 6.170|tagging_loss_fusion: 4.442|total_loss: 37.315 | 63.68 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4220 |tagging_loss_video: 6.213|tagging_loss_audio: 9.362|tagging_loss_text: 16.269|tagging_loss_image: 6.337|tagging_loss_fusion: 5.440|total_loss: 43.622 | Examples/sec: 70.10\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4221 | tagging_loss_video: 6.387|tagging_loss_audio: 7.616|tagging_loss_text: 13.931|tagging_loss_image: 6.041|tagging_loss_fusion: 7.214|total_loss: 41.189 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 4222 | tagging_loss_video: 6.625|tagging_loss_audio: 9.018|tagging_loss_text: 14.767|tagging_loss_image: 6.723|tagging_loss_fusion: 5.737|total_loss: 42.870 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 4223 | tagging_loss_video: 6.055|tagging_loss_audio: 10.261|tagging_loss_text: 15.530|tagging_loss_image: 6.555|tagging_loss_fusion: 4.787|total_loss: 43.188 | 67.77 Examples/sec\n",
      "INFO:tensorflow:training step 4224 | tagging_loss_video: 6.119|tagging_loss_audio: 9.188|tagging_loss_text: 14.150|tagging_loss_image: 5.644|tagging_loss_fusion: 5.317|total_loss: 40.418 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 4225 | tagging_loss_video: 5.437|tagging_loss_audio: 8.862|tagging_loss_text: 16.252|tagging_loss_image: 5.151|tagging_loss_fusion: 4.465|total_loss: 40.168 | 67.49 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4226 | tagging_loss_video: 6.497|tagging_loss_audio: 9.375|tagging_loss_text: 16.747|tagging_loss_image: 5.777|tagging_loss_fusion: 4.903|total_loss: 43.299 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 4227 | tagging_loss_video: 5.626|tagging_loss_audio: 7.832|tagging_loss_text: 15.196|tagging_loss_image: 6.195|tagging_loss_fusion: 3.506|total_loss: 38.356 | 63.42 Examples/sec\n",
      "INFO:tensorflow:training step 4228 | tagging_loss_video: 5.783|tagging_loss_audio: 8.741|tagging_loss_text: 15.312|tagging_loss_image: 5.952|tagging_loss_fusion: 3.749|total_loss: 39.537 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 4229 | tagging_loss_video: 6.642|tagging_loss_audio: 8.784|tagging_loss_text: 16.087|tagging_loss_image: 4.365|tagging_loss_fusion: 4.817|total_loss: 40.694 | 70.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4230 |tagging_loss_video: 6.306|tagging_loss_audio: 9.123|tagging_loss_text: 16.820|tagging_loss_image: 4.854|tagging_loss_fusion: 4.054|total_loss: 41.156 | Examples/sec: 63.79\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4231 | tagging_loss_video: 6.510|tagging_loss_audio: 8.366|tagging_loss_text: 14.757|tagging_loss_image: 5.531|tagging_loss_fusion: 7.274|total_loss: 42.438 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 4232 | tagging_loss_video: 6.243|tagging_loss_audio: 8.839|tagging_loss_text: 15.783|tagging_loss_image: 6.026|tagging_loss_fusion: 5.537|total_loss: 42.429 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 4233 | tagging_loss_video: 5.603|tagging_loss_audio: 8.387|tagging_loss_text: 14.763|tagging_loss_image: 5.923|tagging_loss_fusion: 3.874|total_loss: 38.550 | 64.27 Examples/sec\n",
      "INFO:tensorflow:training step 4234 | tagging_loss_video: 5.523|tagging_loss_audio: 8.719|tagging_loss_text: 16.058|tagging_loss_image: 5.925|tagging_loss_fusion: 4.295|total_loss: 40.520 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 4235 | tagging_loss_video: 5.163|tagging_loss_audio: 9.050|tagging_loss_text: 13.023|tagging_loss_image: 4.662|tagging_loss_fusion: 3.245|total_loss: 35.143 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 4236 | tagging_loss_video: 5.928|tagging_loss_audio: 9.447|tagging_loss_text: 15.676|tagging_loss_image: 6.326|tagging_loss_fusion: 4.992|total_loss: 42.371 | 66.71 Examples/sec\n",
      "INFO:tensorflow:training step 4237 | tagging_loss_video: 6.562|tagging_loss_audio: 8.912|tagging_loss_text: 13.354|tagging_loss_image: 6.822|tagging_loss_fusion: 6.187|total_loss: 41.837 | 67.29 Examples/sec\n",
      "INFO:tensorflow:training step 4238 | tagging_loss_video: 5.882|tagging_loss_audio: 9.464|tagging_loss_text: 16.678|tagging_loss_image: 6.430|tagging_loss_fusion: 4.700|total_loss: 43.154 | 64.71 Examples/sec\n",
      "INFO:tensorflow:training step 4239 | tagging_loss_video: 5.924|tagging_loss_audio: 10.400|tagging_loss_text: 16.156|tagging_loss_image: 6.370|tagging_loss_fusion: 4.363|total_loss: 43.212 | 69.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4240 |tagging_loss_video: 7.156|tagging_loss_audio: 10.236|tagging_loss_text: 15.205|tagging_loss_image: 7.137|tagging_loss_fusion: 6.260|total_loss: 45.994 | Examples/sec: 69.75\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.96 |recall@0.1: 0.95 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4241 | tagging_loss_video: 5.704|tagging_loss_audio: 8.359|tagging_loss_text: 13.187|tagging_loss_image: 5.394|tagging_loss_fusion: 4.455|total_loss: 37.099 | 64.70 Examples/sec\n",
      "INFO:tensorflow:training step 4242 | tagging_loss_video: 4.721|tagging_loss_audio: 8.146|tagging_loss_text: 13.775|tagging_loss_image: 4.051|tagging_loss_fusion: 3.713|total_loss: 34.405 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4243 | tagging_loss_video: 4.997|tagging_loss_audio: 9.215|tagging_loss_text: 15.460|tagging_loss_image: 5.352|tagging_loss_fusion: 3.209|total_loss: 38.233 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 4244 | tagging_loss_video: 4.519|tagging_loss_audio: 7.376|tagging_loss_text: 13.987|tagging_loss_image: 5.290|tagging_loss_fusion: 4.411|total_loss: 35.582 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 4245 | tagging_loss_video: 6.058|tagging_loss_audio: 8.621|tagging_loss_text: 15.421|tagging_loss_image: 3.676|tagging_loss_fusion: 3.477|total_loss: 37.254 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 4246 | tagging_loss_video: 5.582|tagging_loss_audio: 8.321|tagging_loss_text: 13.152|tagging_loss_image: 5.742|tagging_loss_fusion: 4.184|total_loss: 36.981 | 68.40 Examples/sec\n",
      "INFO:tensorflow:training step 4247 | tagging_loss_video: 5.587|tagging_loss_audio: 9.595|tagging_loss_text: 12.630|tagging_loss_image: 5.184|tagging_loss_fusion: 5.399|total_loss: 38.396 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 4248 | tagging_loss_video: 4.635|tagging_loss_audio: 9.151|tagging_loss_text: 15.918|tagging_loss_image: 4.918|tagging_loss_fusion: 3.702|total_loss: 38.324 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 4249 | tagging_loss_video: 7.269|tagging_loss_audio: 8.863|tagging_loss_text: 13.997|tagging_loss_image: 5.716|tagging_loss_fusion: 6.184|total_loss: 42.030 | 69.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4250 |tagging_loss_video: 5.455|tagging_loss_audio: 8.809|tagging_loss_text: 15.047|tagging_loss_image: 6.186|tagging_loss_fusion: 6.177|total_loss: 41.675 | Examples/sec: 70.66\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 4251 | tagging_loss_video: 5.470|tagging_loss_audio: 8.207|tagging_loss_text: 13.015|tagging_loss_image: 4.831|tagging_loss_fusion: 3.981|total_loss: 35.505 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 4252 | tagging_loss_video: 5.896|tagging_loss_audio: 8.243|tagging_loss_text: 15.491|tagging_loss_image: 5.580|tagging_loss_fusion: 4.590|total_loss: 39.800 | 64.94 Examples/sec\n",
      "INFO:tensorflow:training step 4253 | tagging_loss_video: 6.114|tagging_loss_audio: 7.959|tagging_loss_text: 17.275|tagging_loss_image: 3.513|tagging_loss_fusion: 5.063|total_loss: 39.924 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 4254 | tagging_loss_video: 5.849|tagging_loss_audio: 8.679|tagging_loss_text: 15.991|tagging_loss_image: 6.758|tagging_loss_fusion: 4.301|total_loss: 41.578 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 4255 | tagging_loss_video: 6.423|tagging_loss_audio: 8.461|tagging_loss_text: 12.211|tagging_loss_image: 4.550|tagging_loss_fusion: 4.392|total_loss: 36.037 | 64.96 Examples/sec\n",
      "INFO:tensorflow:training step 4256 | tagging_loss_video: 4.801|tagging_loss_audio: 7.849|tagging_loss_text: 16.072|tagging_loss_image: 6.219|tagging_loss_fusion: 3.744|total_loss: 38.685 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 4257 | tagging_loss_video: 5.132|tagging_loss_audio: 8.835|tagging_loss_text: 14.452|tagging_loss_image: 5.929|tagging_loss_fusion: 4.131|total_loss: 38.478 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 4258 | tagging_loss_video: 4.709|tagging_loss_audio: 8.651|tagging_loss_text: 13.690|tagging_loss_image: 5.345|tagging_loss_fusion: 3.171|total_loss: 35.566 | 64.70 Examples/sec\n",
      "INFO:tensorflow:training step 4259 | tagging_loss_video: 5.463|tagging_loss_audio: 8.280|tagging_loss_text: 15.630|tagging_loss_image: 5.725|tagging_loss_fusion: 4.898|total_loss: 39.995 | 69.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4260 |tagging_loss_video: 5.032|tagging_loss_audio: 8.681|tagging_loss_text: 13.854|tagging_loss_image: 5.911|tagging_loss_fusion: 4.559|total_loss: 38.036 | Examples/sec: 72.20\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4261 | tagging_loss_video: 5.471|tagging_loss_audio: 8.197|tagging_loss_text: 13.262|tagging_loss_image: 4.723|tagging_loss_fusion: 3.774|total_loss: 35.426 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 4262 | tagging_loss_video: 5.223|tagging_loss_audio: 7.726|tagging_loss_text: 14.061|tagging_loss_image: 5.203|tagging_loss_fusion: 5.003|total_loss: 37.216 | 71.64 Examples/sec\n",
      "INFO:tensorflow:training step 4263 | tagging_loss_video: 6.444|tagging_loss_audio: 9.977|tagging_loss_text: 16.688|tagging_loss_image: 5.404|tagging_loss_fusion: 5.592|total_loss: 44.105 | 64.01 Examples/sec\n",
      "INFO:tensorflow:training step 4264 | tagging_loss_video: 6.037|tagging_loss_audio: 9.701|tagging_loss_text: 16.851|tagging_loss_image: 4.843|tagging_loss_fusion: 4.392|total_loss: 41.825 | 64.29 Examples/sec\n",
      "INFO:tensorflow:training step 4265 | tagging_loss_video: 6.672|tagging_loss_audio: 9.208|tagging_loss_text: 14.160|tagging_loss_image: 5.714|tagging_loss_fusion: 5.698|total_loss: 41.452 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 4266 | tagging_loss_video: 5.907|tagging_loss_audio: 9.072|tagging_loss_text: 19.308|tagging_loss_image: 5.123|tagging_loss_fusion: 5.013|total_loss: 44.422 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 4267 | tagging_loss_video: 5.468|tagging_loss_audio: 8.800|tagging_loss_text: 10.775|tagging_loss_image: 4.912|tagging_loss_fusion: 4.041|total_loss: 33.997 | 61.76 Examples/sec\n",
      "INFO:tensorflow:training step 4268 | tagging_loss_video: 5.204|tagging_loss_audio: 8.004|tagging_loss_text: 12.451|tagging_loss_image: 5.140|tagging_loss_fusion: 3.620|total_loss: 34.419 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 4269 | tagging_loss_video: 5.907|tagging_loss_audio: 8.464|tagging_loss_text: 17.361|tagging_loss_image: 5.753|tagging_loss_fusion: 5.838|total_loss: 43.322 | 68.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4270 |tagging_loss_video: 6.084|tagging_loss_audio: 7.628|tagging_loss_text: 13.540|tagging_loss_image: 6.370|tagging_loss_fusion: 7.805|total_loss: 41.427 | Examples/sec: 67.79\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.72 | precision@0.5: 0.89 |recall@0.1: 0.94 | recall@0.5: 0.82\n",
      "INFO:tensorflow:training step 4271 | tagging_loss_video: 6.028|tagging_loss_audio: 8.439|tagging_loss_text: 11.728|tagging_loss_image: 5.553|tagging_loss_fusion: 4.856|total_loss: 36.603 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 4272 | tagging_loss_video: 4.472|tagging_loss_audio: 8.339|tagging_loss_text: 15.801|tagging_loss_image: 5.630|tagging_loss_fusion: 3.806|total_loss: 38.047 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 4273 | tagging_loss_video: 4.074|tagging_loss_audio: 8.177|tagging_loss_text: 15.374|tagging_loss_image: 4.883|tagging_loss_fusion: 2.985|total_loss: 35.493 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 4274 | tagging_loss_video: 5.010|tagging_loss_audio: 9.705|tagging_loss_text: 16.671|tagging_loss_image: 5.716|tagging_loss_fusion: 3.724|total_loss: 40.827 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 4275 | tagging_loss_video: 6.421|tagging_loss_audio: 10.477|tagging_loss_text: 13.582|tagging_loss_image: 5.855|tagging_loss_fusion: 5.414|total_loss: 41.749 | 65.85 Examples/sec\n",
      "INFO:tensorflow:training step 4276 | tagging_loss_video: 6.682|tagging_loss_audio: 7.584|tagging_loss_text: 15.374|tagging_loss_image: 5.749|tagging_loss_fusion: 6.157|total_loss: 41.545 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 4277 | tagging_loss_video: 6.457|tagging_loss_audio: 10.285|tagging_loss_text: 13.333|tagging_loss_image: 6.917|tagging_loss_fusion: 5.478|total_loss: 42.469 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 4278 | tagging_loss_video: 6.096|tagging_loss_audio: 10.062|tagging_loss_text: 17.812|tagging_loss_image: 6.092|tagging_loss_fusion: 5.469|total_loss: 45.530 | 62.71 Examples/sec\n",
      "INFO:tensorflow:training step 4279 | tagging_loss_video: 5.714|tagging_loss_audio: 10.173|tagging_loss_text: 13.353|tagging_loss_image: 4.626|tagging_loss_fusion: 4.930|total_loss: 38.797 | 71.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4280 |tagging_loss_video: 5.377|tagging_loss_audio: 8.900|tagging_loss_text: 14.955|tagging_loss_image: 4.300|tagging_loss_fusion: 3.488|total_loss: 37.020 | Examples/sec: 68.31\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4281 | tagging_loss_video: 4.353|tagging_loss_audio: 8.132|tagging_loss_text: 14.268|tagging_loss_image: 5.577|tagging_loss_fusion: 3.327|total_loss: 35.657 | 67.88 Examples/sec\n",
      "INFO:tensorflow:training step 4282 | tagging_loss_video: 6.327|tagging_loss_audio: 10.601|tagging_loss_text: 18.571|tagging_loss_image: 5.158|tagging_loss_fusion: 3.859|total_loss: 44.517 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 4283 | tagging_loss_video: 6.444|tagging_loss_audio: 8.435|tagging_loss_text: 16.211|tagging_loss_image: 5.156|tagging_loss_fusion: 4.734|total_loss: 40.981 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 4284 | tagging_loss_video: 6.374|tagging_loss_audio: 9.779|tagging_loss_text: 16.717|tagging_loss_image: 5.255|tagging_loss_fusion: 6.398|total_loss: 44.524 | 63.88 Examples/sec\n",
      "INFO:tensorflow:training step 4285 | tagging_loss_video: 5.340|tagging_loss_audio: 7.515|tagging_loss_text: 14.801|tagging_loss_image: 4.834|tagging_loss_fusion: 3.663|total_loss: 36.153 | 67.74 Examples/sec\n",
      "INFO:tensorflow:training step 4286 | tagging_loss_video: 6.295|tagging_loss_audio: 10.753|tagging_loss_text: 14.933|tagging_loss_image: 6.343|tagging_loss_fusion: 5.389|total_loss: 43.714 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 4287 | tagging_loss_video: 6.723|tagging_loss_audio: 8.972|tagging_loss_text: 16.433|tagging_loss_image: 5.867|tagging_loss_fusion: 6.136|total_loss: 44.131 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 4288 | tagging_loss_video: 6.127|tagging_loss_audio: 8.913|tagging_loss_text: 18.793|tagging_loss_image: 5.998|tagging_loss_fusion: 5.115|total_loss: 44.946 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 4289 | tagging_loss_video: 5.846|tagging_loss_audio: 9.378|tagging_loss_text: 13.655|tagging_loss_image: 4.922|tagging_loss_fusion: 4.001|total_loss: 37.801 | 62.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4290 |tagging_loss_video: 5.624|tagging_loss_audio: 10.516|tagging_loss_text: 18.080|tagging_loss_image: 6.335|tagging_loss_fusion: 6.367|total_loss: 46.922 | Examples/sec: 69.95\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 4291 | tagging_loss_video: 5.051|tagging_loss_audio: 8.455|tagging_loss_text: 13.191|tagging_loss_image: 5.760|tagging_loss_fusion: 4.552|total_loss: 37.009 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 4292 | tagging_loss_video: 6.289|tagging_loss_audio: 8.472|tagging_loss_text: 13.755|tagging_loss_image: 5.301|tagging_loss_fusion: 4.830|total_loss: 38.648 | 59.40 Examples/sec\n",
      "INFO:tensorflow:training step 4293 | tagging_loss_video: 6.026|tagging_loss_audio: 10.089|tagging_loss_text: 13.380|tagging_loss_image: 6.411|tagging_loss_fusion: 4.076|total_loss: 39.982 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 4294 | tagging_loss_video: 6.119|tagging_loss_audio: 9.386|tagging_loss_text: 14.946|tagging_loss_image: 4.751|tagging_loss_fusion: 3.860|total_loss: 39.061 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 4295 | tagging_loss_video: 5.337|tagging_loss_audio: 10.570|tagging_loss_text: 18.011|tagging_loss_image: 5.130|tagging_loss_fusion: 3.558|total_loss: 42.608 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 4296 | tagging_loss_video: 5.395|tagging_loss_audio: 8.683|tagging_loss_text: 16.602|tagging_loss_image: 6.513|tagging_loss_fusion: 4.782|total_loss: 41.975 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 4297 | tagging_loss_video: 6.429|tagging_loss_audio: 10.512|tagging_loss_text: 17.382|tagging_loss_image: 6.228|tagging_loss_fusion: 5.449|total_loss: 46.000 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 4298 | tagging_loss_video: 6.256|tagging_loss_audio: 8.716|tagging_loss_text: 16.191|tagging_loss_image: 4.826|tagging_loss_fusion: 5.976|total_loss: 41.965 | 60.61 Examples/sec\n",
      "INFO:tensorflow:training step 4299 | tagging_loss_video: 6.957|tagging_loss_audio: 10.686|tagging_loss_text: 13.411|tagging_loss_image: 7.066|tagging_loss_fusion: 6.520|total_loss: 44.641 | 71.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4300 |tagging_loss_video: 5.555|tagging_loss_audio: 9.161|tagging_loss_text: 15.085|tagging_loss_image: 5.358|tagging_loss_fusion: 5.749|total_loss: 40.907 | Examples/sec: 66.36\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 4301 | tagging_loss_video: 6.913|tagging_loss_audio: 10.468|tagging_loss_text: 16.190|tagging_loss_image: 5.656|tagging_loss_fusion: 5.516|total_loss: 44.744 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 4302 | tagging_loss_video: 5.586|tagging_loss_audio: 9.355|tagging_loss_text: 13.623|tagging_loss_image: 5.081|tagging_loss_fusion: 3.801|total_loss: 37.447 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 4303 | tagging_loss_video: 6.376|tagging_loss_audio: 10.514|tagging_loss_text: 17.375|tagging_loss_image: 6.432|tagging_loss_fusion: 5.257|total_loss: 45.955 | 62.04 Examples/sec\n",
      "INFO:tensorflow:training step 4304 | tagging_loss_video: 6.337|tagging_loss_audio: 9.498|tagging_loss_text: 17.986|tagging_loss_image: 7.061|tagging_loss_fusion: 6.210|total_loss: 47.091 | 71.90 Examples/sec\n",
      "INFO:tensorflow:training step 4305 | tagging_loss_video: 6.114|tagging_loss_audio: 9.458|tagging_loss_text: 16.746|tagging_loss_image: 4.920|tagging_loss_fusion: 4.143|total_loss: 41.381 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 4306 | tagging_loss_video: 4.932|tagging_loss_audio: 9.342|tagging_loss_text: 12.549|tagging_loss_image: 6.289|tagging_loss_fusion: 4.843|total_loss: 37.953 | 63.99 Examples/sec\n",
      "INFO:tensorflow:training step 4307 | tagging_loss_video: 6.794|tagging_loss_audio: 11.714|tagging_loss_text: 18.278|tagging_loss_image: 7.074|tagging_loss_fusion: 7.452|total_loss: 51.311 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 4308 | tagging_loss_video: 6.281|tagging_loss_audio: 8.314|tagging_loss_text: 14.526|tagging_loss_image: 5.532|tagging_loss_fusion: 7.192|total_loss: 41.846 | 65.60 Examples/sec\n",
      "INFO:tensorflow:training step 4309 | tagging_loss_video: 4.947|tagging_loss_audio: 9.958|tagging_loss_text: 15.971|tagging_loss_image: 5.700|tagging_loss_fusion: 3.729|total_loss: 40.306 | 62.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4310 |tagging_loss_video: 6.391|tagging_loss_audio: 8.813|tagging_loss_text: 13.412|tagging_loss_image: 6.324|tagging_loss_fusion: 6.601|total_loss: 41.542 | Examples/sec: 71.09\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4311 | tagging_loss_video: 6.349|tagging_loss_audio: 8.654|tagging_loss_text: 13.031|tagging_loss_image: 6.242|tagging_loss_fusion: 5.552|total_loss: 39.828 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 4312 | tagging_loss_video: 6.130|tagging_loss_audio: 8.310|tagging_loss_text: 16.637|tagging_loss_image: 5.073|tagging_loss_fusion: 5.527|total_loss: 41.677 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 4313 | tagging_loss_video: 6.147|tagging_loss_audio: 11.532|tagging_loss_text: 16.326|tagging_loss_image: 6.270|tagging_loss_fusion: 5.605|total_loss: 45.880 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 4314 | tagging_loss_video: 6.796|tagging_loss_audio: 9.044|tagging_loss_text: 16.961|tagging_loss_image: 6.788|tagging_loss_fusion: 4.404|total_loss: 43.995 | 50.87 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 4314.\n",
      "INFO:tensorflow:training step 4315 | tagging_loss_video: 6.837|tagging_loss_audio: 8.640|tagging_loss_text: 13.627|tagging_loss_image: 5.665|tagging_loss_fusion: 6.953|total_loss: 41.722 | 61.97 Examples/sec\n",
      "INFO:tensorflow:training step 4316 | tagging_loss_video: 5.365|tagging_loss_audio: 8.827|tagging_loss_text: 12.849|tagging_loss_image: 4.946|tagging_loss_fusion: 4.752|total_loss: 36.739 | 58.82 Examples/sec\n",
      "INFO:tensorflow:training step 4317 | tagging_loss_video: 6.544|tagging_loss_audio: 10.311|tagging_loss_text: 16.886|tagging_loss_image: 6.740|tagging_loss_fusion: 6.901|total_loss: 47.382 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 4318 | tagging_loss_video: 5.828|tagging_loss_audio: 10.689|tagging_loss_text: 16.870|tagging_loss_image: 5.247|tagging_loss_fusion: 4.473|total_loss: 43.107 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 4319 | tagging_loss_video: 6.179|tagging_loss_audio: 9.383|tagging_loss_text: 15.180|tagging_loss_image: 5.996|tagging_loss_fusion: 5.090|total_loss: 41.828 | 63.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4320 |tagging_loss_video: 6.417|tagging_loss_audio: 8.951|tagging_loss_text: 13.950|tagging_loss_image: 5.649|tagging_loss_fusion: 4.530|total_loss: 39.497 | Examples/sec: 70.77\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4321 | tagging_loss_video: 4.621|tagging_loss_audio: 9.982|tagging_loss_text: 14.156|tagging_loss_image: 5.837|tagging_loss_fusion: 2.715|total_loss: 37.311 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 4322 | tagging_loss_video: 5.431|tagging_loss_audio: 9.078|tagging_loss_text: 14.521|tagging_loss_image: 5.977|tagging_loss_fusion: 4.361|total_loss: 39.369 | 66.63 Examples/sec\n",
      "INFO:tensorflow:training step 4323 | tagging_loss_video: 5.773|tagging_loss_audio: 9.508|tagging_loss_text: 14.561|tagging_loss_image: 5.457|tagging_loss_fusion: 4.098|total_loss: 39.396 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 4324 | tagging_loss_video: 5.138|tagging_loss_audio: 9.383|tagging_loss_text: 17.440|tagging_loss_image: 5.433|tagging_loss_fusion: 3.783|total_loss: 41.177 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 4325 | tagging_loss_video: 6.306|tagging_loss_audio: 9.439|tagging_loss_text: 16.047|tagging_loss_image: 4.783|tagging_loss_fusion: 3.942|total_loss: 40.517 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 4326 | tagging_loss_video: 4.205|tagging_loss_audio: 10.010|tagging_loss_text: 15.216|tagging_loss_image: 5.611|tagging_loss_fusion: 2.528|total_loss: 37.569 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 4327 | tagging_loss_video: 6.647|tagging_loss_audio: 10.179|tagging_loss_text: 12.309|tagging_loss_image: 5.999|tagging_loss_fusion: 5.165|total_loss: 40.299 | 61.86 Examples/sec\n",
      "INFO:tensorflow:training step 4328 | tagging_loss_video: 5.940|tagging_loss_audio: 9.549|tagging_loss_text: 11.346|tagging_loss_image: 5.634|tagging_loss_fusion: 5.747|total_loss: 38.217 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 4329 | tagging_loss_video: 4.275|tagging_loss_audio: 8.281|tagging_loss_text: 16.627|tagging_loss_image: 5.846|tagging_loss_fusion: 3.472|total_loss: 38.502 | 69.68 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4330 |tagging_loss_video: 5.296|tagging_loss_audio: 9.551|tagging_loss_text: 12.173|tagging_loss_image: 4.864|tagging_loss_fusion: 4.019|total_loss: 35.904 | Examples/sec: 65.44\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4331 | tagging_loss_video: 5.019|tagging_loss_audio: 10.415|tagging_loss_text: 15.994|tagging_loss_image: 5.753|tagging_loss_fusion: 4.250|total_loss: 41.431 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 4332 | tagging_loss_video: 6.509|tagging_loss_audio: 10.373|tagging_loss_text: 14.405|tagging_loss_image: 5.726|tagging_loss_fusion: 4.441|total_loss: 41.455 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 4333 | tagging_loss_video: 5.488|tagging_loss_audio: 10.178|tagging_loss_text: 16.116|tagging_loss_image: 6.056|tagging_loss_fusion: 3.655|total_loss: 41.492 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 4334 | tagging_loss_video: 5.923|tagging_loss_audio: 9.587|tagging_loss_text: 11.651|tagging_loss_image: 5.547|tagging_loss_fusion: 3.497|total_loss: 36.205 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4335 | tagging_loss_video: 5.608|tagging_loss_audio: 9.981|tagging_loss_text: 14.250|tagging_loss_image: 5.292|tagging_loss_fusion: 4.087|total_loss: 39.218 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 4336 | tagging_loss_video: 6.298|tagging_loss_audio: 9.118|tagging_loss_text: 17.358|tagging_loss_image: 5.549|tagging_loss_fusion: 5.433|total_loss: 43.757 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 4337 | tagging_loss_video: 7.273|tagging_loss_audio: 10.048|tagging_loss_text: 16.501|tagging_loss_image: 5.741|tagging_loss_fusion: 5.901|total_loss: 45.464 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 4338 | tagging_loss_video: 6.275|tagging_loss_audio: 8.885|tagging_loss_text: 12.542|tagging_loss_image: 5.285|tagging_loss_fusion: 6.440|total_loss: 39.427 | 60.32 Examples/sec\n",
      "INFO:tensorflow:training step 4339 | tagging_loss_video: 7.076|tagging_loss_audio: 9.903|tagging_loss_text: 12.995|tagging_loss_image: 5.772|tagging_loss_fusion: 5.687|total_loss: 41.432 | 66.53 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4340 |tagging_loss_video: 6.632|tagging_loss_audio: 9.627|tagging_loss_text: 16.075|tagging_loss_image: 6.207|tagging_loss_fusion: 6.053|total_loss: 44.593 | Examples/sec: 70.55\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4341 | tagging_loss_video: 4.613|tagging_loss_audio: 7.518|tagging_loss_text: 13.486|tagging_loss_image: 5.420|tagging_loss_fusion: 4.096|total_loss: 35.133 | 63.50 Examples/sec\n",
      "INFO:tensorflow:training step 4342 | tagging_loss_video: 5.976|tagging_loss_audio: 8.417|tagging_loss_text: 17.578|tagging_loss_image: 5.080|tagging_loss_fusion: 5.874|total_loss: 42.926 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 4343 | tagging_loss_video: 4.660|tagging_loss_audio: 9.338|tagging_loss_text: 17.279|tagging_loss_image: 5.796|tagging_loss_fusion: 2.467|total_loss: 39.540 | 66.70 Examples/sec\n",
      "INFO:tensorflow:training step 4344 | tagging_loss_video: 6.094|tagging_loss_audio: 9.227|tagging_loss_text: 16.889|tagging_loss_image: 4.672|tagging_loss_fusion: 5.170|total_loss: 42.052 | 71.46 Examples/sec\n",
      "INFO:tensorflow:training step 4345 | tagging_loss_video: 6.472|tagging_loss_audio: 10.117|tagging_loss_text: 11.848|tagging_loss_image: 6.065|tagging_loss_fusion: 4.594|total_loss: 39.096 | 70.56 Examples/sec\n",
      "INFO:tensorflow:training step 4346 | tagging_loss_video: 4.967|tagging_loss_audio: 9.471|tagging_loss_text: 15.341|tagging_loss_image: 3.909|tagging_loss_fusion: 3.513|total_loss: 37.202 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 4347 | tagging_loss_video: 5.797|tagging_loss_audio: 8.727|tagging_loss_text: 14.972|tagging_loss_image: 4.132|tagging_loss_fusion: 3.300|total_loss: 36.927 | 65.09 Examples/sec\n",
      "INFO:tensorflow:training step 4348 | tagging_loss_video: 6.516|tagging_loss_audio: 9.490|tagging_loss_text: 17.067|tagging_loss_image: 6.215|tagging_loss_fusion: 7.389|total_loss: 46.677 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 4349 | tagging_loss_video: 6.348|tagging_loss_audio: 9.279|tagging_loss_text: 15.448|tagging_loss_image: 5.809|tagging_loss_fusion: 5.284|total_loss: 42.169 | 64.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4350 |tagging_loss_video: 4.420|tagging_loss_audio: 9.596|tagging_loss_text: 14.588|tagging_loss_image: 5.711|tagging_loss_fusion: 3.677|total_loss: 37.993 | Examples/sec: 71.40\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4351 | tagging_loss_video: 4.670|tagging_loss_audio: 8.621|tagging_loss_text: 14.703|tagging_loss_image: 5.236|tagging_loss_fusion: 3.627|total_loss: 36.858 | 66.25 Examples/sec\n",
      "INFO:tensorflow:training step 4352 | tagging_loss_video: 6.142|tagging_loss_audio: 8.352|tagging_loss_text: 17.731|tagging_loss_image: 5.243|tagging_loss_fusion: 3.895|total_loss: 41.362 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 4353 | tagging_loss_video: 5.018|tagging_loss_audio: 9.755|tagging_loss_text: 15.401|tagging_loss_image: 5.186|tagging_loss_fusion: 3.291|total_loss: 38.651 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 4354 | tagging_loss_video: 5.224|tagging_loss_audio: 8.490|tagging_loss_text: 14.936|tagging_loss_image: 5.605|tagging_loss_fusion: 4.157|total_loss: 38.413 | 71.75 Examples/sec\n",
      "INFO:tensorflow:training step 4355 | tagging_loss_video: 5.328|tagging_loss_audio: 8.451|tagging_loss_text: 15.608|tagging_loss_image: 6.207|tagging_loss_fusion: 5.059|total_loss: 40.653 | 58.76 Examples/sec\n",
      "INFO:tensorflow:training step 4356 | tagging_loss_video: 6.073|tagging_loss_audio: 9.273|tagging_loss_text: 18.043|tagging_loss_image: 5.745|tagging_loss_fusion: 3.855|total_loss: 42.990 | 67.63 Examples/sec\n",
      "INFO:tensorflow:training step 4357 | tagging_loss_video: 5.248|tagging_loss_audio: 9.305|tagging_loss_text: 15.147|tagging_loss_image: 6.133|tagging_loss_fusion: 4.278|total_loss: 40.111 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 4358 | tagging_loss_video: 6.262|tagging_loss_audio: 8.353|tagging_loss_text: 15.340|tagging_loss_image: 6.365|tagging_loss_fusion: 6.271|total_loss: 42.591 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 4359 | tagging_loss_video: 6.592|tagging_loss_audio: 9.715|tagging_loss_text: 15.533|tagging_loss_image: 5.577|tagging_loss_fusion: 6.165|total_loss: 43.581 | 70.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4360 |tagging_loss_video: 5.509|tagging_loss_audio: 9.014|tagging_loss_text: 13.279|tagging_loss_image: 4.784|tagging_loss_fusion: 4.600|total_loss: 37.186 | Examples/sec: 71.14\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4361 | tagging_loss_video: 5.893|tagging_loss_audio: 8.508|tagging_loss_text: 17.552|tagging_loss_image: 6.588|tagging_loss_fusion: 6.414|total_loss: 44.956 | 72.14 Examples/sec\n",
      "INFO:tensorflow:training step 4362 | tagging_loss_video: 6.220|tagging_loss_audio: 10.061|tagging_loss_text: 17.626|tagging_loss_image: 5.893|tagging_loss_fusion: 5.868|total_loss: 45.668 | 71.21 Examples/sec\n",
      "INFO:tensorflow:training step 4363 | tagging_loss_video: 5.925|tagging_loss_audio: 9.245|tagging_loss_text: 17.500|tagging_loss_image: 5.852|tagging_loss_fusion: 5.705|total_loss: 44.227 | 60.74 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4364 | tagging_loss_video: 5.993|tagging_loss_audio: 8.573|tagging_loss_text: 16.942|tagging_loss_image: 6.316|tagging_loss_fusion: 4.795|total_loss: 42.620 | 66.41 Examples/sec\n",
      "INFO:tensorflow:training step 4365 | tagging_loss_video: 6.973|tagging_loss_audio: 9.427|tagging_loss_text: 14.636|tagging_loss_image: 7.224|tagging_loss_fusion: 7.162|total_loss: 45.422 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 4366 | tagging_loss_video: 6.785|tagging_loss_audio: 9.272|tagging_loss_text: 16.581|tagging_loss_image: 5.317|tagging_loss_fusion: 4.810|total_loss: 42.765 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 4367 | tagging_loss_video: 6.798|tagging_loss_audio: 8.174|tagging_loss_text: 11.130|tagging_loss_image: 6.803|tagging_loss_fusion: 6.754|total_loss: 39.659 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 4368 | tagging_loss_video: 5.709|tagging_loss_audio: 9.057|tagging_loss_text: 16.071|tagging_loss_image: 5.883|tagging_loss_fusion: 5.168|total_loss: 41.888 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 4369 | tagging_loss_video: 6.446|tagging_loss_audio: 9.353|tagging_loss_text: 15.350|tagging_loss_image: 6.617|tagging_loss_fusion: 5.223|total_loss: 42.989 | 61.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4370 |tagging_loss_video: 6.033|tagging_loss_audio: 9.091|tagging_loss_text: 13.468|tagging_loss_image: 4.284|tagging_loss_fusion: 3.522|total_loss: 36.398 | Examples/sec: 68.52\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4371 | tagging_loss_video: 6.290|tagging_loss_audio: 9.577|tagging_loss_text: 15.096|tagging_loss_image: 6.082|tagging_loss_fusion: 5.345|total_loss: 42.389 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 4372 | tagging_loss_video: 6.290|tagging_loss_audio: 7.895|tagging_loss_text: 15.438|tagging_loss_image: 5.152|tagging_loss_fusion: 5.817|total_loss: 40.591 | 65.35 Examples/sec\n",
      "INFO:tensorflow:training step 4373 | tagging_loss_video: 5.976|tagging_loss_audio: 9.034|tagging_loss_text: 17.462|tagging_loss_image: 6.068|tagging_loss_fusion: 5.799|total_loss: 44.339 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 4374 | tagging_loss_video: 4.565|tagging_loss_audio: 8.721|tagging_loss_text: 16.252|tagging_loss_image: 5.094|tagging_loss_fusion: 2.905|total_loss: 37.536 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 4375 | tagging_loss_video: 6.468|tagging_loss_audio: 9.825|tagging_loss_text: 17.032|tagging_loss_image: 5.416|tagging_loss_fusion: 3.916|total_loss: 42.657 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 4376 | tagging_loss_video: 6.188|tagging_loss_audio: 8.817|tagging_loss_text: 16.319|tagging_loss_image: 6.591|tagging_loss_fusion: 3.890|total_loss: 41.806 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 4377 | tagging_loss_video: 6.715|tagging_loss_audio: 9.944|tagging_loss_text: 16.674|tagging_loss_image: 5.689|tagging_loss_fusion: 5.124|total_loss: 44.146 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 4378 | tagging_loss_video: 5.600|tagging_loss_audio: 9.017|tagging_loss_text: 14.764|tagging_loss_image: 5.941|tagging_loss_fusion: 4.063|total_loss: 39.384 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 4379 | tagging_loss_video: 6.539|tagging_loss_audio: 9.183|tagging_loss_text: 17.492|tagging_loss_image: 6.679|tagging_loss_fusion: 5.432|total_loss: 45.324 | 71.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4380 |tagging_loss_video: 5.991|tagging_loss_audio: 8.150|tagging_loss_text: 14.433|tagging_loss_image: 4.110|tagging_loss_fusion: 4.671|total_loss: 37.356 | Examples/sec: 63.39\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.78 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4381 | tagging_loss_video: 5.579|tagging_loss_audio: 9.268|tagging_loss_text: 15.675|tagging_loss_image: 5.070|tagging_loss_fusion: 5.115|total_loss: 40.707 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 4382 | tagging_loss_video: 6.237|tagging_loss_audio: 9.176|tagging_loss_text: 15.453|tagging_loss_image: 5.523|tagging_loss_fusion: 5.215|total_loss: 41.605 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 4383 | tagging_loss_video: 5.228|tagging_loss_audio: 8.301|tagging_loss_text: 15.197|tagging_loss_image: 5.125|tagging_loss_fusion: 3.604|total_loss: 37.455 | 62.69 Examples/sec\n",
      "INFO:tensorflow:training step 4384 | tagging_loss_video: 6.331|tagging_loss_audio: 9.042|tagging_loss_text: 16.821|tagging_loss_image: 5.268|tagging_loss_fusion: 4.551|total_loss: 42.013 | 66.82 Examples/sec\n",
      "INFO:tensorflow:training step 4385 | tagging_loss_video: 6.137|tagging_loss_audio: 9.250|tagging_loss_text: 15.807|tagging_loss_image: 5.541|tagging_loss_fusion: 4.648|total_loss: 41.383 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 4386 | tagging_loss_video: 5.737|tagging_loss_audio: 8.704|tagging_loss_text: 16.622|tagging_loss_image: 5.030|tagging_loss_fusion: 4.229|total_loss: 40.322 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 4387 | tagging_loss_video: 4.260|tagging_loss_audio: 7.969|tagging_loss_text: 16.527|tagging_loss_image: 5.684|tagging_loss_fusion: 3.510|total_loss: 37.950 | 66.71 Examples/sec\n",
      "INFO:tensorflow:training step 4388 | tagging_loss_video: 5.599|tagging_loss_audio: 9.271|tagging_loss_text: 15.462|tagging_loss_image: 6.436|tagging_loss_fusion: 4.606|total_loss: 41.374 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 4389 | tagging_loss_video: 5.821|tagging_loss_audio: 9.167|tagging_loss_text: 12.321|tagging_loss_image: 5.431|tagging_loss_fusion: 4.874|total_loss: 37.614 | 69.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4390 |tagging_loss_video: 5.513|tagging_loss_audio: 8.647|tagging_loss_text: 13.011|tagging_loss_image: 4.707|tagging_loss_fusion: 4.908|total_loss: 36.786 | Examples/sec: 70.18\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4391 | tagging_loss_video: 4.861|tagging_loss_audio: 7.344|tagging_loss_text: 14.873|tagging_loss_image: 4.513|tagging_loss_fusion: 3.008|total_loss: 34.599 | 62.67 Examples/sec\n",
      "INFO:tensorflow:training step 4392 | tagging_loss_video: 6.858|tagging_loss_audio: 10.646|tagging_loss_text: 16.589|tagging_loss_image: 4.778|tagging_loss_fusion: 5.197|total_loss: 44.068 | 66.68 Examples/sec\n",
      "INFO:tensorflow:training step 4393 | tagging_loss_video: 6.490|tagging_loss_audio: 9.245|tagging_loss_text: 18.889|tagging_loss_image: 6.556|tagging_loss_fusion: 6.646|total_loss: 47.825 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 4394 | tagging_loss_video: 6.127|tagging_loss_audio: 8.710|tagging_loss_text: 14.119|tagging_loss_image: 4.595|tagging_loss_fusion: 4.959|total_loss: 38.509 | 60.31 Examples/sec\n",
      "INFO:tensorflow:training step 4395 | tagging_loss_video: 6.963|tagging_loss_audio: 8.992|tagging_loss_text: 18.110|tagging_loss_image: 4.839|tagging_loss_fusion: 5.388|total_loss: 44.291 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 4396 | tagging_loss_video: 5.345|tagging_loss_audio: 9.131|tagging_loss_text: 13.091|tagging_loss_image: 5.383|tagging_loss_fusion: 3.480|total_loss: 36.431 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 4397 | tagging_loss_video: 4.990|tagging_loss_audio: 8.364|tagging_loss_text: 14.338|tagging_loss_image: 4.741|tagging_loss_fusion: 2.999|total_loss: 35.431 | 63.93 Examples/sec\n",
      "INFO:tensorflow:training step 4398 | tagging_loss_video: 4.753|tagging_loss_audio: 7.813|tagging_loss_text: 11.734|tagging_loss_image: 4.241|tagging_loss_fusion: 3.294|total_loss: 31.834 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 4399 | tagging_loss_video: 5.654|tagging_loss_audio: 8.670|tagging_loss_text: 15.165|tagging_loss_image: 5.200|tagging_loss_fusion: 5.125|total_loss: 39.814 | 70.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4400 |tagging_loss_video: 6.014|tagging_loss_audio: 9.125|tagging_loss_text: 14.596|tagging_loss_image: 5.831|tagging_loss_fusion: 4.669|total_loss: 40.235 | Examples/sec: 70.99\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4401 | tagging_loss_video: 5.057|tagging_loss_audio: 8.131|tagging_loss_text: 13.398|tagging_loss_image: 4.536|tagging_loss_fusion: 3.376|total_loss: 34.498 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 4402 | tagging_loss_video: 6.124|tagging_loss_audio: 9.377|tagging_loss_text: 16.685|tagging_loss_image: 5.396|tagging_loss_fusion: 7.469|total_loss: 45.051 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 4403 | tagging_loss_video: 5.796|tagging_loss_audio: 9.971|tagging_loss_text: 15.446|tagging_loss_image: 5.121|tagging_loss_fusion: 5.446|total_loss: 41.781 | 62.47 Examples/sec\n",
      "INFO:tensorflow:training step 4404 | tagging_loss_video: 6.274|tagging_loss_audio: 9.126|tagging_loss_text: 16.181|tagging_loss_image: 5.853|tagging_loss_fusion: 6.264|total_loss: 43.698 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 4405 | tagging_loss_video: 7.271|tagging_loss_audio: 10.195|tagging_loss_text: 14.988|tagging_loss_image: 6.434|tagging_loss_fusion: 7.322|total_loss: 46.211 | 67.32 Examples/sec\n",
      "INFO:tensorflow:training step 4406 | tagging_loss_video: 5.280|tagging_loss_audio: 8.958|tagging_loss_text: 16.482|tagging_loss_image: 5.249|tagging_loss_fusion: 4.328|total_loss: 40.297 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 4407 | tagging_loss_video: 4.272|tagging_loss_audio: 8.224|tagging_loss_text: 14.832|tagging_loss_image: 4.188|tagging_loss_fusion: 3.720|total_loss: 35.236 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 4408 | tagging_loss_video: 5.954|tagging_loss_audio: 8.889|tagging_loss_text: 14.026|tagging_loss_image: 5.319|tagging_loss_fusion: 5.983|total_loss: 40.170 | 71.10 Examples/sec\n",
      "INFO:tensorflow:training step 4409 | tagging_loss_video: 4.206|tagging_loss_audio: 8.076|tagging_loss_text: 15.456|tagging_loss_image: 5.239|tagging_loss_fusion: 3.957|total_loss: 36.935 | 62.11 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4410 |tagging_loss_video: 5.603|tagging_loss_audio: 9.741|tagging_loss_text: 16.560|tagging_loss_image: 5.181|tagging_loss_fusion: 4.159|total_loss: 41.244 | Examples/sec: 69.24\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4411 | tagging_loss_video: 5.539|tagging_loss_audio: 8.475|tagging_loss_text: 13.178|tagging_loss_image: 5.339|tagging_loss_fusion: 5.962|total_loss: 38.493 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 4412 | tagging_loss_video: 5.311|tagging_loss_audio: 8.292|tagging_loss_text: 16.668|tagging_loss_image: 5.516|tagging_loss_fusion: 3.371|total_loss: 39.157 | 66.14 Examples/sec\n",
      "INFO:tensorflow:training step 4413 | tagging_loss_video: 6.221|tagging_loss_audio: 9.064|tagging_loss_text: 14.559|tagging_loss_image: 4.952|tagging_loss_fusion: 4.991|total_loss: 39.786 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 4414 | tagging_loss_video: 6.103|tagging_loss_audio: 8.193|tagging_loss_text: 16.959|tagging_loss_image: 5.633|tagging_loss_fusion: 5.119|total_loss: 42.008 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 4415 | tagging_loss_video: 6.021|tagging_loss_audio: 8.429|tagging_loss_text: 14.680|tagging_loss_image: 5.438|tagging_loss_fusion: 4.118|total_loss: 38.685 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 4416 | tagging_loss_video: 5.702|tagging_loss_audio: 9.587|tagging_loss_text: 14.818|tagging_loss_image: 6.877|tagging_loss_fusion: 6.325|total_loss: 43.309 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 4417 | tagging_loss_video: 5.592|tagging_loss_audio: 8.625|tagging_loss_text: 17.154|tagging_loss_image: 5.433|tagging_loss_fusion: 5.119|total_loss: 41.925 | 62.54 Examples/sec\n",
      "INFO:tensorflow:training step 4418 | tagging_loss_video: 6.032|tagging_loss_audio: 7.990|tagging_loss_text: 19.059|tagging_loss_image: 4.151|tagging_loss_fusion: 6.688|total_loss: 43.920 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 4419 | tagging_loss_video: 5.684|tagging_loss_audio: 8.043|tagging_loss_text: 14.953|tagging_loss_image: 5.327|tagging_loss_fusion: 5.384|total_loss: 39.390 | 71.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4420 |tagging_loss_video: 5.113|tagging_loss_audio: 8.528|tagging_loss_text: 12.970|tagging_loss_image: 4.918|tagging_loss_fusion: 4.081|total_loss: 35.610 | Examples/sec: 62.31\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4421 | tagging_loss_video: 4.576|tagging_loss_audio: 10.476|tagging_loss_text: 17.151|tagging_loss_image: 6.150|tagging_loss_fusion: 3.782|total_loss: 42.136 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 4422 | tagging_loss_video: 5.609|tagging_loss_audio: 7.746|tagging_loss_text: 16.229|tagging_loss_image: 4.882|tagging_loss_fusion: 4.105|total_loss: 38.571 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 4423 | tagging_loss_video: 4.375|tagging_loss_audio: 8.935|tagging_loss_text: 16.751|tagging_loss_image: 6.323|tagging_loss_fusion: 3.783|total_loss: 40.168 | 65.51 Examples/sec\n",
      "INFO:tensorflow:training step 4424 | tagging_loss_video: 5.208|tagging_loss_audio: 8.017|tagging_loss_text: 13.503|tagging_loss_image: 5.977|tagging_loss_fusion: 4.875|total_loss: 37.580 | 72.95 Examples/sec\n",
      "INFO:tensorflow:training step 4425 | tagging_loss_video: 4.433|tagging_loss_audio: 9.319|tagging_loss_text: 18.446|tagging_loss_image: 6.148|tagging_loss_fusion: 3.607|total_loss: 41.952 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 4426 | tagging_loss_video: 6.567|tagging_loss_audio: 8.982|tagging_loss_text: 15.735|tagging_loss_image: 4.801|tagging_loss_fusion: 5.260|total_loss: 41.345 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 4427 | tagging_loss_video: 5.905|tagging_loss_audio: 8.467|tagging_loss_text: 13.248|tagging_loss_image: 5.914|tagging_loss_fusion: 5.434|total_loss: 38.969 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 4428 | tagging_loss_video: 5.788|tagging_loss_audio: 8.790|tagging_loss_text: 17.702|tagging_loss_image: 5.118|tagging_loss_fusion: 6.048|total_loss: 43.446 | 66.41 Examples/sec\n",
      "INFO:tensorflow:training step 4429 | tagging_loss_video: 6.714|tagging_loss_audio: 10.068|tagging_loss_text: 15.175|tagging_loss_image: 5.712|tagging_loss_fusion: 6.852|total_loss: 44.520 | 70.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4430 |tagging_loss_video: 5.642|tagging_loss_audio: 8.735|tagging_loss_text: 13.841|tagging_loss_image: 5.585|tagging_loss_fusion: 5.022|total_loss: 38.825 | Examples/sec: 69.39\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4431 | tagging_loss_video: 5.822|tagging_loss_audio: 8.578|tagging_loss_text: 16.511|tagging_loss_image: 6.243|tagging_loss_fusion: 4.459|total_loss: 41.613 | 62.61 Examples/sec\n",
      "INFO:tensorflow:training step 4432 | tagging_loss_video: 5.609|tagging_loss_audio: 9.490|tagging_loss_text: 14.109|tagging_loss_image: 5.556|tagging_loss_fusion: 3.063|total_loss: 37.827 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 4433 | tagging_loss_video: 4.647|tagging_loss_audio: 7.986|tagging_loss_text: 15.594|tagging_loss_image: 5.649|tagging_loss_fusion: 4.071|total_loss: 37.947 | 68.44 Examples/sec\n",
      "INFO:tensorflow:training step 4434 | tagging_loss_video: 5.576|tagging_loss_audio: 9.884|tagging_loss_text: 15.242|tagging_loss_image: 5.726|tagging_loss_fusion: 3.645|total_loss: 40.073 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 4435 | tagging_loss_video: 5.111|tagging_loss_audio: 8.841|tagging_loss_text: 15.251|tagging_loss_image: 5.340|tagging_loss_fusion: 4.266|total_loss: 38.809 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 4436 | tagging_loss_video: 5.453|tagging_loss_audio: 10.075|tagging_loss_text: 15.937|tagging_loss_image: 5.873|tagging_loss_fusion: 3.707|total_loss: 41.045 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 4437 | tagging_loss_video: 5.653|tagging_loss_audio: 8.481|tagging_loss_text: 15.477|tagging_loss_image: 5.387|tagging_loss_fusion: 4.816|total_loss: 39.813 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 4438 | tagging_loss_video: 6.200|tagging_loss_audio: 10.533|tagging_loss_text: 17.124|tagging_loss_image: 6.667|tagging_loss_fusion: 5.682|total_loss: 46.206 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 4439 | tagging_loss_video: 6.089|tagging_loss_audio: 9.439|tagging_loss_text: 14.430|tagging_loss_image: 5.339|tagging_loss_fusion: 7.635|total_loss: 42.931 | 66.53 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4440 |tagging_loss_video: 7.165|tagging_loss_audio: 9.935|tagging_loss_text: 17.603|tagging_loss_image: 5.488|tagging_loss_fusion: 5.618|total_loss: 45.810 | Examples/sec: 68.76\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4441 | tagging_loss_video: 6.367|tagging_loss_audio: 9.910|tagging_loss_text: 14.189|tagging_loss_image: 5.209|tagging_loss_fusion: 5.490|total_loss: 41.165 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 4442 | tagging_loss_video: 6.535|tagging_loss_audio: 10.798|tagging_loss_text: 18.059|tagging_loss_image: 6.063|tagging_loss_fusion: 6.071|total_loss: 47.525 | 60.51 Examples/sec\n",
      "INFO:tensorflow:training step 4443 | tagging_loss_video: 5.869|tagging_loss_audio: 10.109|tagging_loss_text: 18.761|tagging_loss_image: 6.406|tagging_loss_fusion: 4.564|total_loss: 45.708 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 4444 | tagging_loss_video: 6.794|tagging_loss_audio: 9.242|tagging_loss_text: 13.845|tagging_loss_image: 5.384|tagging_loss_fusion: 7.175|total_loss: 42.440 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 4445 | tagging_loss_video: 6.326|tagging_loss_audio: 9.059|tagging_loss_text: 13.232|tagging_loss_image: 5.888|tagging_loss_fusion: 6.506|total_loss: 41.010 | 59.35 Examples/sec\n",
      "INFO:tensorflow:training step 4446 | tagging_loss_video: 6.424|tagging_loss_audio: 11.015|tagging_loss_text: 19.087|tagging_loss_image: 6.449|tagging_loss_fusion: 5.294|total_loss: 48.269 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 4447 | tagging_loss_video: 6.244|tagging_loss_audio: 10.163|tagging_loss_text: 15.240|tagging_loss_image: 6.077|tagging_loss_fusion: 5.594|total_loss: 43.319 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 4448 | tagging_loss_video: 5.443|tagging_loss_audio: 9.240|tagging_loss_text: 11.970|tagging_loss_image: 5.891|tagging_loss_fusion: 4.759|total_loss: 37.303 | 66.52 Examples/sec\n",
      "INFO:tensorflow:training step 4449 | tagging_loss_video: 5.119|tagging_loss_audio: 9.307|tagging_loss_text: 14.741|tagging_loss_image: 5.584|tagging_loss_fusion: 4.064|total_loss: 38.816 | 68.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4450 |tagging_loss_video: 6.125|tagging_loss_audio: 8.448|tagging_loss_text: 15.778|tagging_loss_image: 5.960|tagging_loss_fusion: 5.174|total_loss: 41.484 | Examples/sec: 65.34\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4451 | tagging_loss_video: 5.971|tagging_loss_audio: 10.773|tagging_loss_text: 14.639|tagging_loss_image: 5.951|tagging_loss_fusion: 4.763|total_loss: 42.097 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 4452 | tagging_loss_video: 6.809|tagging_loss_audio: 11.460|tagging_loss_text: 16.329|tagging_loss_image: 7.159|tagging_loss_fusion: 6.476|total_loss: 48.232 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 4453 | tagging_loss_video: 6.489|tagging_loss_audio: 9.727|tagging_loss_text: 14.034|tagging_loss_image: 6.725|tagging_loss_fusion: 5.181|total_loss: 42.156 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 4454 | tagging_loss_video: 6.473|tagging_loss_audio: 9.124|tagging_loss_text: 13.771|tagging_loss_image: 5.841|tagging_loss_fusion: 5.882|total_loss: 41.090 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 4455 | tagging_loss_video: 6.724|tagging_loss_audio: 8.895|tagging_loss_text: 14.309|tagging_loss_image: 5.381|tagging_loss_fusion: 6.006|total_loss: 41.314 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 4456 | tagging_loss_video: 5.693|tagging_loss_audio: 8.342|tagging_loss_text: 12.223|tagging_loss_image: 5.285|tagging_loss_fusion: 5.495|total_loss: 37.038 | 62.87 Examples/sec\n",
      "INFO:tensorflow:training step 4457 | tagging_loss_video: 5.914|tagging_loss_audio: 9.968|tagging_loss_text: 14.678|tagging_loss_image: 6.069|tagging_loss_fusion: 4.375|total_loss: 41.004 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 4458 | tagging_loss_video: 6.094|tagging_loss_audio: 10.141|tagging_loss_text: 16.238|tagging_loss_image: 6.172|tagging_loss_fusion: 4.200|total_loss: 42.845 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 4459 | tagging_loss_video: 5.822|tagging_loss_audio: 8.093|tagging_loss_text: 15.844|tagging_loss_image: 5.753|tagging_loss_fusion: 4.149|total_loss: 39.661 | 63.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4460 |tagging_loss_video: 3.810|tagging_loss_audio: 9.004|tagging_loss_text: 13.944|tagging_loss_image: 5.521|tagging_loss_fusion: 2.250|total_loss: 34.529 | Examples/sec: 70.02\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.89 | precision@0.5: 0.99 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 4461 | tagging_loss_video: 5.918|tagging_loss_audio: 8.892|tagging_loss_text: 14.158|tagging_loss_image: 5.601|tagging_loss_fusion: 3.478|total_loss: 38.047 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 4462 | tagging_loss_video: 4.317|tagging_loss_audio: 9.364|tagging_loss_text: 13.861|tagging_loss_image: 5.447|tagging_loss_fusion: 2.932|total_loss: 35.920 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 4463 | tagging_loss_video: 6.283|tagging_loss_audio: 9.919|tagging_loss_text: 15.836|tagging_loss_image: 5.073|tagging_loss_fusion: 4.057|total_loss: 41.168 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 4464 | tagging_loss_video: 5.419|tagging_loss_audio: 8.001|tagging_loss_text: 12.564|tagging_loss_image: 5.422|tagging_loss_fusion: 3.558|total_loss: 34.964 | 64.75 Examples/sec\n",
      "INFO:tensorflow:training step 4465 | tagging_loss_video: 6.299|tagging_loss_audio: 9.730|tagging_loss_text: 12.713|tagging_loss_image: 4.434|tagging_loss_fusion: 4.061|total_loss: 37.238 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 4466 | tagging_loss_video: 6.388|tagging_loss_audio: 10.370|tagging_loss_text: 14.531|tagging_loss_image: 6.330|tagging_loss_fusion: 6.095|total_loss: 43.715 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 4467 | tagging_loss_video: 5.938|tagging_loss_audio: 8.946|tagging_loss_text: 12.298|tagging_loss_image: 5.908|tagging_loss_fusion: 6.473|total_loss: 39.563 | 64.44 Examples/sec\n",
      "INFO:tensorflow:training step 4468 | tagging_loss_video: 5.897|tagging_loss_audio: 8.723|tagging_loss_text: 13.349|tagging_loss_image: 5.919|tagging_loss_fusion: 5.829|total_loss: 39.718 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 4469 | tagging_loss_video: 6.117|tagging_loss_audio: 8.599|tagging_loss_text: 15.941|tagging_loss_image: 6.058|tagging_loss_fusion: 5.842|total_loss: 42.558 | 71.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4470 |tagging_loss_video: 5.265|tagging_loss_audio: 10.648|tagging_loss_text: 14.863|tagging_loss_image: 6.470|tagging_loss_fusion: 4.601|total_loss: 41.847 | Examples/sec: 62.65\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4471 | tagging_loss_video: 5.820|tagging_loss_audio: 10.625|tagging_loss_text: 11.839|tagging_loss_image: 6.325|tagging_loss_fusion: 4.998|total_loss: 39.608 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4472 | tagging_loss_video: 6.348|tagging_loss_audio: 10.558|tagging_loss_text: 16.699|tagging_loss_image: 6.626|tagging_loss_fusion: 7.001|total_loss: 47.232 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 4473 | tagging_loss_video: 6.019|tagging_loss_audio: 10.188|tagging_loss_text: 16.832|tagging_loss_image: 6.189|tagging_loss_fusion: 5.560|total_loss: 44.789 | 72.11 Examples/sec\n",
      "INFO:tensorflow:training step 4474 | tagging_loss_video: 5.887|tagging_loss_audio: 9.505|tagging_loss_text: 14.368|tagging_loss_image: 5.136|tagging_loss_fusion: 4.325|total_loss: 39.222 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 4475 | tagging_loss_video: 6.891|tagging_loss_audio: 10.709|tagging_loss_text: 14.954|tagging_loss_image: 4.805|tagging_loss_fusion: 5.792|total_loss: 43.152 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 4476 | tagging_loss_video: 6.275|tagging_loss_audio: 9.476|tagging_loss_text: 14.018|tagging_loss_image: 6.085|tagging_loss_fusion: 5.201|total_loss: 41.055 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 4477 | tagging_loss_video: 7.588|tagging_loss_audio: 10.399|tagging_loss_text: 16.514|tagging_loss_image: 6.829|tagging_loss_fusion: 9.722|total_loss: 51.051 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 4478 | tagging_loss_video: 5.443|tagging_loss_audio: 8.335|tagging_loss_text: 14.513|tagging_loss_image: 5.562|tagging_loss_fusion: 4.808|total_loss: 38.661 | 64.16 Examples/sec\n",
      "INFO:tensorflow:training step 4479 | tagging_loss_video: 6.407|tagging_loss_audio: 11.189|tagging_loss_text: 14.383|tagging_loss_image: 5.213|tagging_loss_fusion: 6.957|total_loss: 44.150 | 68.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4480 |tagging_loss_video: 7.028|tagging_loss_audio: 9.714|tagging_loss_text: 15.347|tagging_loss_image: 6.648|tagging_loss_fusion: 8.500|total_loss: 47.237 | Examples/sec: 70.58\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.94 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 4481 | tagging_loss_video: 5.324|tagging_loss_audio: 7.366|tagging_loss_text: 11.672|tagging_loss_image: 4.667|tagging_loss_fusion: 3.402|total_loss: 32.432 | 65.42 Examples/sec\n",
      "INFO:tensorflow:training step 4482 | tagging_loss_video: 5.805|tagging_loss_audio: 9.286|tagging_loss_text: 17.592|tagging_loss_image: 5.780|tagging_loss_fusion: 4.780|total_loss: 43.243 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 4483 | tagging_loss_video: 6.214|tagging_loss_audio: 9.206|tagging_loss_text: 14.516|tagging_loss_image: 5.164|tagging_loss_fusion: 4.935|total_loss: 40.034 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 4484 | tagging_loss_video: 5.715|tagging_loss_audio: 8.531|tagging_loss_text: 15.544|tagging_loss_image: 5.293|tagging_loss_fusion: 6.019|total_loss: 41.103 | 63.14 Examples/sec\n",
      "INFO:tensorflow:training step 4485 | tagging_loss_video: 4.889|tagging_loss_audio: 10.096|tagging_loss_text: 19.206|tagging_loss_image: 5.356|tagging_loss_fusion: 2.945|total_loss: 42.492 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 4486 | tagging_loss_video: 5.184|tagging_loss_audio: 8.531|tagging_loss_text: 13.364|tagging_loss_image: 5.290|tagging_loss_fusion: 4.399|total_loss: 36.768 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 4487 | tagging_loss_video: 4.048|tagging_loss_audio: 8.796|tagging_loss_text: 15.635|tagging_loss_image: 5.751|tagging_loss_fusion: 2.135|total_loss: 36.365 | 61.43 Examples/sec\n",
      "INFO:tensorflow:training step 4488 | tagging_loss_video: 6.640|tagging_loss_audio: 9.164|tagging_loss_text: 15.648|tagging_loss_image: 6.124|tagging_loss_fusion: 4.422|total_loss: 41.999 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 4489 | tagging_loss_video: 5.871|tagging_loss_audio: 10.369|tagging_loss_text: 14.797|tagging_loss_image: 4.854|tagging_loss_fusion: 4.294|total_loss: 40.186 | 70.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4490 |tagging_loss_video: 6.062|tagging_loss_audio: 8.575|tagging_loss_text: 14.562|tagging_loss_image: 5.878|tagging_loss_fusion: 5.164|total_loss: 40.241 | Examples/sec: 72.19\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4491 | tagging_loss_video: 5.708|tagging_loss_audio: 8.592|tagging_loss_text: 12.024|tagging_loss_image: 5.708|tagging_loss_fusion: 4.004|total_loss: 36.036 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 4492 | tagging_loss_video: 5.805|tagging_loss_audio: 8.873|tagging_loss_text: 14.334|tagging_loss_image: 4.424|tagging_loss_fusion: 4.363|total_loss: 37.799 | 64.22 Examples/sec\n",
      "INFO:tensorflow:training step 4493 | tagging_loss_video: 5.094|tagging_loss_audio: 8.674|tagging_loss_text: 14.072|tagging_loss_image: 5.929|tagging_loss_fusion: 2.558|total_loss: 36.327 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 4494 | tagging_loss_video: 6.197|tagging_loss_audio: 9.339|tagging_loss_text: 14.956|tagging_loss_image: 5.762|tagging_loss_fusion: 5.514|total_loss: 41.768 | 67.53 Examples/sec\n",
      "INFO:tensorflow:training step 4495 | tagging_loss_video: 4.861|tagging_loss_audio: 7.909|tagging_loss_text: 16.259|tagging_loss_image: 4.876|tagging_loss_fusion: 3.121|total_loss: 37.027 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 4496 | tagging_loss_video: 5.703|tagging_loss_audio: 10.404|tagging_loss_text: 15.256|tagging_loss_image: 5.702|tagging_loss_fusion: 4.501|total_loss: 41.566 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 4497 | tagging_loss_video: 6.099|tagging_loss_audio: 8.297|tagging_loss_text: 18.588|tagging_loss_image: 4.978|tagging_loss_fusion: 5.501|total_loss: 43.464 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 4498 | tagging_loss_video: 6.132|tagging_loss_audio: 8.503|tagging_loss_text: 13.940|tagging_loss_image: 5.751|tagging_loss_fusion: 5.412|total_loss: 39.737 | 62.51 Examples/sec\n",
      "INFO:tensorflow:training step 4499 | tagging_loss_video: 4.671|tagging_loss_audio: 10.153|tagging_loss_text: 17.732|tagging_loss_image: 4.428|tagging_loss_fusion: 3.122|total_loss: 40.107 | 68.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4500 |tagging_loss_video: 5.862|tagging_loss_audio: 8.556|tagging_loss_text: 13.265|tagging_loss_image: 5.536|tagging_loss_fusion: 4.568|total_loss: 37.787 | Examples/sec: 71.60\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4501 | tagging_loss_video: 6.347|tagging_loss_audio: 9.212|tagging_loss_text: 14.049|tagging_loss_image: 4.934|tagging_loss_fusion: 4.570|total_loss: 39.112 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 4502 | tagging_loss_video: 4.939|tagging_loss_audio: 9.511|tagging_loss_text: 16.225|tagging_loss_image: 6.653|tagging_loss_fusion: 4.617|total_loss: 41.946 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 4503 | tagging_loss_video: 5.989|tagging_loss_audio: 9.101|tagging_loss_text: 15.480|tagging_loss_image: 5.484|tagging_loss_fusion: 5.074|total_loss: 41.129 | 61.97 Examples/sec\n",
      "INFO:tensorflow:training step 4504 | tagging_loss_video: 6.351|tagging_loss_audio: 9.260|tagging_loss_text: 19.070|tagging_loss_image: 5.345|tagging_loss_fusion: 4.279|total_loss: 44.305 | 66.41 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4505 | tagging_loss_video: 5.714|tagging_loss_audio: 8.401|tagging_loss_text: 15.339|tagging_loss_image: 5.182|tagging_loss_fusion: 3.694|total_loss: 38.330 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 4506 | tagging_loss_video: 5.980|tagging_loss_audio: 9.049|tagging_loss_text: 17.303|tagging_loss_image: 6.272|tagging_loss_fusion: 5.426|total_loss: 44.030 | 62.84 Examples/sec\n",
      "INFO:tensorflow:training step 4507 | tagging_loss_video: 5.975|tagging_loss_audio: 7.809|tagging_loss_text: 15.982|tagging_loss_image: 6.406|tagging_loss_fusion: 6.539|total_loss: 42.711 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 4508 | tagging_loss_video: 4.609|tagging_loss_audio: 9.362|tagging_loss_text: 16.057|tagging_loss_image: 5.345|tagging_loss_fusion: 4.367|total_loss: 39.740 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 4509 | tagging_loss_video: 5.706|tagging_loss_audio: 9.483|tagging_loss_text: 16.099|tagging_loss_image: 6.790|tagging_loss_fusion: 3.143|total_loss: 41.221 | 60.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4510 |tagging_loss_video: 5.426|tagging_loss_audio: 8.777|tagging_loss_text: 16.026|tagging_loss_image: 5.805|tagging_loss_fusion: 5.324|total_loss: 41.358 | Examples/sec: 68.28\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.96 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4511 | tagging_loss_video: 6.773|tagging_loss_audio: 9.963|tagging_loss_text: 15.762|tagging_loss_image: 5.630|tagging_loss_fusion: 5.231|total_loss: 43.360 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 4512 | tagging_loss_video: 6.331|tagging_loss_audio: 8.181|tagging_loss_text: 18.131|tagging_loss_image: 5.122|tagging_loss_fusion: 5.889|total_loss: 43.653 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 4513 | tagging_loss_video: 4.575|tagging_loss_audio: 9.793|tagging_loss_text: 16.066|tagging_loss_image: 5.811|tagging_loss_fusion: 3.804|total_loss: 40.050 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 4514 | tagging_loss_video: 5.735|tagging_loss_audio: 8.595|tagging_loss_text: 13.662|tagging_loss_image: 5.429|tagging_loss_fusion: 4.826|total_loss: 38.247 | 64.91 Examples/sec\n",
      "INFO:tensorflow:training step 4515 | tagging_loss_video: 5.984|tagging_loss_audio: 7.753|tagging_loss_text: 11.587|tagging_loss_image: 5.012|tagging_loss_fusion: 4.270|total_loss: 34.606 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 4516 | tagging_loss_video: 6.450|tagging_loss_audio: 8.853|tagging_loss_text: 19.288|tagging_loss_image: 6.364|tagging_loss_fusion: 4.613|total_loss: 45.568 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 4517 | tagging_loss_video: 6.307|tagging_loss_audio: 7.900|tagging_loss_text: 17.400|tagging_loss_image: 6.872|tagging_loss_fusion: 6.434|total_loss: 44.912 | 65.47 Examples/sec\n",
      "INFO:tensorflow:training step 4518 | tagging_loss_video: 4.125|tagging_loss_audio: 9.821|tagging_loss_text: 14.053|tagging_loss_image: 5.732|tagging_loss_fusion: 3.562|total_loss: 37.293 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 4519 | tagging_loss_video: 5.949|tagging_loss_audio: 9.679|tagging_loss_text: 15.205|tagging_loss_image: 5.521|tagging_loss_fusion: 5.744|total_loss: 42.098 | 71.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4520 |tagging_loss_video: 5.099|tagging_loss_audio: 7.784|tagging_loss_text: 12.567|tagging_loss_image: 4.453|tagging_loss_fusion: 3.088|total_loss: 32.991 | Examples/sec: 61.27\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4521 | tagging_loss_video: 5.476|tagging_loss_audio: 8.108|tagging_loss_text: 13.772|tagging_loss_image: 5.130|tagging_loss_fusion: 4.988|total_loss: 37.473 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 4522 | tagging_loss_video: 5.755|tagging_loss_audio: 9.554|tagging_loss_text: 17.465|tagging_loss_image: 3.915|tagging_loss_fusion: 4.403|total_loss: 41.091 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 4523 | tagging_loss_video: 4.896|tagging_loss_audio: 7.649|tagging_loss_text: 14.602|tagging_loss_image: 3.916|tagging_loss_fusion: 3.667|total_loss: 34.729 | 62.08 Examples/sec\n",
      "INFO:tensorflow:training step 4524 | tagging_loss_video: 5.299|tagging_loss_audio: 8.322|tagging_loss_text: 15.428|tagging_loss_image: 6.165|tagging_loss_fusion: 4.255|total_loss: 39.468 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 4525 | tagging_loss_video: 5.754|tagging_loss_audio: 7.953|tagging_loss_text: 12.497|tagging_loss_image: 5.809|tagging_loss_fusion: 5.214|total_loss: 37.227 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 4526 | tagging_loss_video: 5.937|tagging_loss_audio: 8.489|tagging_loss_text: 17.955|tagging_loss_image: 5.781|tagging_loss_fusion: 4.304|total_loss: 42.467 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 4527 | tagging_loss_video: 5.634|tagging_loss_audio: 9.265|tagging_loss_text: 12.103|tagging_loss_image: 5.681|tagging_loss_fusion: 4.838|total_loss: 37.521 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 4528 | tagging_loss_video: 7.181|tagging_loss_audio: 9.764|tagging_loss_text: 19.166|tagging_loss_image: 5.451|tagging_loss_fusion: 6.234|total_loss: 47.796 | 66.56 Examples/sec\n",
      "INFO:tensorflow:training step 4529 | tagging_loss_video: 4.479|tagging_loss_audio: 8.686|tagging_loss_text: 17.088|tagging_loss_image: 6.215|tagging_loss_fusion: 3.360|total_loss: 39.829 | 68.35 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4530 |tagging_loss_video: 5.288|tagging_loss_audio: 7.847|tagging_loss_text: 12.990|tagging_loss_image: 5.071|tagging_loss_fusion: 4.714|total_loss: 35.910 | Examples/sec: 68.51\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4531 | tagging_loss_video: 5.887|tagging_loss_audio: 8.498|tagging_loss_text: 11.797|tagging_loss_image: 4.837|tagging_loss_fusion: 5.626|total_loss: 36.644 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 4532 | tagging_loss_video: 6.120|tagging_loss_audio: 8.251|tagging_loss_text: 13.797|tagging_loss_image: 5.173|tagging_loss_fusion: 6.109|total_loss: 39.451 | 67.58 Examples/sec\n",
      "INFO:tensorflow:training step 4533 | tagging_loss_video: 6.351|tagging_loss_audio: 9.024|tagging_loss_text: 21.067|tagging_loss_image: 5.933|tagging_loss_fusion: 4.154|total_loss: 46.530 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 4534 | tagging_loss_video: 5.718|tagging_loss_audio: 8.694|tagging_loss_text: 17.349|tagging_loss_image: 5.356|tagging_loss_fusion: 4.274|total_loss: 41.390 | 61.43 Examples/sec\n",
      "INFO:tensorflow:training step 4535 | tagging_loss_video: 4.814|tagging_loss_audio: 8.189|tagging_loss_text: 16.091|tagging_loss_image: 6.003|tagging_loss_fusion: 3.431|total_loss: 38.528 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 4536 | tagging_loss_video: 5.494|tagging_loss_audio: 9.375|tagging_loss_text: 16.515|tagging_loss_image: 5.322|tagging_loss_fusion: 4.871|total_loss: 41.577 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 4537 | tagging_loss_video: 6.240|tagging_loss_audio: 8.276|tagging_loss_text: 15.648|tagging_loss_image: 5.604|tagging_loss_fusion: 6.094|total_loss: 41.862 | 63.70 Examples/sec\n",
      "INFO:tensorflow:training step 4538 | tagging_loss_video: 5.394|tagging_loss_audio: 7.787|tagging_loss_text: 12.385|tagging_loss_image: 5.137|tagging_loss_fusion: 5.099|total_loss: 35.802 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 4539 | tagging_loss_video: 5.963|tagging_loss_audio: 8.450|tagging_loss_text: 17.177|tagging_loss_image: 5.463|tagging_loss_fusion: 4.559|total_loss: 41.611 | 70.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4540 |tagging_loss_video: 6.224|tagging_loss_audio: 8.597|tagging_loss_text: 15.912|tagging_loss_image: 5.506|tagging_loss_fusion: 4.653|total_loss: 40.892 | Examples/sec: 71.05\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4541 | tagging_loss_video: 4.130|tagging_loss_audio: 8.632|tagging_loss_text: 14.741|tagging_loss_image: 5.502|tagging_loss_fusion: 2.106|total_loss: 35.112 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 4542 | tagging_loss_video: 5.801|tagging_loss_audio: 8.466|tagging_loss_text: 13.204|tagging_loss_image: 6.272|tagging_loss_fusion: 5.634|total_loss: 39.377 | 67.33 Examples/sec\n",
      "INFO:tensorflow:training step 4543 | tagging_loss_video: 5.566|tagging_loss_audio: 9.496|tagging_loss_text: 13.340|tagging_loss_image: 4.663|tagging_loss_fusion: 3.418|total_loss: 36.483 | 61.49 Examples/sec\n",
      "INFO:tensorflow:training step 4544 | tagging_loss_video: 4.450|tagging_loss_audio: 8.313|tagging_loss_text: 14.171|tagging_loss_image: 4.315|tagging_loss_fusion: 4.008|total_loss: 35.256 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 4545 | tagging_loss_video: 6.192|tagging_loss_audio: 9.342|tagging_loss_text: 14.277|tagging_loss_image: 6.310|tagging_loss_fusion: 6.244|total_loss: 42.364 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 4546 | tagging_loss_video: 6.210|tagging_loss_audio: 5.762|tagging_loss_text: 13.944|tagging_loss_image: 4.973|tagging_loss_fusion: 5.101|total_loss: 35.990 | 63.19 Examples/sec\n",
      "INFO:tensorflow:training step 4547 | tagging_loss_video: 4.968|tagging_loss_audio: 7.899|tagging_loss_text: 13.040|tagging_loss_image: 4.586|tagging_loss_fusion: 3.869|total_loss: 34.363 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 4548 | tagging_loss_video: 5.889|tagging_loss_audio: 8.685|tagging_loss_text: 14.011|tagging_loss_image: 4.492|tagging_loss_fusion: 4.757|total_loss: 37.835 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 4549 | tagging_loss_video: 6.145|tagging_loss_audio: 7.998|tagging_loss_text: 10.960|tagging_loss_image: 4.829|tagging_loss_fusion: 5.073|total_loss: 35.006 | 59.95 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4550 |tagging_loss_video: 5.074|tagging_loss_audio: 8.553|tagging_loss_text: 16.554|tagging_loss_image: 5.498|tagging_loss_fusion: 4.000|total_loss: 39.680 | Examples/sec: 69.60\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4551 | tagging_loss_video: 4.534|tagging_loss_audio: 8.677|tagging_loss_text: 17.130|tagging_loss_image: 6.098|tagging_loss_fusion: 4.718|total_loss: 41.157 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 4552 | tagging_loss_video: 4.761|tagging_loss_audio: 8.430|tagging_loss_text: 12.820|tagging_loss_image: 5.691|tagging_loss_fusion: 3.375|total_loss: 35.077 | 64.75 Examples/sec\n",
      "INFO:tensorflow:training step 4553 | tagging_loss_video: 4.758|tagging_loss_audio: 8.586|tagging_loss_text: 13.876|tagging_loss_image: 5.081|tagging_loss_fusion: 3.533|total_loss: 35.835 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 4554 | tagging_loss_video: 6.126|tagging_loss_audio: 9.732|tagging_loss_text: 14.935|tagging_loss_image: 5.925|tagging_loss_fusion: 5.318|total_loss: 42.037 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 4555 | tagging_loss_video: 5.536|tagging_loss_audio: 9.018|tagging_loss_text: 17.485|tagging_loss_image: 5.769|tagging_loss_fusion: 4.141|total_loss: 41.949 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 4556 | tagging_loss_video: 6.562|tagging_loss_audio: 10.244|tagging_loss_text: 20.762|tagging_loss_image: 5.977|tagging_loss_fusion: 5.369|total_loss: 48.914 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 4557 | tagging_loss_video: 6.859|tagging_loss_audio: 8.248|tagging_loss_text: 15.774|tagging_loss_image: 5.601|tagging_loss_fusion: 4.831|total_loss: 41.312 | 63.88 Examples/sec\n",
      "INFO:tensorflow:training step 4558 | tagging_loss_video: 4.737|tagging_loss_audio: 8.858|tagging_loss_text: 15.265|tagging_loss_image: 5.147|tagging_loss_fusion: 3.705|total_loss: 37.712 | 68.61 Examples/sec\n",
      "INFO:tensorflow:training step 4559 | tagging_loss_video: 3.809|tagging_loss_audio: 7.635|tagging_loss_text: 14.970|tagging_loss_image: 3.759|tagging_loss_fusion: 2.475|total_loss: 32.647 | 70.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4560 |tagging_loss_video: 4.875|tagging_loss_audio: 8.625|tagging_loss_text: 15.577|tagging_loss_image: 4.734|tagging_loss_fusion: 3.622|total_loss: 37.433 | Examples/sec: 63.56\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4561 | tagging_loss_video: 5.093|tagging_loss_audio: 7.927|tagging_loss_text: 18.595|tagging_loss_image: 6.526|tagging_loss_fusion: 3.388|total_loss: 41.529 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 4562 | tagging_loss_video: 5.856|tagging_loss_audio: 8.714|tagging_loss_text: 12.843|tagging_loss_image: 5.498|tagging_loss_fusion: 5.193|total_loss: 38.104 | 66.40 Examples/sec\n",
      "INFO:tensorflow:training step 4563 | tagging_loss_video: 6.226|tagging_loss_audio: 9.192|tagging_loss_text: 16.759|tagging_loss_image: 5.709|tagging_loss_fusion: 4.596|total_loss: 42.483 | 64.45 Examples/sec\n",
      "INFO:tensorflow:training step 4564 | tagging_loss_video: 5.107|tagging_loss_audio: 8.313|tagging_loss_text: 16.094|tagging_loss_image: 4.169|tagging_loss_fusion: 3.071|total_loss: 36.754 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 4565 | tagging_loss_video: 4.494|tagging_loss_audio: 10.266|tagging_loss_text: 13.466|tagging_loss_image: 7.079|tagging_loss_fusion: 3.575|total_loss: 38.881 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 4566 | tagging_loss_video: 6.130|tagging_loss_audio: 8.727|tagging_loss_text: 15.742|tagging_loss_image: 6.527|tagging_loss_fusion: 6.220|total_loss: 43.346 | 68.68 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 4566.\n",
      "INFO:tensorflow:training step 4567 | tagging_loss_video: 6.417|tagging_loss_audio: 9.036|tagging_loss_text: 12.252|tagging_loss_image: 5.943|tagging_loss_fusion: 6.048|total_loss: 39.697 | 43.49 Examples/sec\n",
      "INFO:tensorflow:training step 4568 | tagging_loss_video: 6.570|tagging_loss_audio: 10.299|tagging_loss_text: 18.088|tagging_loss_image: 6.314|tagging_loss_fusion: 6.260|total_loss: 47.531 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 4569 | tagging_loss_video: 4.731|tagging_loss_audio: 9.531|tagging_loss_text: 16.494|tagging_loss_image: 5.729|tagging_loss_fusion: 4.900|total_loss: 41.384 | 69.99 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4570 |tagging_loss_video: 5.549|tagging_loss_audio: 8.908|tagging_loss_text: 15.799|tagging_loss_image: 5.042|tagging_loss_fusion: 4.376|total_loss: 39.674 | Examples/sec: 61.59\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.98 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4571 | tagging_loss_video: 5.695|tagging_loss_audio: 10.367|tagging_loss_text: 17.103|tagging_loss_image: 5.587|tagging_loss_fusion: 4.374|total_loss: 43.125 | 71.76 Examples/sec\n",
      "INFO:tensorflow:training step 4572 | tagging_loss_video: 5.479|tagging_loss_audio: 8.876|tagging_loss_text: 14.284|tagging_loss_image: 5.233|tagging_loss_fusion: 4.777|total_loss: 38.649 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 4573 | tagging_loss_video: 6.619|tagging_loss_audio: 9.416|tagging_loss_text: 12.485|tagging_loss_image: 6.242|tagging_loss_fusion: 6.220|total_loss: 40.981 | 66.52 Examples/sec\n",
      "INFO:tensorflow:training step 4574 | tagging_loss_video: 5.228|tagging_loss_audio: 9.289|tagging_loss_text: 15.917|tagging_loss_image: 6.361|tagging_loss_fusion: 3.885|total_loss: 40.680 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 4575 | tagging_loss_video: 5.574|tagging_loss_audio: 8.212|tagging_loss_text: 14.529|tagging_loss_image: 6.620|tagging_loss_fusion: 4.036|total_loss: 38.971 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 4576 | tagging_loss_video: 5.961|tagging_loss_audio: 9.170|tagging_loss_text: 15.488|tagging_loss_image: 4.502|tagging_loss_fusion: 4.103|total_loss: 39.224 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 4577 | tagging_loss_video: 6.979|tagging_loss_audio: 9.624|tagging_loss_text: 15.649|tagging_loss_image: 6.783|tagging_loss_fusion: 6.746|total_loss: 45.781 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 4578 | tagging_loss_video: 5.776|tagging_loss_audio: 9.182|tagging_loss_text: 16.436|tagging_loss_image: 3.891|tagging_loss_fusion: 4.081|total_loss: 39.368 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 4579 | tagging_loss_video: 6.480|tagging_loss_audio: 9.318|tagging_loss_text: 15.520|tagging_loss_image: 5.989|tagging_loss_fusion: 4.489|total_loss: 41.797 | 67.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4580 |tagging_loss_video: 5.428|tagging_loss_audio: 8.148|tagging_loss_text: 11.734|tagging_loss_image: 5.139|tagging_loss_fusion: 3.041|total_loss: 33.490 | Examples/sec: 69.61\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 4581 | tagging_loss_video: 6.630|tagging_loss_audio: 9.728|tagging_loss_text: 16.622|tagging_loss_image: 6.012|tagging_loss_fusion: 6.498|total_loss: 45.490 | 66.32 Examples/sec\n",
      "INFO:tensorflow:training step 4582 | tagging_loss_video: 5.956|tagging_loss_audio: 10.536|tagging_loss_text: 17.291|tagging_loss_image: 7.383|tagging_loss_fusion: 6.174|total_loss: 47.340 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 4583 | tagging_loss_video: 6.468|tagging_loss_audio: 9.078|tagging_loss_text: 12.371|tagging_loss_image: 5.770|tagging_loss_fusion: 4.538|total_loss: 38.226 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 4584 | tagging_loss_video: 6.306|tagging_loss_audio: 9.653|tagging_loss_text: 12.539|tagging_loss_image: 4.920|tagging_loss_fusion: 6.167|total_loss: 39.586 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 4585 | tagging_loss_video: 5.963|tagging_loss_audio: 11.628|tagging_loss_text: 19.082|tagging_loss_image: 6.886|tagging_loss_fusion: 4.384|total_loss: 47.943 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 4586 | tagging_loss_video: 5.160|tagging_loss_audio: 9.059|tagging_loss_text: 17.295|tagging_loss_image: 4.458|tagging_loss_fusion: 2.931|total_loss: 38.903 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 4587 | tagging_loss_video: 6.200|tagging_loss_audio: 8.967|tagging_loss_text: 14.636|tagging_loss_image: 6.049|tagging_loss_fusion: 5.713|total_loss: 41.565 | 62.73 Examples/sec\n",
      "INFO:tensorflow:training step 4588 | tagging_loss_video: 3.979|tagging_loss_audio: 8.781|tagging_loss_text: 13.415|tagging_loss_image: 5.774|tagging_loss_fusion: 3.633|total_loss: 35.583 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 4589 | tagging_loss_video: 5.771|tagging_loss_audio: 10.173|tagging_loss_text: 16.463|tagging_loss_image: 6.053|tagging_loss_fusion: 4.915|total_loss: 43.375 | 67.36 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4590 |tagging_loss_video: 4.957|tagging_loss_audio: 9.340|tagging_loss_text: 13.973|tagging_loss_image: 5.631|tagging_loss_fusion: 3.934|total_loss: 37.834 | Examples/sec: 70.35\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4591 | tagging_loss_video: 6.725|tagging_loss_audio: 10.208|tagging_loss_text: 20.011|tagging_loss_image: 5.635|tagging_loss_fusion: 4.078|total_loss: 46.657 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 4592 | tagging_loss_video: 7.503|tagging_loss_audio: 8.646|tagging_loss_text: 15.525|tagging_loss_image: 5.519|tagging_loss_fusion: 7.083|total_loss: 44.277 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4593 | tagging_loss_video: 5.987|tagging_loss_audio: 9.734|tagging_loss_text: 13.759|tagging_loss_image: 5.429|tagging_loss_fusion: 5.223|total_loss: 40.132 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 4594 | tagging_loss_video: 5.696|tagging_loss_audio: 8.529|tagging_loss_text: 15.670|tagging_loss_image: 5.757|tagging_loss_fusion: 5.375|total_loss: 41.027 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 4595 | tagging_loss_video: 5.194|tagging_loss_audio: 8.250|tagging_loss_text: 15.411|tagging_loss_image: 4.883|tagging_loss_fusion: 2.944|total_loss: 36.682 | 62.73 Examples/sec\n",
      "INFO:tensorflow:training step 4596 | tagging_loss_video: 6.893|tagging_loss_audio: 9.973|tagging_loss_text: 16.151|tagging_loss_image: 6.108|tagging_loss_fusion: 6.839|total_loss: 45.964 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 4597 | tagging_loss_video: 5.090|tagging_loss_audio: 9.931|tagging_loss_text: 15.551|tagging_loss_image: 4.684|tagging_loss_fusion: 3.035|total_loss: 38.291 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 4598 | tagging_loss_video: 6.010|tagging_loss_audio: 7.953|tagging_loss_text: 17.146|tagging_loss_image: 5.059|tagging_loss_fusion: 3.783|total_loss: 39.952 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 4599 | tagging_loss_video: 6.099|tagging_loss_audio: 8.267|tagging_loss_text: 13.251|tagging_loss_image: 5.392|tagging_loss_fusion: 5.373|total_loss: 38.382 | 68.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4600 |tagging_loss_video: 5.416|tagging_loss_audio: 8.564|tagging_loss_text: 13.490|tagging_loss_image: 5.852|tagging_loss_fusion: 4.608|total_loss: 37.930 | Examples/sec: 72.25\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4601 | tagging_loss_video: 5.686|tagging_loss_audio: 8.785|tagging_loss_text: 14.504|tagging_loss_image: 5.544|tagging_loss_fusion: 4.922|total_loss: 39.442 | 61.98 Examples/sec\n",
      "INFO:tensorflow:training step 4602 | tagging_loss_video: 5.251|tagging_loss_audio: 8.360|tagging_loss_text: 13.281|tagging_loss_image: 5.854|tagging_loss_fusion: 4.966|total_loss: 37.712 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 4603 | tagging_loss_video: 5.329|tagging_loss_audio: 8.213|tagging_loss_text: 14.657|tagging_loss_image: 4.273|tagging_loss_fusion: 3.373|total_loss: 35.846 | 65.99 Examples/sec\n",
      "INFO:tensorflow:training step 4604 | tagging_loss_video: 6.225|tagging_loss_audio: 8.197|tagging_loss_text: 13.400|tagging_loss_image: 4.703|tagging_loss_fusion: 4.869|total_loss: 37.394 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 4605 | tagging_loss_video: 6.440|tagging_loss_audio: 9.403|tagging_loss_text: 15.196|tagging_loss_image: 4.587|tagging_loss_fusion: 4.792|total_loss: 40.418 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 4606 | tagging_loss_video: 5.055|tagging_loss_audio: 8.829|tagging_loss_text: 14.245|tagging_loss_image: 5.367|tagging_loss_fusion: 2.998|total_loss: 36.494 | 63.53 Examples/sec\n",
      "INFO:tensorflow:training step 4607 | tagging_loss_video: 5.347|tagging_loss_audio: 8.431|tagging_loss_text: 14.691|tagging_loss_image: 5.403|tagging_loss_fusion: 3.650|total_loss: 37.522 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 4608 | tagging_loss_video: 5.405|tagging_loss_audio: 7.185|tagging_loss_text: 17.256|tagging_loss_image: 6.023|tagging_loss_fusion: 4.164|total_loss: 40.033 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 4609 | tagging_loss_video: 5.604|tagging_loss_audio: 8.937|tagging_loss_text: 16.225|tagging_loss_image: 4.957|tagging_loss_fusion: 4.279|total_loss: 40.002 | 66.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4610 |tagging_loss_video: 6.651|tagging_loss_audio: 8.580|tagging_loss_text: 17.418|tagging_loss_image: 5.056|tagging_loss_fusion: 5.166|total_loss: 42.872 | Examples/sec: 69.67\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4611 | tagging_loss_video: 7.645|tagging_loss_audio: 10.279|tagging_loss_text: 15.978|tagging_loss_image: 6.021|tagging_loss_fusion: 6.079|total_loss: 46.001 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 4612 | tagging_loss_video: 5.331|tagging_loss_audio: 8.929|tagging_loss_text: 13.430|tagging_loss_image: 5.851|tagging_loss_fusion: 4.325|total_loss: 37.866 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 4613 | tagging_loss_video: 6.262|tagging_loss_audio: 10.540|tagging_loss_text: 16.446|tagging_loss_image: 5.741|tagging_loss_fusion: 4.988|total_loss: 43.978 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 4614 | tagging_loss_video: 5.285|tagging_loss_audio: 10.927|tagging_loss_text: 19.258|tagging_loss_image: 6.017|tagging_loss_fusion: 4.281|total_loss: 45.769 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 4615 | tagging_loss_video: 6.479|tagging_loss_audio: 8.868|tagging_loss_text: 14.670|tagging_loss_image: 5.218|tagging_loss_fusion: 3.639|total_loss: 38.873 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 4616 | tagging_loss_video: 6.409|tagging_loss_audio: 10.398|tagging_loss_text: 16.514|tagging_loss_image: 5.593|tagging_loss_fusion: 5.626|total_loss: 44.540 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 4617 | tagging_loss_video: 5.541|tagging_loss_audio: 7.817|tagging_loss_text: 12.541|tagging_loss_image: 5.005|tagging_loss_fusion: 3.943|total_loss: 34.847 | 59.13 Examples/sec\n",
      "INFO:tensorflow:training step 4618 | tagging_loss_video: 5.793|tagging_loss_audio: 10.744|tagging_loss_text: 17.809|tagging_loss_image: 4.536|tagging_loss_fusion: 3.940|total_loss: 42.821 | 67.62 Examples/sec\n",
      "INFO:tensorflow:training step 4619 | tagging_loss_video: 6.408|tagging_loss_audio: 8.380|tagging_loss_text: 13.257|tagging_loss_image: 5.342|tagging_loss_fusion: 5.109|total_loss: 38.496 | 69.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4620 |tagging_loss_video: 4.964|tagging_loss_audio: 7.368|tagging_loss_text: 12.280|tagging_loss_image: 4.725|tagging_loss_fusion: 5.054|total_loss: 34.391 | Examples/sec: 64.10\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4621 | tagging_loss_video: 4.835|tagging_loss_audio: 8.770|tagging_loss_text: 14.200|tagging_loss_image: 5.830|tagging_loss_fusion: 3.457|total_loss: 37.092 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 4622 | tagging_loss_video: 5.624|tagging_loss_audio: 9.161|tagging_loss_text: 16.565|tagging_loss_image: 5.175|tagging_loss_fusion: 3.402|total_loss: 39.927 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 4623 | tagging_loss_video: 4.943|tagging_loss_audio: 9.063|tagging_loss_text: 15.514|tagging_loss_image: 5.316|tagging_loss_fusion: 3.649|total_loss: 38.485 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 4624 | tagging_loss_video: 6.952|tagging_loss_audio: 9.794|tagging_loss_text: 17.017|tagging_loss_image: 6.995|tagging_loss_fusion: 6.524|total_loss: 47.282 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 4625 | tagging_loss_video: 4.759|tagging_loss_audio: 9.140|tagging_loss_text: 14.693|tagging_loss_image: 4.513|tagging_loss_fusion: 3.440|total_loss: 36.546 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 4626 | tagging_loss_video: 6.011|tagging_loss_audio: 8.572|tagging_loss_text: 14.977|tagging_loss_image: 5.050|tagging_loss_fusion: 6.522|total_loss: 41.133 | 63.48 Examples/sec\n",
      "INFO:tensorflow:training step 4627 | tagging_loss_video: 6.112|tagging_loss_audio: 9.071|tagging_loss_text: 15.658|tagging_loss_image: 6.244|tagging_loss_fusion: 6.286|total_loss: 43.370 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 4628 | tagging_loss_video: 4.985|tagging_loss_audio: 8.252|tagging_loss_text: 16.152|tagging_loss_image: 5.702|tagging_loss_fusion: 2.821|total_loss: 37.913 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 4629 | tagging_loss_video: 5.356|tagging_loss_audio: 9.911|tagging_loss_text: 18.715|tagging_loss_image: 5.247|tagging_loss_fusion: 3.838|total_loss: 43.067 | 69.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4630 |tagging_loss_video: 5.629|tagging_loss_audio: 8.613|tagging_loss_text: 14.708|tagging_loss_image: 5.197|tagging_loss_fusion: 4.025|total_loss: 38.172 | Examples/sec: 68.73\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4631 | tagging_loss_video: 5.311|tagging_loss_audio: 8.290|tagging_loss_text: 17.053|tagging_loss_image: 6.080|tagging_loss_fusion: 4.808|total_loss: 41.542 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 4632 | tagging_loss_video: 5.634|tagging_loss_audio: 9.155|tagging_loss_text: 15.417|tagging_loss_image: 3.530|tagging_loss_fusion: 3.807|total_loss: 37.543 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 4633 | tagging_loss_video: 4.951|tagging_loss_audio: 8.194|tagging_loss_text: 13.596|tagging_loss_image: 5.795|tagging_loss_fusion: 4.632|total_loss: 37.168 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 4634 | tagging_loss_video: 5.736|tagging_loss_audio: 7.951|tagging_loss_text: 15.610|tagging_loss_image: 5.572|tagging_loss_fusion: 4.463|total_loss: 39.331 | 61.77 Examples/sec\n",
      "INFO:tensorflow:training step 4635 | tagging_loss_video: 4.983|tagging_loss_audio: 9.699|tagging_loss_text: 20.810|tagging_loss_image: 5.517|tagging_loss_fusion: 2.782|total_loss: 43.790 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 4636 | tagging_loss_video: 6.037|tagging_loss_audio: 8.834|tagging_loss_text: 15.166|tagging_loss_image: 5.704|tagging_loss_fusion: 6.810|total_loss: 42.551 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 4637 | tagging_loss_video: 6.423|tagging_loss_audio: 8.069|tagging_loss_text: 17.444|tagging_loss_image: 6.397|tagging_loss_fusion: 5.673|total_loss: 44.006 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 4638 | tagging_loss_video: 5.903|tagging_loss_audio: 8.281|tagging_loss_text: 17.013|tagging_loss_image: 5.942|tagging_loss_fusion: 7.278|total_loss: 44.416 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 4639 | tagging_loss_video: 4.736|tagging_loss_audio: 9.031|tagging_loss_text: 17.257|tagging_loss_image: 6.116|tagging_loss_fusion: 3.245|total_loss: 40.385 | 68.03 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4640 |tagging_loss_video: 6.911|tagging_loss_audio: 8.549|tagging_loss_text: 17.567|tagging_loss_image: 5.428|tagging_loss_fusion: 6.598|total_loss: 45.052 | Examples/sec: 68.23\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.76 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 4641 | tagging_loss_video: 6.944|tagging_loss_audio: 9.068|tagging_loss_text: 16.904|tagging_loss_image: 5.609|tagging_loss_fusion: 6.459|total_loss: 44.985 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 4642 | tagging_loss_video: 6.069|tagging_loss_audio: 8.739|tagging_loss_text: 16.841|tagging_loss_image: 5.467|tagging_loss_fusion: 4.471|total_loss: 41.586 | 62.11 Examples/sec\n",
      "INFO:tensorflow:training step 4643 | tagging_loss_video: 6.399|tagging_loss_audio: 7.722|tagging_loss_text: 16.939|tagging_loss_image: 6.244|tagging_loss_fusion: 5.881|total_loss: 43.185 | 70.57 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4644 | tagging_loss_video: 6.086|tagging_loss_audio: 8.450|tagging_loss_text: 14.643|tagging_loss_image: 5.478|tagging_loss_fusion: 4.445|total_loss: 39.101 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 4645 | tagging_loss_video: 5.139|tagging_loss_audio: 9.146|tagging_loss_text: 16.589|tagging_loss_image: 4.894|tagging_loss_fusion: 3.611|total_loss: 39.378 | 63.28 Examples/sec\n",
      "INFO:tensorflow:training step 4646 | tagging_loss_video: 4.922|tagging_loss_audio: 9.700|tagging_loss_text: 12.517|tagging_loss_image: 5.490|tagging_loss_fusion: 2.946|total_loss: 35.574 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 4647 | tagging_loss_video: 5.920|tagging_loss_audio: 9.589|tagging_loss_text: 18.093|tagging_loss_image: 5.764|tagging_loss_fusion: 4.976|total_loss: 44.341 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 4648 | tagging_loss_video: 6.659|tagging_loss_audio: 8.036|tagging_loss_text: 13.902|tagging_loss_image: 5.739|tagging_loss_fusion: 4.083|total_loss: 38.419 | 63.84 Examples/sec\n",
      "INFO:tensorflow:training step 4649 | tagging_loss_video: 6.179|tagging_loss_audio: 8.321|tagging_loss_text: 15.378|tagging_loss_image: 3.892|tagging_loss_fusion: 6.204|total_loss: 39.974 | 68.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4650 |tagging_loss_video: 5.372|tagging_loss_audio: 9.148|tagging_loss_text: 10.972|tagging_loss_image: 5.920|tagging_loss_fusion: 4.530|total_loss: 35.941 | Examples/sec: 69.06\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4651 | tagging_loss_video: 6.205|tagging_loss_audio: 8.688|tagging_loss_text: 16.101|tagging_loss_image: 5.352|tagging_loss_fusion: 4.792|total_loss: 41.139 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 4652 | tagging_loss_video: 6.066|tagging_loss_audio: 8.698|tagging_loss_text: 16.077|tagging_loss_image: 5.420|tagging_loss_fusion: 5.901|total_loss: 42.163 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 4653 | tagging_loss_video: 4.408|tagging_loss_audio: 7.802|tagging_loss_text: 14.954|tagging_loss_image: 5.707|tagging_loss_fusion: 3.377|total_loss: 36.248 | 66.15 Examples/sec\n",
      "INFO:tensorflow:training step 4654 | tagging_loss_video: 5.838|tagging_loss_audio: 10.074|tagging_loss_text: 16.342|tagging_loss_image: 5.287|tagging_loss_fusion: 5.447|total_loss: 42.988 | 71.87 Examples/sec\n",
      "INFO:tensorflow:training step 4655 | tagging_loss_video: 4.817|tagging_loss_audio: 9.225|tagging_loss_text: 19.262|tagging_loss_image: 4.775|tagging_loss_fusion: 3.197|total_loss: 41.276 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 4656 | tagging_loss_video: 5.906|tagging_loss_audio: 9.206|tagging_loss_text: 11.600|tagging_loss_image: 6.911|tagging_loss_fusion: 5.539|total_loss: 39.163 | 61.11 Examples/sec\n",
      "INFO:tensorflow:training step 4657 | tagging_loss_video: 5.117|tagging_loss_audio: 9.308|tagging_loss_text: 14.765|tagging_loss_image: 5.879|tagging_loss_fusion: 4.565|total_loss: 39.635 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 4658 | tagging_loss_video: 5.523|tagging_loss_audio: 10.198|tagging_loss_text: 18.246|tagging_loss_image: 5.805|tagging_loss_fusion: 4.098|total_loss: 43.871 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 4659 | tagging_loss_video: 5.774|tagging_loss_audio: 7.601|tagging_loss_text: 13.189|tagging_loss_image: 4.922|tagging_loss_fusion: 4.638|total_loss: 36.124 | 63.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4660 |tagging_loss_video: 5.283|tagging_loss_audio: 7.425|tagging_loss_text: 11.270|tagging_loss_image: 4.424|tagging_loss_fusion: 3.560|total_loss: 31.962 | Examples/sec: 68.71\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4661 | tagging_loss_video: 6.402|tagging_loss_audio: 7.716|tagging_loss_text: 16.143|tagging_loss_image: 5.428|tagging_loss_fusion: 6.319|total_loss: 42.008 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 4662 | tagging_loss_video: 4.433|tagging_loss_audio: 7.796|tagging_loss_text: 12.163|tagging_loss_image: 5.422|tagging_loss_fusion: 2.979|total_loss: 32.793 | 62.28 Examples/sec\n",
      "INFO:tensorflow:training step 4663 | tagging_loss_video: 4.867|tagging_loss_audio: 9.190|tagging_loss_text: 13.318|tagging_loss_image: 5.111|tagging_loss_fusion: 3.391|total_loss: 35.877 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 4664 | tagging_loss_video: 6.409|tagging_loss_audio: 8.068|tagging_loss_text: 12.492|tagging_loss_image: 4.945|tagging_loss_fusion: 5.599|total_loss: 37.513 | 65.02 Examples/sec\n",
      "INFO:tensorflow:training step 4665 | tagging_loss_video: 4.781|tagging_loss_audio: 8.503|tagging_loss_text: 15.290|tagging_loss_image: 3.593|tagging_loss_fusion: 3.105|total_loss: 35.271 | 68.75 Examples/sec\n",
      "INFO:tensorflow:training step 4666 | tagging_loss_video: 4.009|tagging_loss_audio: 8.836|tagging_loss_text: 15.277|tagging_loss_image: 4.965|tagging_loss_fusion: 2.860|total_loss: 35.947 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 4667 | tagging_loss_video: 5.902|tagging_loss_audio: 8.789|tagging_loss_text: 15.494|tagging_loss_image: 6.043|tagging_loss_fusion: 4.044|total_loss: 40.272 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 4668 | tagging_loss_video: 6.568|tagging_loss_audio: 9.289|tagging_loss_text: 16.415|tagging_loss_image: 4.702|tagging_loss_fusion: 4.486|total_loss: 41.460 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 4669 | tagging_loss_video: 4.398|tagging_loss_audio: 8.866|tagging_loss_text: 12.396|tagging_loss_image: 5.361|tagging_loss_fusion: 3.579|total_loss: 34.600 | 69.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4670 |tagging_loss_video: 4.854|tagging_loss_audio: 8.055|tagging_loss_text: 16.132|tagging_loss_image: 5.724|tagging_loss_fusion: 3.984|total_loss: 38.750 | Examples/sec: 63.22\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4671 | tagging_loss_video: 5.796|tagging_loss_audio: 8.746|tagging_loss_text: 14.497|tagging_loss_image: 5.909|tagging_loss_fusion: 5.354|total_loss: 40.302 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 4672 | tagging_loss_video: 5.956|tagging_loss_audio: 8.533|tagging_loss_text: 16.713|tagging_loss_image: 6.365|tagging_loss_fusion: 4.875|total_loss: 42.441 | 66.30 Examples/sec\n",
      "INFO:tensorflow:training step 4673 | tagging_loss_video: 4.558|tagging_loss_audio: 7.808|tagging_loss_text: 14.805|tagging_loss_image: 4.810|tagging_loss_fusion: 2.581|total_loss: 34.562 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 4674 | tagging_loss_video: 6.233|tagging_loss_audio: 7.668|tagging_loss_text: 16.083|tagging_loss_image: 4.154|tagging_loss_fusion: 5.056|total_loss: 39.194 | 68.19 Examples/sec\n",
      "INFO:tensorflow:training step 4675 | tagging_loss_video: 6.155|tagging_loss_audio: 7.888|tagging_loss_text: 17.928|tagging_loss_image: 3.891|tagging_loss_fusion: 3.971|total_loss: 39.833 | 71.59 Examples/sec\n",
      "INFO:tensorflow:training step 4676 | tagging_loss_video: 5.723|tagging_loss_audio: 8.221|tagging_loss_text: 12.377|tagging_loss_image: 4.963|tagging_loss_fusion: 5.467|total_loss: 36.750 | 64.07 Examples/sec\n",
      "INFO:tensorflow:training step 4677 | tagging_loss_video: 6.168|tagging_loss_audio: 8.160|tagging_loss_text: 13.049|tagging_loss_image: 4.698|tagging_loss_fusion: 4.674|total_loss: 36.749 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 4678 | tagging_loss_video: 5.977|tagging_loss_audio: 8.903|tagging_loss_text: 16.494|tagging_loss_image: 5.174|tagging_loss_fusion: 5.886|total_loss: 42.435 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 4679 | tagging_loss_video: 4.636|tagging_loss_audio: 9.323|tagging_loss_text: 13.260|tagging_loss_image: 5.363|tagging_loss_fusion: 3.950|total_loss: 36.531 | 71.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4680 |tagging_loss_video: 5.534|tagging_loss_audio: 7.969|tagging_loss_text: 14.741|tagging_loss_image: 5.575|tagging_loss_fusion: 5.312|total_loss: 39.130 | Examples/sec: 70.53\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4681 | tagging_loss_video: 6.157|tagging_loss_audio: 8.462|tagging_loss_text: 11.831|tagging_loss_image: 5.432|tagging_loss_fusion: 5.519|total_loss: 37.401 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 4682 | tagging_loss_video: 6.168|tagging_loss_audio: 9.591|tagging_loss_text: 13.335|tagging_loss_image: 6.324|tagging_loss_fusion: 5.682|total_loss: 41.100 | 63.84 Examples/sec\n",
      "INFO:tensorflow:training step 4683 | tagging_loss_video: 4.209|tagging_loss_audio: 8.617|tagging_loss_text: 12.156|tagging_loss_image: 5.733|tagging_loss_fusion: 3.094|total_loss: 33.809 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 4684 | tagging_loss_video: 6.710|tagging_loss_audio: 8.925|tagging_loss_text: 14.994|tagging_loss_image: 6.341|tagging_loss_fusion: 6.861|total_loss: 43.830 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 4685 | tagging_loss_video: 5.805|tagging_loss_audio: 8.620|tagging_loss_text: 15.215|tagging_loss_image: 5.389|tagging_loss_fusion: 5.287|total_loss: 40.315 | 61.45 Examples/sec\n",
      "INFO:tensorflow:training step 4686 | tagging_loss_video: 4.905|tagging_loss_audio: 8.251|tagging_loss_text: 13.658|tagging_loss_image: 4.969|tagging_loss_fusion: 3.819|total_loss: 35.603 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 4687 | tagging_loss_video: 5.760|tagging_loss_audio: 8.299|tagging_loss_text: 15.335|tagging_loss_image: 5.908|tagging_loss_fusion: 5.765|total_loss: 41.066 | 68.63 Examples/sec\n",
      "INFO:tensorflow:training step 4688 | tagging_loss_video: 5.881|tagging_loss_audio: 7.932|tagging_loss_text: 13.552|tagging_loss_image: 5.317|tagging_loss_fusion: 4.896|total_loss: 37.578 | 71.96 Examples/sec\n",
      "INFO:tensorflow:training step 4689 | tagging_loss_video: 5.675|tagging_loss_audio: 8.959|tagging_loss_text: 17.243|tagging_loss_image: 5.685|tagging_loss_fusion: 4.880|total_loss: 42.442 | 70.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4690 |tagging_loss_video: 4.968|tagging_loss_audio: 8.919|tagging_loss_text: 16.470|tagging_loss_image: 4.822|tagging_loss_fusion: 4.899|total_loss: 40.078 | Examples/sec: 71.92\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4691 | tagging_loss_video: 5.620|tagging_loss_audio: 7.904|tagging_loss_text: 13.466|tagging_loss_image: 5.859|tagging_loss_fusion: 7.045|total_loss: 39.895 | 66.52 Examples/sec\n",
      "INFO:tensorflow:training step 4692 | tagging_loss_video: 6.780|tagging_loss_audio: 8.784|tagging_loss_text: 13.866|tagging_loss_image: 5.969|tagging_loss_fusion: 8.069|total_loss: 43.467 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 4693 | tagging_loss_video: 5.002|tagging_loss_audio: 8.577|tagging_loss_text: 18.992|tagging_loss_image: 5.730|tagging_loss_fusion: 3.812|total_loss: 42.112 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 4694 | tagging_loss_video: 5.587|tagging_loss_audio: 8.509|tagging_loss_text: 18.865|tagging_loss_image: 5.277|tagging_loss_fusion: 4.533|total_loss: 42.770 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 4695 | tagging_loss_video: 5.881|tagging_loss_audio: 10.463|tagging_loss_text: 16.291|tagging_loss_image: 6.392|tagging_loss_fusion: 3.760|total_loss: 42.787 | 66.71 Examples/sec\n",
      "INFO:tensorflow:training step 4696 | tagging_loss_video: 5.865|tagging_loss_audio: 8.786|tagging_loss_text: 15.756|tagging_loss_image: 5.646|tagging_loss_fusion: 4.345|total_loss: 40.399 | 62.17 Examples/sec\n",
      "INFO:tensorflow:training step 4697 | tagging_loss_video: 4.013|tagging_loss_audio: 8.439|tagging_loss_text: 13.993|tagging_loss_image: 5.773|tagging_loss_fusion: 3.492|total_loss: 35.710 | 67.81 Examples/sec\n",
      "INFO:tensorflow:training step 4698 | tagging_loss_video: 5.668|tagging_loss_audio: 9.210|tagging_loss_text: 17.458|tagging_loss_image: 4.176|tagging_loss_fusion: 3.107|total_loss: 39.620 | 72.46 Examples/sec\n",
      "INFO:tensorflow:training step 4699 | tagging_loss_video: 4.630|tagging_loss_audio: 8.566|tagging_loss_text: 12.329|tagging_loss_image: 5.053|tagging_loss_fusion: 3.354|total_loss: 33.931 | 59.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4700 |tagging_loss_video: 5.146|tagging_loss_audio: 9.928|tagging_loss_text: 17.151|tagging_loss_image: 5.664|tagging_loss_fusion: 3.376|total_loss: 41.265 | Examples/sec: 71.27\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4701 | tagging_loss_video: 5.901|tagging_loss_audio: 8.407|tagging_loss_text: 15.554|tagging_loss_image: 5.051|tagging_loss_fusion: 6.858|total_loss: 41.771 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 4702 | tagging_loss_video: 5.569|tagging_loss_audio: 9.383|tagging_loss_text: 13.953|tagging_loss_image: 5.374|tagging_loss_fusion: 5.755|total_loss: 40.034 | 60.93 Examples/sec\n",
      "INFO:tensorflow:training step 4703 | tagging_loss_video: 5.599|tagging_loss_audio: 8.252|tagging_loss_text: 16.725|tagging_loss_image: 4.827|tagging_loss_fusion: 3.544|total_loss: 38.947 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 4704 | tagging_loss_video: 6.008|tagging_loss_audio: 9.962|tagging_loss_text: 17.063|tagging_loss_image: 3.506|tagging_loss_fusion: 3.604|total_loss: 40.143 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 4705 | tagging_loss_video: 6.242|tagging_loss_audio: 8.598|tagging_loss_text: 17.179|tagging_loss_image: 6.614|tagging_loss_fusion: 5.702|total_loss: 44.335 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 4706 | tagging_loss_video: 6.445|tagging_loss_audio: 9.443|tagging_loss_text: 16.752|tagging_loss_image: 5.725|tagging_loss_fusion: 6.651|total_loss: 45.015 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 4707 | tagging_loss_video: 5.111|tagging_loss_audio: 7.714|tagging_loss_text: 12.937|tagging_loss_image: 5.829|tagging_loss_fusion: 4.581|total_loss: 36.172 | 63.18 Examples/sec\n",
      "INFO:tensorflow:training step 4708 | tagging_loss_video: 7.045|tagging_loss_audio: 9.856|tagging_loss_text: 14.467|tagging_loss_image: 6.587|tagging_loss_fusion: 7.182|total_loss: 45.138 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 4709 | tagging_loss_video: 5.285|tagging_loss_audio: 8.765|tagging_loss_text: 15.172|tagging_loss_image: 3.351|tagging_loss_fusion: 4.420|total_loss: 36.992 | 70.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4710 |tagging_loss_video: 7.240|tagging_loss_audio: 8.772|tagging_loss_text: 15.115|tagging_loss_image: 5.076|tagging_loss_fusion: 6.128|total_loss: 42.331 | Examples/sec: 66.23\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.78 | precision@0.5: 0.92 |recall@0.1: 0.95 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4711 | tagging_loss_video: 5.420|tagging_loss_audio: 10.052|tagging_loss_text: 17.104|tagging_loss_image: 5.354|tagging_loss_fusion: 3.807|total_loss: 41.738 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 4712 | tagging_loss_video: 4.415|tagging_loss_audio: 9.008|tagging_loss_text: 14.942|tagging_loss_image: 5.414|tagging_loss_fusion: 2.632|total_loss: 36.410 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 4713 | tagging_loss_video: 5.557|tagging_loss_audio: 9.467|tagging_loss_text: 19.403|tagging_loss_image: 5.523|tagging_loss_fusion: 4.668|total_loss: 44.617 | 65.68 Examples/sec\n",
      "INFO:tensorflow:training step 4714 | tagging_loss_video: 6.245|tagging_loss_audio: 9.189|tagging_loss_text: 17.991|tagging_loss_image: 5.221|tagging_loss_fusion: 5.042|total_loss: 43.688 | 72.34 Examples/sec\n",
      "INFO:tensorflow:training step 4715 | tagging_loss_video: 6.396|tagging_loss_audio: 10.391|tagging_loss_text: 13.793|tagging_loss_image: 6.233|tagging_loss_fusion: 5.033|total_loss: 41.846 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 4716 | tagging_loss_video: 6.004|tagging_loss_audio: 9.507|tagging_loss_text: 16.874|tagging_loss_image: 5.582|tagging_loss_fusion: 4.110|total_loss: 42.078 | 67.38 Examples/sec\n",
      "INFO:tensorflow:training step 4717 | tagging_loss_video: 5.450|tagging_loss_audio: 10.033|tagging_loss_text: 14.144|tagging_loss_image: 6.412|tagging_loss_fusion: 4.928|total_loss: 40.967 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 4718 | tagging_loss_video: 5.479|tagging_loss_audio: 10.133|tagging_loss_text: 13.769|tagging_loss_image: 4.994|tagging_loss_fusion: 3.813|total_loss: 38.189 | 65.64 Examples/sec\n",
      "INFO:tensorflow:training step 4719 | tagging_loss_video: 4.698|tagging_loss_audio: 8.987|tagging_loss_text: 14.079|tagging_loss_image: 5.807|tagging_loss_fusion: 3.862|total_loss: 37.432 | 68.45 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4720 |tagging_loss_video: 5.143|tagging_loss_audio: 9.233|tagging_loss_text: 12.366|tagging_loss_image: 5.477|tagging_loss_fusion: 3.883|total_loss: 36.102 | Examples/sec: 71.57\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4721 | tagging_loss_video: 6.984|tagging_loss_audio: 9.797|tagging_loss_text: 13.726|tagging_loss_image: 5.209|tagging_loss_fusion: 5.982|total_loss: 41.698 | 62.95 Examples/sec\n",
      "INFO:tensorflow:training step 4722 | tagging_loss_video: 4.590|tagging_loss_audio: 10.012|tagging_loss_text: 12.242|tagging_loss_image: 6.584|tagging_loss_fusion: 3.777|total_loss: 37.205 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 4723 | tagging_loss_video: 6.601|tagging_loss_audio: 8.996|tagging_loss_text: 14.584|tagging_loss_image: 6.634|tagging_loss_fusion: 6.146|total_loss: 42.961 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 4724 | tagging_loss_video: 6.089|tagging_loss_audio: 10.391|tagging_loss_text: 19.499|tagging_loss_image: 6.524|tagging_loss_fusion: 5.118|total_loss: 47.620 | 63.10 Examples/sec\n",
      "INFO:tensorflow:training step 4725 | tagging_loss_video: 6.695|tagging_loss_audio: 11.040|tagging_loss_text: 16.008|tagging_loss_image: 6.969|tagging_loss_fusion: 8.135|total_loss: 48.847 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 4726 | tagging_loss_video: 5.812|tagging_loss_audio: 10.098|tagging_loss_text: 15.902|tagging_loss_image: 5.510|tagging_loss_fusion: 3.889|total_loss: 41.212 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 4727 | tagging_loss_video: 6.312|tagging_loss_audio: 8.871|tagging_loss_text: 16.652|tagging_loss_image: 5.848|tagging_loss_fusion: 7.115|total_loss: 44.798 | 64.83 Examples/sec\n",
      "INFO:tensorflow:training step 4728 | tagging_loss_video: 5.831|tagging_loss_audio: 9.685|tagging_loss_text: 13.404|tagging_loss_image: 5.997|tagging_loss_fusion: 5.977|total_loss: 40.895 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 4729 | tagging_loss_video: 6.505|tagging_loss_audio: 9.879|tagging_loss_text: 16.460|tagging_loss_image: 6.308|tagging_loss_fusion: 6.022|total_loss: 45.174 | 68.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4730 |tagging_loss_video: 6.519|tagging_loss_audio: 9.501|tagging_loss_text: 13.314|tagging_loss_image: 4.884|tagging_loss_fusion: 4.829|total_loss: 39.048 | Examples/sec: 68.94\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4731 | tagging_loss_video: 6.852|tagging_loss_audio: 10.785|tagging_loss_text: 18.571|tagging_loss_image: 6.684|tagging_loss_fusion: 7.450|total_loss: 50.343 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 4732 | tagging_loss_video: 6.856|tagging_loss_audio: 8.888|tagging_loss_text: 17.741|tagging_loss_image: 6.593|tagging_loss_fusion: 7.042|total_loss: 47.119 | 66.62 Examples/sec\n",
      "INFO:tensorflow:training step 4733 | tagging_loss_video: 5.640|tagging_loss_audio: 9.319|tagging_loss_text: 13.106|tagging_loss_image: 5.537|tagging_loss_fusion: 4.791|total_loss: 38.393 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 4734 | tagging_loss_video: 4.672|tagging_loss_audio: 8.810|tagging_loss_text: 16.328|tagging_loss_image: 6.014|tagging_loss_fusion: 3.353|total_loss: 39.178 | 72.54 Examples/sec\n",
      "INFO:tensorflow:training step 4735 | tagging_loss_video: 5.401|tagging_loss_audio: 8.841|tagging_loss_text: 10.279|tagging_loss_image: 5.944|tagging_loss_fusion: 3.941|total_loss: 34.406 | 60.04 Examples/sec\n",
      "INFO:tensorflow:training step 4736 | tagging_loss_video: 6.531|tagging_loss_audio: 10.740|tagging_loss_text: 11.742|tagging_loss_image: 6.265|tagging_loss_fusion: 5.937|total_loss: 41.215 | 66.13 Examples/sec\n",
      "INFO:tensorflow:training step 4737 | tagging_loss_video: 6.013|tagging_loss_audio: 7.530|tagging_loss_text: 14.885|tagging_loss_image: 5.698|tagging_loss_fusion: 4.921|total_loss: 39.047 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 4738 | tagging_loss_video: 6.137|tagging_loss_audio: 8.604|tagging_loss_text: 13.207|tagging_loss_image: 6.244|tagging_loss_fusion: 7.540|total_loss: 41.731 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 4739 | tagging_loss_video: 5.135|tagging_loss_audio: 8.122|tagging_loss_text: 15.252|tagging_loss_image: 5.157|tagging_loss_fusion: 3.691|total_loss: 37.356 | 67.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4740 |tagging_loss_video: 5.943|tagging_loss_audio: 8.967|tagging_loss_text: 15.505|tagging_loss_image: 5.822|tagging_loss_fusion: 4.903|total_loss: 41.140 | Examples/sec: 71.75\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4741 | tagging_loss_video: 5.862|tagging_loss_audio: 7.905|tagging_loss_text: 16.489|tagging_loss_image: 5.898|tagging_loss_fusion: 4.269|total_loss: 40.424 | 65.23 Examples/sec\n",
      "INFO:tensorflow:training step 4742 | tagging_loss_video: 5.876|tagging_loss_audio: 8.336|tagging_loss_text: 14.560|tagging_loss_image: 5.297|tagging_loss_fusion: 4.834|total_loss: 38.903 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 4743 | tagging_loss_video: 6.277|tagging_loss_audio: 9.231|tagging_loss_text: 17.448|tagging_loss_image: 6.445|tagging_loss_fusion: 6.119|total_loss: 45.521 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 4744 | tagging_loss_video: 6.575|tagging_loss_audio: 10.395|tagging_loss_text: 16.072|tagging_loss_image: 4.973|tagging_loss_fusion: 4.621|total_loss: 42.635 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 4745 | tagging_loss_video: 5.873|tagging_loss_audio: 10.198|tagging_loss_text: 14.549|tagging_loss_image: 5.838|tagging_loss_fusion: 5.591|total_loss: 42.049 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 4746 | tagging_loss_video: 5.260|tagging_loss_audio: 9.074|tagging_loss_text: 15.541|tagging_loss_image: 4.829|tagging_loss_fusion: 4.790|total_loss: 39.494 | 60.49 Examples/sec\n",
      "INFO:tensorflow:training step 4747 | tagging_loss_video: 5.991|tagging_loss_audio: 8.141|tagging_loss_text: 13.338|tagging_loss_image: 4.394|tagging_loss_fusion: 4.547|total_loss: 36.411 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 4748 | tagging_loss_video: 5.461|tagging_loss_audio: 8.829|tagging_loss_text: 13.276|tagging_loss_image: 5.895|tagging_loss_fusion: 4.882|total_loss: 38.343 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 4749 | tagging_loss_video: 4.296|tagging_loss_audio: 9.856|tagging_loss_text: 16.213|tagging_loss_image: 4.613|tagging_loss_fusion: 2.884|total_loss: 37.862 | 64.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4750 |tagging_loss_video: 6.435|tagging_loss_audio: 9.921|tagging_loss_text: 16.016|tagging_loss_image: 5.721|tagging_loss_fusion: 6.312|total_loss: 44.404 | Examples/sec: 72.04\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4751 | tagging_loss_video: 6.295|tagging_loss_audio: 8.722|tagging_loss_text: 17.493|tagging_loss_image: 6.044|tagging_loss_fusion: 4.509|total_loss: 43.064 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 4752 | tagging_loss_video: 6.895|tagging_loss_audio: 9.662|tagging_loss_text: 14.102|tagging_loss_image: 5.151|tagging_loss_fusion: 5.913|total_loss: 41.722 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 4753 | tagging_loss_video: 6.155|tagging_loss_audio: 9.367|tagging_loss_text: 13.706|tagging_loss_image: 5.422|tagging_loss_fusion: 3.958|total_loss: 38.607 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 4754 | tagging_loss_video: 4.755|tagging_loss_audio: 10.461|tagging_loss_text: 17.119|tagging_loss_image: 5.833|tagging_loss_fusion: 3.567|total_loss: 41.734 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 4755 | tagging_loss_video: 5.516|tagging_loss_audio: 9.561|tagging_loss_text: 14.034|tagging_loss_image: 4.949|tagging_loss_fusion: 3.203|total_loss: 37.263 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 4756 | tagging_loss_video: 5.638|tagging_loss_audio: 11.081|tagging_loss_text: 12.196|tagging_loss_image: 5.552|tagging_loss_fusion: 3.394|total_loss: 37.861 | 66.01 Examples/sec\n",
      "INFO:tensorflow:training step 4757 | tagging_loss_video: 4.904|tagging_loss_audio: 8.944|tagging_loss_text: 16.488|tagging_loss_image: 5.617|tagging_loss_fusion: 3.395|total_loss: 39.348 | 65.00 Examples/sec\n",
      "INFO:tensorflow:training step 4758 | tagging_loss_video: 5.642|tagging_loss_audio: 9.751|tagging_loss_text: 16.473|tagging_loss_image: 5.053|tagging_loss_fusion: 3.827|total_loss: 40.745 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 4759 | tagging_loss_video: 6.019|tagging_loss_audio: 9.496|tagging_loss_text: 18.855|tagging_loss_image: 5.063|tagging_loss_fusion: 4.210|total_loss: 43.644 | 70.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4760 |tagging_loss_video: 5.641|tagging_loss_audio: 7.537|tagging_loss_text: 14.731|tagging_loss_image: 5.132|tagging_loss_fusion: 5.499|total_loss: 38.540 | Examples/sec: 62.15\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4761 | tagging_loss_video: 5.940|tagging_loss_audio: 7.234|tagging_loss_text: 13.511|tagging_loss_image: 4.920|tagging_loss_fusion: 4.577|total_loss: 36.182 | 72.50 Examples/sec\n",
      "INFO:tensorflow:training step 4762 | tagging_loss_video: 6.793|tagging_loss_audio: 10.111|tagging_loss_text: 13.135|tagging_loss_image: 5.008|tagging_loss_fusion: 5.366|total_loss: 40.414 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 4763 | tagging_loss_video: 4.529|tagging_loss_audio: 9.271|tagging_loss_text: 15.532|tagging_loss_image: 5.175|tagging_loss_fusion: 3.229|total_loss: 37.736 | 65.04 Examples/sec\n",
      "INFO:tensorflow:training step 4764 | tagging_loss_video: 5.365|tagging_loss_audio: 9.534|tagging_loss_text: 16.256|tagging_loss_image: 6.245|tagging_loss_fusion: 3.629|total_loss: 41.029 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 4765 | tagging_loss_video: 6.188|tagging_loss_audio: 9.794|tagging_loss_text: 15.355|tagging_loss_image: 5.086|tagging_loss_fusion: 4.682|total_loss: 41.106 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 4766 | tagging_loss_video: 5.370|tagging_loss_audio: 8.809|tagging_loss_text: 13.609|tagging_loss_image: 4.462|tagging_loss_fusion: 3.039|total_loss: 35.289 | 65.71 Examples/sec\n",
      "INFO:tensorflow:training step 4767 | tagging_loss_video: 5.627|tagging_loss_audio: 8.832|tagging_loss_text: 12.796|tagging_loss_image: 6.442|tagging_loss_fusion: 4.783|total_loss: 38.480 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 4768 | tagging_loss_video: 5.374|tagging_loss_audio: 9.140|tagging_loss_text: 14.123|tagging_loss_image: 5.516|tagging_loss_fusion: 3.373|total_loss: 37.526 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 4769 | tagging_loss_video: 5.357|tagging_loss_audio: 7.964|tagging_loss_text: 18.023|tagging_loss_image: 5.141|tagging_loss_fusion: 4.796|total_loss: 41.280 | 71.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4770 |tagging_loss_video: 5.876|tagging_loss_audio: 8.885|tagging_loss_text: 16.035|tagging_loss_image: 5.917|tagging_loss_fusion: 6.838|total_loss: 43.550 | Examples/sec: 71.89\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.78 | precision@0.5: 0.88 |recall@0.1: 0.95 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 4771 | tagging_loss_video: 5.690|tagging_loss_audio: 8.917|tagging_loss_text: 17.036|tagging_loss_image: 5.182|tagging_loss_fusion: 6.816|total_loss: 43.641 | 59.58 Examples/sec\n",
      "INFO:tensorflow:training step 4772 | tagging_loss_video: 5.599|tagging_loss_audio: 9.264|tagging_loss_text: 16.072|tagging_loss_image: 4.705|tagging_loss_fusion: 4.077|total_loss: 39.717 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 4773 | tagging_loss_video: 6.218|tagging_loss_audio: 9.164|tagging_loss_text: 15.641|tagging_loss_image: 5.028|tagging_loss_fusion: 5.611|total_loss: 41.662 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 4774 | tagging_loss_video: 4.622|tagging_loss_audio: 7.876|tagging_loss_text: 16.243|tagging_loss_image: 4.870|tagging_loss_fusion: 3.384|total_loss: 36.996 | 63.39 Examples/sec\n",
      "INFO:tensorflow:training step 4775 | tagging_loss_video: 5.318|tagging_loss_audio: 9.300|tagging_loss_text: 15.271|tagging_loss_image: 5.297|tagging_loss_fusion: 4.539|total_loss: 39.725 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 4776 | tagging_loss_video: 5.164|tagging_loss_audio: 8.634|tagging_loss_text: 13.776|tagging_loss_image: 5.294|tagging_loss_fusion: 3.938|total_loss: 36.806 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 4777 | tagging_loss_video: 5.347|tagging_loss_audio: 9.016|tagging_loss_text: 16.741|tagging_loss_image: 5.280|tagging_loss_fusion: 4.034|total_loss: 40.417 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 4778 | tagging_loss_video: 5.128|tagging_loss_audio: 10.488|tagging_loss_text: 14.796|tagging_loss_image: 5.832|tagging_loss_fusion: 4.127|total_loss: 40.371 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 4779 | tagging_loss_video: 5.159|tagging_loss_audio: 8.290|tagging_loss_text: 16.572|tagging_loss_image: 5.330|tagging_loss_fusion: 3.589|total_loss: 38.940 | 63.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4780 |tagging_loss_video: 5.504|tagging_loss_audio: 8.623|tagging_loss_text: 14.755|tagging_loss_image: 5.870|tagging_loss_fusion: 3.689|total_loss: 38.441 | Examples/sec: 70.31\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4781 | tagging_loss_video: 6.263|tagging_loss_audio: 9.160|tagging_loss_text: 14.095|tagging_loss_image: 5.347|tagging_loss_fusion: 5.160|total_loss: 40.025 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 4782 | tagging_loss_video: 5.775|tagging_loss_audio: 8.612|tagging_loss_text: 10.783|tagging_loss_image: 5.932|tagging_loss_fusion: 5.417|total_loss: 36.519 | 60.29 Examples/sec\n",
      "INFO:tensorflow:training step 4783 | tagging_loss_video: 5.537|tagging_loss_audio: 7.790|tagging_loss_text: 16.230|tagging_loss_image: 5.926|tagging_loss_fusion: 3.799|total_loss: 39.283 | 72.07 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4784 | tagging_loss_video: 5.796|tagging_loss_audio: 8.396|tagging_loss_text: 9.058|tagging_loss_image: 4.713|tagging_loss_fusion: 4.079|total_loss: 32.041 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 4785 | tagging_loss_video: 6.269|tagging_loss_audio: 8.857|tagging_loss_text: 15.907|tagging_loss_image: 5.486|tagging_loss_fusion: 5.665|total_loss: 42.185 | 61.05 Examples/sec\n",
      "INFO:tensorflow:training step 4786 | tagging_loss_video: 5.847|tagging_loss_audio: 8.367|tagging_loss_text: 18.089|tagging_loss_image: 5.191|tagging_loss_fusion: 3.182|total_loss: 40.677 | 72.49 Examples/sec\n",
      "INFO:tensorflow:training step 4787 | tagging_loss_video: 5.076|tagging_loss_audio: 8.976|tagging_loss_text: 16.718|tagging_loss_image: 5.405|tagging_loss_fusion: 4.873|total_loss: 41.047 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 4788 | tagging_loss_video: 6.757|tagging_loss_audio: 8.982|tagging_loss_text: 13.884|tagging_loss_image: 6.609|tagging_loss_fusion: 6.158|total_loss: 42.391 | 66.39 Examples/sec\n",
      "INFO:tensorflow:training step 4789 | tagging_loss_video: 6.177|tagging_loss_audio: 8.184|tagging_loss_text: 13.459|tagging_loss_image: 4.467|tagging_loss_fusion: 4.370|total_loss: 36.656 | 69.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4790 |tagging_loss_video: 5.799|tagging_loss_audio: 9.017|tagging_loss_text: 13.717|tagging_loss_image: 5.432|tagging_loss_fusion: 5.763|total_loss: 39.727 | Examples/sec: 70.42\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.95 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4791 | tagging_loss_video: 6.039|tagging_loss_audio: 9.402|tagging_loss_text: 16.775|tagging_loss_image: 5.225|tagging_loss_fusion: 5.225|total_loss: 42.668 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 4792 | tagging_loss_video: 5.446|tagging_loss_audio: 8.529|tagging_loss_text: 13.963|tagging_loss_image: 5.715|tagging_loss_fusion: 4.834|total_loss: 38.487 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 4793 | tagging_loss_video: 5.465|tagging_loss_audio: 8.616|tagging_loss_text: 18.201|tagging_loss_image: 5.724|tagging_loss_fusion: 4.865|total_loss: 42.871 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 4794 | tagging_loss_video: 6.418|tagging_loss_audio: 9.395|tagging_loss_text: 16.341|tagging_loss_image: 4.989|tagging_loss_fusion: 6.068|total_loss: 43.211 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 4795 | tagging_loss_video: 6.302|tagging_loss_audio: 10.030|tagging_loss_text: 16.330|tagging_loss_image: 5.436|tagging_loss_fusion: 4.714|total_loss: 42.813 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 4796 | tagging_loss_video: 6.308|tagging_loss_audio: 9.939|tagging_loss_text: 13.783|tagging_loss_image: 6.252|tagging_loss_fusion: 5.471|total_loss: 41.753 | 61.17 Examples/sec\n",
      "INFO:tensorflow:training step 4797 | tagging_loss_video: 6.037|tagging_loss_audio: 8.764|tagging_loss_text: 15.439|tagging_loss_image: 6.166|tagging_loss_fusion: 5.530|total_loss: 41.937 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 4798 | tagging_loss_video: 5.904|tagging_loss_audio: 9.047|tagging_loss_text: 16.721|tagging_loss_image: 7.333|tagging_loss_fusion: 5.211|total_loss: 44.215 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 4799 | tagging_loss_video: 5.919|tagging_loss_audio: 7.747|tagging_loss_text: 14.443|tagging_loss_image: 4.991|tagging_loss_fusion: 4.495|total_loss: 37.595 | 66.63 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4800 |tagging_loss_video: 5.596|tagging_loss_audio: 7.797|tagging_loss_text: 15.668|tagging_loss_image: 3.640|tagging_loss_fusion: 4.232|total_loss: 36.933 | Examples/sec: 70.06\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4801 | tagging_loss_video: 5.259|tagging_loss_audio: 8.451|tagging_loss_text: 14.775|tagging_loss_image: 4.895|tagging_loss_fusion: 4.114|total_loss: 37.494 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 4802 | tagging_loss_video: 5.336|tagging_loss_audio: 7.235|tagging_loss_text: 12.759|tagging_loss_image: 4.402|tagging_loss_fusion: 4.345|total_loss: 34.077 | 65.38 Examples/sec\n",
      "INFO:tensorflow:training step 4803 | tagging_loss_video: 5.494|tagging_loss_audio: 9.181|tagging_loss_text: 16.845|tagging_loss_image: 5.977|tagging_loss_fusion: 4.943|total_loss: 42.440 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 4804 | tagging_loss_video: 5.177|tagging_loss_audio: 8.018|tagging_loss_text: 13.168|tagging_loss_image: 5.806|tagging_loss_fusion: 4.135|total_loss: 36.304 | 66.57 Examples/sec\n",
      "INFO:tensorflow:training step 4805 | tagging_loss_video: 6.428|tagging_loss_audio: 8.214|tagging_loss_text: 15.966|tagging_loss_image: 4.203|tagging_loss_fusion: 4.379|total_loss: 39.190 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 4806 | tagging_loss_video: 5.784|tagging_loss_audio: 7.437|tagging_loss_text: 15.301|tagging_loss_image: 3.692|tagging_loss_fusion: 4.282|total_loss: 36.495 | 66.78 Examples/sec\n",
      "INFO:tensorflow:training step 4807 | tagging_loss_video: 5.640|tagging_loss_audio: 9.431|tagging_loss_text: 17.700|tagging_loss_image: 6.369|tagging_loss_fusion: 4.833|total_loss: 43.973 | 65.08 Examples/sec\n",
      "INFO:tensorflow:training step 4808 | tagging_loss_video: 5.514|tagging_loss_audio: 8.219|tagging_loss_text: 17.781|tagging_loss_image: 5.204|tagging_loss_fusion: 4.663|total_loss: 41.381 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 4809 | tagging_loss_video: 5.102|tagging_loss_audio: 7.983|tagging_loss_text: 13.621|tagging_loss_image: 4.785|tagging_loss_fusion: 4.032|total_loss: 35.525 | 71.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4810 |tagging_loss_video: 5.211|tagging_loss_audio: 7.628|tagging_loss_text: 14.248|tagging_loss_image: 4.467|tagging_loss_fusion: 3.827|total_loss: 35.382 | Examples/sec: 62.37\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 4811 | tagging_loss_video: 5.787|tagging_loss_audio: 8.447|tagging_loss_text: 13.112|tagging_loss_image: 6.039|tagging_loss_fusion: 5.018|total_loss: 38.402 | 67.10 Examples/sec\n",
      "INFO:tensorflow:training step 4812 | tagging_loss_video: 5.722|tagging_loss_audio: 9.367|tagging_loss_text: 15.967|tagging_loss_image: 5.467|tagging_loss_fusion: 3.711|total_loss: 40.234 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4813 | tagging_loss_video: 6.314|tagging_loss_audio: 7.884|tagging_loss_text: 16.060|tagging_loss_image: 4.268|tagging_loss_fusion: 4.410|total_loss: 38.935 | 68.40 Examples/sec\n",
      "INFO:tensorflow:training step 4814 | tagging_loss_video: 5.378|tagging_loss_audio: 8.375|tagging_loss_text: 12.732|tagging_loss_image: 5.500|tagging_loss_fusion: 3.962|total_loss: 35.947 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 4815 | tagging_loss_video: 5.959|tagging_loss_audio: 8.428|tagging_loss_text: 16.513|tagging_loss_image: 6.414|tagging_loss_fusion: 5.857|total_loss: 43.170 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 4816 | tagging_loss_video: 5.227|tagging_loss_audio: 7.821|tagging_loss_text: 14.341|tagging_loss_image: 5.898|tagging_loss_fusion: 4.109|total_loss: 37.395 | 63.68 Examples/sec\n",
      "INFO:tensorflow:training step 4817 | tagging_loss_video: 4.163|tagging_loss_audio: 7.753|tagging_loss_text: 11.727|tagging_loss_image: 6.254|tagging_loss_fusion: 3.539|total_loss: 33.435 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 4818 | tagging_loss_video: 6.091|tagging_loss_audio: 8.424|tagging_loss_text: 13.849|tagging_loss_image: 4.464|tagging_loss_fusion: 6.208|total_loss: 39.037 | 71.73 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 4819.\n",
      "INFO:tensorflow:training step 4819 | tagging_loss_video: 5.998|tagging_loss_audio: 8.097|tagging_loss_text: 17.238|tagging_loss_image: 5.035|tagging_loss_fusion: 4.978|total_loss: 41.346 | 53.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4820 |tagging_loss_video: 6.743|tagging_loss_audio: 9.461|tagging_loss_text: 18.789|tagging_loss_image: 5.807|tagging_loss_fusion: 6.117|total_loss: 46.917 | Examples/sec: 57.27\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4821 | tagging_loss_video: 5.396|tagging_loss_audio: 9.098|tagging_loss_text: 14.736|tagging_loss_image: 5.254|tagging_loss_fusion: 3.580|total_loss: 38.064 | 64.06 Examples/sec\n",
      "INFO:tensorflow:training step 4822 | tagging_loss_video: 4.728|tagging_loss_audio: 8.653|tagging_loss_text: 13.515|tagging_loss_image: 6.193|tagging_loss_fusion: 3.615|total_loss: 36.704 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 4823 | tagging_loss_video: 6.679|tagging_loss_audio: 9.665|tagging_loss_text: 15.705|tagging_loss_image: 6.027|tagging_loss_fusion: 5.095|total_loss: 43.171 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 4824 | tagging_loss_video: 5.371|tagging_loss_audio: 8.881|tagging_loss_text: 16.462|tagging_loss_image: 5.358|tagging_loss_fusion: 6.816|total_loss: 42.888 | 69.38 Examples/sec\n",
      "INFO:tensorflow:training step 4825 | tagging_loss_video: 4.571|tagging_loss_audio: 7.820|tagging_loss_text: 12.447|tagging_loss_image: 4.911|tagging_loss_fusion: 4.552|total_loss: 34.300 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 4826 | tagging_loss_video: 5.738|tagging_loss_audio: 8.304|tagging_loss_text: 12.001|tagging_loss_image: 6.170|tagging_loss_fusion: 5.148|total_loss: 37.362 | 72.04 Examples/sec\n",
      "INFO:tensorflow:training step 4827 | tagging_loss_video: 5.065|tagging_loss_audio: 8.499|tagging_loss_text: 14.180|tagging_loss_image: 4.896|tagging_loss_fusion: 3.928|total_loss: 36.568 | 59.12 Examples/sec\n",
      "INFO:tensorflow:training step 4828 | tagging_loss_video: 4.055|tagging_loss_audio: 8.709|tagging_loss_text: 16.569|tagging_loss_image: 4.994|tagging_loss_fusion: 2.036|total_loss: 36.362 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 4829 | tagging_loss_video: 3.798|tagging_loss_audio: 8.074|tagging_loss_text: 12.519|tagging_loss_image: 4.666|tagging_loss_fusion: 3.102|total_loss: 32.159 | 71.86 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4830 |tagging_loss_video: 4.571|tagging_loss_audio: 7.946|tagging_loss_text: 12.196|tagging_loss_image: 5.604|tagging_loss_fusion: 3.877|total_loss: 34.194 | Examples/sec: 64.64\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4831 | tagging_loss_video: 6.563|tagging_loss_audio: 9.689|tagging_loss_text: 17.342|tagging_loss_image: 4.499|tagging_loss_fusion: 4.561|total_loss: 42.653 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 4832 | tagging_loss_video: 5.711|tagging_loss_audio: 8.291|tagging_loss_text: 12.904|tagging_loss_image: 6.377|tagging_loss_fusion: 4.264|total_loss: 37.547 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 4833 | tagging_loss_video: 6.219|tagging_loss_audio: 8.931|tagging_loss_text: 14.661|tagging_loss_image: 5.608|tagging_loss_fusion: 6.546|total_loss: 41.965 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 4834 | tagging_loss_video: 5.734|tagging_loss_audio: 9.547|tagging_loss_text: 14.099|tagging_loss_image: 6.016|tagging_loss_fusion: 5.985|total_loss: 41.381 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 4835 | tagging_loss_video: 5.180|tagging_loss_audio: 8.409|tagging_loss_text: 14.402|tagging_loss_image: 5.983|tagging_loss_fusion: 4.678|total_loss: 38.652 | 60.55 Examples/sec\n",
      "INFO:tensorflow:training step 4836 | tagging_loss_video: 5.081|tagging_loss_audio: 8.136|tagging_loss_text: 13.992|tagging_loss_image: 4.756|tagging_loss_fusion: 2.977|total_loss: 34.942 | 67.00 Examples/sec\n",
      "INFO:tensorflow:training step 4837 | tagging_loss_video: 5.191|tagging_loss_audio: 8.306|tagging_loss_text: 13.722|tagging_loss_image: 3.930|tagging_loss_fusion: 3.214|total_loss: 34.364 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 4838 | tagging_loss_video: 5.650|tagging_loss_audio: 8.385|tagging_loss_text: 15.576|tagging_loss_image: 4.948|tagging_loss_fusion: 4.502|total_loss: 39.061 | 59.58 Examples/sec\n",
      "INFO:tensorflow:training step 4839 | tagging_loss_video: 5.408|tagging_loss_audio: 8.913|tagging_loss_text: 13.552|tagging_loss_image: 3.914|tagging_loss_fusion: 4.407|total_loss: 36.194 | 69.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4840 |tagging_loss_video: 7.291|tagging_loss_audio: 7.776|tagging_loss_text: 12.864|tagging_loss_image: 4.445|tagging_loss_fusion: 6.838|total_loss: 39.213 | Examples/sec: 69.75\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.95 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4841 | tagging_loss_video: 5.732|tagging_loss_audio: 9.165|tagging_loss_text: 16.748|tagging_loss_image: 5.297|tagging_loss_fusion: 5.189|total_loss: 42.131 | 66.83 Examples/sec\n",
      "INFO:tensorflow:training step 4842 | tagging_loss_video: 4.127|tagging_loss_audio: 8.515|tagging_loss_text: 14.163|tagging_loss_image: 4.272|tagging_loss_fusion: 2.311|total_loss: 33.388 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 4843 | tagging_loss_video: 5.963|tagging_loss_audio: 8.737|tagging_loss_text: 15.629|tagging_loss_image: 4.010|tagging_loss_fusion: 4.873|total_loss: 39.213 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 4844 | tagging_loss_video: 5.984|tagging_loss_audio: 9.851|tagging_loss_text: 17.174|tagging_loss_image: 4.970|tagging_loss_fusion: 4.135|total_loss: 42.114 | 68.49 Examples/sec\n",
      "INFO:tensorflow:training step 4845 | tagging_loss_video: 6.032|tagging_loss_audio: 8.765|tagging_loss_text: 17.424|tagging_loss_image: 5.988|tagging_loss_fusion: 4.141|total_loss: 42.351 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 4846 | tagging_loss_video: 6.042|tagging_loss_audio: 8.629|tagging_loss_text: 16.363|tagging_loss_image: 5.816|tagging_loss_fusion: 4.774|total_loss: 41.624 | 62.94 Examples/sec\n",
      "INFO:tensorflow:training step 4847 | tagging_loss_video: 5.821|tagging_loss_audio: 9.615|tagging_loss_text: 15.182|tagging_loss_image: 5.849|tagging_loss_fusion: 6.802|total_loss: 43.269 | 68.22 Examples/sec\n",
      "INFO:tensorflow:training step 4848 | tagging_loss_video: 6.740|tagging_loss_audio: 8.764|tagging_loss_text: 17.138|tagging_loss_image: 4.643|tagging_loss_fusion: 4.756|total_loss: 42.041 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 4849 | tagging_loss_video: 6.542|tagging_loss_audio: 10.233|tagging_loss_text: 15.121|tagging_loss_image: 6.140|tagging_loss_fusion: 5.569|total_loss: 43.605 | 64.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4850 |tagging_loss_video: 6.486|tagging_loss_audio: 9.010|tagging_loss_text: 20.088|tagging_loss_image: 6.706|tagging_loss_fusion: 5.426|total_loss: 47.714 | Examples/sec: 66.43\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4851 | tagging_loss_video: 5.816|tagging_loss_audio: 8.455|tagging_loss_text: 14.308|tagging_loss_image: 5.288|tagging_loss_fusion: 4.862|total_loss: 38.729 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 4852 | tagging_loss_video: 5.360|tagging_loss_audio: 10.011|tagging_loss_text: 18.000|tagging_loss_image: 5.916|tagging_loss_fusion: 4.648|total_loss: 43.934 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 4853 | tagging_loss_video: 5.745|tagging_loss_audio: 10.091|tagging_loss_text: 13.844|tagging_loss_image: 5.243|tagging_loss_fusion: 3.543|total_loss: 38.466 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 4854 | tagging_loss_video: 6.279|tagging_loss_audio: 10.106|tagging_loss_text: 16.685|tagging_loss_image: 6.068|tagging_loss_fusion: 4.334|total_loss: 43.470 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 4855 | tagging_loss_video: 6.042|tagging_loss_audio: 7.945|tagging_loss_text: 18.301|tagging_loss_image: 5.740|tagging_loss_fusion: 3.602|total_loss: 41.630 | 65.32 Examples/sec\n",
      "INFO:tensorflow:training step 4856 | tagging_loss_video: 6.610|tagging_loss_audio: 10.482|tagging_loss_text: 16.399|tagging_loss_image: 4.688|tagging_loss_fusion: 5.316|total_loss: 43.495 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 4857 | tagging_loss_video: 4.454|tagging_loss_audio: 8.997|tagging_loss_text: 17.068|tagging_loss_image: 5.542|tagging_loss_fusion: 3.984|total_loss: 40.046 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 4858 | tagging_loss_video: 6.242|tagging_loss_audio: 9.739|tagging_loss_text: 14.089|tagging_loss_image: 5.813|tagging_loss_fusion: 4.837|total_loss: 40.720 | 67.53 Examples/sec\n",
      "INFO:tensorflow:training step 4859 | tagging_loss_video: 4.810|tagging_loss_audio: 8.814|tagging_loss_text: 12.357|tagging_loss_image: 4.757|tagging_loss_fusion: 3.555|total_loss: 34.292 | 70.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4860 |tagging_loss_video: 6.504|tagging_loss_audio: 9.079|tagging_loss_text: 17.350|tagging_loss_image: 5.850|tagging_loss_fusion: 6.187|total_loss: 44.970 | Examples/sec: 64.06\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4861 | tagging_loss_video: 6.153|tagging_loss_audio: 11.526|tagging_loss_text: 13.691|tagging_loss_image: 6.359|tagging_loss_fusion: 5.299|total_loss: 43.028 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 4862 | tagging_loss_video: 5.429|tagging_loss_audio: 9.676|tagging_loss_text: 12.378|tagging_loss_image: 4.094|tagging_loss_fusion: 4.161|total_loss: 35.737 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 4863 | tagging_loss_video: 6.155|tagging_loss_audio: 8.892|tagging_loss_text: 16.748|tagging_loss_image: 6.061|tagging_loss_fusion: 4.793|total_loss: 42.649 | 61.53 Examples/sec\n",
      "INFO:tensorflow:training step 4864 | tagging_loss_video: 5.577|tagging_loss_audio: 11.196|tagging_loss_text: 17.538|tagging_loss_image: 5.869|tagging_loss_fusion: 4.487|total_loss: 44.668 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 4865 | tagging_loss_video: 4.955|tagging_loss_audio: 9.099|tagging_loss_text: 13.837|tagging_loss_image: 4.742|tagging_loss_fusion: 3.864|total_loss: 36.497 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 4866 | tagging_loss_video: 5.477|tagging_loss_audio: 10.198|tagging_loss_text: 13.339|tagging_loss_image: 3.775|tagging_loss_fusion: 4.216|total_loss: 37.005 | 65.16 Examples/sec\n",
      "INFO:tensorflow:training step 4867 | tagging_loss_video: 5.835|tagging_loss_audio: 10.292|tagging_loss_text: 16.080|tagging_loss_image: 5.897|tagging_loss_fusion: 4.867|total_loss: 42.971 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 4868 | tagging_loss_video: 6.167|tagging_loss_audio: 10.029|tagging_loss_text: 15.773|tagging_loss_image: 6.030|tagging_loss_fusion: 6.805|total_loss: 44.804 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 4869 | tagging_loss_video: 5.643|tagging_loss_audio: 10.057|tagging_loss_text: 17.965|tagging_loss_image: 4.924|tagging_loss_fusion: 5.214|total_loss: 43.802 | 70.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4870 |tagging_loss_video: 7.094|tagging_loss_audio: 10.236|tagging_loss_text: 14.829|tagging_loss_image: 5.341|tagging_loss_fusion: 5.568|total_loss: 43.069 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4871 | tagging_loss_video: 6.568|tagging_loss_audio: 9.816|tagging_loss_text: 12.569|tagging_loss_image: 4.974|tagging_loss_fusion: 5.235|total_loss: 39.163 | 62.61 Examples/sec\n",
      "INFO:tensorflow:training step 4872 | tagging_loss_video: 5.260|tagging_loss_audio: 8.191|tagging_loss_text: 15.739|tagging_loss_image: 5.209|tagging_loss_fusion: 3.142|total_loss: 37.540 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 4873 | tagging_loss_video: 4.582|tagging_loss_audio: 8.312|tagging_loss_text: 12.944|tagging_loss_image: 5.591|tagging_loss_fusion: 2.742|total_loss: 34.171 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 4874 | tagging_loss_video: 5.255|tagging_loss_audio: 8.608|tagging_loss_text: 16.709|tagging_loss_image: 5.772|tagging_loss_fusion: 5.330|total_loss: 41.674 | 61.13 Examples/sec\n",
      "INFO:tensorflow:training step 4875 | tagging_loss_video: 6.568|tagging_loss_audio: 8.533|tagging_loss_text: 19.066|tagging_loss_image: 6.190|tagging_loss_fusion: 6.165|total_loss: 46.522 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 4876 | tagging_loss_video: 5.563|tagging_loss_audio: 9.286|tagging_loss_text: 15.543|tagging_loss_image: 6.008|tagging_loss_fusion: 3.939|total_loss: 40.339 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 4877 | tagging_loss_video: 5.122|tagging_loss_audio: 9.018|tagging_loss_text: 15.845|tagging_loss_image: 5.137|tagging_loss_fusion: 2.747|total_loss: 37.869 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 4878 | tagging_loss_video: 5.175|tagging_loss_audio: 8.316|tagging_loss_text: 15.918|tagging_loss_image: 4.436|tagging_loss_fusion: 3.039|total_loss: 36.885 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 4879 | tagging_loss_video: 6.528|tagging_loss_audio: 9.478|tagging_loss_text: 14.159|tagging_loss_image: 4.015|tagging_loss_fusion: 3.682|total_loss: 37.862 | 68.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4880 |tagging_loss_video: 5.256|tagging_loss_audio: 7.862|tagging_loss_text: 11.871|tagging_loss_image: 5.056|tagging_loss_fusion: 4.077|total_loss: 34.122 | Examples/sec: 64.42\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4881 | tagging_loss_video: 5.200|tagging_loss_audio: 8.347|tagging_loss_text: 14.559|tagging_loss_image: 5.973|tagging_loss_fusion: 4.985|total_loss: 39.062 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 4882 | tagging_loss_video: 5.832|tagging_loss_audio: 8.237|tagging_loss_text: 13.956|tagging_loss_image: 4.576|tagging_loss_fusion: 4.689|total_loss: 37.291 | 72.06 Examples/sec\n",
      "INFO:tensorflow:training step 4883 | tagging_loss_video: 5.864|tagging_loss_audio: 9.649|tagging_loss_text: 12.037|tagging_loss_image: 5.333|tagging_loss_fusion: 2.952|total_loss: 35.834 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 4884 | tagging_loss_video: 5.515|tagging_loss_audio: 8.948|tagging_loss_text: 16.518|tagging_loss_image: 5.431|tagging_loss_fusion: 4.890|total_loss: 41.301 | 63.23 Examples/sec\n",
      "INFO:tensorflow:training step 4885 | tagging_loss_video: 5.818|tagging_loss_audio: 8.962|tagging_loss_text: 16.181|tagging_loss_image: 5.423|tagging_loss_fusion: 6.260|total_loss: 42.645 | 62.88 Examples/sec\n",
      "INFO:tensorflow:training step 4886 | tagging_loss_video: 5.412|tagging_loss_audio: 8.509|tagging_loss_text: 18.003|tagging_loss_image: 5.730|tagging_loss_fusion: 4.147|total_loss: 41.801 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 4887 | tagging_loss_video: 5.522|tagging_loss_audio: 8.238|tagging_loss_text: 15.929|tagging_loss_image: 6.055|tagging_loss_fusion: 3.526|total_loss: 39.271 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 4888 | tagging_loss_video: 5.771|tagging_loss_audio: 9.803|tagging_loss_text: 16.898|tagging_loss_image: 6.015|tagging_loss_fusion: 5.683|total_loss: 44.169 | 61.45 Examples/sec\n",
      "INFO:tensorflow:training step 4889 | tagging_loss_video: 5.983|tagging_loss_audio: 9.446|tagging_loss_text: 16.040|tagging_loss_image: 6.343|tagging_loss_fusion: 5.371|total_loss: 43.183 | 70.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4890 |tagging_loss_video: 4.313|tagging_loss_audio: 9.550|tagging_loss_text: 19.005|tagging_loss_image: 5.940|tagging_loss_fusion: 3.166|total_loss: 41.974 | Examples/sec: 70.27\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4891 | tagging_loss_video: 6.388|tagging_loss_audio: 8.998|tagging_loss_text: 14.109|tagging_loss_image: 4.916|tagging_loss_fusion: 6.344|total_loss: 40.755 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 4892 | tagging_loss_video: 6.005|tagging_loss_audio: 10.128|tagging_loss_text: 15.073|tagging_loss_image: 5.948|tagging_loss_fusion: 5.410|total_loss: 42.564 | 69.07 Examples/sec\n",
      "INFO:tensorflow:training step 4893 | tagging_loss_video: 5.743|tagging_loss_audio: 10.450|tagging_loss_text: 14.236|tagging_loss_image: 6.251|tagging_loss_fusion: 4.923|total_loss: 41.603 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 4894 | tagging_loss_video: 5.559|tagging_loss_audio: 8.684|tagging_loss_text: 14.014|tagging_loss_image: 6.309|tagging_loss_fusion: 5.750|total_loss: 40.317 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 4895 | tagging_loss_video: 5.189|tagging_loss_audio: 9.894|tagging_loss_text: 13.628|tagging_loss_image: 6.428|tagging_loss_fusion: 5.573|total_loss: 40.712 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 4896 | tagging_loss_video: 5.007|tagging_loss_audio: 8.663|tagging_loss_text: 16.500|tagging_loss_image: 4.915|tagging_loss_fusion: 4.025|total_loss: 39.110 | 63.51 Examples/sec\n",
      "INFO:tensorflow:training step 4897 | tagging_loss_video: 6.925|tagging_loss_audio: 9.836|tagging_loss_text: 17.129|tagging_loss_image: 5.138|tagging_loss_fusion: 5.746|total_loss: 44.773 | 69.43 Examples/sec\n",
      "INFO:tensorflow:training step 4898 | tagging_loss_video: 5.802|tagging_loss_audio: 9.004|tagging_loss_text: 14.661|tagging_loss_image: 5.082|tagging_loss_fusion: 3.758|total_loss: 38.307 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 4899 | tagging_loss_video: 4.713|tagging_loss_audio: 7.952|tagging_loss_text: 12.282|tagging_loss_image: 3.763|tagging_loss_fusion: 2.647|total_loss: 31.357 | 64.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4900 |tagging_loss_video: 5.852|tagging_loss_audio: 8.326|tagging_loss_text: 16.243|tagging_loss_image: 5.258|tagging_loss_fusion: 4.192|total_loss: 39.870 | Examples/sec: 70.28\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4901 | tagging_loss_video: 5.984|tagging_loss_audio: 8.609|tagging_loss_text: 17.287|tagging_loss_image: 4.862|tagging_loss_fusion: 4.992|total_loss: 41.735 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 4902 | tagging_loss_video: 6.297|tagging_loss_audio: 8.192|tagging_loss_text: 12.838|tagging_loss_image: 6.043|tagging_loss_fusion: 7.306|total_loss: 40.675 | 58.10 Examples/sec\n",
      "INFO:tensorflow:training step 4903 | tagging_loss_video: 6.709|tagging_loss_audio: 10.180|tagging_loss_text: 15.548|tagging_loss_image: 5.994|tagging_loss_fusion: 6.466|total_loss: 44.897 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 4904 | tagging_loss_video: 4.230|tagging_loss_audio: 11.268|tagging_loss_text: 16.687|tagging_loss_image: 5.736|tagging_loss_fusion: 3.154|total_loss: 41.076 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 4905 | tagging_loss_video: 6.097|tagging_loss_audio: 8.870|tagging_loss_text: 16.326|tagging_loss_image: 4.168|tagging_loss_fusion: 4.277|total_loss: 39.738 | 67.08 Examples/sec\n",
      "INFO:tensorflow:training step 4906 | tagging_loss_video: 5.508|tagging_loss_audio: 9.670|tagging_loss_text: 17.072|tagging_loss_image: 4.875|tagging_loss_fusion: 3.714|total_loss: 40.840 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 4907 | tagging_loss_video: 6.031|tagging_loss_audio: 9.223|tagging_loss_text: 15.453|tagging_loss_image: 5.397|tagging_loss_fusion: 5.161|total_loss: 41.266 | 64.44 Examples/sec\n",
      "INFO:tensorflow:training step 4908 | tagging_loss_video: 5.974|tagging_loss_audio: 8.757|tagging_loss_text: 13.162|tagging_loss_image: 5.691|tagging_loss_fusion: 5.517|total_loss: 39.102 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 4909 | tagging_loss_video: 4.898|tagging_loss_audio: 8.506|tagging_loss_text: 14.042|tagging_loss_image: 5.383|tagging_loss_fusion: 3.785|total_loss: 36.614 | 68.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4910 |tagging_loss_video: 6.128|tagging_loss_audio: 8.336|tagging_loss_text: 14.326|tagging_loss_image: 5.921|tagging_loss_fusion: 6.313|total_loss: 41.023 | Examples/sec: 64.52\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 4911 | tagging_loss_video: 4.851|tagging_loss_audio: 9.062|tagging_loss_text: 12.073|tagging_loss_image: 4.170|tagging_loss_fusion: 2.858|total_loss: 33.014 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 4912 | tagging_loss_video: 5.292|tagging_loss_audio: 9.464|tagging_loss_text: 14.274|tagging_loss_image: 3.729|tagging_loss_fusion: 3.980|total_loss: 36.738 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 4913 | tagging_loss_video: 5.432|tagging_loss_audio: 7.798|tagging_loss_text: 13.010|tagging_loss_image: 5.920|tagging_loss_fusion: 5.359|total_loss: 37.519 | 65.98 Examples/sec\n",
      "INFO:tensorflow:training step 4914 | tagging_loss_video: 5.427|tagging_loss_audio: 8.398|tagging_loss_text: 16.639|tagging_loss_image: 5.206|tagging_loss_fusion: 4.058|total_loss: 39.728 | 66.41 Examples/sec\n",
      "INFO:tensorflow:training step 4915 | tagging_loss_video: 6.001|tagging_loss_audio: 8.536|tagging_loss_text: 13.092|tagging_loss_image: 4.410|tagging_loss_fusion: 4.451|total_loss: 36.491 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 4916 | tagging_loss_video: 5.021|tagging_loss_audio: 7.962|tagging_loss_text: 14.652|tagging_loss_image: 6.109|tagging_loss_fusion: 4.726|total_loss: 38.469 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 4917 | tagging_loss_video: 6.671|tagging_loss_audio: 8.657|tagging_loss_text: 18.470|tagging_loss_image: 6.130|tagging_loss_fusion: 5.056|total_loss: 44.985 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 4918 | tagging_loss_video: 5.403|tagging_loss_audio: 7.624|tagging_loss_text: 16.602|tagging_loss_image: 4.918|tagging_loss_fusion: 5.147|total_loss: 39.693 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 4919 | tagging_loss_video: 6.382|tagging_loss_audio: 8.277|tagging_loss_text: 13.336|tagging_loss_image: 6.280|tagging_loss_fusion: 7.332|total_loss: 41.608 | 69.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4920 |tagging_loss_video: 5.648|tagging_loss_audio: 10.020|tagging_loss_text: 16.930|tagging_loss_image: 5.416|tagging_loss_fusion: 4.528|total_loss: 42.542 | Examples/sec: 66.88\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 4921 | tagging_loss_video: 5.477|tagging_loss_audio: 8.835|tagging_loss_text: 16.161|tagging_loss_image: 6.036|tagging_loss_fusion: 3.761|total_loss: 40.270 | 61.46 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 4922 | tagging_loss_video: 5.966|tagging_loss_audio: 7.810|tagging_loss_text: 17.658|tagging_loss_image: 5.326|tagging_loss_fusion: 5.592|total_loss: 42.350 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 4923 | tagging_loss_video: 5.144|tagging_loss_audio: 7.158|tagging_loss_text: 13.239|tagging_loss_image: 5.820|tagging_loss_fusion: 3.087|total_loss: 34.447 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 4924 | tagging_loss_video: 5.338|tagging_loss_audio: 9.224|tagging_loss_text: 14.501|tagging_loss_image: 6.134|tagging_loss_fusion: 3.763|total_loss: 38.961 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 4925 | tagging_loss_video: 6.565|tagging_loss_audio: 9.100|tagging_loss_text: 12.515|tagging_loss_image: 4.981|tagging_loss_fusion: 6.638|total_loss: 39.800 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 4926 | tagging_loss_video: 6.232|tagging_loss_audio: 8.742|tagging_loss_text: 18.076|tagging_loss_image: 4.871|tagging_loss_fusion: 4.780|total_loss: 42.701 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 4927 | tagging_loss_video: 5.370|tagging_loss_audio: 8.300|tagging_loss_text: 18.299|tagging_loss_image: 6.572|tagging_loss_fusion: 3.903|total_loss: 42.445 | 64.14 Examples/sec\n",
      "INFO:tensorflow:training step 4928 | tagging_loss_video: 5.257|tagging_loss_audio: 8.015|tagging_loss_text: 16.020|tagging_loss_image: 4.064|tagging_loss_fusion: 4.282|total_loss: 37.638 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 4929 | tagging_loss_video: 5.827|tagging_loss_audio: 9.532|tagging_loss_text: 12.351|tagging_loss_image: 6.384|tagging_loss_fusion: 5.170|total_loss: 39.264 | 69.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4930 |tagging_loss_video: 6.261|tagging_loss_audio: 7.985|tagging_loss_text: 15.433|tagging_loss_image: 3.573|tagging_loss_fusion: 4.224|total_loss: 37.477 | Examples/sec: 67.80\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 4931 | tagging_loss_video: 4.796|tagging_loss_audio: 9.245|tagging_loss_text: 11.878|tagging_loss_image: 6.294|tagging_loss_fusion: 3.844|total_loss: 36.058 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 4932 | tagging_loss_video: 5.463|tagging_loss_audio: 7.886|tagging_loss_text: 17.561|tagging_loss_image: 3.047|tagging_loss_fusion: 3.842|total_loss: 37.799 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 4933 | tagging_loss_video: 5.593|tagging_loss_audio: 10.085|tagging_loss_text: 18.386|tagging_loss_image: 5.492|tagging_loss_fusion: 4.409|total_loss: 43.964 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 4934 | tagging_loss_video: 5.735|tagging_loss_audio: 9.398|tagging_loss_text: 17.790|tagging_loss_image: 4.805|tagging_loss_fusion: 3.682|total_loss: 41.411 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 4935 | tagging_loss_video: 5.436|tagging_loss_audio: 9.484|tagging_loss_text: 15.229|tagging_loss_image: 5.732|tagging_loss_fusion: 3.777|total_loss: 39.658 | 61.51 Examples/sec\n",
      "INFO:tensorflow:training step 4936 | tagging_loss_video: 6.969|tagging_loss_audio: 9.437|tagging_loss_text: 18.288|tagging_loss_image: 5.670|tagging_loss_fusion: 5.399|total_loss: 45.763 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 4937 | tagging_loss_video: 6.936|tagging_loss_audio: 10.120|tagging_loss_text: 15.208|tagging_loss_image: 7.006|tagging_loss_fusion: 5.854|total_loss: 45.124 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 4938 | tagging_loss_video: 4.159|tagging_loss_audio: 8.268|tagging_loss_text: 13.189|tagging_loss_image: 4.769|tagging_loss_fusion: 2.889|total_loss: 33.275 | 64.83 Examples/sec\n",
      "INFO:tensorflow:training step 4939 | tagging_loss_video: 5.051|tagging_loss_audio: 7.259|tagging_loss_text: 12.531|tagging_loss_image: 3.909|tagging_loss_fusion: 2.727|total_loss: 31.476 | 70.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4940 |tagging_loss_video: 5.139|tagging_loss_audio: 8.820|tagging_loss_text: 12.763|tagging_loss_image: 5.978|tagging_loss_fusion: 3.017|total_loss: 35.717 | Examples/sec: 71.08\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 4941 | tagging_loss_video: 4.871|tagging_loss_audio: 8.596|tagging_loss_text: 13.390|tagging_loss_image: 4.886|tagging_loss_fusion: 2.748|total_loss: 34.491 | 64.01 Examples/sec\n",
      "INFO:tensorflow:training step 4942 | tagging_loss_video: 5.268|tagging_loss_audio: 8.532|tagging_loss_text: 16.115|tagging_loss_image: 5.093|tagging_loss_fusion: 4.050|total_loss: 39.057 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 4943 | tagging_loss_video: 5.789|tagging_loss_audio: 8.440|tagging_loss_text: 14.461|tagging_loss_image: 5.816|tagging_loss_fusion: 5.938|total_loss: 40.444 | 66.70 Examples/sec\n",
      "INFO:tensorflow:training step 4944 | tagging_loss_video: 6.519|tagging_loss_audio: 8.311|tagging_loss_text: 15.289|tagging_loss_image: 4.746|tagging_loss_fusion: 5.527|total_loss: 40.393 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 4945 | tagging_loss_video: 5.169|tagging_loss_audio: 7.950|tagging_loss_text: 14.013|tagging_loss_image: 5.413|tagging_loss_fusion: 3.110|total_loss: 35.654 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 4946 | tagging_loss_video: 5.502|tagging_loss_audio: 9.360|tagging_loss_text: 16.218|tagging_loss_image: 5.177|tagging_loss_fusion: 3.300|total_loss: 39.556 | 64.56 Examples/sec\n",
      "INFO:tensorflow:training step 4947 | tagging_loss_video: 5.314|tagging_loss_audio: 8.627|tagging_loss_text: 12.998|tagging_loss_image: 4.426|tagging_loss_fusion: 4.148|total_loss: 35.514 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 4948 | tagging_loss_video: 4.771|tagging_loss_audio: 8.142|tagging_loss_text: 13.635|tagging_loss_image: 5.209|tagging_loss_fusion: 3.805|total_loss: 35.562 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 4949 | tagging_loss_video: 5.121|tagging_loss_audio: 7.295|tagging_loss_text: 13.644|tagging_loss_image: 5.056|tagging_loss_fusion: 3.970|total_loss: 35.086 | 60.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4950 |tagging_loss_video: 6.635|tagging_loss_audio: 8.105|tagging_loss_text: 13.121|tagging_loss_image: 4.688|tagging_loss_fusion: 5.248|total_loss: 37.797 | Examples/sec: 67.27\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 4951 | tagging_loss_video: 5.303|tagging_loss_audio: 9.001|tagging_loss_text: 16.724|tagging_loss_image: 5.717|tagging_loss_fusion: 3.475|total_loss: 40.220 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 4952 | tagging_loss_video: 4.805|tagging_loss_audio: 8.282|tagging_loss_text: 13.503|tagging_loss_image: 5.396|tagging_loss_fusion: 3.873|total_loss: 35.858 | 63.08 Examples/sec\n",
      "INFO:tensorflow:training step 4953 | tagging_loss_video: 5.730|tagging_loss_audio: 8.650|tagging_loss_text: 16.755|tagging_loss_image: 5.109|tagging_loss_fusion: 4.270|total_loss: 40.514 | 67.25 Examples/sec\n",
      "INFO:tensorflow:training step 4954 | tagging_loss_video: 5.426|tagging_loss_audio: 8.582|tagging_loss_text: 17.227|tagging_loss_image: 5.708|tagging_loss_fusion: 3.304|total_loss: 40.248 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 4955 | tagging_loss_video: 5.088|tagging_loss_audio: 8.289|tagging_loss_text: 13.689|tagging_loss_image: 4.311|tagging_loss_fusion: 2.836|total_loss: 34.213 | 62.34 Examples/sec\n",
      "INFO:tensorflow:training step 4956 | tagging_loss_video: 5.638|tagging_loss_audio: 7.513|tagging_loss_text: 12.385|tagging_loss_image: 5.067|tagging_loss_fusion: 4.759|total_loss: 35.363 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 4957 | tagging_loss_video: 5.016|tagging_loss_audio: 7.824|tagging_loss_text: 13.209|tagging_loss_image: 4.645|tagging_loss_fusion: 4.073|total_loss: 34.767 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 4958 | tagging_loss_video: 5.548|tagging_loss_audio: 7.480|tagging_loss_text: 17.920|tagging_loss_image: 4.771|tagging_loss_fusion: 4.139|total_loss: 39.858 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 4959 | tagging_loss_video: 4.920|tagging_loss_audio: 7.909|tagging_loss_text: 16.755|tagging_loss_image: 5.149|tagging_loss_fusion: 3.569|total_loss: 38.302 | 72.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4960 |tagging_loss_video: 6.146|tagging_loss_audio: 9.320|tagging_loss_text: 17.378|tagging_loss_image: 5.375|tagging_loss_fusion: 6.193|total_loss: 44.411 | Examples/sec: 70.63\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 4961 | tagging_loss_video: 5.803|tagging_loss_audio: 10.138|tagging_loss_text: 16.851|tagging_loss_image: 4.924|tagging_loss_fusion: 4.376|total_loss: 42.092 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 4962 | tagging_loss_video: 5.452|tagging_loss_audio: 8.434|tagging_loss_text: 15.540|tagging_loss_image: 5.922|tagging_loss_fusion: 4.507|total_loss: 39.855 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 4963 | tagging_loss_video: 5.855|tagging_loss_audio: 9.636|tagging_loss_text: 17.860|tagging_loss_image: 6.281|tagging_loss_fusion: 4.371|total_loss: 44.004 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 4964 | tagging_loss_video: 5.500|tagging_loss_audio: 8.119|tagging_loss_text: 13.305|tagging_loss_image: 5.384|tagging_loss_fusion: 5.402|total_loss: 37.710 | 65.50 Examples/sec\n",
      "INFO:tensorflow:training step 4965 | tagging_loss_video: 4.175|tagging_loss_audio: 7.261|tagging_loss_text: 10.675|tagging_loss_image: 3.934|tagging_loss_fusion: 2.103|total_loss: 28.148 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 4966 | tagging_loss_video: 5.777|tagging_loss_audio: 9.171|tagging_loss_text: 15.354|tagging_loss_image: 4.621|tagging_loss_fusion: 4.616|total_loss: 39.539 | 66.78 Examples/sec\n",
      "INFO:tensorflow:training step 4967 | tagging_loss_video: 4.377|tagging_loss_audio: 8.684|tagging_loss_text: 16.760|tagging_loss_image: 5.065|tagging_loss_fusion: 3.357|total_loss: 38.243 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 4968 | tagging_loss_video: 5.257|tagging_loss_audio: 9.365|tagging_loss_text: 16.549|tagging_loss_image: 5.333|tagging_loss_fusion: 2.838|total_loss: 39.342 | 67.78 Examples/sec\n",
      "INFO:tensorflow:training step 4969 | tagging_loss_video: 5.467|tagging_loss_audio: 8.529|tagging_loss_text: 17.782|tagging_loss_image: 4.625|tagging_loss_fusion: 3.023|total_loss: 39.426 | 69.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4970 |tagging_loss_video: 5.696|tagging_loss_audio: 8.294|tagging_loss_text: 13.476|tagging_loss_image: 4.390|tagging_loss_fusion: 5.092|total_loss: 36.948 | Examples/sec: 66.49\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 4971 | tagging_loss_video: 4.723|tagging_loss_audio: 8.242|tagging_loss_text: 15.936|tagging_loss_image: 6.150|tagging_loss_fusion: 4.382|total_loss: 39.434 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 4972 | tagging_loss_video: 5.444|tagging_loss_audio: 9.643|tagging_loss_text: 13.583|tagging_loss_image: 5.327|tagging_loss_fusion: 3.623|total_loss: 37.620 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 4973 | tagging_loss_video: 6.321|tagging_loss_audio: 8.475|tagging_loss_text: 14.674|tagging_loss_image: 4.186|tagging_loss_fusion: 3.974|total_loss: 37.629 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 4974 | tagging_loss_video: 6.030|tagging_loss_audio: 10.197|tagging_loss_text: 16.311|tagging_loss_image: 4.013|tagging_loss_fusion: 4.254|total_loss: 40.805 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 4975 | tagging_loss_video: 4.205|tagging_loss_audio: 8.331|tagging_loss_text: 16.455|tagging_loss_image: 4.936|tagging_loss_fusion: 2.606|total_loss: 36.534 | 62.48 Examples/sec\n",
      "INFO:tensorflow:training step 4976 | tagging_loss_video: 5.355|tagging_loss_audio: 9.107|tagging_loss_text: 13.991|tagging_loss_image: 5.646|tagging_loss_fusion: 4.278|total_loss: 38.377 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 4977 | tagging_loss_video: 4.786|tagging_loss_audio: 8.384|tagging_loss_text: 12.469|tagging_loss_image: 4.704|tagging_loss_fusion: 3.385|total_loss: 33.728 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 4978 | tagging_loss_video: 4.738|tagging_loss_audio: 7.862|tagging_loss_text: 12.967|tagging_loss_image: 3.923|tagging_loss_fusion: 2.187|total_loss: 31.677 | 60.73 Examples/sec\n",
      "INFO:tensorflow:training step 4979 | tagging_loss_video: 5.637|tagging_loss_audio: 9.548|tagging_loss_text: 15.736|tagging_loss_image: 6.677|tagging_loss_fusion: 5.061|total_loss: 42.659 | 69.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4980 |tagging_loss_video: 3.908|tagging_loss_audio: 8.229|tagging_loss_text: 16.225|tagging_loss_image: 5.239|tagging_loss_fusion: 2.754|total_loss: 36.356 | Examples/sec: 70.55\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 4981 | tagging_loss_video: 5.864|tagging_loss_audio: 10.165|tagging_loss_text: 15.338|tagging_loss_image: 4.461|tagging_loss_fusion: 4.527|total_loss: 40.355 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 4982 | tagging_loss_video: 5.954|tagging_loss_audio: 9.283|tagging_loss_text: 16.094|tagging_loss_image: 5.022|tagging_loss_fusion: 5.242|total_loss: 41.594 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 4983 | tagging_loss_video: 6.757|tagging_loss_audio: 9.212|tagging_loss_text: 16.335|tagging_loss_image: 5.689|tagging_loss_fusion: 6.444|total_loss: 44.438 | 64.54 Examples/sec\n",
      "INFO:tensorflow:training step 4984 | tagging_loss_video: 6.113|tagging_loss_audio: 8.666|tagging_loss_text: 20.047|tagging_loss_image: 4.391|tagging_loss_fusion: 4.283|total_loss: 43.500 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 4985 | tagging_loss_video: 5.761|tagging_loss_audio: 9.337|tagging_loss_text: 16.034|tagging_loss_image: 5.369|tagging_loss_fusion: 4.500|total_loss: 41.001 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 4986 | tagging_loss_video: 6.402|tagging_loss_audio: 9.649|tagging_loss_text: 14.297|tagging_loss_image: 5.006|tagging_loss_fusion: 4.731|total_loss: 40.084 | 66.55 Examples/sec\n",
      "INFO:tensorflow:training step 4987 | tagging_loss_video: 6.215|tagging_loss_audio: 10.518|tagging_loss_text: 17.349|tagging_loss_image: 6.288|tagging_loss_fusion: 5.445|total_loss: 45.815 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 4988 | tagging_loss_video: 4.711|tagging_loss_audio: 8.726|tagging_loss_text: 17.135|tagging_loss_image: 5.108|tagging_loss_fusion: 3.502|total_loss: 39.182 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 4989 | tagging_loss_video: 5.107|tagging_loss_audio: 9.732|tagging_loss_text: 14.423|tagging_loss_image: 5.670|tagging_loss_fusion: 5.119|total_loss: 40.050 | 65.30 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 4990 |tagging_loss_video: 4.374|tagging_loss_audio: 9.461|tagging_loss_text: 17.080|tagging_loss_image: 6.606|tagging_loss_fusion: 3.099|total_loss: 40.620 | Examples/sec: 68.41\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.90 | precision@0.5: 0.98 |recall@0.1: 0.97 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 4991 | tagging_loss_video: 5.864|tagging_loss_audio: 9.069|tagging_loss_text: 13.636|tagging_loss_image: 4.492|tagging_loss_fusion: 3.858|total_loss: 36.920 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 4992 | tagging_loss_video: 4.522|tagging_loss_audio: 9.272|tagging_loss_text: 16.631|tagging_loss_image: 5.076|tagging_loss_fusion: 3.032|total_loss: 38.532 | 63.35 Examples/sec\n",
      "INFO:tensorflow:training step 4993 | tagging_loss_video: 6.498|tagging_loss_audio: 8.594|tagging_loss_text: 16.600|tagging_loss_image: 6.506|tagging_loss_fusion: 7.689|total_loss: 45.887 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 4994 | tagging_loss_video: 7.013|tagging_loss_audio: 8.649|tagging_loss_text: 13.789|tagging_loss_image: 6.346|tagging_loss_fusion: 5.619|total_loss: 41.416 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 4995 | tagging_loss_video: 5.786|tagging_loss_audio: 8.313|tagging_loss_text: 16.894|tagging_loss_image: 6.237|tagging_loss_fusion: 4.812|total_loss: 42.043 | 66.46 Examples/sec\n",
      "INFO:tensorflow:training step 4996 | tagging_loss_video: 5.091|tagging_loss_audio: 10.951|tagging_loss_text: 14.891|tagging_loss_image: 5.358|tagging_loss_fusion: 4.321|total_loss: 40.611 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 4997 | tagging_loss_video: 5.760|tagging_loss_audio: 8.806|tagging_loss_text: 13.769|tagging_loss_image: 4.910|tagging_loss_fusion: 4.525|total_loss: 37.771 | 71.44 Examples/sec\n",
      "INFO:tensorflow:training step 4998 | tagging_loss_video: 6.153|tagging_loss_audio: 9.946|tagging_loss_text: 16.903|tagging_loss_image: 5.255|tagging_loss_fusion: 4.286|total_loss: 42.544 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 4999 | tagging_loss_video: 5.388|tagging_loss_audio: 9.470|tagging_loss_text: 14.221|tagging_loss_image: 5.371|tagging_loss_fusion: 3.471|total_loss: 37.922 | 70.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5000 |tagging_loss_video: 5.571|tagging_loss_audio: 8.948|tagging_loss_text: 14.443|tagging_loss_image: 6.367|tagging_loss_fusion: 4.276|total_loss: 39.605 | Examples/sec: 63.37\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.719|loss: 32.505|GAP: 0.734|examples_per_second: 93.248\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.736|loss: 29.108|GAP: 0.782|examples_per_second: 94.341\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 1.000|perr: 0.714|loss: 38.877|GAP: 0.714|examples_per_second: 99.379\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.773|loss: 31.564|GAP: 0.758|examples_per_second: 99.283\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.748|loss: 33.145|GAP: 0.748|examples_per_second: 100.298\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.694|loss: 31.915|GAP: 0.725|examples_per_second: 97.986\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.741|loss: 35.158|GAP: 0.733|examples_per_second: 100.502\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.763|loss: 31.420|GAP: 0.763|examples_per_second: 99.168\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 1.000|perr: 0.723|loss: 33.786|GAP: 0.737|examples_per_second: 100.733\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.688|loss: 34.898|GAP: 0.708|examples_per_second: 98.850\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.745|loss: 26.811|GAP: 0.784|examples_per_second: 92.950\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 1.000|perr: 0.773|loss: 29.885|GAP: 0.769|examples_per_second: 99.605\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.740|loss: 32.742|GAP: 0.740|examples_per_second: 101.323\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.752|loss: 32.750|GAP: 0.753|examples_per_second: 101.830\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.730|loss: 31.996|GAP: 0.749|examples_per_second: 86.027\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 5000 | MAP: 0.319 | GAP: 0.738 | p@0.1: 0.708 | p@0.5:0.817 | r@0.1:0.744 | r@0.5: 0.624 | Avg_Loss: 27.686301\n",
      "INFO:tensorflow:epoch/eval number 5000 | MAP: 0.270 | GAP: 0.670 | p@0.1: 0.550 | p@0.5:0.744 | r@0.1:0.805 | r@0.5: 0.593 | Avg_Loss: 23.202900\n",
      "INFO:tensorflow:epoch/eval number 5000 | MAP: 0.112 | GAP: 0.575 | p@0.1: 0.341 | p@0.5:0.784 | r@0.1:0.868 | r@0.5: 0.406 | Avg_Loss: 22.040960\n",
      "INFO:tensorflow:epoch/eval number 5000 | MAP: 0.272 | GAP: 0.656 | p@0.1: 0.635 | p@0.5:0.733 | r@0.1:0.718 | r@0.5: 0.628 | Avg_Loss: 38.551128\n",
      "INFO:tensorflow:epoch/eval number 5000 | MAP: 0.339 | GAP: 0.746 | p@0.1: 0.739 | p@0.5:0.814 | r@0.1:0.731 | r@0.5: 0.646 | Avg_Loss: 32.437295\n",
      "INFO:tensorflow:validation score on val799 is : 0.7461\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-5000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_5000_0.7461/saved_model.pb\n",
      "INFO:tensorflow:training step 5001 | tagging_loss_video: 5.148|tagging_loss_audio: 8.383|tagging_loss_text: 18.240|tagging_loss_image: 5.154|tagging_loss_fusion: 3.202|total_loss: 40.127 | 65.19 Examples/sec\n",
      "INFO:tensorflow:training step 5002 | tagging_loss_video: 6.496|tagging_loss_audio: 8.974|tagging_loss_text: 15.186|tagging_loss_image: 5.655|tagging_loss_fusion: 4.920|total_loss: 41.232 | 68.00 Examples/sec\n",
      "INFO:tensorflow:training step 5003 | tagging_loss_video: 6.128|tagging_loss_audio: 7.858|tagging_loss_text: 15.240|tagging_loss_image: 4.302|tagging_loss_fusion: 5.098|total_loss: 38.626 | 71.63 Examples/sec\n",
      "INFO:tensorflow:training step 5004 | tagging_loss_video: 5.572|tagging_loss_audio: 8.786|tagging_loss_text: 15.498|tagging_loss_image: 4.748|tagging_loss_fusion: 2.929|total_loss: 37.533 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 5005 | tagging_loss_video: 6.200|tagging_loss_audio: 8.823|tagging_loss_text: 11.863|tagging_loss_image: 4.973|tagging_loss_fusion: 5.379|total_loss: 37.237 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 5006 | tagging_loss_video: 5.122|tagging_loss_audio: 8.377|tagging_loss_text: 13.304|tagging_loss_image: 4.251|tagging_loss_fusion: 3.940|total_loss: 34.994 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 5007 | tagging_loss_video: 4.717|tagging_loss_audio: 8.096|tagging_loss_text: 16.730|tagging_loss_image: 5.561|tagging_loss_fusion: 2.538|total_loss: 37.643 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 5008 | tagging_loss_video: 5.260|tagging_loss_audio: 10.046|tagging_loss_text: 12.712|tagging_loss_image: 4.606|tagging_loss_fusion: 4.214|total_loss: 36.837 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 5009 | tagging_loss_video: 5.863|tagging_loss_audio: 9.439|tagging_loss_text: 13.863|tagging_loss_image: 5.296|tagging_loss_fusion: 3.124|total_loss: 37.585 | 70.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5010 |tagging_loss_video: 5.370|tagging_loss_audio: 8.878|tagging_loss_text: 16.796|tagging_loss_image: 4.103|tagging_loss_fusion: 2.994|total_loss: 38.142 | Examples/sec: 69.01\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5011 | tagging_loss_video: 5.775|tagging_loss_audio: 8.306|tagging_loss_text: 14.000|tagging_loss_image: 5.853|tagging_loss_fusion: 3.437|total_loss: 37.371 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 5012 | tagging_loss_video: 5.923|tagging_loss_audio: 8.230|tagging_loss_text: 13.942|tagging_loss_image: 5.763|tagging_loss_fusion: 6.018|total_loss: 39.875 | 69.65 Examples/sec\n",
      "INFO:tensorflow:training step 5013 | tagging_loss_video: 3.698|tagging_loss_audio: 9.063|tagging_loss_text: 12.837|tagging_loss_image: 5.271|tagging_loss_fusion: 2.237|total_loss: 33.106 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5014 | tagging_loss_video: 5.410|tagging_loss_audio: 9.863|tagging_loss_text: 18.829|tagging_loss_image: 5.533|tagging_loss_fusion: 5.490|total_loss: 45.125 | 69.03 Examples/sec\n",
      "INFO:tensorflow:training step 5015 | tagging_loss_video: 6.197|tagging_loss_audio: 9.095|tagging_loss_text: 17.513|tagging_loss_image: 5.619|tagging_loss_fusion: 4.599|total_loss: 43.023 | 71.79 Examples/sec\n",
      "INFO:tensorflow:training step 5016 | tagging_loss_video: 5.971|tagging_loss_audio: 8.153|tagging_loss_text: 16.793|tagging_loss_image: 5.239|tagging_loss_fusion: 4.798|total_loss: 40.955 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 5017 | tagging_loss_video: 5.207|tagging_loss_audio: 9.231|tagging_loss_text: 13.012|tagging_loss_image: 6.081|tagging_loss_fusion: 5.453|total_loss: 38.984 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 5018 | tagging_loss_video: 6.480|tagging_loss_audio: 9.388|tagging_loss_text: 14.247|tagging_loss_image: 6.246|tagging_loss_fusion: 5.413|total_loss: 41.774 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 5019 | tagging_loss_video: 6.720|tagging_loss_audio: 9.346|tagging_loss_text: 14.690|tagging_loss_image: 4.297|tagging_loss_fusion: 5.049|total_loss: 40.103 | 68.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5020 |tagging_loss_video: 5.558|tagging_loss_audio: 9.498|tagging_loss_text: 18.656|tagging_loss_image: 4.344|tagging_loss_fusion: 3.608|total_loss: 41.664 | Examples/sec: 71.10\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5021 | tagging_loss_video: 5.673|tagging_loss_audio: 8.509|tagging_loss_text: 15.190|tagging_loss_image: 4.818|tagging_loss_fusion: 5.497|total_loss: 39.687 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 5022 | tagging_loss_video: 6.576|tagging_loss_audio: 9.049|tagging_loss_text: 15.773|tagging_loss_image: 5.502|tagging_loss_fusion: 4.593|total_loss: 41.493 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 5023 | tagging_loss_video: 6.231|tagging_loss_audio: 7.941|tagging_loss_text: 16.751|tagging_loss_image: 4.356|tagging_loss_fusion: 4.833|total_loss: 40.113 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 5024 | tagging_loss_video: 4.855|tagging_loss_audio: 7.760|tagging_loss_text: 13.494|tagging_loss_image: 5.311|tagging_loss_fusion: 4.014|total_loss: 35.434 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 5025 | tagging_loss_video: 6.089|tagging_loss_audio: 9.088|tagging_loss_text: 14.876|tagging_loss_image: 5.260|tagging_loss_fusion: 5.905|total_loss: 41.218 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 5026 | tagging_loss_video: 5.168|tagging_loss_audio: 8.712|tagging_loss_text: 14.524|tagging_loss_image: 5.519|tagging_loss_fusion: 4.460|total_loss: 38.383 | 71.01 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 5027.\n",
      "INFO:tensorflow:training step 5027 | tagging_loss_video: 5.871|tagging_loss_audio: 8.285|tagging_loss_text: 15.516|tagging_loss_image: 5.102|tagging_loss_fusion: 4.430|total_loss: 39.203 | 3.82 Examples/sec\n",
      "INFO:tensorflow:training step 5028 | tagging_loss_video: 5.905|tagging_loss_audio: 8.983|tagging_loss_text: 15.363|tagging_loss_image: 6.500|tagging_loss_fusion: 6.025|total_loss: 42.776 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 5029 | tagging_loss_video: 5.642|tagging_loss_audio: 9.298|tagging_loss_text: 12.921|tagging_loss_image: 5.364|tagging_loss_fusion: 4.757|total_loss: 37.982 | 68.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5030 |tagging_loss_video: 5.616|tagging_loss_audio: 8.510|tagging_loss_text: 17.073|tagging_loss_image: 4.476|tagging_loss_fusion: 4.388|total_loss: 40.064 | Examples/sec: 71.63\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5031 | tagging_loss_video: 5.847|tagging_loss_audio: 9.388|tagging_loss_text: 12.092|tagging_loss_image: 5.572|tagging_loss_fusion: 6.233|total_loss: 39.133 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 5032 | tagging_loss_video: 6.041|tagging_loss_audio: 8.519|tagging_loss_text: 16.641|tagging_loss_image: 5.397|tagging_loss_fusion: 5.375|total_loss: 41.973 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 5033 | tagging_loss_video: 5.621|tagging_loss_audio: 8.667|tagging_loss_text: 14.704|tagging_loss_image: 5.465|tagging_loss_fusion: 4.035|total_loss: 38.493 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 5034 | tagging_loss_video: 4.846|tagging_loss_audio: 8.192|tagging_loss_text: 14.323|tagging_loss_image: 5.888|tagging_loss_fusion: 2.153|total_loss: 35.401 | 69.90 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5035 | tagging_loss_video: 5.218|tagging_loss_audio: 7.208|tagging_loss_text: 15.407|tagging_loss_image: 4.360|tagging_loss_fusion: 3.340|total_loss: 35.533 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 5036 | tagging_loss_video: 4.375|tagging_loss_audio: 8.981|tagging_loss_text: 16.315|tagging_loss_image: 5.539|tagging_loss_fusion: 2.804|total_loss: 38.015 | 72.08 Examples/sec\n",
      "INFO:tensorflow:training step 5037 | tagging_loss_video: 5.357|tagging_loss_audio: 7.510|tagging_loss_text: 16.915|tagging_loss_image: 5.009|tagging_loss_fusion: 3.765|total_loss: 38.556 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 5038 | tagging_loss_video: 4.889|tagging_loss_audio: 8.268|tagging_loss_text: 15.946|tagging_loss_image: 4.720|tagging_loss_fusion: 4.287|total_loss: 38.110 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 5039 | tagging_loss_video: 4.803|tagging_loss_audio: 8.543|tagging_loss_text: 15.139|tagging_loss_image: 4.930|tagging_loss_fusion: 2.684|total_loss: 36.098 | 71.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5040 |tagging_loss_video: 6.157|tagging_loss_audio: 8.664|tagging_loss_text: 16.038|tagging_loss_image: 5.500|tagging_loss_fusion: 6.494|total_loss: 42.853 | Examples/sec: 69.50\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.78 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5041 | tagging_loss_video: 5.882|tagging_loss_audio: 9.407|tagging_loss_text: 13.295|tagging_loss_image: 5.160|tagging_loss_fusion: 3.938|total_loss: 37.682 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 5042 | tagging_loss_video: 5.126|tagging_loss_audio: 7.680|tagging_loss_text: 16.570|tagging_loss_image: 4.257|tagging_loss_fusion: 2.577|total_loss: 36.209 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 5043 | tagging_loss_video: 6.075|tagging_loss_audio: 8.520|tagging_loss_text: 15.466|tagging_loss_image: 6.136|tagging_loss_fusion: 4.514|total_loss: 40.710 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 5044 | tagging_loss_video: 4.440|tagging_loss_audio: 9.183|tagging_loss_text: 17.659|tagging_loss_image: 6.206|tagging_loss_fusion: 3.018|total_loss: 40.507 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 5045 | tagging_loss_video: 6.056|tagging_loss_audio: 8.590|tagging_loss_text: 17.492|tagging_loss_image: 5.776|tagging_loss_fusion: 4.943|total_loss: 42.858 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 5046 | tagging_loss_video: 6.667|tagging_loss_audio: 7.806|tagging_loss_text: 17.647|tagging_loss_image: 6.761|tagging_loss_fusion: 5.243|total_loss: 44.124 | 66.95 Examples/sec\n",
      "INFO:tensorflow:training step 5047 | tagging_loss_video: 6.634|tagging_loss_audio: 8.682|tagging_loss_text: 15.310|tagging_loss_image: 5.358|tagging_loss_fusion: 5.105|total_loss: 41.088 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 5048 | tagging_loss_video: 5.939|tagging_loss_audio: 9.153|tagging_loss_text: 15.231|tagging_loss_image: 5.998|tagging_loss_fusion: 4.520|total_loss: 40.842 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 5049 | tagging_loss_video: 6.172|tagging_loss_audio: 8.400|tagging_loss_text: 14.616|tagging_loss_image: 5.793|tagging_loss_fusion: 3.831|total_loss: 38.812 | 68.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5050 |tagging_loss_video: 5.334|tagging_loss_audio: 8.345|tagging_loss_text: 15.395|tagging_loss_image: 5.681|tagging_loss_fusion: 4.157|total_loss: 38.912 | Examples/sec: 68.59\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5051 | tagging_loss_video: 6.533|tagging_loss_audio: 8.936|tagging_loss_text: 16.851|tagging_loss_image: 6.076|tagging_loss_fusion: 5.153|total_loss: 43.549 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 5052 | tagging_loss_video: 6.023|tagging_loss_audio: 8.580|tagging_loss_text: 14.760|tagging_loss_image: 4.919|tagging_loss_fusion: 4.836|total_loss: 39.118 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 5053 | tagging_loss_video: 5.109|tagging_loss_audio: 8.661|tagging_loss_text: 15.784|tagging_loss_image: 5.632|tagging_loss_fusion: 3.207|total_loss: 38.392 | 72.19 Examples/sec\n",
      "INFO:tensorflow:training step 5054 | tagging_loss_video: 6.630|tagging_loss_audio: 8.144|tagging_loss_text: 13.438|tagging_loss_image: 4.825|tagging_loss_fusion: 4.956|total_loss: 37.993 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 5055 | tagging_loss_video: 5.937|tagging_loss_audio: 8.822|tagging_loss_text: 14.672|tagging_loss_image: 5.925|tagging_loss_fusion: 4.932|total_loss: 40.289 | 72.16 Examples/sec\n",
      "INFO:tensorflow:training step 5056 | tagging_loss_video: 5.996|tagging_loss_audio: 7.829|tagging_loss_text: 18.205|tagging_loss_image: 4.063|tagging_loss_fusion: 4.709|total_loss: 40.802 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 5057 | tagging_loss_video: 5.684|tagging_loss_audio: 9.529|tagging_loss_text: 18.368|tagging_loss_image: 6.215|tagging_loss_fusion: 5.743|total_loss: 45.539 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 5058 | tagging_loss_video: 3.998|tagging_loss_audio: 8.759|tagging_loss_text: 15.587|tagging_loss_image: 6.847|tagging_loss_fusion: 3.640|total_loss: 38.831 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 5059 | tagging_loss_video: 6.506|tagging_loss_audio: 9.303|tagging_loss_text: 15.240|tagging_loss_image: 5.256|tagging_loss_fusion: 4.898|total_loss: 41.203 | 71.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5060 |tagging_loss_video: 5.486|tagging_loss_audio: 9.712|tagging_loss_text: 15.460|tagging_loss_image: 5.087|tagging_loss_fusion: 3.908|total_loss: 39.653 | Examples/sec: 70.56\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5061 | tagging_loss_video: 5.367|tagging_loss_audio: 9.652|tagging_loss_text: 18.239|tagging_loss_image: 6.857|tagging_loss_fusion: 3.613|total_loss: 43.729 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 5062 | tagging_loss_video: 5.825|tagging_loss_audio: 7.067|tagging_loss_text: 15.683|tagging_loss_image: 4.509|tagging_loss_fusion: 4.306|total_loss: 37.389 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 5063 | tagging_loss_video: 4.137|tagging_loss_audio: 8.190|tagging_loss_text: 15.035|tagging_loss_image: 5.042|tagging_loss_fusion: 2.318|total_loss: 34.722 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 5064 | tagging_loss_video: 5.523|tagging_loss_audio: 9.010|tagging_loss_text: 16.781|tagging_loss_image: 5.700|tagging_loss_fusion: 3.580|total_loss: 40.594 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 5065 | tagging_loss_video: 5.495|tagging_loss_audio: 7.466|tagging_loss_text: 13.389|tagging_loss_image: 4.287|tagging_loss_fusion: 3.552|total_loss: 34.189 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 5066 | tagging_loss_video: 5.902|tagging_loss_audio: 9.618|tagging_loss_text: 18.238|tagging_loss_image: 5.871|tagging_loss_fusion: 5.366|total_loss: 44.994 | 66.92 Examples/sec\n",
      "INFO:tensorflow:training step 5067 | tagging_loss_video: 3.365|tagging_loss_audio: 7.654|tagging_loss_text: 15.111|tagging_loss_image: 5.148|tagging_loss_fusion: 2.532|total_loss: 33.810 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 5068 | tagging_loss_video: 6.344|tagging_loss_audio: 8.371|tagging_loss_text: 13.297|tagging_loss_image: 4.420|tagging_loss_fusion: 4.482|total_loss: 36.914 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 5069 | tagging_loss_video: 4.806|tagging_loss_audio: 7.958|tagging_loss_text: 15.928|tagging_loss_image: 5.499|tagging_loss_fusion: 4.589|total_loss: 38.780 | 71.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5070 |tagging_loss_video: 5.685|tagging_loss_audio: 8.794|tagging_loss_text: 16.239|tagging_loss_image: 4.909|tagging_loss_fusion: 3.734|total_loss: 39.362 | Examples/sec: 69.44\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5071 | tagging_loss_video: 5.038|tagging_loss_audio: 8.927|tagging_loss_text: 17.069|tagging_loss_image: 5.287|tagging_loss_fusion: 3.666|total_loss: 39.988 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 5072 | tagging_loss_video: 5.251|tagging_loss_audio: 8.066|tagging_loss_text: 14.235|tagging_loss_image: 4.371|tagging_loss_fusion: 3.581|total_loss: 35.504 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 5073 | tagging_loss_video: 4.363|tagging_loss_audio: 8.520|tagging_loss_text: 15.495|tagging_loss_image: 4.549|tagging_loss_fusion: 3.388|total_loss: 36.314 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 5074 | tagging_loss_video: 6.219|tagging_loss_audio: 9.522|tagging_loss_text: 15.219|tagging_loss_image: 5.325|tagging_loss_fusion: 4.212|total_loss: 40.497 | 68.29 Examples/sec\n",
      "INFO:tensorflow:training step 5075 | tagging_loss_video: 6.009|tagging_loss_audio: 8.929|tagging_loss_text: 19.631|tagging_loss_image: 5.878|tagging_loss_fusion: 4.829|total_loss: 45.275 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 5076 | tagging_loss_video: 5.018|tagging_loss_audio: 8.499|tagging_loss_text: 14.760|tagging_loss_image: 5.064|tagging_loss_fusion: 4.155|total_loss: 37.497 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 5077 | tagging_loss_video: 6.561|tagging_loss_audio: 7.752|tagging_loss_text: 12.742|tagging_loss_image: 5.850|tagging_loss_fusion: 7.010|total_loss: 39.916 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 5078 | tagging_loss_video: 6.240|tagging_loss_audio: 8.387|tagging_loss_text: 16.528|tagging_loss_image: 4.220|tagging_loss_fusion: 4.413|total_loss: 39.788 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 5079 | tagging_loss_video: 5.680|tagging_loss_audio: 8.293|tagging_loss_text: 15.631|tagging_loss_image: 4.053|tagging_loss_fusion: 3.485|total_loss: 37.142 | 71.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5080 |tagging_loss_video: 5.392|tagging_loss_audio: 8.695|tagging_loss_text: 12.367|tagging_loss_image: 5.259|tagging_loss_fusion: 3.649|total_loss: 35.361 | Examples/sec: 70.77\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5081 | tagging_loss_video: 5.722|tagging_loss_audio: 8.939|tagging_loss_text: 13.873|tagging_loss_image: 6.086|tagging_loss_fusion: 6.333|total_loss: 40.954 | 72.16 Examples/sec\n",
      "INFO:tensorflow:training step 5082 | tagging_loss_video: 4.391|tagging_loss_audio: 8.579|tagging_loss_text: 18.568|tagging_loss_image: 4.772|tagging_loss_fusion: 2.665|total_loss: 38.975 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 5083 | tagging_loss_video: 5.603|tagging_loss_audio: 8.701|tagging_loss_text: 14.739|tagging_loss_image: 5.387|tagging_loss_fusion: 5.477|total_loss: 39.906 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 5084 | tagging_loss_video: 5.929|tagging_loss_audio: 9.283|tagging_loss_text: 15.302|tagging_loss_image: 5.364|tagging_loss_fusion: 5.530|total_loss: 41.408 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 5085 | tagging_loss_video: 5.585|tagging_loss_audio: 8.429|tagging_loss_text: 15.458|tagging_loss_image: 6.326|tagging_loss_fusion: 3.752|total_loss: 39.551 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 5086 | tagging_loss_video: 5.263|tagging_loss_audio: 8.495|tagging_loss_text: 12.822|tagging_loss_image: 5.006|tagging_loss_fusion: 4.130|total_loss: 35.716 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 5087 | tagging_loss_video: 5.449|tagging_loss_audio: 8.881|tagging_loss_text: 13.569|tagging_loss_image: 5.291|tagging_loss_fusion: 4.349|total_loss: 37.540 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 5088 | tagging_loss_video: 5.355|tagging_loss_audio: 7.358|tagging_loss_text: 12.684|tagging_loss_image: 3.875|tagging_loss_fusion: 3.685|total_loss: 32.957 | 61.35 Examples/sec\n",
      "INFO:tensorflow:training step 5089 | tagging_loss_video: 4.665|tagging_loss_audio: 7.602|tagging_loss_text: 13.049|tagging_loss_image: 3.490|tagging_loss_fusion: 2.456|total_loss: 31.261 | 70.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5090 |tagging_loss_video: 4.422|tagging_loss_audio: 8.541|tagging_loss_text: 15.334|tagging_loss_image: 4.837|tagging_loss_fusion: 2.552|total_loss: 35.686 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.88 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5091 | tagging_loss_video: 5.788|tagging_loss_audio: 9.022|tagging_loss_text: 12.247|tagging_loss_image: 4.520|tagging_loss_fusion: 4.601|total_loss: 36.179 | 66.20 Examples/sec\n",
      "INFO:tensorflow:training step 5092 | tagging_loss_video: 5.497|tagging_loss_audio: 9.460|tagging_loss_text: 17.260|tagging_loss_image: 4.966|tagging_loss_fusion: 3.819|total_loss: 41.002 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 5093 | tagging_loss_video: 5.609|tagging_loss_audio: 7.848|tagging_loss_text: 11.867|tagging_loss_image: 6.296|tagging_loss_fusion: 5.995|total_loss: 37.615 | 72.73 Examples/sec\n",
      "INFO:tensorflow:training step 5094 | tagging_loss_video: 5.372|tagging_loss_audio: 8.451|tagging_loss_text: 10.897|tagging_loss_image: 5.354|tagging_loss_fusion: 4.469|total_loss: 34.543 | 60.78 Examples/sec\n",
      "INFO:tensorflow:training step 5095 | tagging_loss_video: 4.472|tagging_loss_audio: 8.013|tagging_loss_text: 17.317|tagging_loss_image: 5.543|tagging_loss_fusion: 3.290|total_loss: 38.636 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 5096 | tagging_loss_video: 5.115|tagging_loss_audio: 8.128|tagging_loss_text: 16.290|tagging_loss_image: 4.493|tagging_loss_fusion: 3.481|total_loss: 37.507 | 71.84 Examples/sec\n",
      "INFO:tensorflow:training step 5097 | tagging_loss_video: 5.526|tagging_loss_audio: 8.630|tagging_loss_text: 14.000|tagging_loss_image: 5.917|tagging_loss_fusion: 4.641|total_loss: 38.714 | 66.33 Examples/sec\n",
      "INFO:tensorflow:training step 5098 | tagging_loss_video: 6.194|tagging_loss_audio: 9.951|tagging_loss_text: 18.505|tagging_loss_image: 5.166|tagging_loss_fusion: 4.779|total_loss: 44.595 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 5099 | tagging_loss_video: 5.048|tagging_loss_audio: 8.714|tagging_loss_text: 16.447|tagging_loss_image: 5.572|tagging_loss_fusion: 3.123|total_loss: 38.905 | 61.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5100 |tagging_loss_video: 5.084|tagging_loss_audio: 9.248|tagging_loss_text: 13.352|tagging_loss_image: 5.811|tagging_loss_fusion: 5.352|total_loss: 38.847 | Examples/sec: 69.29\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5101 | tagging_loss_video: 5.119|tagging_loss_audio: 8.448|tagging_loss_text: 13.714|tagging_loss_image: 4.490|tagging_loss_fusion: 4.288|total_loss: 36.058 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 5102 | tagging_loss_video: 4.444|tagging_loss_audio: 8.554|tagging_loss_text: 14.913|tagging_loss_image: 5.322|tagging_loss_fusion: 3.614|total_loss: 36.848 | 61.81 Examples/sec\n",
      "INFO:tensorflow:training step 5103 | tagging_loss_video: 5.084|tagging_loss_audio: 8.790|tagging_loss_text: 14.269|tagging_loss_image: 5.887|tagging_loss_fusion: 3.388|total_loss: 37.418 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 5104 | tagging_loss_video: 5.244|tagging_loss_audio: 8.023|tagging_loss_text: 16.206|tagging_loss_image: 5.125|tagging_loss_fusion: 3.412|total_loss: 38.009 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 5105 | tagging_loss_video: 5.614|tagging_loss_audio: 8.165|tagging_loss_text: 17.429|tagging_loss_image: 5.958|tagging_loss_fusion: 5.033|total_loss: 42.199 | 66.85 Examples/sec\n",
      "INFO:tensorflow:training step 5106 | tagging_loss_video: 5.747|tagging_loss_audio: 8.335|tagging_loss_text: 13.515|tagging_loss_image: 4.436|tagging_loss_fusion: 4.159|total_loss: 36.192 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 5107 | tagging_loss_video: 6.427|tagging_loss_audio: 10.053|tagging_loss_text: 14.903|tagging_loss_image: 4.976|tagging_loss_fusion: 5.326|total_loss: 41.685 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 5108 | tagging_loss_video: 5.803|tagging_loss_audio: 9.187|tagging_loss_text: 15.006|tagging_loss_image: 3.889|tagging_loss_fusion: 4.462|total_loss: 38.348 | 68.90 Examples/sec\n",
      "INFO:tensorflow:training step 5109 | tagging_loss_video: 5.982|tagging_loss_audio: 8.886|tagging_loss_text: 13.949|tagging_loss_image: 5.001|tagging_loss_fusion: 6.083|total_loss: 39.902 | 70.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5110 |tagging_loss_video: 6.392|tagging_loss_audio: 8.642|tagging_loss_text: 12.941|tagging_loss_image: 5.370|tagging_loss_fusion: 4.579|total_loss: 37.926 | Examples/sec: 60.94\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5111 | tagging_loss_video: 6.706|tagging_loss_audio: 9.758|tagging_loss_text: 17.341|tagging_loss_image: 5.320|tagging_loss_fusion: 6.841|total_loss: 45.966 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 5112 | tagging_loss_video: 5.652|tagging_loss_audio: 8.222|tagging_loss_text: 13.188|tagging_loss_image: 5.572|tagging_loss_fusion: 5.355|total_loss: 37.989 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 5113 | tagging_loss_video: 5.733|tagging_loss_audio: 9.206|tagging_loss_text: 13.754|tagging_loss_image: 5.450|tagging_loss_fusion: 3.897|total_loss: 38.040 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 5114 | tagging_loss_video: 6.907|tagging_loss_audio: 9.850|tagging_loss_text: 20.072|tagging_loss_image: 5.870|tagging_loss_fusion: 5.909|total_loss: 48.607 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 5115 | tagging_loss_video: 5.466|tagging_loss_audio: 8.686|tagging_loss_text: 16.899|tagging_loss_image: 4.973|tagging_loss_fusion: 3.958|total_loss: 39.981 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 5116 | tagging_loss_video: 5.542|tagging_loss_audio: 9.162|tagging_loss_text: 15.936|tagging_loss_image: 5.635|tagging_loss_fusion: 5.356|total_loss: 41.631 | 63.77 Examples/sec\n",
      "INFO:tensorflow:training step 5117 | tagging_loss_video: 6.296|tagging_loss_audio: 9.791|tagging_loss_text: 18.695|tagging_loss_image: 6.241|tagging_loss_fusion: 6.006|total_loss: 47.029 | 71.85 Examples/sec\n",
      "INFO:tensorflow:training step 5118 | tagging_loss_video: 5.826|tagging_loss_audio: 9.445|tagging_loss_text: 16.680|tagging_loss_image: 6.232|tagging_loss_fusion: 3.070|total_loss: 41.253 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 5119 | tagging_loss_video: 6.802|tagging_loss_audio: 8.466|tagging_loss_text: 14.086|tagging_loss_image: 5.969|tagging_loss_fusion: 6.760|total_loss: 42.082 | 64.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5120 |tagging_loss_video: 6.036|tagging_loss_audio: 10.158|tagging_loss_text: 16.394|tagging_loss_image: 5.521|tagging_loss_fusion: 4.919|total_loss: 43.027 | Examples/sec: 69.54\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5121 | tagging_loss_video: 5.371|tagging_loss_audio: 9.834|tagging_loss_text: 15.101|tagging_loss_image: 5.943|tagging_loss_fusion: 3.658|total_loss: 39.906 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 5122 | tagging_loss_video: 6.094|tagging_loss_audio: 10.003|tagging_loss_text: 13.380|tagging_loss_image: 5.024|tagging_loss_fusion: 5.241|total_loss: 39.742 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 5123 | tagging_loss_video: 5.551|tagging_loss_audio: 7.785|tagging_loss_text: 12.987|tagging_loss_image: 5.230|tagging_loss_fusion: 4.354|total_loss: 35.906 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 5124 | tagging_loss_video: 6.459|tagging_loss_audio: 9.505|tagging_loss_text: 16.613|tagging_loss_image: 5.888|tagging_loss_fusion: 6.209|total_loss: 44.673 | 64.28 Examples/sec\n",
      "INFO:tensorflow:training step 5125 | tagging_loss_video: 6.400|tagging_loss_audio: 9.585|tagging_loss_text: 16.574|tagging_loss_image: 6.072|tagging_loss_fusion: 5.084|total_loss: 43.715 | 68.75 Examples/sec\n",
      "INFO:tensorflow:training step 5126 | tagging_loss_video: 6.907|tagging_loss_audio: 9.900|tagging_loss_text: 14.546|tagging_loss_image: 5.503|tagging_loss_fusion: 6.172|total_loss: 43.028 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 5127 | tagging_loss_video: 5.709|tagging_loss_audio: 9.718|tagging_loss_text: 15.328|tagging_loss_image: 5.289|tagging_loss_fusion: 4.810|total_loss: 40.855 | 63.23 Examples/sec\n",
      "INFO:tensorflow:training step 5128 | tagging_loss_video: 6.242|tagging_loss_audio: 10.968|tagging_loss_text: 22.127|tagging_loss_image: 6.481|tagging_loss_fusion: 4.617|total_loss: 50.435 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 5129 | tagging_loss_video: 5.209|tagging_loss_audio: 9.992|tagging_loss_text: 14.532|tagging_loss_image: 4.749|tagging_loss_fusion: 3.768|total_loss: 38.250 | 68.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5130 |tagging_loss_video: 5.856|tagging_loss_audio: 8.850|tagging_loss_text: 15.994|tagging_loss_image: 6.073|tagging_loss_fusion: 5.384|total_loss: 42.156 | Examples/sec: 70.90\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.94 |recall@0.1: 0.96 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5131 | tagging_loss_video: 5.300|tagging_loss_audio: 8.416|tagging_loss_text: 14.748|tagging_loss_image: 5.542|tagging_loss_fusion: 4.257|total_loss: 38.262 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 5132 | tagging_loss_video: 7.079|tagging_loss_audio: 9.174|tagging_loss_text: 15.777|tagging_loss_image: 5.638|tagging_loss_fusion: 5.577|total_loss: 43.245 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 5133 | tagging_loss_video: 6.647|tagging_loss_audio: 9.593|tagging_loss_text: 17.292|tagging_loss_image: 4.947|tagging_loss_fusion: 4.788|total_loss: 43.267 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 5134 | tagging_loss_video: 5.345|tagging_loss_audio: 10.589|tagging_loss_text: 15.594|tagging_loss_image: 6.091|tagging_loss_fusion: 4.447|total_loss: 42.067 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 5135 | tagging_loss_video: 6.116|tagging_loss_audio: 9.123|tagging_loss_text: 18.479|tagging_loss_image: 4.826|tagging_loss_fusion: 4.155|total_loss: 42.699 | 62.49 Examples/sec\n",
      "INFO:tensorflow:training step 5136 | tagging_loss_video: 4.462|tagging_loss_audio: 9.040|tagging_loss_text: 15.736|tagging_loss_image: 6.494|tagging_loss_fusion: 4.534|total_loss: 40.266 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 5137 | tagging_loss_video: 5.044|tagging_loss_audio: 8.884|tagging_loss_text: 10.909|tagging_loss_image: 5.979|tagging_loss_fusion: 3.813|total_loss: 34.629 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 5138 | tagging_loss_video: 5.951|tagging_loss_audio: 9.403|tagging_loss_text: 14.140|tagging_loss_image: 4.682|tagging_loss_fusion: 4.094|total_loss: 38.271 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 5139 | tagging_loss_video: 6.175|tagging_loss_audio: 11.019|tagging_loss_text: 15.403|tagging_loss_image: 5.779|tagging_loss_fusion: 4.839|total_loss: 43.216 | 68.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5140 |tagging_loss_video: 6.067|tagging_loss_audio: 9.671|tagging_loss_text: 18.236|tagging_loss_image: 5.884|tagging_loss_fusion: 4.621|total_loss: 44.479 | Examples/sec: 70.99\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5141 | tagging_loss_video: 6.445|tagging_loss_audio: 8.083|tagging_loss_text: 11.220|tagging_loss_image: 5.480|tagging_loss_fusion: 5.969|total_loss: 37.196 | 66.14 Examples/sec\n",
      "INFO:tensorflow:training step 5142 | tagging_loss_video: 5.440|tagging_loss_audio: 8.259|tagging_loss_text: 13.258|tagging_loss_image: 5.410|tagging_loss_fusion: 5.291|total_loss: 37.658 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 5143 | tagging_loss_video: 5.690|tagging_loss_audio: 8.760|tagging_loss_text: 12.789|tagging_loss_image: 5.648|tagging_loss_fusion: 4.937|total_loss: 37.825 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 5144 | tagging_loss_video: 4.050|tagging_loss_audio: 8.394|tagging_loss_text: 15.837|tagging_loss_image: 5.345|tagging_loss_fusion: 2.239|total_loss: 35.864 | 64.29 Examples/sec\n",
      "INFO:tensorflow:training step 5145 | tagging_loss_video: 5.331|tagging_loss_audio: 8.437|tagging_loss_text: 12.031|tagging_loss_image: 5.115|tagging_loss_fusion: 5.490|total_loss: 36.404 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 5146 | tagging_loss_video: 5.503|tagging_loss_audio: 8.862|tagging_loss_text: 16.734|tagging_loss_image: 5.351|tagging_loss_fusion: 3.694|total_loss: 40.144 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 5147 | tagging_loss_video: 7.079|tagging_loss_audio: 8.899|tagging_loss_text: 14.040|tagging_loss_image: 5.408|tagging_loss_fusion: 6.272|total_loss: 41.697 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 5148 | tagging_loss_video: 6.147|tagging_loss_audio: 9.527|tagging_loss_text: 13.214|tagging_loss_image: 5.084|tagging_loss_fusion: 5.594|total_loss: 39.565 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 5149 | tagging_loss_video: 6.394|tagging_loss_audio: 8.964|tagging_loss_text: 10.350|tagging_loss_image: 5.533|tagging_loss_fusion: 5.497|total_loss: 36.738 | 60.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5150 |tagging_loss_video: 5.845|tagging_loss_audio: 9.137|tagging_loss_text: 14.005|tagging_loss_image: 5.218|tagging_loss_fusion: 3.783|total_loss: 37.987 | Examples/sec: 67.63\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5151 | tagging_loss_video: 6.098|tagging_loss_audio: 9.211|tagging_loss_text: 17.917|tagging_loss_image: 4.997|tagging_loss_fusion: 4.631|total_loss: 42.854 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 5152 | tagging_loss_video: 6.122|tagging_loss_audio: 8.905|tagging_loss_text: 13.519|tagging_loss_image: 5.489|tagging_loss_fusion: 6.063|total_loss: 40.098 | 60.62 Examples/sec\n",
      "INFO:tensorflow:training step 5153 | tagging_loss_video: 6.271|tagging_loss_audio: 9.631|tagging_loss_text: 16.032|tagging_loss_image: 5.590|tagging_loss_fusion: 6.578|total_loss: 44.102 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 5154 | tagging_loss_video: 6.634|tagging_loss_audio: 9.353|tagging_loss_text: 16.717|tagging_loss_image: 6.351|tagging_loss_fusion: 5.891|total_loss: 44.946 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 5155 | tagging_loss_video: 5.653|tagging_loss_audio: 9.778|tagging_loss_text: 10.746|tagging_loss_image: 5.165|tagging_loss_fusion: 4.644|total_loss: 35.987 | 65.14 Examples/sec\n",
      "INFO:tensorflow:training step 5156 | tagging_loss_video: 5.784|tagging_loss_audio: 8.345|tagging_loss_text: 10.973|tagging_loss_image: 5.675|tagging_loss_fusion: 3.766|total_loss: 34.543 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 5157 | tagging_loss_video: 5.116|tagging_loss_audio: 9.141|tagging_loss_text: 14.245|tagging_loss_image: 6.051|tagging_loss_fusion: 4.408|total_loss: 38.962 | 72.05 Examples/sec\n",
      "INFO:tensorflow:training step 5158 | tagging_loss_video: 6.446|tagging_loss_audio: 8.156|tagging_loss_text: 14.689|tagging_loss_image: 4.699|tagging_loss_fusion: 4.298|total_loss: 38.289 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 5159 | tagging_loss_video: 6.630|tagging_loss_audio: 10.295|tagging_loss_text: 17.909|tagging_loss_image: 6.048|tagging_loss_fusion: 4.482|total_loss: 45.362 | 71.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5160 |tagging_loss_video: 5.886|tagging_loss_audio: 8.708|tagging_loss_text: 13.211|tagging_loss_image: 4.854|tagging_loss_fusion: 3.663|total_loss: 36.322 | Examples/sec: 59.62\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5161 | tagging_loss_video: 4.693|tagging_loss_audio: 8.984|tagging_loss_text: 17.143|tagging_loss_image: 3.888|tagging_loss_fusion: 3.359|total_loss: 38.066 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 5162 | tagging_loss_video: 5.645|tagging_loss_audio: 8.362|tagging_loss_text: 16.748|tagging_loss_image: 5.724|tagging_loss_fusion: 5.156|total_loss: 41.636 | 68.74 Examples/sec\n",
      "INFO:tensorflow:training step 5163 | tagging_loss_video: 5.078|tagging_loss_audio: 7.483|tagging_loss_text: 12.282|tagging_loss_image: 4.021|tagging_loss_fusion: 3.472|total_loss: 32.335 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 5164 | tagging_loss_video: 5.952|tagging_loss_audio: 8.600|tagging_loss_text: 16.242|tagging_loss_image: 4.487|tagging_loss_fusion: 4.129|total_loss: 39.409 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 5165 | tagging_loss_video: 4.637|tagging_loss_audio: 9.766|tagging_loss_text: 13.829|tagging_loss_image: 5.088|tagging_loss_fusion: 4.042|total_loss: 37.362 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 5166 | tagging_loss_video: 6.433|tagging_loss_audio: 8.387|tagging_loss_text: 16.206|tagging_loss_image: 5.401|tagging_loss_fusion: 5.133|total_loss: 41.561 | 60.04 Examples/sec\n",
      "INFO:tensorflow:training step 5167 | tagging_loss_video: 7.223|tagging_loss_audio: 10.108|tagging_loss_text: 15.526|tagging_loss_image: 3.786|tagging_loss_fusion: 5.762|total_loss: 42.406 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 5168 | tagging_loss_video: 5.092|tagging_loss_audio: 8.647|tagging_loss_text: 14.707|tagging_loss_image: 5.687|tagging_loss_fusion: 3.639|total_loss: 37.772 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5169 | tagging_loss_video: 5.316|tagging_loss_audio: 8.781|tagging_loss_text: 12.921|tagging_loss_image: 5.320|tagging_loss_fusion: 3.583|total_loss: 35.920 | 64.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5170 |tagging_loss_video: 5.806|tagging_loss_audio: 9.069|tagging_loss_text: 12.789|tagging_loss_image: 5.973|tagging_loss_fusion: 4.817|total_loss: 38.454 | Examples/sec: 68.44\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5171 | tagging_loss_video: 6.345|tagging_loss_audio: 9.346|tagging_loss_text: 15.457|tagging_loss_image: 6.092|tagging_loss_fusion: 6.815|total_loss: 44.054 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 5172 | tagging_loss_video: 5.494|tagging_loss_audio: 9.951|tagging_loss_text: 17.314|tagging_loss_image: 5.142|tagging_loss_fusion: 3.235|total_loss: 41.137 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 5173 | tagging_loss_video: 4.489|tagging_loss_audio: 7.841|tagging_loss_text: 15.369|tagging_loss_image: 4.535|tagging_loss_fusion: 2.623|total_loss: 34.857 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 5174 | tagging_loss_video: 5.601|tagging_loss_audio: 8.516|tagging_loss_text: 15.674|tagging_loss_image: 5.093|tagging_loss_fusion: 3.026|total_loss: 37.909 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 5175 | tagging_loss_video: 5.146|tagging_loss_audio: 9.477|tagging_loss_text: 14.082|tagging_loss_image: 4.162|tagging_loss_fusion: 3.875|total_loss: 36.742 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 5176 | tagging_loss_video: 4.695|tagging_loss_audio: 8.715|tagging_loss_text: 12.919|tagging_loss_image: 5.508|tagging_loss_fusion: 3.368|total_loss: 35.206 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 5177 | tagging_loss_video: 5.095|tagging_loss_audio: 7.936|tagging_loss_text: 13.641|tagging_loss_image: 5.625|tagging_loss_fusion: 3.648|total_loss: 35.945 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 5178 | tagging_loss_video: 4.744|tagging_loss_audio: 8.707|tagging_loss_text: 18.030|tagging_loss_image: 5.347|tagging_loss_fusion: 3.505|total_loss: 40.332 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 5179 | tagging_loss_video: 5.801|tagging_loss_audio: 8.394|tagging_loss_text: 13.770|tagging_loss_image: 3.303|tagging_loss_fusion: 4.522|total_loss: 35.790 | 70.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5180 |tagging_loss_video: 5.214|tagging_loss_audio: 7.925|tagging_loss_text: 16.050|tagging_loss_image: 6.179|tagging_loss_fusion: 4.601|total_loss: 39.968 | Examples/sec: 64.13\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5181 | tagging_loss_video: 6.263|tagging_loss_audio: 9.023|tagging_loss_text: 11.092|tagging_loss_image: 6.021|tagging_loss_fusion: 5.549|total_loss: 37.948 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 5182 | tagging_loss_video: 6.166|tagging_loss_audio: 7.816|tagging_loss_text: 15.253|tagging_loss_image: 4.404|tagging_loss_fusion: 4.945|total_loss: 38.585 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 5183 | tagging_loss_video: 6.127|tagging_loss_audio: 7.819|tagging_loss_text: 14.048|tagging_loss_image: 5.892|tagging_loss_fusion: 5.683|total_loss: 39.568 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 5184 | tagging_loss_video: 6.096|tagging_loss_audio: 8.832|tagging_loss_text: 16.916|tagging_loss_image: 6.058|tagging_loss_fusion: 5.179|total_loss: 43.081 | 67.69 Examples/sec\n",
      "INFO:tensorflow:training step 5185 | tagging_loss_video: 6.266|tagging_loss_audio: 8.521|tagging_loss_text: 15.491|tagging_loss_image: 4.730|tagging_loss_fusion: 6.407|total_loss: 41.414 | 70.46 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5186 | tagging_loss_video: 5.316|tagging_loss_audio: 7.223|tagging_loss_text: 15.529|tagging_loss_image: 6.381|tagging_loss_fusion: 4.115|total_loss: 38.564 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 5187 | tagging_loss_video: 5.189|tagging_loss_audio: 8.589|tagging_loss_text: 16.045|tagging_loss_image: 4.724|tagging_loss_fusion: 2.871|total_loss: 37.418 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 5188 | tagging_loss_video: 6.336|tagging_loss_audio: 8.624|tagging_loss_text: 14.530|tagging_loss_image: 5.390|tagging_loss_fusion: 4.475|total_loss: 39.356 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 5189 | tagging_loss_video: 6.433|tagging_loss_audio: 8.961|tagging_loss_text: 17.378|tagging_loss_image: 5.404|tagging_loss_fusion: 4.597|total_loss: 42.772 | 71.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5190 |tagging_loss_video: 5.294|tagging_loss_audio: 9.400|tagging_loss_text: 12.736|tagging_loss_image: 5.509|tagging_loss_fusion: 3.611|total_loss: 36.550 | Examples/sec: 67.83\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.88 | precision@0.5: 0.98 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5191 | tagging_loss_video: 6.854|tagging_loss_audio: 8.439|tagging_loss_text: 16.096|tagging_loss_image: 5.050|tagging_loss_fusion: 4.893|total_loss: 41.331 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 5192 | tagging_loss_video: 6.032|tagging_loss_audio: 7.757|tagging_loss_text: 13.470|tagging_loss_image: 4.944|tagging_loss_fusion: 5.980|total_loss: 38.183 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 5193 | tagging_loss_video: 4.904|tagging_loss_audio: 8.797|tagging_loss_text: 15.082|tagging_loss_image: 5.051|tagging_loss_fusion: 2.507|total_loss: 36.340 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 5194 | tagging_loss_video: 4.740|tagging_loss_audio: 8.508|tagging_loss_text: 16.774|tagging_loss_image: 5.462|tagging_loss_fusion: 3.493|total_loss: 38.977 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 5195 | tagging_loss_video: 5.734|tagging_loss_audio: 8.724|tagging_loss_text: 13.260|tagging_loss_image: 6.344|tagging_loss_fusion: 4.068|total_loss: 38.130 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 5196 | tagging_loss_video: 5.318|tagging_loss_audio: 8.513|tagging_loss_text: 16.252|tagging_loss_image: 3.791|tagging_loss_fusion: 3.697|total_loss: 37.571 | 64.33 Examples/sec\n",
      "INFO:tensorflow:training step 5197 | tagging_loss_video: 6.627|tagging_loss_audio: 9.574|tagging_loss_text: 16.321|tagging_loss_image: 5.185|tagging_loss_fusion: 5.901|total_loss: 43.607 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 5198 | tagging_loss_video: 7.021|tagging_loss_audio: 7.715|tagging_loss_text: 11.868|tagging_loss_image: 5.819|tagging_loss_fusion: 4.202|total_loss: 36.625 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 5199 | tagging_loss_video: 6.067|tagging_loss_audio: 9.466|tagging_loss_text: 16.648|tagging_loss_image: 5.989|tagging_loss_fusion: 4.251|total_loss: 42.420 | 62.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5200 |tagging_loss_video: 4.920|tagging_loss_audio: 8.854|tagging_loss_text: 12.653|tagging_loss_image: 5.651|tagging_loss_fusion: 3.651|total_loss: 35.729 | Examples/sec: 68.82\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5201 | tagging_loss_video: 6.058|tagging_loss_audio: 8.919|tagging_loss_text: 15.214|tagging_loss_image: 7.239|tagging_loss_fusion: 5.652|total_loss: 43.080 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 5202 | tagging_loss_video: 4.925|tagging_loss_audio: 7.481|tagging_loss_text: 15.074|tagging_loss_image: 4.298|tagging_loss_fusion: 2.665|total_loss: 34.444 | 63.70 Examples/sec\n",
      "INFO:tensorflow:training step 5203 | tagging_loss_video: 4.726|tagging_loss_audio: 8.220|tagging_loss_text: 13.159|tagging_loss_image: 2.983|tagging_loss_fusion: 2.691|total_loss: 31.779 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 5204 | tagging_loss_video: 5.871|tagging_loss_audio: 8.124|tagging_loss_text: 14.107|tagging_loss_image: 5.346|tagging_loss_fusion: 4.083|total_loss: 37.531 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 5205 | tagging_loss_video: 4.066|tagging_loss_audio: 7.814|tagging_loss_text: 15.220|tagging_loss_image: 3.719|tagging_loss_fusion: 2.859|total_loss: 33.679 | 63.02 Examples/sec\n",
      "INFO:tensorflow:training step 5206 | tagging_loss_video: 5.943|tagging_loss_audio: 8.458|tagging_loss_text: 17.544|tagging_loss_image: 5.256|tagging_loss_fusion: 3.750|total_loss: 40.951 | 68.67 Examples/sec\n",
      "INFO:tensorflow:training step 5207 | tagging_loss_video: 6.149|tagging_loss_audio: 7.933|tagging_loss_text: 13.162|tagging_loss_image: 5.111|tagging_loss_fusion: 5.846|total_loss: 38.200 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 5208 | tagging_loss_video: 6.340|tagging_loss_audio: 8.288|tagging_loss_text: 16.615|tagging_loss_image: 3.520|tagging_loss_fusion: 4.873|total_loss: 39.635 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 5209 | tagging_loss_video: 5.009|tagging_loss_audio: 8.145|tagging_loss_text: 13.376|tagging_loss_image: 5.290|tagging_loss_fusion: 4.625|total_loss: 36.445 | 68.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5210 |tagging_loss_video: 6.716|tagging_loss_audio: 10.002|tagging_loss_text: 16.953|tagging_loss_image: 5.609|tagging_loss_fusion: 5.053|total_loss: 44.332 | Examples/sec: 60.80\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5211 | tagging_loss_video: 5.676|tagging_loss_audio: 8.702|tagging_loss_text: 15.759|tagging_loss_image: 4.975|tagging_loss_fusion: 4.843|total_loss: 39.954 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 5212 | tagging_loss_video: 5.283|tagging_loss_audio: 8.003|tagging_loss_text: 10.526|tagging_loss_image: 4.624|tagging_loss_fusion: 4.720|total_loss: 33.156 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 5213 | tagging_loss_video: 4.636|tagging_loss_audio: 7.842|tagging_loss_text: 13.007|tagging_loss_image: 3.884|tagging_loss_fusion: 2.937|total_loss: 32.306 | 61.57 Examples/sec\n",
      "INFO:tensorflow:training step 5214 | tagging_loss_video: 5.520|tagging_loss_audio: 8.270|tagging_loss_text: 14.509|tagging_loss_image: 5.363|tagging_loss_fusion: 3.776|total_loss: 37.438 | 72.82 Examples/sec\n",
      "INFO:tensorflow:training step 5215 | tagging_loss_video: 5.746|tagging_loss_audio: 8.478|tagging_loss_text: 13.798|tagging_loss_image: 5.969|tagging_loss_fusion: 3.876|total_loss: 37.867 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 5216 | tagging_loss_video: 6.275|tagging_loss_audio: 7.742|tagging_loss_text: 13.484|tagging_loss_image: 4.187|tagging_loss_fusion: 4.395|total_loss: 36.083 | 67.00 Examples/sec\n",
      "INFO:tensorflow:training step 5217 | tagging_loss_video: 5.540|tagging_loss_audio: 7.996|tagging_loss_text: 15.427|tagging_loss_image: 4.476|tagging_loss_fusion: 4.008|total_loss: 37.447 | 63.02 Examples/sec\n",
      "INFO:tensorflow:training step 5218 | tagging_loss_video: 5.799|tagging_loss_audio: 8.512|tagging_loss_text: 15.856|tagging_loss_image: 4.127|tagging_loss_fusion: 3.086|total_loss: 37.380 | 69.25 Examples/sec\n",
      "INFO:tensorflow:training step 5219 | tagging_loss_video: 5.998|tagging_loss_audio: 8.020|tagging_loss_text: 15.002|tagging_loss_image: 5.749|tagging_loss_fusion: 6.525|total_loss: 41.293 | 71.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5220 |tagging_loss_video: 3.692|tagging_loss_audio: 8.147|tagging_loss_text: 12.394|tagging_loss_image: 4.171|tagging_loss_fusion: 2.813|total_loss: 31.217 | Examples/sec: 69.97\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5221 | tagging_loss_video: 6.320|tagging_loss_audio: 8.620|tagging_loss_text: 17.170|tagging_loss_image: 4.486|tagging_loss_fusion: 4.169|total_loss: 40.765 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 5222 | tagging_loss_video: 3.708|tagging_loss_audio: 7.716|tagging_loss_text: 15.912|tagging_loss_image: 5.057|tagging_loss_fusion: 2.761|total_loss: 35.154 | 71.38 Examples/sec\n",
      "INFO:tensorflow:training step 5223 | tagging_loss_video: 6.019|tagging_loss_audio: 8.106|tagging_loss_text: 14.734|tagging_loss_image: 4.833|tagging_loss_fusion: 4.842|total_loss: 38.534 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 5224 | tagging_loss_video: 6.153|tagging_loss_audio: 8.425|tagging_loss_text: 16.011|tagging_loss_image: 4.972|tagging_loss_fusion: 3.597|total_loss: 39.159 | 66.42 Examples/sec\n",
      "INFO:tensorflow:training step 5225 | tagging_loss_video: 6.494|tagging_loss_audio: 8.348|tagging_loss_text: 18.983|tagging_loss_image: 5.826|tagging_loss_fusion: 4.913|total_loss: 44.563 | 61.41 Examples/sec\n",
      "INFO:tensorflow:training step 5226 | tagging_loss_video: 5.839|tagging_loss_audio: 8.130|tagging_loss_text: 14.187|tagging_loss_image: 4.720|tagging_loss_fusion: 4.956|total_loss: 37.833 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 5227 | tagging_loss_video: 4.328|tagging_loss_audio: 9.098|tagging_loss_text: 14.994|tagging_loss_image: 5.576|tagging_loss_fusion: 2.369|total_loss: 36.365 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 5228 | tagging_loss_video: 5.413|tagging_loss_audio: 8.132|tagging_loss_text: 17.122|tagging_loss_image: 4.577|tagging_loss_fusion: 3.245|total_loss: 38.489 | 62.65 Examples/sec\n",
      "INFO:tensorflow:training step 5229 | tagging_loss_video: 4.457|tagging_loss_audio: 7.054|tagging_loss_text: 14.243|tagging_loss_image: 4.626|tagging_loss_fusion: 2.678|total_loss: 33.058 | 71.17 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5230 |tagging_loss_video: 5.894|tagging_loss_audio: 8.319|tagging_loss_text: 18.006|tagging_loss_image: 4.236|tagging_loss_fusion: 3.672|total_loss: 40.127 | Examples/sec: 63.76\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5231 | tagging_loss_video: 4.215|tagging_loss_audio: 8.453|tagging_loss_text: 12.239|tagging_loss_image: 3.599|tagging_loss_fusion: 2.978|total_loss: 31.484 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 5232 | tagging_loss_video: 4.955|tagging_loss_audio: 8.448|tagging_loss_text: 11.730|tagging_loss_image: 4.636|tagging_loss_fusion: 4.564|total_loss: 34.332 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 5233 | tagging_loss_video: 6.098|tagging_loss_audio: 8.856|tagging_loss_text: 13.818|tagging_loss_image: 3.929|tagging_loss_fusion: 3.779|total_loss: 36.482 | 68.11 Examples/sec\n",
      "INFO:tensorflow:training step 5234 | tagging_loss_video: 5.266|tagging_loss_audio: 8.770|tagging_loss_text: 16.033|tagging_loss_image: 3.888|tagging_loss_fusion: 4.598|total_loss: 38.555 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 5235 | tagging_loss_video: 6.102|tagging_loss_audio: 9.024|tagging_loss_text: 13.885|tagging_loss_image: 5.476|tagging_loss_fusion: 5.660|total_loss: 40.148 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 5236 | tagging_loss_video: 5.342|tagging_loss_audio: 8.198|tagging_loss_text: 15.615|tagging_loss_image: 5.641|tagging_loss_fusion: 3.600|total_loss: 38.396 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 5237 | tagging_loss_video: 5.213|tagging_loss_audio: 8.555|tagging_loss_text: 16.773|tagging_loss_image: 4.703|tagging_loss_fusion: 2.995|total_loss: 38.239 | 68.67 Examples/sec\n",
      "INFO:tensorflow:training step 5238 | tagging_loss_video: 5.608|tagging_loss_audio: 9.322|tagging_loss_text: 14.065|tagging_loss_image: 5.010|tagging_loss_fusion: 3.954|total_loss: 37.959 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 5239 | tagging_loss_video: 5.343|tagging_loss_audio: 9.438|tagging_loss_text: 15.757|tagging_loss_image: 5.938|tagging_loss_fusion: 4.531|total_loss: 41.006 | 65.63 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5240 |tagging_loss_video: 5.804|tagging_loss_audio: 9.006|tagging_loss_text: 15.884|tagging_loss_image: 5.727|tagging_loss_fusion: 5.885|total_loss: 42.307 | Examples/sec: 69.85\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.80 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5241 | tagging_loss_video: 5.667|tagging_loss_audio: 8.654|tagging_loss_text: 16.191|tagging_loss_image: 4.459|tagging_loss_fusion: 4.334|total_loss: 39.304 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 5242 | tagging_loss_video: 5.516|tagging_loss_audio: 8.558|tagging_loss_text: 12.312|tagging_loss_image: 3.059|tagging_loss_fusion: 3.561|total_loss: 33.006 | 61.39 Examples/sec\n",
      "INFO:tensorflow:training step 5243 | tagging_loss_video: 6.252|tagging_loss_audio: 9.245|tagging_loss_text: 19.282|tagging_loss_image: 4.771|tagging_loss_fusion: 4.936|total_loss: 44.486 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5244 | tagging_loss_video: 5.514|tagging_loss_audio: 7.572|tagging_loss_text: 17.563|tagging_loss_image: 4.628|tagging_loss_fusion: 3.648|total_loss: 38.924 | 67.37 Examples/sec\n",
      "INFO:tensorflow:training step 5245 | tagging_loss_video: 4.994|tagging_loss_audio: 9.060|tagging_loss_text: 16.053|tagging_loss_image: 6.228|tagging_loss_fusion: 5.553|total_loss: 41.888 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 5246 | tagging_loss_video: 5.675|tagging_loss_audio: 8.553|tagging_loss_text: 16.101|tagging_loss_image: 5.303|tagging_loss_fusion: 4.621|total_loss: 40.253 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 5247 | tagging_loss_video: 5.989|tagging_loss_audio: 8.503|tagging_loss_text: 15.647|tagging_loss_image: 5.757|tagging_loss_fusion: 5.489|total_loss: 41.385 | 65.77 Examples/sec\n",
      "INFO:tensorflow:training step 5248 | tagging_loss_video: 5.177|tagging_loss_audio: 9.621|tagging_loss_text: 18.597|tagging_loss_image: 6.318|tagging_loss_fusion: 3.789|total_loss: 43.501 | 70.44 Examples/sec\n",
      "INFO:tensorflow:training step 5249 | tagging_loss_video: 5.629|tagging_loss_audio: 8.574|tagging_loss_text: 16.042|tagging_loss_image: 5.254|tagging_loss_fusion: 5.753|total_loss: 41.252 | 69.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5250 |tagging_loss_video: 6.388|tagging_loss_audio: 8.903|tagging_loss_text: 14.978|tagging_loss_image: 6.786|tagging_loss_fusion: 7.314|total_loss: 44.369 | Examples/sec: 65.01\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 5251 | tagging_loss_video: 6.118|tagging_loss_audio: 9.706|tagging_loss_text: 15.913|tagging_loss_image: 6.743|tagging_loss_fusion: 5.328|total_loss: 43.807 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 5252 | tagging_loss_video: 5.897|tagging_loss_audio: 8.331|tagging_loss_text: 12.523|tagging_loss_image: 5.199|tagging_loss_fusion: 5.915|total_loss: 37.865 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 5253 | tagging_loss_video: 4.503|tagging_loss_audio: 8.892|tagging_loss_text: 15.124|tagging_loss_image: 4.268|tagging_loss_fusion: 3.166|total_loss: 35.953 | 72.59 Examples/sec\n",
      "INFO:tensorflow:training step 5254 | tagging_loss_video: 5.723|tagging_loss_audio: 9.892|tagging_loss_text: 14.862|tagging_loss_image: 5.971|tagging_loss_fusion: 5.467|total_loss: 41.915 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 5255 | tagging_loss_video: 5.702|tagging_loss_audio: 8.408|tagging_loss_text: 9.748|tagging_loss_image: 5.352|tagging_loss_fusion: 5.871|total_loss: 35.081 | 63.65 Examples/sec\n",
      "INFO:tensorflow:training step 5256 | tagging_loss_video: 6.369|tagging_loss_audio: 9.544|tagging_loss_text: 14.546|tagging_loss_image: 5.874|tagging_loss_fusion: 5.558|total_loss: 41.891 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 5257 | tagging_loss_video: 5.620|tagging_loss_audio: 10.250|tagging_loss_text: 17.278|tagging_loss_image: 5.947|tagging_loss_fusion: 5.731|total_loss: 44.825 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 5258 | tagging_loss_video: 4.046|tagging_loss_audio: 8.626|tagging_loss_text: 14.507|tagging_loss_image: 4.157|tagging_loss_fusion: 2.223|total_loss: 33.559 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 5259 | tagging_loss_video: 5.180|tagging_loss_audio: 9.004|tagging_loss_text: 13.366|tagging_loss_image: 4.790|tagging_loss_fusion: 3.657|total_loss: 35.997 | 65.99 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5260 |tagging_loss_video: 7.653|tagging_loss_audio: 9.228|tagging_loss_text: 14.909|tagging_loss_image: 6.502|tagging_loss_fusion: 6.348|total_loss: 44.641 | Examples/sec: 69.22\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5261 | tagging_loss_video: 4.451|tagging_loss_audio: 7.705|tagging_loss_text: 16.408|tagging_loss_image: 5.657|tagging_loss_fusion: 3.825|total_loss: 38.045 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 5262 | tagging_loss_video: 6.685|tagging_loss_audio: 10.593|tagging_loss_text: 17.623|tagging_loss_image: 5.511|tagging_loss_fusion: 6.112|total_loss: 46.523 | 66.24 Examples/sec\n",
      "INFO:tensorflow:training step 5263 | tagging_loss_video: 5.148|tagging_loss_audio: 8.542|tagging_loss_text: 14.215|tagging_loss_image: 4.561|tagging_loss_fusion: 3.735|total_loss: 36.202 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 5264 | tagging_loss_video: 5.513|tagging_loss_audio: 9.728|tagging_loss_text: 15.892|tagging_loss_image: 5.683|tagging_loss_fusion: 4.249|total_loss: 41.066 | 61.60 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 5265.\n",
      "INFO:tensorflow:training step 5265 | tagging_loss_video: 5.739|tagging_loss_audio: 9.890|tagging_loss_text: 15.125|tagging_loss_image: 6.542|tagging_loss_fusion: 6.326|total_loss: 43.623 | 51.44 Examples/sec\n",
      "INFO:tensorflow:training step 5266 | tagging_loss_video: 5.918|tagging_loss_audio: 8.988|tagging_loss_text: 17.406|tagging_loss_image: 6.276|tagging_loss_fusion: 5.415|total_loss: 44.004 | 56.06 Examples/sec\n",
      "INFO:tensorflow:training step 5267 | tagging_loss_video: 6.634|tagging_loss_audio: 11.292|tagging_loss_text: 17.557|tagging_loss_image: 6.658|tagging_loss_fusion: 5.744|total_loss: 47.885 | 71.77 Examples/sec\n",
      "INFO:tensorflow:training step 5268 | tagging_loss_video: 6.113|tagging_loss_audio: 8.226|tagging_loss_text: 15.910|tagging_loss_image: 5.683|tagging_loss_fusion: 4.682|total_loss: 40.614 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 5269 | tagging_loss_video: 5.854|tagging_loss_audio: 9.443|tagging_loss_text: 15.305|tagging_loss_image: 3.633|tagging_loss_fusion: 4.206|total_loss: 38.440 | 64.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5270 |tagging_loss_video: 5.980|tagging_loss_audio: 8.381|tagging_loss_text: 16.771|tagging_loss_image: 5.381|tagging_loss_fusion: 3.954|total_loss: 40.467 | Examples/sec: 68.70\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5271 | tagging_loss_video: 5.781|tagging_loss_audio: 9.605|tagging_loss_text: 16.448|tagging_loss_image: 5.175|tagging_loss_fusion: 4.409|total_loss: 41.419 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 5272 | tagging_loss_video: 6.224|tagging_loss_audio: 9.081|tagging_loss_text: 14.633|tagging_loss_image: 5.416|tagging_loss_fusion: 6.060|total_loss: 41.415 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 5273 | tagging_loss_video: 5.724|tagging_loss_audio: 10.877|tagging_loss_text: 20.059|tagging_loss_image: 6.944|tagging_loss_fusion: 5.990|total_loss: 49.594 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 5274 | tagging_loss_video: 6.325|tagging_loss_audio: 9.488|tagging_loss_text: 17.006|tagging_loss_image: 4.164|tagging_loss_fusion: 3.675|total_loss: 40.657 | 61.72 Examples/sec\n",
      "INFO:tensorflow:training step 5275 | tagging_loss_video: 6.586|tagging_loss_audio: 8.891|tagging_loss_text: 13.117|tagging_loss_image: 3.773|tagging_loss_fusion: 3.950|total_loss: 36.317 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 5276 | tagging_loss_video: 5.015|tagging_loss_audio: 8.584|tagging_loss_text: 14.991|tagging_loss_image: 5.268|tagging_loss_fusion: 3.296|total_loss: 37.154 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 5277 | tagging_loss_video: 5.917|tagging_loss_audio: 7.704|tagging_loss_text: 16.062|tagging_loss_image: 4.746|tagging_loss_fusion: 5.620|total_loss: 40.049 | 59.87 Examples/sec\n",
      "INFO:tensorflow:training step 5278 | tagging_loss_video: 4.644|tagging_loss_audio: 9.974|tagging_loss_text: 13.922|tagging_loss_image: 5.075|tagging_loss_fusion: 3.193|total_loss: 36.809 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 5279 | tagging_loss_video: 6.542|tagging_loss_audio: 8.901|tagging_loss_text: 16.888|tagging_loss_image: 5.232|tagging_loss_fusion: 4.771|total_loss: 42.333 | 71.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5280 |tagging_loss_video: 5.630|tagging_loss_audio: 7.853|tagging_loss_text: 17.186|tagging_loss_image: 4.420|tagging_loss_fusion: 4.722|total_loss: 39.811 | Examples/sec: 70.68\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.97 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5281 | tagging_loss_video: 5.542|tagging_loss_audio: 8.379|tagging_loss_text: 16.606|tagging_loss_image: 4.817|tagging_loss_fusion: 3.955|total_loss: 39.299 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 5282 | tagging_loss_video: 6.110|tagging_loss_audio: 8.945|tagging_loss_text: 17.547|tagging_loss_image: 5.256|tagging_loss_fusion: 4.604|total_loss: 42.461 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 5283 | tagging_loss_video: 6.046|tagging_loss_audio: 8.028|tagging_loss_text: 15.181|tagging_loss_image: 5.148|tagging_loss_fusion: 5.373|total_loss: 39.776 | 63.21 Examples/sec\n",
      "INFO:tensorflow:training step 5284 | tagging_loss_video: 5.516|tagging_loss_audio: 8.622|tagging_loss_text: 13.929|tagging_loss_image: 5.466|tagging_loss_fusion: 4.331|total_loss: 37.864 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 5285 | tagging_loss_video: 6.452|tagging_loss_audio: 8.329|tagging_loss_text: 13.262|tagging_loss_image: 6.007|tagging_loss_fusion: 4.000|total_loss: 38.050 | 66.23 Examples/sec\n",
      "INFO:tensorflow:training step 5286 | tagging_loss_video: 5.758|tagging_loss_audio: 9.676|tagging_loss_text: 12.047|tagging_loss_image: 4.423|tagging_loss_fusion: 3.318|total_loss: 35.222 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 5287 | tagging_loss_video: 5.843|tagging_loss_audio: 10.204|tagging_loss_text: 16.511|tagging_loss_image: 5.757|tagging_loss_fusion: 4.850|total_loss: 43.166 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 5288 | tagging_loss_video: 5.947|tagging_loss_audio: 8.400|tagging_loss_text: 14.236|tagging_loss_image: 4.828|tagging_loss_fusion: 4.169|total_loss: 37.580 | 63.04 Examples/sec\n",
      "INFO:tensorflow:training step 5289 | tagging_loss_video: 4.878|tagging_loss_audio: 8.836|tagging_loss_text: 14.670|tagging_loss_image: 5.257|tagging_loss_fusion: 3.477|total_loss: 37.119 | 71.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5290 |tagging_loss_video: 4.403|tagging_loss_audio: 7.760|tagging_loss_text: 15.279|tagging_loss_image: 5.721|tagging_loss_fusion: 2.739|total_loss: 35.901 | Examples/sec: 69.18\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 5291 | tagging_loss_video: 4.945|tagging_loss_audio: 9.179|tagging_loss_text: 16.910|tagging_loss_image: 4.854|tagging_loss_fusion: 3.397|total_loss: 39.285 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 5292 | tagging_loss_video: 6.479|tagging_loss_audio: 9.332|tagging_loss_text: 11.848|tagging_loss_image: 4.619|tagging_loss_fusion: 4.026|total_loss: 36.305 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 5293 | tagging_loss_video: 6.727|tagging_loss_audio: 9.330|tagging_loss_text: 15.957|tagging_loss_image: 6.265|tagging_loss_fusion: 6.164|total_loss: 44.443 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 5294 | tagging_loss_video: 6.307|tagging_loss_audio: 9.389|tagging_loss_text: 14.113|tagging_loss_image: 5.840|tagging_loss_fusion: 6.010|total_loss: 41.659 | 65.53 Examples/sec\n",
      "INFO:tensorflow:training step 5295 | tagging_loss_video: 4.956|tagging_loss_audio: 8.769|tagging_loss_text: 14.389|tagging_loss_image: 5.450|tagging_loss_fusion: 4.204|total_loss: 37.768 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 5296 | tagging_loss_video: 6.398|tagging_loss_audio: 9.249|tagging_loss_text: 15.674|tagging_loss_image: 6.107|tagging_loss_fusion: 4.325|total_loss: 41.753 | 71.31 Examples/sec\n",
      "INFO:tensorflow:training step 5297 | tagging_loss_video: 6.439|tagging_loss_audio: 8.777|tagging_loss_text: 12.012|tagging_loss_image: 5.608|tagging_loss_fusion: 6.437|total_loss: 39.274 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 5298 | tagging_loss_video: 5.402|tagging_loss_audio: 8.777|tagging_loss_text: 17.216|tagging_loss_image: 5.179|tagging_loss_fusion: 3.251|total_loss: 39.825 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 5299 | tagging_loss_video: 5.209|tagging_loss_audio: 8.229|tagging_loss_text: 13.195|tagging_loss_image: 5.420|tagging_loss_fusion: 3.857|total_loss: 35.909 | 61.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5300 |tagging_loss_video: 3.979|tagging_loss_audio: 9.432|tagging_loss_text: 15.086|tagging_loss_image: 6.078|tagging_loss_fusion: 2.443|total_loss: 37.018 | Examples/sec: 70.08\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.88 | precision@0.5: 0.97 |recall@0.1: 1.00 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5301 | tagging_loss_video: 6.067|tagging_loss_audio: 8.212|tagging_loss_text: 17.456|tagging_loss_image: 5.978|tagging_loss_fusion: 6.129|total_loss: 43.842 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 5302 | tagging_loss_video: 4.785|tagging_loss_audio: 7.488|tagging_loss_text: 14.720|tagging_loss_image: 5.190|tagging_loss_fusion: 2.770|total_loss: 34.953 | 64.77 Examples/sec\n",
      "INFO:tensorflow:training step 5303 | tagging_loss_video: 3.804|tagging_loss_audio: 8.637|tagging_loss_text: 14.874|tagging_loss_image: 5.188|tagging_loss_fusion: 2.573|total_loss: 35.075 | 69.12 Examples/sec\n",
      "INFO:tensorflow:training step 5304 | tagging_loss_video: 6.294|tagging_loss_audio: 8.826|tagging_loss_text: 13.816|tagging_loss_image: 5.554|tagging_loss_fusion: 5.297|total_loss: 39.788 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 5305 | tagging_loss_video: 5.253|tagging_loss_audio: 8.262|tagging_loss_text: 13.526|tagging_loss_image: 4.972|tagging_loss_fusion: 4.623|total_loss: 36.636 | 64.23 Examples/sec\n",
      "INFO:tensorflow:training step 5306 | tagging_loss_video: 4.109|tagging_loss_audio: 9.764|tagging_loss_text: 16.283|tagging_loss_image: 5.763|tagging_loss_fusion: 2.778|total_loss: 38.697 | 69.43 Examples/sec\n",
      "INFO:tensorflow:training step 5307 | tagging_loss_video: 6.169|tagging_loss_audio: 9.158|tagging_loss_text: 16.691|tagging_loss_image: 5.086|tagging_loss_fusion: 3.653|total_loss: 40.758 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 5308 | tagging_loss_video: 5.771|tagging_loss_audio: 9.557|tagging_loss_text: 18.378|tagging_loss_image: 2.649|tagging_loss_fusion: 3.438|total_loss: 39.792 | 65.89 Examples/sec\n",
      "INFO:tensorflow:training step 5309 | tagging_loss_video: 4.317|tagging_loss_audio: 8.966|tagging_loss_text: 14.238|tagging_loss_image: 5.426|tagging_loss_fusion: 3.306|total_loss: 36.254 | 68.63 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5310 |tagging_loss_video: 5.311|tagging_loss_audio: 9.045|tagging_loss_text: 14.112|tagging_loss_image: 5.868|tagging_loss_fusion: 4.008|total_loss: 38.344 | Examples/sec: 68.06\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5311 | tagging_loss_video: 6.421|tagging_loss_audio: 8.543|tagging_loss_text: 16.634|tagging_loss_image: 5.319|tagging_loss_fusion: 4.614|total_loss: 41.532 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 5312 | tagging_loss_video: 5.258|tagging_loss_audio: 9.117|tagging_loss_text: 14.042|tagging_loss_image: 4.265|tagging_loss_fusion: 3.995|total_loss: 36.677 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 5313 | tagging_loss_video: 5.181|tagging_loss_audio: 8.613|tagging_loss_text: 13.666|tagging_loss_image: 5.548|tagging_loss_fusion: 4.533|total_loss: 37.540 | 65.36 Examples/sec\n",
      "INFO:tensorflow:training step 5314 | tagging_loss_video: 4.265|tagging_loss_audio: 8.306|tagging_loss_text: 14.074|tagging_loss_image: 3.736|tagging_loss_fusion: 3.301|total_loss: 33.682 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 5315 | tagging_loss_video: 5.688|tagging_loss_audio: 7.858|tagging_loss_text: 16.298|tagging_loss_image: 4.931|tagging_loss_fusion: 4.662|total_loss: 39.437 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 5316 | tagging_loss_video: 5.238|tagging_loss_audio: 8.477|tagging_loss_text: 16.279|tagging_loss_image: 4.815|tagging_loss_fusion: 3.082|total_loss: 37.891 | 66.77 Examples/sec\n",
      "INFO:tensorflow:training step 5317 | tagging_loss_video: 6.011|tagging_loss_audio: 8.431|tagging_loss_text: 17.337|tagging_loss_image: 5.337|tagging_loss_fusion: 4.751|total_loss: 41.868 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 5318 | tagging_loss_video: 6.026|tagging_loss_audio: 8.301|tagging_loss_text: 15.133|tagging_loss_image: 4.386|tagging_loss_fusion: 5.218|total_loss: 39.065 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 5319 | tagging_loss_video: 5.601|tagging_loss_audio: 7.705|tagging_loss_text: 17.460|tagging_loss_image: 4.406|tagging_loss_fusion: 4.349|total_loss: 39.521 | 61.52 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5320 |tagging_loss_video: 5.840|tagging_loss_audio: 9.884|tagging_loss_text: 18.493|tagging_loss_image: 5.748|tagging_loss_fusion: 5.065|total_loss: 45.030 | Examples/sec: 70.33\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5321 | tagging_loss_video: 5.235|tagging_loss_audio: 8.815|tagging_loss_text: 15.918|tagging_loss_image: 5.829|tagging_loss_fusion: 4.277|total_loss: 40.072 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5322 | tagging_loss_video: 5.993|tagging_loss_audio: 8.587|tagging_loss_text: 17.556|tagging_loss_image: 5.229|tagging_loss_fusion: 4.692|total_loss: 42.057 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 5323 | tagging_loss_video: 5.730|tagging_loss_audio: 10.089|tagging_loss_text: 18.341|tagging_loss_image: 4.485|tagging_loss_fusion: 4.404|total_loss: 43.048 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 5324 | tagging_loss_video: 5.441|tagging_loss_audio: 8.973|tagging_loss_text: 14.147|tagging_loss_image: 5.192|tagging_loss_fusion: 4.022|total_loss: 37.774 | 61.61 Examples/sec\n",
      "INFO:tensorflow:training step 5325 | tagging_loss_video: 4.876|tagging_loss_audio: 8.617|tagging_loss_text: 14.121|tagging_loss_image: 6.221|tagging_loss_fusion: 3.415|total_loss: 37.250 | 71.15 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5326 | tagging_loss_video: 5.695|tagging_loss_audio: 8.625|tagging_loss_text: 14.637|tagging_loss_image: 6.901|tagging_loss_fusion: 4.578|total_loss: 40.437 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 5327 | tagging_loss_video: 4.587|tagging_loss_audio: 8.330|tagging_loss_text: 17.290|tagging_loss_image: 6.393|tagging_loss_fusion: 3.313|total_loss: 39.913 | 62.72 Examples/sec\n",
      "INFO:tensorflow:training step 5328 | tagging_loss_video: 5.569|tagging_loss_audio: 8.609|tagging_loss_text: 13.908|tagging_loss_image: 5.215|tagging_loss_fusion: 4.575|total_loss: 37.875 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 5329 | tagging_loss_video: 5.779|tagging_loss_audio: 9.041|tagging_loss_text: 15.416|tagging_loss_image: 5.023|tagging_loss_fusion: 5.442|total_loss: 40.701 | 70.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5330 |tagging_loss_video: 7.121|tagging_loss_audio: 9.314|tagging_loss_text: 19.766|tagging_loss_image: 5.978|tagging_loss_fusion: 5.751|total_loss: 47.930 | Examples/sec: 63.77\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5331 | tagging_loss_video: 5.242|tagging_loss_audio: 8.467|tagging_loss_text: 14.117|tagging_loss_image: 4.365|tagging_loss_fusion: 4.260|total_loss: 36.452 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 5332 | tagging_loss_video: 6.020|tagging_loss_audio: 8.118|tagging_loss_text: 17.145|tagging_loss_image: 5.275|tagging_loss_fusion: 5.137|total_loss: 41.696 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 5333 | tagging_loss_video: 5.558|tagging_loss_audio: 8.315|tagging_loss_text: 14.777|tagging_loss_image: 5.549|tagging_loss_fusion: 9.403|total_loss: 43.603 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 5334 | tagging_loss_video: 5.195|tagging_loss_audio: 8.385|tagging_loss_text: 13.276|tagging_loss_image: 5.380|tagging_loss_fusion: 3.914|total_loss: 36.149 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 5335 | tagging_loss_video: 6.035|tagging_loss_audio: 8.092|tagging_loss_text: 13.684|tagging_loss_image: 4.834|tagging_loss_fusion: 4.929|total_loss: 37.574 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 5336 | tagging_loss_video: 4.030|tagging_loss_audio: 8.698|tagging_loss_text: 16.348|tagging_loss_image: 5.090|tagging_loss_fusion: 2.146|total_loss: 36.312 | 71.63 Examples/sec\n",
      "INFO:tensorflow:training step 5337 | tagging_loss_video: 6.145|tagging_loss_audio: 8.976|tagging_loss_text: 14.096|tagging_loss_image: 6.100|tagging_loss_fusion: 5.122|total_loss: 40.438 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 5338 | tagging_loss_video: 6.635|tagging_loss_audio: 9.727|tagging_loss_text: 15.947|tagging_loss_image: 4.593|tagging_loss_fusion: 5.588|total_loss: 42.491 | 65.80 Examples/sec\n",
      "INFO:tensorflow:training step 5339 | tagging_loss_video: 5.479|tagging_loss_audio: 8.649|tagging_loss_text: 15.470|tagging_loss_image: 5.090|tagging_loss_fusion: 4.678|total_loss: 39.366 | 70.36 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5340 |tagging_loss_video: 6.520|tagging_loss_audio: 9.453|tagging_loss_text: 17.504|tagging_loss_image: 6.634|tagging_loss_fusion: 4.009|total_loss: 44.121 | Examples/sec: 71.56\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5341 | tagging_loss_video: 5.125|tagging_loss_audio: 7.206|tagging_loss_text: 13.823|tagging_loss_image: 5.174|tagging_loss_fusion: 4.721|total_loss: 36.049 | 65.30 Examples/sec\n",
      "INFO:tensorflow:training step 5342 | tagging_loss_video: 4.597|tagging_loss_audio: 8.190|tagging_loss_text: 16.294|tagging_loss_image: 2.633|tagging_loss_fusion: 2.937|total_loss: 34.652 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 5343 | tagging_loss_video: 6.537|tagging_loss_audio: 8.619|tagging_loss_text: 14.099|tagging_loss_image: 5.018|tagging_loss_fusion: 5.881|total_loss: 40.154 | 66.90 Examples/sec\n",
      "INFO:tensorflow:training step 5344 | tagging_loss_video: 4.770|tagging_loss_audio: 8.028|tagging_loss_text: 14.591|tagging_loss_image: 5.254|tagging_loss_fusion: 3.400|total_loss: 36.043 | 64.33 Examples/sec\n",
      "INFO:tensorflow:training step 5345 | tagging_loss_video: 6.009|tagging_loss_audio: 8.749|tagging_loss_text: 15.409|tagging_loss_image: 6.229|tagging_loss_fusion: 6.063|total_loss: 42.459 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 5346 | tagging_loss_video: 4.542|tagging_loss_audio: 8.858|tagging_loss_text: 15.793|tagging_loss_image: 4.167|tagging_loss_fusion: 3.051|total_loss: 36.412 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 5347 | tagging_loss_video: 5.309|tagging_loss_audio: 8.317|tagging_loss_text: 13.300|tagging_loss_image: 3.228|tagging_loss_fusion: 3.682|total_loss: 33.837 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 5348 | tagging_loss_video: 4.378|tagging_loss_audio: 8.386|tagging_loss_text: 16.549|tagging_loss_image: 4.982|tagging_loss_fusion: 3.505|total_loss: 37.800 | 67.51 Examples/sec\n",
      "INFO:tensorflow:training step 5349 | tagging_loss_video: 6.851|tagging_loss_audio: 8.560|tagging_loss_text: 18.419|tagging_loss_image: 5.481|tagging_loss_fusion: 5.110|total_loss: 44.420 | 62.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5350 |tagging_loss_video: 5.180|tagging_loss_audio: 8.564|tagging_loss_text: 17.789|tagging_loss_image: 5.533|tagging_loss_fusion: 2.539|total_loss: 39.604 | Examples/sec: 70.44\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5351 | tagging_loss_video: 5.527|tagging_loss_audio: 8.252|tagging_loss_text: 13.012|tagging_loss_image: 4.889|tagging_loss_fusion: 4.888|total_loss: 36.568 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 5352 | tagging_loss_video: 5.384|tagging_loss_audio: 8.350|tagging_loss_text: 16.736|tagging_loss_image: 5.354|tagging_loss_fusion: 4.348|total_loss: 40.173 | 63.78 Examples/sec\n",
      "INFO:tensorflow:training step 5353 | tagging_loss_video: 6.555|tagging_loss_audio: 9.292|tagging_loss_text: 15.198|tagging_loss_image: 5.248|tagging_loss_fusion: 4.940|total_loss: 41.233 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 5354 | tagging_loss_video: 5.733|tagging_loss_audio: 9.309|tagging_loss_text: 17.452|tagging_loss_image: 5.106|tagging_loss_fusion: 4.936|total_loss: 42.536 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 5355 | tagging_loss_video: 4.053|tagging_loss_audio: 7.961|tagging_loss_text: 12.216|tagging_loss_image: 4.900|tagging_loss_fusion: 2.702|total_loss: 31.832 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 5356 | tagging_loss_video: 5.473|tagging_loss_audio: 7.712|tagging_loss_text: 15.409|tagging_loss_image: 5.645|tagging_loss_fusion: 4.559|total_loss: 38.797 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 5357 | tagging_loss_video: 5.438|tagging_loss_audio: 8.231|tagging_loss_text: 14.473|tagging_loss_image: 4.857|tagging_loss_fusion: 3.648|total_loss: 36.648 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 5358 | tagging_loss_video: 5.390|tagging_loss_audio: 8.036|tagging_loss_text: 15.661|tagging_loss_image: 5.624|tagging_loss_fusion: 3.831|total_loss: 38.543 | 65.07 Examples/sec\n",
      "INFO:tensorflow:training step 5359 | tagging_loss_video: 6.273|tagging_loss_audio: 7.684|tagging_loss_text: 12.385|tagging_loss_image: 5.800|tagging_loss_fusion: 6.061|total_loss: 38.203 | 69.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5360 |tagging_loss_video: 5.479|tagging_loss_audio: 7.702|tagging_loss_text: 11.882|tagging_loss_image: 5.741|tagging_loss_fusion: 4.136|total_loss: 34.939 | Examples/sec: 67.57\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5361 | tagging_loss_video: 5.627|tagging_loss_audio: 8.009|tagging_loss_text: 13.281|tagging_loss_image: 5.317|tagging_loss_fusion: 4.567|total_loss: 36.801 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 5362 | tagging_loss_video: 5.768|tagging_loss_audio: 8.324|tagging_loss_text: 17.416|tagging_loss_image: 4.875|tagging_loss_fusion: 4.299|total_loss: 40.682 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 5363 | tagging_loss_video: 4.990|tagging_loss_audio: 9.361|tagging_loss_text: 17.393|tagging_loss_image: 5.405|tagging_loss_fusion: 2.857|total_loss: 40.005 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 5364 | tagging_loss_video: 6.989|tagging_loss_audio: 9.606|tagging_loss_text: 15.452|tagging_loss_image: 4.632|tagging_loss_fusion: 5.168|total_loss: 41.847 | 60.76 Examples/sec\n",
      "INFO:tensorflow:training step 5365 | tagging_loss_video: 5.910|tagging_loss_audio: 8.817|tagging_loss_text: 13.487|tagging_loss_image: 5.157|tagging_loss_fusion: 6.475|total_loss: 39.845 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 5366 | tagging_loss_video: 6.080|tagging_loss_audio: 9.311|tagging_loss_text: 12.140|tagging_loss_image: 5.965|tagging_loss_fusion: 5.416|total_loss: 38.911 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 5367 | tagging_loss_video: 5.649|tagging_loss_audio: 7.636|tagging_loss_text: 13.311|tagging_loss_image: 4.201|tagging_loss_fusion: 4.254|total_loss: 35.051 | 60.18 Examples/sec\n",
      "INFO:tensorflow:training step 5368 | tagging_loss_video: 4.616|tagging_loss_audio: 6.842|tagging_loss_text: 10.082|tagging_loss_image: 4.677|tagging_loss_fusion: 3.941|total_loss: 30.158 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 5369 | tagging_loss_video: 6.017|tagging_loss_audio: 8.872|tagging_loss_text: 16.672|tagging_loss_image: 5.582|tagging_loss_fusion: 8.413|total_loss: 45.556 | 71.03 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5370 |tagging_loss_video: 5.587|tagging_loss_audio: 8.893|tagging_loss_text: 14.831|tagging_loss_image: 5.527|tagging_loss_fusion: 4.483|total_loss: 39.320 | Examples/sec: 61.34\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5371 | tagging_loss_video: 4.320|tagging_loss_audio: 9.337|tagging_loss_text: 15.178|tagging_loss_image: 5.088|tagging_loss_fusion: 2.429|total_loss: 36.351 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 5372 | tagging_loss_video: 5.478|tagging_loss_audio: 7.510|tagging_loss_text: 16.450|tagging_loss_image: 6.321|tagging_loss_fusion: 5.179|total_loss: 40.937 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 5373 | tagging_loss_video: 4.486|tagging_loss_audio: 8.238|tagging_loss_text: 16.035|tagging_loss_image: 5.461|tagging_loss_fusion: 3.395|total_loss: 37.615 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 5374 | tagging_loss_video: 5.704|tagging_loss_audio: 8.930|tagging_loss_text: 14.564|tagging_loss_image: 5.492|tagging_loss_fusion: 4.299|total_loss: 38.988 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 5375 | tagging_loss_video: 5.606|tagging_loss_audio: 9.286|tagging_loss_text: 16.974|tagging_loss_image: 5.250|tagging_loss_fusion: 4.603|total_loss: 41.719 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 5376 | tagging_loss_video: 5.999|tagging_loss_audio: 8.153|tagging_loss_text: 15.374|tagging_loss_image: 5.652|tagging_loss_fusion: 4.646|total_loss: 39.823 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 5377 | tagging_loss_video: 6.181|tagging_loss_audio: 9.404|tagging_loss_text: 14.810|tagging_loss_image: 5.800|tagging_loss_fusion: 4.740|total_loss: 40.936 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 5378 | tagging_loss_video: 5.389|tagging_loss_audio: 8.518|tagging_loss_text: 15.073|tagging_loss_image: 4.342|tagging_loss_fusion: 4.233|total_loss: 37.556 | 66.80 Examples/sec\n",
      "INFO:tensorflow:training step 5379 | tagging_loss_video: 4.880|tagging_loss_audio: 7.988|tagging_loss_text: 12.076|tagging_loss_image: 4.432|tagging_loss_fusion: 3.350|total_loss: 32.726 | 70.23 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5380 |tagging_loss_video: 5.286|tagging_loss_audio: 8.361|tagging_loss_text: 13.089|tagging_loss_image: 4.702|tagging_loss_fusion: 3.227|total_loss: 34.665 | Examples/sec: 69.61\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5381 | tagging_loss_video: 5.687|tagging_loss_audio: 8.737|tagging_loss_text: 16.222|tagging_loss_image: 4.988|tagging_loss_fusion: 3.284|total_loss: 38.918 | 64.38 Examples/sec\n",
      "INFO:tensorflow:training step 5382 | tagging_loss_video: 5.228|tagging_loss_audio: 8.731|tagging_loss_text: 19.272|tagging_loss_image: 5.310|tagging_loss_fusion: 2.524|total_loss: 41.064 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 5383 | tagging_loss_video: 5.192|tagging_loss_audio: 7.893|tagging_loss_text: 14.865|tagging_loss_image: 4.410|tagging_loss_fusion: 3.703|total_loss: 36.061 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 5384 | tagging_loss_video: 6.047|tagging_loss_audio: 9.145|tagging_loss_text: 13.945|tagging_loss_image: 5.707|tagging_loss_fusion: 3.170|total_loss: 38.014 | 65.86 Examples/sec\n",
      "INFO:tensorflow:training step 5385 | tagging_loss_video: 5.643|tagging_loss_audio: 8.392|tagging_loss_text: 13.520|tagging_loss_image: 5.311|tagging_loss_fusion: 4.550|total_loss: 37.415 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 5386 | tagging_loss_video: 5.976|tagging_loss_audio: 9.876|tagging_loss_text: 14.944|tagging_loss_image: 5.622|tagging_loss_fusion: 5.273|total_loss: 41.692 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 5387 | tagging_loss_video: 5.421|tagging_loss_audio: 9.576|tagging_loss_text: 12.879|tagging_loss_image: 5.473|tagging_loss_fusion: 3.505|total_loss: 36.854 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 5388 | tagging_loss_video: 4.720|tagging_loss_audio: 8.935|tagging_loss_text: 15.334|tagging_loss_image: 4.990|tagging_loss_fusion: 3.275|total_loss: 37.254 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 5389 | tagging_loss_video: 6.234|tagging_loss_audio: 8.257|tagging_loss_text: 13.633|tagging_loss_image: 5.928|tagging_loss_fusion: 6.561|total_loss: 40.613 | 60.86 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5390 |tagging_loss_video: 6.965|tagging_loss_audio: 8.587|tagging_loss_text: 16.628|tagging_loss_image: 6.100|tagging_loss_fusion: 7.012|total_loss: 45.291 | Examples/sec: 71.87\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 5391 | tagging_loss_video: 5.370|tagging_loss_audio: 9.411|tagging_loss_text: 14.506|tagging_loss_image: 4.686|tagging_loss_fusion: 3.704|total_loss: 37.677 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 5392 | tagging_loss_video: 5.098|tagging_loss_audio: 9.687|tagging_loss_text: 18.550|tagging_loss_image: 6.207|tagging_loss_fusion: 3.758|total_loss: 43.300 | 62.64 Examples/sec\n",
      "INFO:tensorflow:training step 5393 | tagging_loss_video: 6.410|tagging_loss_audio: 9.448|tagging_loss_text: 16.361|tagging_loss_image: 6.179|tagging_loss_fusion: 5.691|total_loss: 44.090 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 5394 | tagging_loss_video: 4.861|tagging_loss_audio: 8.061|tagging_loss_text: 14.951|tagging_loss_image: 4.929|tagging_loss_fusion: 4.214|total_loss: 37.016 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 5395 | tagging_loss_video: 6.433|tagging_loss_audio: 8.651|tagging_loss_text: 15.246|tagging_loss_image: 5.354|tagging_loss_fusion: 4.837|total_loss: 40.520 | 63.52 Examples/sec\n",
      "INFO:tensorflow:training step 5396 | tagging_loss_video: 5.713|tagging_loss_audio: 9.412|tagging_loss_text: 15.223|tagging_loss_image: 5.379|tagging_loss_fusion: 4.066|total_loss: 39.793 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 5397 | tagging_loss_video: 6.360|tagging_loss_audio: 8.707|tagging_loss_text: 18.840|tagging_loss_image: 6.407|tagging_loss_fusion: 5.238|total_loss: 45.552 | 68.48 Examples/sec\n",
      "INFO:tensorflow:training step 5398 | tagging_loss_video: 5.204|tagging_loss_audio: 9.766|tagging_loss_text: 16.892|tagging_loss_image: 5.488|tagging_loss_fusion: 3.485|total_loss: 40.836 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 5399 | tagging_loss_video: 6.577|tagging_loss_audio: 10.264|tagging_loss_text: 13.408|tagging_loss_image: 5.996|tagging_loss_fusion: 5.817|total_loss: 42.063 | 67.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5400 |tagging_loss_video: 5.610|tagging_loss_audio: 10.821|tagging_loss_text: 14.445|tagging_loss_image: 4.148|tagging_loss_fusion: 4.256|total_loss: 39.280 | Examples/sec: 70.56\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.96 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5401 | tagging_loss_video: 5.279|tagging_loss_audio: 8.719|tagging_loss_text: 16.920|tagging_loss_image: 5.988|tagging_loss_fusion: 4.639|total_loss: 41.545 | 66.95 Examples/sec\n",
      "INFO:tensorflow:training step 5402 | tagging_loss_video: 5.585|tagging_loss_audio: 7.626|tagging_loss_text: 13.583|tagging_loss_image: 4.571|tagging_loss_fusion: 3.251|total_loss: 34.616 | 68.32 Examples/sec\n",
      "INFO:tensorflow:training step 5403 | tagging_loss_video: 6.610|tagging_loss_audio: 8.696|tagging_loss_text: 18.781|tagging_loss_image: 5.103|tagging_loss_fusion: 6.650|total_loss: 45.840 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 5404 | tagging_loss_video: 5.744|tagging_loss_audio: 9.430|tagging_loss_text: 15.143|tagging_loss_image: 5.524|tagging_loss_fusion: 4.433|total_loss: 40.274 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 5405 | tagging_loss_video: 6.677|tagging_loss_audio: 8.793|tagging_loss_text: 20.379|tagging_loss_image: 6.253|tagging_loss_fusion: 6.280|total_loss: 48.382 | 65.33 Examples/sec\n",
      "INFO:tensorflow:training step 5406 | tagging_loss_video: 5.579|tagging_loss_audio: 9.329|tagging_loss_text: 17.418|tagging_loss_image: 6.252|tagging_loss_fusion: 5.031|total_loss: 43.608 | 61.16 Examples/sec\n",
      "INFO:tensorflow:training step 5407 | tagging_loss_video: 5.624|tagging_loss_audio: 11.037|tagging_loss_text: 16.787|tagging_loss_image: 5.468|tagging_loss_fusion: 4.428|total_loss: 43.344 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 5408 | tagging_loss_video: 6.531|tagging_loss_audio: 10.505|tagging_loss_text: 14.527|tagging_loss_image: 4.571|tagging_loss_fusion: 4.204|total_loss: 40.337 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 5409 | tagging_loss_video: 4.772|tagging_loss_audio: 9.505|tagging_loss_text: 13.978|tagging_loss_image: 3.716|tagging_loss_fusion: 3.645|total_loss: 35.616 | 65.48 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5410 |tagging_loss_video: 5.002|tagging_loss_audio: 8.944|tagging_loss_text: 15.418|tagging_loss_image: 6.225|tagging_loss_fusion: 4.216|total_loss: 39.806 | Examples/sec: 69.63\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5411 | tagging_loss_video: 5.638|tagging_loss_audio: 8.915|tagging_loss_text: 17.134|tagging_loss_image: 5.994|tagging_loss_fusion: 6.182|total_loss: 43.863 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 5412 | tagging_loss_video: 5.854|tagging_loss_audio: 9.867|tagging_loss_text: 17.283|tagging_loss_image: 5.980|tagging_loss_fusion: 4.541|total_loss: 43.525 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 5413 | tagging_loss_video: 6.255|tagging_loss_audio: 10.997|tagging_loss_text: 17.802|tagging_loss_image: 6.047|tagging_loss_fusion: 4.890|total_loss: 45.992 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 5414 | tagging_loss_video: 6.047|tagging_loss_audio: 9.861|tagging_loss_text: 14.043|tagging_loss_image: 6.630|tagging_loss_fusion: 4.670|total_loss: 41.252 | 61.77 Examples/sec\n",
      "INFO:tensorflow:training step 5415 | tagging_loss_video: 5.689|tagging_loss_audio: 9.116|tagging_loss_text: 15.069|tagging_loss_image: 5.243|tagging_loss_fusion: 3.801|total_loss: 38.918 | 69.20 Examples/sec\n",
      "INFO:tensorflow:training step 5416 | tagging_loss_video: 4.207|tagging_loss_audio: 8.924|tagging_loss_text: 14.997|tagging_loss_image: 5.785|tagging_loss_fusion: 3.930|total_loss: 37.843 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 5417 | tagging_loss_video: 5.731|tagging_loss_audio: 8.170|tagging_loss_text: 15.414|tagging_loss_image: 5.741|tagging_loss_fusion: 3.911|total_loss: 38.967 | 60.72 Examples/sec\n",
      "INFO:tensorflow:training step 5418 | tagging_loss_video: 6.233|tagging_loss_audio: 9.851|tagging_loss_text: 13.940|tagging_loss_image: 4.260|tagging_loss_fusion: 3.911|total_loss: 38.195 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 5419 | tagging_loss_video: 5.172|tagging_loss_audio: 9.203|tagging_loss_text: 14.857|tagging_loss_image: 5.152|tagging_loss_fusion: 3.965|total_loss: 38.348 | 69.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5420 |tagging_loss_video: 5.998|tagging_loss_audio: 8.599|tagging_loss_text: 13.205|tagging_loss_image: 5.028|tagging_loss_fusion: 4.692|total_loss: 37.523 | Examples/sec: 66.64\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5421 | tagging_loss_video: 4.530|tagging_loss_audio: 9.046|tagging_loss_text: 12.609|tagging_loss_image: 5.738|tagging_loss_fusion: 3.616|total_loss: 35.538 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 5422 | tagging_loss_video: 4.134|tagging_loss_audio: 9.168|tagging_loss_text: 14.842|tagging_loss_image: 5.003|tagging_loss_fusion: 3.110|total_loss: 36.257 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 5423 | tagging_loss_video: 4.880|tagging_loss_audio: 8.751|tagging_loss_text: 14.519|tagging_loss_image: 5.818|tagging_loss_fusion: 4.144|total_loss: 38.112 | 65.28 Examples/sec\n",
      "INFO:tensorflow:training step 5424 | tagging_loss_video: 4.926|tagging_loss_audio: 8.045|tagging_loss_text: 13.297|tagging_loss_image: 5.219|tagging_loss_fusion: 3.128|total_loss: 34.616 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 5425 | tagging_loss_video: 5.656|tagging_loss_audio: 9.006|tagging_loss_text: 18.857|tagging_loss_image: 6.270|tagging_loss_fusion: 5.811|total_loss: 45.599 | 66.73 Examples/sec\n",
      "INFO:tensorflow:training step 5426 | tagging_loss_video: 5.201|tagging_loss_audio: 8.226|tagging_loss_text: 14.720|tagging_loss_image: 4.683|tagging_loss_fusion: 3.376|total_loss: 36.206 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 5427 | tagging_loss_video: 5.735|tagging_loss_audio: 7.596|tagging_loss_text: 11.238|tagging_loss_image: 5.798|tagging_loss_fusion: 5.913|total_loss: 36.281 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 5428 | tagging_loss_video: 5.768|tagging_loss_audio: 8.682|tagging_loss_text: 10.996|tagging_loss_image: 5.500|tagging_loss_fusion: 5.194|total_loss: 36.141 | 59.26 Examples/sec\n",
      "INFO:tensorflow:training step 5429 | tagging_loss_video: 4.835|tagging_loss_audio: 7.436|tagging_loss_text: 15.359|tagging_loss_image: 5.110|tagging_loss_fusion: 3.467|total_loss: 36.208 | 70.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5430 |tagging_loss_video: 4.550|tagging_loss_audio: 8.212|tagging_loss_text: 9.959|tagging_loss_image: 3.779|tagging_loss_fusion: 2.114|total_loss: 28.614 | Examples/sec: 69.16\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.88 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5431 | tagging_loss_video: 6.343|tagging_loss_audio: 8.949|tagging_loss_text: 15.560|tagging_loss_image: 4.373|tagging_loss_fusion: 5.153|total_loss: 40.378 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 5432 | tagging_loss_video: 4.806|tagging_loss_audio: 10.052|tagging_loss_text: 18.110|tagging_loss_image: 5.766|tagging_loss_fusion: 3.708|total_loss: 42.442 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 5433 | tagging_loss_video: 5.988|tagging_loss_audio: 9.775|tagging_loss_text: 17.474|tagging_loss_image: 5.402|tagging_loss_fusion: 3.154|total_loss: 41.792 | 68.63 Examples/sec\n",
      "INFO:tensorflow:training step 5434 | tagging_loss_video: 4.795|tagging_loss_audio: 8.819|tagging_loss_text: 14.119|tagging_loss_image: 5.038|tagging_loss_fusion: 2.940|total_loss: 35.710 | 62.73 Examples/sec\n",
      "INFO:tensorflow:training step 5435 | tagging_loss_video: 5.409|tagging_loss_audio: 8.883|tagging_loss_text: 17.806|tagging_loss_image: 5.113|tagging_loss_fusion: 3.766|total_loss: 40.976 | 66.89 Examples/sec\n",
      "INFO:tensorflow:training step 5436 | tagging_loss_video: 6.338|tagging_loss_audio: 9.494|tagging_loss_text: 17.103|tagging_loss_image: 4.261|tagging_loss_fusion: 4.177|total_loss: 41.371 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 5437 | tagging_loss_video: 5.891|tagging_loss_audio: 8.160|tagging_loss_text: 18.682|tagging_loss_image: 4.801|tagging_loss_fusion: 4.450|total_loss: 41.984 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 5438 | tagging_loss_video: 6.372|tagging_loss_audio: 9.508|tagging_loss_text: 15.071|tagging_loss_image: 6.009|tagging_loss_fusion: 6.240|total_loss: 43.200 | 65.88 Examples/sec\n",
      "INFO:tensorflow:training step 5439 | tagging_loss_video: 4.495|tagging_loss_audio: 8.268|tagging_loss_text: 11.224|tagging_loss_image: 5.869|tagging_loss_fusion: 4.120|total_loss: 33.977 | 69.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5440 |tagging_loss_video: 6.475|tagging_loss_audio: 9.212|tagging_loss_text: 17.128|tagging_loss_image: 5.361|tagging_loss_fusion: 5.739|total_loss: 43.914 | Examples/sec: 70.73\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5441 | tagging_loss_video: 6.423|tagging_loss_audio: 8.686|tagging_loss_text: 14.655|tagging_loss_image: 5.668|tagging_loss_fusion: 5.600|total_loss: 41.031 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 5442 | tagging_loss_video: 4.503|tagging_loss_audio: 7.844|tagging_loss_text: 12.284|tagging_loss_image: 4.945|tagging_loss_fusion: 2.759|total_loss: 32.335 | 64.40 Examples/sec\n",
      "INFO:tensorflow:training step 5443 | tagging_loss_video: 4.715|tagging_loss_audio: 8.274|tagging_loss_text: 14.207|tagging_loss_image: 4.541|tagging_loss_fusion: 4.403|total_loss: 36.139 | 68.83 Examples/sec\n",
      "INFO:tensorflow:training step 5444 | tagging_loss_video: 6.366|tagging_loss_audio: 9.383|tagging_loss_text: 16.590|tagging_loss_image: 5.733|tagging_loss_fusion: 5.105|total_loss: 43.177 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 5445 | tagging_loss_video: 4.273|tagging_loss_audio: 7.782|tagging_loss_text: 12.840|tagging_loss_image: 4.467|tagging_loss_fusion: 2.574|total_loss: 31.936 | 59.97 Examples/sec\n",
      "INFO:tensorflow:training step 5446 | tagging_loss_video: 5.950|tagging_loss_audio: 10.156|tagging_loss_text: 17.007|tagging_loss_image: 3.761|tagging_loss_fusion: 4.633|total_loss: 41.506 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 5447 | tagging_loss_video: 5.656|tagging_loss_audio: 9.911|tagging_loss_text: 14.024|tagging_loss_image: 5.011|tagging_loss_fusion: 4.714|total_loss: 39.317 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 5448 | tagging_loss_video: 6.097|tagging_loss_audio: 7.910|tagging_loss_text: 14.285|tagging_loss_image: 5.137|tagging_loss_fusion: 4.772|total_loss: 38.201 | 65.58 Examples/sec\n",
      "INFO:tensorflow:training step 5449 | tagging_loss_video: 5.823|tagging_loss_audio: 8.436|tagging_loss_text: 15.651|tagging_loss_image: 5.844|tagging_loss_fusion: 4.025|total_loss: 39.778 | 69.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5450 |tagging_loss_video: 4.495|tagging_loss_audio: 9.875|tagging_loss_text: 13.448|tagging_loss_image: 4.793|tagging_loss_fusion: 2.797|total_loss: 35.408 | Examples/sec: 70.69\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5451 | tagging_loss_video: 6.149|tagging_loss_audio: 8.143|tagging_loss_text: 13.855|tagging_loss_image: 5.090|tagging_loss_fusion: 5.464|total_loss: 38.700 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 5452 | tagging_loss_video: 4.840|tagging_loss_audio: 8.237|tagging_loss_text: 15.359|tagging_loss_image: 5.484|tagging_loss_fusion: 2.400|total_loss: 36.319 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 5453 | tagging_loss_video: 5.237|tagging_loss_audio: 9.080|tagging_loss_text: 15.681|tagging_loss_image: 5.854|tagging_loss_fusion: 3.537|total_loss: 39.390 | 62.88 Examples/sec\n",
      "INFO:tensorflow:training step 5454 | tagging_loss_video: 5.838|tagging_loss_audio: 9.255|tagging_loss_text: 13.410|tagging_loss_image: 5.610|tagging_loss_fusion: 4.846|total_loss: 38.960 | 71.65 Examples/sec\n",
      "INFO:tensorflow:training step 5455 | tagging_loss_video: 4.947|tagging_loss_audio: 8.345|tagging_loss_text: 14.954|tagging_loss_image: 5.330|tagging_loss_fusion: 3.257|total_loss: 36.833 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 5456 | tagging_loss_video: 6.118|tagging_loss_audio: 8.139|tagging_loss_text: 14.291|tagging_loss_image: 5.965|tagging_loss_fusion: 6.179|total_loss: 40.692 | 63.11 Examples/sec\n",
      "INFO:tensorflow:training step 5457 | tagging_loss_video: 5.653|tagging_loss_audio: 7.086|tagging_loss_text: 15.930|tagging_loss_image: 4.815|tagging_loss_fusion: 4.196|total_loss: 37.680 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 5458 | tagging_loss_video: 6.270|tagging_loss_audio: 8.494|tagging_loss_text: 14.454|tagging_loss_image: 5.794|tagging_loss_fusion: 6.101|total_loss: 41.112 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 5459 | tagging_loss_video: 5.840|tagging_loss_audio: 7.845|tagging_loss_text: 13.267|tagging_loss_image: 5.809|tagging_loss_fusion: 3.763|total_loss: 36.524 | 62.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5460 |tagging_loss_video: 5.153|tagging_loss_audio: 10.668|tagging_loss_text: 15.548|tagging_loss_image: 5.810|tagging_loss_fusion: 3.963|total_loss: 41.142 | Examples/sec: 70.49\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5461 | tagging_loss_video: 5.575|tagging_loss_audio: 7.793|tagging_loss_text: 14.603|tagging_loss_image: 6.038|tagging_loss_fusion: 4.916|total_loss: 38.926 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 5462 | tagging_loss_video: 4.858|tagging_loss_audio: 8.315|tagging_loss_text: 14.750|tagging_loss_image: 4.803|tagging_loss_fusion: 3.189|total_loss: 35.914 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 5463 | tagging_loss_video: 6.683|tagging_loss_audio: 8.685|tagging_loss_text: 16.227|tagging_loss_image: 5.232|tagging_loss_fusion: 5.288|total_loss: 42.115 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 5464 | tagging_loss_video: 4.670|tagging_loss_audio: 8.679|tagging_loss_text: 15.511|tagging_loss_image: 5.314|tagging_loss_fusion: 3.980|total_loss: 38.154 | 62.24 Examples/sec\n",
      "INFO:tensorflow:training step 5465 | tagging_loss_video: 5.670|tagging_loss_audio: 8.190|tagging_loss_text: 18.356|tagging_loss_image: 6.325|tagging_loss_fusion: 4.533|total_loss: 43.073 | 69.85 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5466 | tagging_loss_video: 6.088|tagging_loss_audio: 9.776|tagging_loss_text: 14.641|tagging_loss_image: 6.362|tagging_loss_fusion: 4.805|total_loss: 41.672 | 71.99 Examples/sec\n",
      "INFO:tensorflow:training step 5467 | tagging_loss_video: 6.030|tagging_loss_audio: 9.197|tagging_loss_text: 17.292|tagging_loss_image: 4.926|tagging_loss_fusion: 4.612|total_loss: 42.057 | 65.60 Examples/sec\n",
      "INFO:tensorflow:training step 5468 | tagging_loss_video: 6.524|tagging_loss_audio: 8.475|tagging_loss_text: 15.302|tagging_loss_image: 5.154|tagging_loss_fusion: 6.542|total_loss: 41.998 | 68.46 Examples/sec\n",
      "INFO:tensorflow:training step 5469 | tagging_loss_video: 5.639|tagging_loss_audio: 8.895|tagging_loss_text: 14.730|tagging_loss_image: 5.767|tagging_loss_fusion: 4.572|total_loss: 39.603 | 70.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5470 |tagging_loss_video: 4.918|tagging_loss_audio: 8.303|tagging_loss_text: 15.361|tagging_loss_image: 5.546|tagging_loss_fusion: 3.150|total_loss: 37.279 | Examples/sec: 66.13\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.90 | precision@0.5: 0.99 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5471 | tagging_loss_video: 6.246|tagging_loss_audio: 8.334|tagging_loss_text: 13.464|tagging_loss_image: 5.003|tagging_loss_fusion: 4.688|total_loss: 37.734 | 70.62 Examples/sec\n",
      "INFO:tensorflow:training step 5472 | tagging_loss_video: 5.872|tagging_loss_audio: 8.709|tagging_loss_text: 15.783|tagging_loss_image: 5.464|tagging_loss_fusion: 4.725|total_loss: 40.554 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 5473 | tagging_loss_video: 6.102|tagging_loss_audio: 8.603|tagging_loss_text: 16.784|tagging_loss_image: 4.414|tagging_loss_fusion: 5.906|total_loss: 41.808 | 64.60 Examples/sec\n",
      "INFO:tensorflow:training step 5474 | tagging_loss_video: 5.032|tagging_loss_audio: 8.626|tagging_loss_text: 13.274|tagging_loss_image: 5.368|tagging_loss_fusion: 4.152|total_loss: 36.452 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 5475 | tagging_loss_video: 3.742|tagging_loss_audio: 8.725|tagging_loss_text: 14.972|tagging_loss_image: 4.053|tagging_loss_fusion: 2.843|total_loss: 34.335 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 5476 | tagging_loss_video: 5.482|tagging_loss_audio: 9.627|tagging_loss_text: 14.302|tagging_loss_image: 4.319|tagging_loss_fusion: 4.037|total_loss: 37.766 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 5477 | tagging_loss_video: 5.833|tagging_loss_audio: 9.445|tagging_loss_text: 16.314|tagging_loss_image: 5.452|tagging_loss_fusion: 4.126|total_loss: 41.169 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 5478 | tagging_loss_video: 6.096|tagging_loss_audio: 9.239|tagging_loss_text: 17.405|tagging_loss_image: 5.801|tagging_loss_fusion: 4.767|total_loss: 43.308 | 64.22 Examples/sec\n",
      "INFO:tensorflow:training step 5479 | tagging_loss_video: 5.689|tagging_loss_audio: 8.887|tagging_loss_text: 16.866|tagging_loss_image: 4.856|tagging_loss_fusion: 3.545|total_loss: 39.843 | 70.65 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5480 |tagging_loss_video: 4.703|tagging_loss_audio: 8.875|tagging_loss_text: 17.487|tagging_loss_image: 6.660|tagging_loss_fusion: 3.712|total_loss: 41.436 | Examples/sec: 70.95\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5481 | tagging_loss_video: 5.329|tagging_loss_audio: 7.490|tagging_loss_text: 15.072|tagging_loss_image: 4.376|tagging_loss_fusion: 3.853|total_loss: 36.120 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 5482 | tagging_loss_video: 5.364|tagging_loss_audio: 8.153|tagging_loss_text: 9.394|tagging_loss_image: 5.752|tagging_loss_fusion: 6.185|total_loss: 34.848 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 5483 | tagging_loss_video: 5.596|tagging_loss_audio: 9.334|tagging_loss_text: 13.440|tagging_loss_image: 5.672|tagging_loss_fusion: 5.338|total_loss: 39.380 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 5484 | tagging_loss_video: 4.605|tagging_loss_audio: 7.668|tagging_loss_text: 16.420|tagging_loss_image: 4.558|tagging_loss_fusion: 3.196|total_loss: 36.446 | 67.56 Examples/sec\n",
      "INFO:tensorflow:training step 5485 | tagging_loss_video: 5.577|tagging_loss_audio: 7.605|tagging_loss_text: 15.428|tagging_loss_image: 6.271|tagging_loss_fusion: 6.185|total_loss: 41.066 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 5486 | tagging_loss_video: 4.700|tagging_loss_audio: 8.291|tagging_loss_text: 12.506|tagging_loss_image: 4.099|tagging_loss_fusion: 2.838|total_loss: 32.434 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 5487 | tagging_loss_video: 6.442|tagging_loss_audio: 8.263|tagging_loss_text: 13.284|tagging_loss_image: 6.220|tagging_loss_fusion: 7.274|total_loss: 41.483 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 5488 | tagging_loss_video: 5.273|tagging_loss_audio: 8.599|tagging_loss_text: 14.013|tagging_loss_image: 4.909|tagging_loss_fusion: 4.534|total_loss: 37.329 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 5489 | tagging_loss_video: 4.972|tagging_loss_audio: 8.560|tagging_loss_text: 16.950|tagging_loss_image: 5.754|tagging_loss_fusion: 4.074|total_loss: 40.310 | 61.57 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5490 |tagging_loss_video: 5.637|tagging_loss_audio: 8.552|tagging_loss_text: 15.748|tagging_loss_image: 5.290|tagging_loss_fusion: 4.875|total_loss: 40.103 | Examples/sec: 70.49\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5491 | tagging_loss_video: 5.234|tagging_loss_audio: 8.514|tagging_loss_text: 16.132|tagging_loss_image: 5.230|tagging_loss_fusion: 4.338|total_loss: 39.449 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 5492 | tagging_loss_video: 4.874|tagging_loss_audio: 7.856|tagging_loss_text: 13.651|tagging_loss_image: 5.404|tagging_loss_fusion: 3.687|total_loss: 35.472 | 62.37 Examples/sec\n",
      "INFO:tensorflow:training step 5493 | tagging_loss_video: 4.417|tagging_loss_audio: 9.103|tagging_loss_text: 15.881|tagging_loss_image: 3.739|tagging_loss_fusion: 2.039|total_loss: 35.178 | 67.09 Examples/sec\n",
      "INFO:tensorflow:training step 5494 | tagging_loss_video: 5.800|tagging_loss_audio: 8.324|tagging_loss_text: 15.267|tagging_loss_image: 5.242|tagging_loss_fusion: 3.563|total_loss: 38.196 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 5495 | tagging_loss_video: 5.092|tagging_loss_audio: 8.711|tagging_loss_text: 12.210|tagging_loss_image: 4.529|tagging_loss_fusion: 3.669|total_loss: 34.210 | 63.35 Examples/sec\n",
      "INFO:tensorflow:training step 5496 | tagging_loss_video: 5.621|tagging_loss_audio: 8.160|tagging_loss_text: 11.396|tagging_loss_image: 4.846|tagging_loss_fusion: 4.257|total_loss: 34.280 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 5497 | tagging_loss_video: 4.385|tagging_loss_audio: 8.367|tagging_loss_text: 16.537|tagging_loss_image: 5.099|tagging_loss_fusion: 2.705|total_loss: 37.093 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 5498 | tagging_loss_video: 4.263|tagging_loss_audio: 8.149|tagging_loss_text: 11.733|tagging_loss_image: 4.298|tagging_loss_fusion: 3.001|total_loss: 31.444 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 5499 | tagging_loss_video: 5.225|tagging_loss_audio: 7.915|tagging_loss_text: 11.731|tagging_loss_image: 4.704|tagging_loss_fusion: 4.611|total_loss: 34.185 | 68.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5500 |tagging_loss_video: 4.546|tagging_loss_audio: 7.717|tagging_loss_text: 15.180|tagging_loss_image: 5.438|tagging_loss_fusion: 3.541|total_loss: 36.422 | Examples/sec: 67.08\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5501 | tagging_loss_video: 4.828|tagging_loss_audio: 7.308|tagging_loss_text: 15.272|tagging_loss_image: 5.395|tagging_loss_fusion: 3.499|total_loss: 36.303 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 5502 | tagging_loss_video: 4.580|tagging_loss_audio: 8.205|tagging_loss_text: 18.761|tagging_loss_image: 3.566|tagging_loss_fusion: 2.909|total_loss: 38.021 | 68.39 Examples/sec\n",
      "INFO:tensorflow:training step 5503 | tagging_loss_video: 5.809|tagging_loss_audio: 8.800|tagging_loss_text: 13.211|tagging_loss_image: 4.895|tagging_loss_fusion: 4.462|total_loss: 37.176 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 5504 | tagging_loss_video: 6.403|tagging_loss_audio: 8.964|tagging_loss_text: 16.856|tagging_loss_image: 5.702|tagging_loss_fusion: 8.362|total_loss: 46.286 | 63.03 Examples/sec\n",
      "INFO:tensorflow:training step 5505 | tagging_loss_video: 5.902|tagging_loss_audio: 8.380|tagging_loss_text: 16.205|tagging_loss_image: 5.038|tagging_loss_fusion: 6.331|total_loss: 41.856 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 5506 | tagging_loss_video: 6.006|tagging_loss_audio: 8.700|tagging_loss_text: 17.839|tagging_loss_image: 4.292|tagging_loss_fusion: 4.283|total_loss: 41.120 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 5507 | tagging_loss_video: 3.524|tagging_loss_audio: 8.149|tagging_loss_text: 13.310|tagging_loss_image: 4.773|tagging_loss_fusion: 1.999|total_loss: 31.755 | 70.98 Examples/sec\n",
      "INFO:tensorflow:training step 5508 | tagging_loss_video: 5.014|tagging_loss_audio: 8.441|tagging_loss_text: 13.639|tagging_loss_image: 3.640|tagging_loss_fusion: 4.277|total_loss: 35.009 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 5509 | tagging_loss_video: 5.955|tagging_loss_audio: 8.192|tagging_loss_text: 14.685|tagging_loss_image: 5.523|tagging_loss_fusion: 6.049|total_loss: 40.404 | 70.16 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5510 |tagging_loss_video: 5.629|tagging_loss_audio: 7.939|tagging_loss_text: 16.128|tagging_loss_image: 3.749|tagging_loss_fusion: 4.579|total_loss: 38.022 | Examples/sec: 66.59\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5511 | tagging_loss_video: 6.062|tagging_loss_audio: 8.826|tagging_loss_text: 15.189|tagging_loss_image: 5.931|tagging_loss_fusion: 6.342|total_loss: 42.350 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 5512 | tagging_loss_video: 4.457|tagging_loss_audio: 8.930|tagging_loss_text: 13.821|tagging_loss_image: 5.455|tagging_loss_fusion: 4.438|total_loss: 37.101 | 67.08 Examples/sec\n",
      "INFO:tensorflow:training step 5513 | tagging_loss_video: 4.908|tagging_loss_audio: 8.455|tagging_loss_text: 14.107|tagging_loss_image: 5.289|tagging_loss_fusion: 4.215|total_loss: 36.974 | 67.21 Examples/sec\n",
      "INFO:tensorflow:training step 5514 | tagging_loss_video: 4.865|tagging_loss_audio: 9.352|tagging_loss_text: 12.499|tagging_loss_image: 5.087|tagging_loss_fusion: 3.405|total_loss: 35.208 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 5515 | tagging_loss_video: 5.627|tagging_loss_audio: 9.180|tagging_loss_text: 17.642|tagging_loss_image: 4.479|tagging_loss_fusion: 3.955|total_loss: 40.883 | 63.70 Examples/sec\n",
      "INFO:tensorflow:training step 5516 | tagging_loss_video: 6.169|tagging_loss_audio: 8.518|tagging_loss_text: 13.984|tagging_loss_image: 5.429|tagging_loss_fusion: 4.950|total_loss: 39.050 | 68.87 Examples/sec\n",
      "INFO:tensorflow:training step 5517 | tagging_loss_video: 3.952|tagging_loss_audio: 9.347|tagging_loss_text: 15.562|tagging_loss_image: 5.997|tagging_loss_fusion: 3.016|total_loss: 37.875 | 70.35 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 5517.\n",
      "INFO:tensorflow:training step 5518 | tagging_loss_video: 5.641|tagging_loss_audio: 7.741|tagging_loss_text: 17.141|tagging_loss_image: 5.082|tagging_loss_fusion: 3.746|total_loss: 39.352 | 47.04 Examples/sec\n",
      "INFO:tensorflow:training step 5519 | tagging_loss_video: 4.011|tagging_loss_audio: 7.780|tagging_loss_text: 14.958|tagging_loss_image: 4.346|tagging_loss_fusion: 1.661|total_loss: 32.756 | 62.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5520 |tagging_loss_video: 5.979|tagging_loss_audio: 8.587|tagging_loss_text: 13.627|tagging_loss_image: 4.642|tagging_loss_fusion: 3.209|total_loss: 36.044 | Examples/sec: 59.76\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5521 | tagging_loss_video: 5.458|tagging_loss_audio: 8.357|tagging_loss_text: 14.285|tagging_loss_image: 5.660|tagging_loss_fusion: 5.224|total_loss: 38.984 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 5522 | tagging_loss_video: 5.448|tagging_loss_audio: 7.554|tagging_loss_text: 14.857|tagging_loss_image: 5.223|tagging_loss_fusion: 5.007|total_loss: 38.089 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 5523 | tagging_loss_video: 5.155|tagging_loss_audio: 9.507|tagging_loss_text: 17.448|tagging_loss_image: 5.668|tagging_loss_fusion: 3.011|total_loss: 40.789 | 67.76 Examples/sec\n",
      "INFO:tensorflow:training step 5524 | tagging_loss_video: 5.274|tagging_loss_audio: 6.987|tagging_loss_text: 14.164|tagging_loss_image: 5.075|tagging_loss_fusion: 4.900|total_loss: 36.400 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 5525 | tagging_loss_video: 6.256|tagging_loss_audio: 8.731|tagging_loss_text: 15.626|tagging_loss_image: 5.431|tagging_loss_fusion: 6.592|total_loss: 42.635 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 5526 | tagging_loss_video: 5.482|tagging_loss_audio: 7.547|tagging_loss_text: 14.316|tagging_loss_image: 4.986|tagging_loss_fusion: 5.544|total_loss: 37.875 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 5527 | tagging_loss_video: 6.279|tagging_loss_audio: 9.559|tagging_loss_text: 15.332|tagging_loss_image: 5.453|tagging_loss_fusion: 5.569|total_loss: 42.192 | 72.26 Examples/sec\n",
      "INFO:tensorflow:training step 5528 | tagging_loss_video: 5.399|tagging_loss_audio: 8.775|tagging_loss_text: 14.989|tagging_loss_image: 4.728|tagging_loss_fusion: 3.835|total_loss: 37.726 | 62.78 Examples/sec\n",
      "INFO:tensorflow:training step 5529 | tagging_loss_video: 5.990|tagging_loss_audio: 9.390|tagging_loss_text: 17.354|tagging_loss_image: 4.503|tagging_loss_fusion: 4.838|total_loss: 42.075 | 70.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5530 |tagging_loss_video: 6.113|tagging_loss_audio: 8.543|tagging_loss_text: 13.195|tagging_loss_image: 5.575|tagging_loss_fusion: 6.497|total_loss: 39.923 | Examples/sec: 70.09\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.94 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5531 | tagging_loss_video: 6.946|tagging_loss_audio: 8.735|tagging_loss_text: 14.456|tagging_loss_image: 5.104|tagging_loss_fusion: 5.157|total_loss: 40.398 | 65.62 Examples/sec\n",
      "INFO:tensorflow:training step 5532 | tagging_loss_video: 5.624|tagging_loss_audio: 9.464|tagging_loss_text: 18.576|tagging_loss_image: 5.844|tagging_loss_fusion: 4.457|total_loss: 43.966 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 5533 | tagging_loss_video: 4.968|tagging_loss_audio: 7.660|tagging_loss_text: 14.954|tagging_loss_image: 4.177|tagging_loss_fusion: 2.932|total_loss: 34.691 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 5534 | tagging_loss_video: 5.449|tagging_loss_audio: 9.947|tagging_loss_text: 14.562|tagging_loss_image: 4.790|tagging_loss_fusion: 4.028|total_loss: 38.776 | 65.07 Examples/sec\n",
      "INFO:tensorflow:training step 5535 | tagging_loss_video: 6.164|tagging_loss_audio: 9.546|tagging_loss_text: 13.148|tagging_loss_image: 5.310|tagging_loss_fusion: 5.271|total_loss: 39.439 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 5536 | tagging_loss_video: 5.819|tagging_loss_audio: 9.718|tagging_loss_text: 15.236|tagging_loss_image: 5.874|tagging_loss_fusion: 4.386|total_loss: 41.033 | 70.20 Examples/sec\n",
      "INFO:tensorflow:training step 5537 | tagging_loss_video: 4.928|tagging_loss_audio: 9.051|tagging_loss_text: 16.189|tagging_loss_image: 5.383|tagging_loss_fusion: 3.425|total_loss: 38.974 | 64.41 Examples/sec\n",
      "INFO:tensorflow:training step 5538 | tagging_loss_video: 4.330|tagging_loss_audio: 10.711|tagging_loss_text: 20.857|tagging_loss_image: 6.273|tagging_loss_fusion: 3.085|total_loss: 45.256 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 5539 | tagging_loss_video: 4.896|tagging_loss_audio: 9.798|tagging_loss_text: 13.127|tagging_loss_image: 4.451|tagging_loss_fusion: 3.120|total_loss: 35.392 | 70.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5540 |tagging_loss_video: 5.852|tagging_loss_audio: 9.291|tagging_loss_text: 12.669|tagging_loss_image: 4.870|tagging_loss_fusion: 4.992|total_loss: 37.674 | Examples/sec: 70.72\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 5541 | tagging_loss_video: 4.565|tagging_loss_audio: 8.908|tagging_loss_text: 12.980|tagging_loss_image: 4.285|tagging_loss_fusion: 2.813|total_loss: 33.551 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 5542 | tagging_loss_video: 6.226|tagging_loss_audio: 9.610|tagging_loss_text: 17.347|tagging_loss_image: 5.839|tagging_loss_fusion: 3.928|total_loss: 42.949 | 65.16 Examples/sec\n",
      "INFO:tensorflow:training step 5543 | tagging_loss_video: 6.516|tagging_loss_audio: 10.058|tagging_loss_text: 17.295|tagging_loss_image: 6.428|tagging_loss_fusion: 7.559|total_loss: 47.855 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 5544 | tagging_loss_video: 6.079|tagging_loss_audio: 9.795|tagging_loss_text: 16.745|tagging_loss_image: 5.034|tagging_loss_fusion: 4.663|total_loss: 42.316 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 5545 | tagging_loss_video: 5.372|tagging_loss_audio: 9.803|tagging_loss_text: 15.327|tagging_loss_image: 5.446|tagging_loss_fusion: 4.935|total_loss: 40.883 | 64.40 Examples/sec\n",
      "INFO:tensorflow:training step 5546 | tagging_loss_video: 4.959|tagging_loss_audio: 9.848|tagging_loss_text: 16.783|tagging_loss_image: 5.822|tagging_loss_fusion: 3.454|total_loss: 40.866 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 5547 | tagging_loss_video: 4.990|tagging_loss_audio: 9.317|tagging_loss_text: 13.148|tagging_loss_image: 4.027|tagging_loss_fusion: 3.639|total_loss: 35.121 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 5548 | tagging_loss_video: 4.835|tagging_loss_audio: 8.827|tagging_loss_text: 11.978|tagging_loss_image: 5.011|tagging_loss_fusion: 4.483|total_loss: 35.133 | 65.52 Examples/sec\n",
      "INFO:tensorflow:training step 5549 | tagging_loss_video: 5.277|tagging_loss_audio: 8.227|tagging_loss_text: 14.748|tagging_loss_image: 4.073|tagging_loss_fusion: 2.974|total_loss: 35.299 | 68.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5550 |tagging_loss_video: 4.369|tagging_loss_audio: 9.904|tagging_loss_text: 15.074|tagging_loss_image: 5.757|tagging_loss_fusion: 3.273|total_loss: 38.377 | Examples/sec: 71.15\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5551 | tagging_loss_video: 5.890|tagging_loss_audio: 8.472|tagging_loss_text: 14.621|tagging_loss_image: 5.760|tagging_loss_fusion: 4.968|total_loss: 39.711 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 5552 | tagging_loss_video: 5.920|tagging_loss_audio: 10.778|tagging_loss_text: 17.790|tagging_loss_image: 5.965|tagging_loss_fusion: 5.654|total_loss: 46.108 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 5553 | tagging_loss_video: 5.531|tagging_loss_audio: 8.987|tagging_loss_text: 15.509|tagging_loss_image: 5.827|tagging_loss_fusion: 3.386|total_loss: 39.239 | 62.27 Examples/sec\n",
      "INFO:tensorflow:training step 5554 | tagging_loss_video: 5.967|tagging_loss_audio: 8.707|tagging_loss_text: 16.392|tagging_loss_image: 6.183|tagging_loss_fusion: 4.176|total_loss: 41.425 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 5555 | tagging_loss_video: 5.221|tagging_loss_audio: 8.545|tagging_loss_text: 14.302|tagging_loss_image: 5.723|tagging_loss_fusion: 4.073|total_loss: 37.863 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 5556 | tagging_loss_video: 5.680|tagging_loss_audio: 8.184|tagging_loss_text: 15.422|tagging_loss_image: 5.053|tagging_loss_fusion: 4.675|total_loss: 39.014 | 64.20 Examples/sec\n",
      "INFO:tensorflow:training step 5557 | tagging_loss_video: 5.382|tagging_loss_audio: 9.558|tagging_loss_text: 16.874|tagging_loss_image: 6.647|tagging_loss_fusion: 4.426|total_loss: 42.886 | 67.92 Examples/sec\n",
      "INFO:tensorflow:training step 5558 | tagging_loss_video: 5.259|tagging_loss_audio: 8.372|tagging_loss_text: 16.234|tagging_loss_image: 5.234|tagging_loss_fusion: 3.242|total_loss: 38.342 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 5559 | tagging_loss_video: 5.470|tagging_loss_audio: 8.270|tagging_loss_text: 14.515|tagging_loss_image: 5.606|tagging_loss_fusion: 4.173|total_loss: 38.032 | 63.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5560 |tagging_loss_video: 5.402|tagging_loss_audio: 9.382|tagging_loss_text: 15.264|tagging_loss_image: 5.784|tagging_loss_fusion: 5.425|total_loss: 41.257 | Examples/sec: 70.21\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.83 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5561 | tagging_loss_video: 6.343|tagging_loss_audio: 8.430|tagging_loss_text: 14.156|tagging_loss_image: 4.930|tagging_loss_fusion: 4.775|total_loss: 38.634 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 5562 | tagging_loss_video: 5.368|tagging_loss_audio: 8.950|tagging_loss_text: 14.508|tagging_loss_image: 4.583|tagging_loss_fusion: 3.884|total_loss: 37.294 | 66.46 Examples/sec\n",
      "INFO:tensorflow:training step 5563 | tagging_loss_video: 5.054|tagging_loss_audio: 7.873|tagging_loss_text: 11.405|tagging_loss_image: 4.609|tagging_loss_fusion: 2.924|total_loss: 31.866 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 5564 | tagging_loss_video: 5.534|tagging_loss_audio: 8.339|tagging_loss_text: 15.334|tagging_loss_image: 5.472|tagging_loss_fusion: 4.134|total_loss: 38.812 | 65.01 Examples/sec\n",
      "INFO:tensorflow:training step 5565 | tagging_loss_video: 5.492|tagging_loss_audio: 8.847|tagging_loss_text: 15.387|tagging_loss_image: 4.781|tagging_loss_fusion: 3.901|total_loss: 38.409 | 71.83 Examples/sec\n",
      "INFO:tensorflow:training step 5566 | tagging_loss_video: 4.454|tagging_loss_audio: 9.817|tagging_loss_text: 13.884|tagging_loss_image: 5.517|tagging_loss_fusion: 3.548|total_loss: 37.220 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 5567 | tagging_loss_video: 5.916|tagging_loss_audio: 8.429|tagging_loss_text: 18.132|tagging_loss_image: 4.504|tagging_loss_fusion: 6.151|total_loss: 43.132 | 65.46 Examples/sec\n",
      "INFO:tensorflow:training step 5568 | tagging_loss_video: 5.462|tagging_loss_audio: 8.385|tagging_loss_text: 14.691|tagging_loss_image: 5.622|tagging_loss_fusion: 4.795|total_loss: 38.954 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 5569 | tagging_loss_video: 4.684|tagging_loss_audio: 7.725|tagging_loss_text: 15.260|tagging_loss_image: 4.881|tagging_loss_fusion: 3.114|total_loss: 35.664 | 70.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5570 |tagging_loss_video: 5.108|tagging_loss_audio: 8.056|tagging_loss_text: 10.809|tagging_loss_image: 4.128|tagging_loss_fusion: 3.944|total_loss: 32.045 | Examples/sec: 65.44\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5571 | tagging_loss_video: 6.604|tagging_loss_audio: 8.753|tagging_loss_text: 15.309|tagging_loss_image: 5.055|tagging_loss_fusion: 5.605|total_loss: 41.327 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 5572 | tagging_loss_video: 6.026|tagging_loss_audio: 9.207|tagging_loss_text: 11.419|tagging_loss_image: 4.924|tagging_loss_fusion: 5.898|total_loss: 37.473 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 5573 | tagging_loss_video: 5.775|tagging_loss_audio: 8.968|tagging_loss_text: 14.790|tagging_loss_image: 5.223|tagging_loss_fusion: 3.853|total_loss: 38.609 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 5574 | tagging_loss_video: 6.125|tagging_loss_audio: 8.706|tagging_loss_text: 15.728|tagging_loss_image: 5.369|tagging_loss_fusion: 4.547|total_loss: 40.475 | 68.97 Examples/sec\n",
      "INFO:tensorflow:training step 5575 | tagging_loss_video: 6.088|tagging_loss_audio: 9.110|tagging_loss_text: 14.968|tagging_loss_image: 6.132|tagging_loss_fusion: 5.504|total_loss: 41.802 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 5576 | tagging_loss_video: 4.706|tagging_loss_audio: 7.759|tagging_loss_text: 14.680|tagging_loss_image: 6.024|tagging_loss_fusion: 4.163|total_loss: 37.332 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 5577 | tagging_loss_video: 5.006|tagging_loss_audio: 10.729|tagging_loss_text: 12.920|tagging_loss_image: 6.024|tagging_loss_fusion: 3.769|total_loss: 38.447 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 5578 | tagging_loss_video: 5.173|tagging_loss_audio: 7.702|tagging_loss_text: 17.184|tagging_loss_image: 4.834|tagging_loss_fusion: 4.925|total_loss: 39.818 | 64.08 Examples/sec\n",
      "INFO:tensorflow:training step 5579 | tagging_loss_video: 5.650|tagging_loss_audio: 8.748|tagging_loss_text: 16.452|tagging_loss_image: 4.871|tagging_loss_fusion: 4.944|total_loss: 40.666 | 68.32 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5580 |tagging_loss_video: 6.513|tagging_loss_audio: 8.286|tagging_loss_text: 11.856|tagging_loss_image: 4.407|tagging_loss_fusion: 5.273|total_loss: 36.334 | Examples/sec: 70.66\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5581 | tagging_loss_video: 4.797|tagging_loss_audio: 7.428|tagging_loss_text: 11.659|tagging_loss_image: 5.202|tagging_loss_fusion: 3.679|total_loss: 32.764 | 62.70 Examples/sec\n",
      "INFO:tensorflow:training step 5582 | tagging_loss_video: 4.684|tagging_loss_audio: 8.422|tagging_loss_text: 14.899|tagging_loss_image: 5.209|tagging_loss_fusion: 3.587|total_loss: 36.801 | 66.89 Examples/sec\n",
      "INFO:tensorflow:training step 5583 | tagging_loss_video: 5.481|tagging_loss_audio: 8.845|tagging_loss_text: 15.891|tagging_loss_image: 5.499|tagging_loss_fusion: 2.871|total_loss: 38.586 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 5584 | tagging_loss_video: 6.207|tagging_loss_audio: 9.062|tagging_loss_text: 14.857|tagging_loss_image: 4.705|tagging_loss_fusion: 6.106|total_loss: 40.938 | 66.54 Examples/sec\n",
      "INFO:tensorflow:training step 5585 | tagging_loss_video: 6.516|tagging_loss_audio: 8.793|tagging_loss_text: 17.777|tagging_loss_image: 7.398|tagging_loss_fusion: 6.780|total_loss: 47.264 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 5586 | tagging_loss_video: 5.838|tagging_loss_audio: 10.195|tagging_loss_text: 15.358|tagging_loss_image: 4.940|tagging_loss_fusion: 5.562|total_loss: 41.893 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 5587 | tagging_loss_video: 5.013|tagging_loss_audio: 8.464|tagging_loss_text: 15.646|tagging_loss_image: 3.191|tagging_loss_fusion: 3.025|total_loss: 35.339 | 64.44 Examples/sec\n",
      "INFO:tensorflow:training step 5588 | tagging_loss_video: 6.084|tagging_loss_audio: 8.800|tagging_loss_text: 19.890|tagging_loss_image: 5.963|tagging_loss_fusion: 3.944|total_loss: 44.681 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 5589 | tagging_loss_video: 5.310|tagging_loss_audio: 8.454|tagging_loss_text: 14.116|tagging_loss_image: 4.027|tagging_loss_fusion: 3.012|total_loss: 34.920 | 67.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5590 |tagging_loss_video: 6.088|tagging_loss_audio: 9.121|tagging_loss_text: 18.738|tagging_loss_image: 5.375|tagging_loss_fusion: 4.677|total_loss: 43.999 | Examples/sec: 70.15\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5591 | tagging_loss_video: 5.630|tagging_loss_audio: 7.486|tagging_loss_text: 17.372|tagging_loss_image: 3.738|tagging_loss_fusion: 3.781|total_loss: 38.006 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 5592 | tagging_loss_video: 5.378|tagging_loss_audio: 8.567|tagging_loss_text: 14.330|tagging_loss_image: 5.584|tagging_loss_fusion: 4.220|total_loss: 38.079 | 52.85 Examples/sec\n",
      "INFO:tensorflow:training step 5593 | tagging_loss_video: 5.713|tagging_loss_audio: 8.586|tagging_loss_text: 16.751|tagging_loss_image: 5.137|tagging_loss_fusion: 3.458|total_loss: 39.645 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 5594 | tagging_loss_video: 5.720|tagging_loss_audio: 8.365|tagging_loss_text: 16.986|tagging_loss_image: 5.994|tagging_loss_fusion: 3.966|total_loss: 41.031 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 5595 | tagging_loss_video: 4.182|tagging_loss_audio: 7.216|tagging_loss_text: 16.898|tagging_loss_image: 5.423|tagging_loss_fusion: 3.463|total_loss: 37.181 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 5596 | tagging_loss_video: 5.876|tagging_loss_audio: 9.781|tagging_loss_text: 17.332|tagging_loss_image: 3.976|tagging_loss_fusion: 5.046|total_loss: 42.011 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 5597 | tagging_loss_video: 5.575|tagging_loss_audio: 8.149|tagging_loss_text: 14.460|tagging_loss_image: 5.569|tagging_loss_fusion: 5.071|total_loss: 38.823 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 5598 | tagging_loss_video: 5.583|tagging_loss_audio: 7.835|tagging_loss_text: 16.046|tagging_loss_image: 5.390|tagging_loss_fusion: 3.558|total_loss: 38.412 | 65.86 Examples/sec\n",
      "INFO:tensorflow:training step 5599 | tagging_loss_video: 4.962|tagging_loss_audio: 9.680|tagging_loss_text: 16.256|tagging_loss_image: 6.318|tagging_loss_fusion: 3.076|total_loss: 40.292 | 68.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5600 |tagging_loss_video: 5.111|tagging_loss_audio: 8.668|tagging_loss_text: 14.590|tagging_loss_image: 6.145|tagging_loss_fusion: 3.887|total_loss: 38.402 | Examples/sec: 68.95\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5601 | tagging_loss_video: 4.802|tagging_loss_audio: 8.531|tagging_loss_text: 12.636|tagging_loss_image: 5.511|tagging_loss_fusion: 2.935|total_loss: 34.416 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 5602 | tagging_loss_video: 5.991|tagging_loss_audio: 10.045|tagging_loss_text: 14.816|tagging_loss_image: 6.511|tagging_loss_fusion: 4.721|total_loss: 42.085 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 5603 | tagging_loss_video: 4.264|tagging_loss_audio: 8.445|tagging_loss_text: 14.128|tagging_loss_image: 5.367|tagging_loss_fusion: 2.142|total_loss: 34.346 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 5604 | tagging_loss_video: 4.960|tagging_loss_audio: 8.302|tagging_loss_text: 13.405|tagging_loss_image: 4.957|tagging_loss_fusion: 3.197|total_loss: 34.822 | 69.99 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5605 | tagging_loss_video: 7.122|tagging_loss_audio: 9.449|tagging_loss_text: 15.349|tagging_loss_image: 5.646|tagging_loss_fusion: 6.808|total_loss: 44.374 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 5606 | tagging_loss_video: 5.742|tagging_loss_audio: 8.994|tagging_loss_text: 15.215|tagging_loss_image: 6.380|tagging_loss_fusion: 4.753|total_loss: 41.084 | 63.32 Examples/sec\n",
      "INFO:tensorflow:training step 5607 | tagging_loss_video: 4.904|tagging_loss_audio: 8.527|tagging_loss_text: 18.777|tagging_loss_image: 6.544|tagging_loss_fusion: 3.906|total_loss: 42.657 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 5608 | tagging_loss_video: 5.390|tagging_loss_audio: 8.910|tagging_loss_text: 16.063|tagging_loss_image: 5.394|tagging_loss_fusion: 3.372|total_loss: 39.128 | 71.60 Examples/sec\n",
      "INFO:tensorflow:training step 5609 | tagging_loss_video: 5.506|tagging_loss_audio: 9.300|tagging_loss_text: 14.630|tagging_loss_image: 5.448|tagging_loss_fusion: 3.346|total_loss: 38.230 | 63.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5610 |tagging_loss_video: 5.536|tagging_loss_audio: 7.865|tagging_loss_text: 14.105|tagging_loss_image: 4.523|tagging_loss_fusion: 3.670|total_loss: 35.698 | Examples/sec: 69.81\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5611 | tagging_loss_video: 4.210|tagging_loss_audio: 7.977|tagging_loss_text: 15.092|tagging_loss_image: 5.178|tagging_loss_fusion: 2.885|total_loss: 35.342 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 5612 | tagging_loss_video: 6.395|tagging_loss_audio: 7.631|tagging_loss_text: 14.759|tagging_loss_image: 5.016|tagging_loss_fusion: 4.893|total_loss: 38.694 | 65.93 Examples/sec\n",
      "INFO:tensorflow:training step 5613 | tagging_loss_video: 4.973|tagging_loss_audio: 7.969|tagging_loss_text: 13.280|tagging_loss_image: 3.932|tagging_loss_fusion: 3.037|total_loss: 33.190 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 5614 | tagging_loss_video: 4.535|tagging_loss_audio: 8.493|tagging_loss_text: 11.721|tagging_loss_image: 4.474|tagging_loss_fusion: 2.462|total_loss: 31.687 | 68.05 Examples/sec\n",
      "INFO:tensorflow:training step 5615 | tagging_loss_video: 5.612|tagging_loss_audio: 9.210|tagging_loss_text: 16.352|tagging_loss_image: 4.902|tagging_loss_fusion: 4.668|total_loss: 40.744 | 68.14 Examples/sec\n",
      "INFO:tensorflow:training step 5616 | tagging_loss_video: 5.859|tagging_loss_audio: 9.042|tagging_loss_text: 18.548|tagging_loss_image: 5.411|tagging_loss_fusion: 4.998|total_loss: 43.858 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 5617 | tagging_loss_video: 6.025|tagging_loss_audio: 9.127|tagging_loss_text: 18.130|tagging_loss_image: 6.715|tagging_loss_fusion: 5.943|total_loss: 45.941 | 61.18 Examples/sec\n",
      "INFO:tensorflow:training step 5618 | tagging_loss_video: 5.154|tagging_loss_audio: 8.540|tagging_loss_text: 18.976|tagging_loss_image: 4.467|tagging_loss_fusion: 3.960|total_loss: 41.096 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 5619 | tagging_loss_video: 7.173|tagging_loss_audio: 9.206|tagging_loss_text: 18.977|tagging_loss_image: 6.629|tagging_loss_fusion: 5.305|total_loss: 47.290 | 70.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5620 |tagging_loss_video: 5.067|tagging_loss_audio: 7.878|tagging_loss_text: 11.311|tagging_loss_image: 5.079|tagging_loss_fusion: 4.123|total_loss: 33.458 | Examples/sec: 63.17\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5621 | tagging_loss_video: 5.353|tagging_loss_audio: 7.339|tagging_loss_text: 13.778|tagging_loss_image: 5.327|tagging_loss_fusion: 5.968|total_loss: 37.765 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 5622 | tagging_loss_video: 5.808|tagging_loss_audio: 8.197|tagging_loss_text: 16.123|tagging_loss_image: 5.561|tagging_loss_fusion: 4.203|total_loss: 39.891 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 5623 | tagging_loss_video: 5.059|tagging_loss_audio: 8.062|tagging_loss_text: 14.604|tagging_loss_image: 4.735|tagging_loss_fusion: 4.555|total_loss: 37.016 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 5624 | tagging_loss_video: 5.778|tagging_loss_audio: 8.731|tagging_loss_text: 16.821|tagging_loss_image: 4.777|tagging_loss_fusion: 4.097|total_loss: 40.205 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 5625 | tagging_loss_video: 5.142|tagging_loss_audio: 7.727|tagging_loss_text: 15.771|tagging_loss_image: 5.243|tagging_loss_fusion: 6.077|total_loss: 39.960 | 68.75 Examples/sec\n",
      "INFO:tensorflow:training step 5626 | tagging_loss_video: 5.749|tagging_loss_audio: 8.110|tagging_loss_text: 13.305|tagging_loss_image: 4.213|tagging_loss_fusion: 3.917|total_loss: 35.294 | 70.79 Examples/sec\n",
      "INFO:tensorflow:training step 5627 | tagging_loss_video: 4.445|tagging_loss_audio: 7.842|tagging_loss_text: 13.382|tagging_loss_image: 4.157|tagging_loss_fusion: 2.862|total_loss: 32.688 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 5628 | tagging_loss_video: 5.981|tagging_loss_audio: 9.474|tagging_loss_text: 16.941|tagging_loss_image: 6.235|tagging_loss_fusion: 5.192|total_loss: 43.823 | 65.03 Examples/sec\n",
      "INFO:tensorflow:training step 5629 | tagging_loss_video: 4.759|tagging_loss_audio: 8.070|tagging_loss_text: 15.743|tagging_loss_image: 4.759|tagging_loss_fusion: 2.663|total_loss: 35.994 | 68.47 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5630 |tagging_loss_video: 5.757|tagging_loss_audio: 8.535|tagging_loss_text: 14.257|tagging_loss_image: 4.710|tagging_loss_fusion: 5.344|total_loss: 38.604 | Examples/sec: 71.77\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5631 | tagging_loss_video: 5.121|tagging_loss_audio: 8.005|tagging_loss_text: 13.643|tagging_loss_image: 5.028|tagging_loss_fusion: 3.580|total_loss: 35.378 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 5632 | tagging_loss_video: 6.133|tagging_loss_audio: 8.505|tagging_loss_text: 14.523|tagging_loss_image: 5.083|tagging_loss_fusion: 4.964|total_loss: 39.208 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 5633 | tagging_loss_video: 6.048|tagging_loss_audio: 8.777|tagging_loss_text: 15.987|tagging_loss_image: 5.973|tagging_loss_fusion: 4.967|total_loss: 41.751 | 67.37 Examples/sec\n",
      "INFO:tensorflow:training step 5634 | tagging_loss_video: 4.655|tagging_loss_audio: 7.713|tagging_loss_text: 11.573|tagging_loss_image: 4.799|tagging_loss_fusion: 3.230|total_loss: 31.971 | 63.35 Examples/sec\n",
      "INFO:tensorflow:training step 5635 | tagging_loss_video: 3.639|tagging_loss_audio: 7.947|tagging_loss_text: 16.093|tagging_loss_image: 5.648|tagging_loss_fusion: 3.153|total_loss: 36.479 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 5636 | tagging_loss_video: 4.869|tagging_loss_audio: 9.681|tagging_loss_text: 15.851|tagging_loss_image: 6.103|tagging_loss_fusion: 3.884|total_loss: 40.388 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 5637 | tagging_loss_video: 4.772|tagging_loss_audio: 8.672|tagging_loss_text: 13.688|tagging_loss_image: 3.787|tagging_loss_fusion: 3.445|total_loss: 34.364 | 67.48 Examples/sec\n",
      "INFO:tensorflow:training step 5638 | tagging_loss_video: 4.327|tagging_loss_audio: 7.922|tagging_loss_text: 9.777|tagging_loss_image: 3.869|tagging_loss_fusion: 2.807|total_loss: 28.701 | 67.66 Examples/sec\n",
      "INFO:tensorflow:training step 5639 | tagging_loss_video: 4.914|tagging_loss_audio: 9.041|tagging_loss_text: 17.166|tagging_loss_image: 4.574|tagging_loss_fusion: 2.485|total_loss: 38.180 | 69.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5640 |tagging_loss_video: 5.386|tagging_loss_audio: 8.090|tagging_loss_text: 13.933|tagging_loss_image: 5.226|tagging_loss_fusion: 4.419|total_loss: 37.053 | Examples/sec: 70.79\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5641 | tagging_loss_video: 4.489|tagging_loss_audio: 7.271|tagging_loss_text: 14.725|tagging_loss_image: 4.692|tagging_loss_fusion: 3.218|total_loss: 34.395 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 5642 | tagging_loss_video: 4.641|tagging_loss_audio: 8.072|tagging_loss_text: 12.521|tagging_loss_image: 5.503|tagging_loss_fusion: 4.024|total_loss: 34.760 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 5643 | tagging_loss_video: 6.519|tagging_loss_audio: 8.099|tagging_loss_text: 18.230|tagging_loss_image: 4.824|tagging_loss_fusion: 4.664|total_loss: 42.335 | 61.75 Examples/sec\n",
      "INFO:tensorflow:training step 5644 | tagging_loss_video: 5.708|tagging_loss_audio: 8.174|tagging_loss_text: 16.182|tagging_loss_image: 5.411|tagging_loss_fusion: 3.199|total_loss: 38.675 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 5645 | tagging_loss_video: 6.174|tagging_loss_audio: 9.164|tagging_loss_text: 13.565|tagging_loss_image: 4.456|tagging_loss_fusion: 5.275|total_loss: 38.634 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 5646 | tagging_loss_video: 3.744|tagging_loss_audio: 7.987|tagging_loss_text: 15.215|tagging_loss_image: 4.132|tagging_loss_fusion: 2.395|total_loss: 33.473 | 64.64 Examples/sec\n",
      "INFO:tensorflow:training step 5647 | tagging_loss_video: 4.930|tagging_loss_audio: 8.408|tagging_loss_text: 13.062|tagging_loss_image: 4.092|tagging_loss_fusion: 3.768|total_loss: 34.260 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 5648 | tagging_loss_video: 5.586|tagging_loss_audio: 7.732|tagging_loss_text: 14.667|tagging_loss_image: 4.413|tagging_loss_fusion: 3.962|total_loss: 36.360 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 5649 | tagging_loss_video: 5.179|tagging_loss_audio: 7.975|tagging_loss_text: 14.836|tagging_loss_image: 5.224|tagging_loss_fusion: 4.602|total_loss: 37.817 | 62.81 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5650 |tagging_loss_video: 5.786|tagging_loss_audio: 8.843|tagging_loss_text: 13.799|tagging_loss_image: 4.859|tagging_loss_fusion: 4.219|total_loss: 37.506 | Examples/sec: 70.84\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.96 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5651 | tagging_loss_video: 5.967|tagging_loss_audio: 7.726|tagging_loss_text: 10.535|tagging_loss_image: 4.948|tagging_loss_fusion: 5.219|total_loss: 34.396 | 71.51 Examples/sec\n",
      "INFO:tensorflow:training step 5652 | tagging_loss_video: 5.526|tagging_loss_audio: 7.827|tagging_loss_text: 15.386|tagging_loss_image: 4.209|tagging_loss_fusion: 3.567|total_loss: 36.515 | 64.84 Examples/sec\n",
      "INFO:tensorflow:training step 5653 | tagging_loss_video: 6.053|tagging_loss_audio: 8.726|tagging_loss_text: 16.648|tagging_loss_image: 6.050|tagging_loss_fusion: 6.184|total_loss: 43.661 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 5654 | tagging_loss_video: 5.453|tagging_loss_audio: 7.933|tagging_loss_text: 18.324|tagging_loss_image: 5.852|tagging_loss_fusion: 4.769|total_loss: 42.333 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 5655 | tagging_loss_video: 5.683|tagging_loss_audio: 7.627|tagging_loss_text: 16.076|tagging_loss_image: 6.153|tagging_loss_fusion: 5.085|total_loss: 40.624 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 5656 | tagging_loss_video: 5.899|tagging_loss_audio: 9.610|tagging_loss_text: 18.526|tagging_loss_image: 4.745|tagging_loss_fusion: 5.577|total_loss: 44.356 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5657 | tagging_loss_video: 5.683|tagging_loss_audio: 9.021|tagging_loss_text: 13.023|tagging_loss_image: 4.901|tagging_loss_fusion: 3.663|total_loss: 36.290 | 62.66 Examples/sec\n",
      "INFO:tensorflow:training step 5658 | tagging_loss_video: 4.651|tagging_loss_audio: 9.031|tagging_loss_text: 15.895|tagging_loss_image: 4.065|tagging_loss_fusion: 3.245|total_loss: 36.887 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 5659 | tagging_loss_video: 5.239|tagging_loss_audio: 7.930|tagging_loss_text: 11.844|tagging_loss_image: 4.989|tagging_loss_fusion: 3.094|total_loss: 33.096 | 70.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5660 |tagging_loss_video: 5.986|tagging_loss_audio: 7.890|tagging_loss_text: 14.922|tagging_loss_image: 4.108|tagging_loss_fusion: 5.154|total_loss: 38.060 | Examples/sec: 68.63\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5661 | tagging_loss_video: 6.113|tagging_loss_audio: 8.700|tagging_loss_text: 17.132|tagging_loss_image: 5.264|tagging_loss_fusion: 4.812|total_loss: 42.021 | 71.85 Examples/sec\n",
      "INFO:tensorflow:training step 5662 | tagging_loss_video: 5.184|tagging_loss_audio: 7.807|tagging_loss_text: 15.543|tagging_loss_image: 4.678|tagging_loss_fusion: 3.017|total_loss: 36.229 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 5663 | tagging_loss_video: 5.718|tagging_loss_audio: 9.724|tagging_loss_text: 18.145|tagging_loss_image: 5.570|tagging_loss_fusion: 3.397|total_loss: 42.554 | 66.59 Examples/sec\n",
      "INFO:tensorflow:training step 5664 | tagging_loss_video: 5.118|tagging_loss_audio: 6.779|tagging_loss_text: 12.227|tagging_loss_image: 4.755|tagging_loss_fusion: 3.203|total_loss: 32.082 | 66.01 Examples/sec\n",
      "INFO:tensorflow:training step 5665 | tagging_loss_video: 5.887|tagging_loss_audio: 9.167|tagging_loss_text: 9.948|tagging_loss_image: 4.819|tagging_loss_fusion: 6.197|total_loss: 36.018 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 5666 | tagging_loss_video: 5.740|tagging_loss_audio: 9.065|tagging_loss_text: 12.885|tagging_loss_image: 6.170|tagging_loss_fusion: 5.834|total_loss: 39.695 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 5667 | tagging_loss_video: 6.299|tagging_loss_audio: 8.362|tagging_loss_text: 13.942|tagging_loss_image: 5.446|tagging_loss_fusion: 5.084|total_loss: 39.133 | 66.97 Examples/sec\n",
      "INFO:tensorflow:training step 5668 | tagging_loss_video: 5.804|tagging_loss_audio: 8.336|tagging_loss_text: 14.981|tagging_loss_image: 5.044|tagging_loss_fusion: 3.857|total_loss: 38.022 | 71.29 Examples/sec\n",
      "INFO:tensorflow:training step 5669 | tagging_loss_video: 5.631|tagging_loss_audio: 9.501|tagging_loss_text: 15.188|tagging_loss_image: 5.376|tagging_loss_fusion: 4.632|total_loss: 40.328 | 67.98 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5670 |tagging_loss_video: 5.352|tagging_loss_audio: 7.635|tagging_loss_text: 15.822|tagging_loss_image: 5.067|tagging_loss_fusion: 5.816|total_loss: 39.691 | Examples/sec: 68.04\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.94 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5671 | tagging_loss_video: 5.527|tagging_loss_audio: 8.357|tagging_loss_text: 14.432|tagging_loss_image: 5.343|tagging_loss_fusion: 5.100|total_loss: 38.759 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 5672 | tagging_loss_video: 6.241|tagging_loss_audio: 10.234|tagging_loss_text: 17.076|tagging_loss_image: 5.006|tagging_loss_fusion: 3.324|total_loss: 41.880 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 5673 | tagging_loss_video: 5.436|tagging_loss_audio: 9.394|tagging_loss_text: 14.963|tagging_loss_image: 5.341|tagging_loss_fusion: 4.388|total_loss: 39.522 | 71.95 Examples/sec\n",
      "INFO:tensorflow:training step 5674 | tagging_loss_video: 5.851|tagging_loss_audio: 8.902|tagging_loss_text: 13.862|tagging_loss_image: 4.560|tagging_loss_fusion: 3.806|total_loss: 36.981 | 58.14 Examples/sec\n",
      "INFO:tensorflow:training step 5675 | tagging_loss_video: 6.048|tagging_loss_audio: 9.536|tagging_loss_text: 17.326|tagging_loss_image: 5.311|tagging_loss_fusion: 4.139|total_loss: 42.361 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 5676 | tagging_loss_video: 6.248|tagging_loss_audio: 9.332|tagging_loss_text: 20.327|tagging_loss_image: 5.961|tagging_loss_fusion: 4.490|total_loss: 46.357 | 68.17 Examples/sec\n",
      "INFO:tensorflow:training step 5677 | tagging_loss_video: 5.868|tagging_loss_audio: 8.558|tagging_loss_text: 16.882|tagging_loss_image: 6.300|tagging_loss_fusion: 7.265|total_loss: 44.873 | 67.38 Examples/sec\n",
      "INFO:tensorflow:training step 5678 | tagging_loss_video: 6.706|tagging_loss_audio: 10.308|tagging_loss_text: 13.411|tagging_loss_image: 3.611|tagging_loss_fusion: 4.007|total_loss: 38.042 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 5679 | tagging_loss_video: 6.189|tagging_loss_audio: 8.792|tagging_loss_text: 13.131|tagging_loss_image: 5.396|tagging_loss_fusion: 5.857|total_loss: 39.364 | 61.34 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5680 |tagging_loss_video: 5.903|tagging_loss_audio: 8.474|tagging_loss_text: 16.201|tagging_loss_image: 4.916|tagging_loss_fusion: 3.456|total_loss: 38.949 | Examples/sec: 70.30\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.97 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5681 | tagging_loss_video: 4.913|tagging_loss_audio: 8.662|tagging_loss_text: 13.605|tagging_loss_image: 5.360|tagging_loss_fusion: 3.088|total_loss: 35.628 | 67.00 Examples/sec\n",
      "INFO:tensorflow:training step 5682 | tagging_loss_video: 5.099|tagging_loss_audio: 8.700|tagging_loss_text: 13.734|tagging_loss_image: 4.948|tagging_loss_fusion: 3.804|total_loss: 36.284 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 5683 | tagging_loss_video: 5.286|tagging_loss_audio: 10.298|tagging_loss_text: 16.562|tagging_loss_image: 5.940|tagging_loss_fusion: 3.901|total_loss: 41.988 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 5684 | tagging_loss_video: 6.583|tagging_loss_audio: 9.251|tagging_loss_text: 15.275|tagging_loss_image: 4.719|tagging_loss_fusion: 4.816|total_loss: 40.643 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 5685 | tagging_loss_video: 6.432|tagging_loss_audio: 8.473|tagging_loss_text: 16.716|tagging_loss_image: 5.920|tagging_loss_fusion: 6.324|total_loss: 43.865 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 5686 | tagging_loss_video: 6.363|tagging_loss_audio: 9.827|tagging_loss_text: 16.796|tagging_loss_image: 6.821|tagging_loss_fusion: 5.431|total_loss: 45.238 | 71.84 Examples/sec\n",
      "INFO:tensorflow:training step 5687 | tagging_loss_video: 4.851|tagging_loss_audio: 8.493|tagging_loss_text: 19.361|tagging_loss_image: 3.917|tagging_loss_fusion: 3.281|total_loss: 39.903 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 5688 | tagging_loss_video: 6.147|tagging_loss_audio: 8.826|tagging_loss_text: 12.659|tagging_loss_image: 4.698|tagging_loss_fusion: 4.088|total_loss: 36.417 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 5689 | tagging_loss_video: 5.020|tagging_loss_audio: 9.714|tagging_loss_text: 14.749|tagging_loss_image: 4.832|tagging_loss_fusion: 3.353|total_loss: 37.669 | 68.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5690 |tagging_loss_video: 6.020|tagging_loss_audio: 8.979|tagging_loss_text: 15.076|tagging_loss_image: 6.029|tagging_loss_fusion: 6.294|total_loss: 42.399 | Examples/sec: 70.94\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.95 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5691 | tagging_loss_video: 5.627|tagging_loss_audio: 8.666|tagging_loss_text: 13.958|tagging_loss_image: 5.324|tagging_loss_fusion: 3.554|total_loss: 37.129 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 5692 | tagging_loss_video: 6.437|tagging_loss_audio: 10.887|tagging_loss_text: 15.577|tagging_loss_image: 5.836|tagging_loss_fusion: 6.559|total_loss: 45.296 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 5693 | tagging_loss_video: 6.772|tagging_loss_audio: 9.558|tagging_loss_text: 16.261|tagging_loss_image: 4.293|tagging_loss_fusion: 4.509|total_loss: 41.393 | 60.12 Examples/sec\n",
      "INFO:tensorflow:training step 5694 | tagging_loss_video: 3.639|tagging_loss_audio: 8.283|tagging_loss_text: 13.758|tagging_loss_image: 5.887|tagging_loss_fusion: 3.990|total_loss: 35.557 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 5695 | tagging_loss_video: 5.226|tagging_loss_audio: 9.430|tagging_loss_text: 15.665|tagging_loss_image: 6.544|tagging_loss_fusion: 4.270|total_loss: 41.136 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 5696 | tagging_loss_video: 6.237|tagging_loss_audio: 8.047|tagging_loss_text: 12.838|tagging_loss_image: 4.630|tagging_loss_fusion: 5.621|total_loss: 37.373 | 63.93 Examples/sec\n",
      "INFO:tensorflow:training step 5697 | tagging_loss_video: 4.317|tagging_loss_audio: 10.569|tagging_loss_text: 14.672|tagging_loss_image: 3.892|tagging_loss_fusion: 2.260|total_loss: 35.710 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 5698 | tagging_loss_video: 4.830|tagging_loss_audio: 8.847|tagging_loss_text: 14.877|tagging_loss_image: 3.966|tagging_loss_fusion: 3.249|total_loss: 35.769 | 68.49 Examples/sec\n",
      "INFO:tensorflow:training step 5699 | tagging_loss_video: 5.271|tagging_loss_audio: 8.617|tagging_loss_text: 17.822|tagging_loss_image: 5.224|tagging_loss_fusion: 3.249|total_loss: 40.183 | 63.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5700 |tagging_loss_video: 4.722|tagging_loss_audio: 8.123|tagging_loss_text: 16.572|tagging_loss_image: 5.040|tagging_loss_fusion: 2.640|total_loss: 37.096 | Examples/sec: 69.50\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 5701 | tagging_loss_video: 6.052|tagging_loss_audio: 9.389|tagging_loss_text: 14.160|tagging_loss_image: 5.095|tagging_loss_fusion: 4.769|total_loss: 39.465 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 5702 | tagging_loss_video: 5.690|tagging_loss_audio: 8.765|tagging_loss_text: 16.488|tagging_loss_image: 4.476|tagging_loss_fusion: 5.372|total_loss: 40.792 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 5703 | tagging_loss_video: 4.561|tagging_loss_audio: 8.115|tagging_loss_text: 16.480|tagging_loss_image: 4.884|tagging_loss_fusion: 3.134|total_loss: 37.173 | 68.14 Examples/sec\n",
      "INFO:tensorflow:training step 5704 | tagging_loss_video: 5.944|tagging_loss_audio: 7.732|tagging_loss_text: 15.334|tagging_loss_image: 5.176|tagging_loss_fusion: 4.213|total_loss: 38.399 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 5705 | tagging_loss_video: 4.780|tagging_loss_audio: 9.472|tagging_loss_text: 14.046|tagging_loss_image: 5.089|tagging_loss_fusion: 3.050|total_loss: 36.437 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 5706 | tagging_loss_video: 6.238|tagging_loss_audio: 10.348|tagging_loss_text: 13.210|tagging_loss_image: 5.490|tagging_loss_fusion: 5.518|total_loss: 40.805 | 65.27 Examples/sec\n",
      "INFO:tensorflow:training step 5707 | tagging_loss_video: 5.391|tagging_loss_audio: 8.953|tagging_loss_text: 15.540|tagging_loss_image: 5.456|tagging_loss_fusion: 3.970|total_loss: 39.310 | 59.28 Examples/sec\n",
      "INFO:tensorflow:training step 5708 | tagging_loss_video: 6.495|tagging_loss_audio: 7.187|tagging_loss_text: 13.342|tagging_loss_image: 4.860|tagging_loss_fusion: 5.889|total_loss: 37.773 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 5709 | tagging_loss_video: 3.545|tagging_loss_audio: 7.929|tagging_loss_text: 14.605|tagging_loss_image: 5.202|tagging_loss_fusion: 2.168|total_loss: 33.450 | 67.71 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5710 |tagging_loss_video: 5.525|tagging_loss_audio: 8.717|tagging_loss_text: 13.518|tagging_loss_image: 3.198|tagging_loss_fusion: 4.016|total_loss: 34.975 | Examples/sec: 63.72\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5711 | tagging_loss_video: 6.111|tagging_loss_audio: 9.150|tagging_loss_text: 16.722|tagging_loss_image: 5.556|tagging_loss_fusion: 4.977|total_loss: 42.516 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 5712 | tagging_loss_video: 6.567|tagging_loss_audio: 9.469|tagging_loss_text: 16.735|tagging_loss_image: 6.545|tagging_loss_fusion: 6.425|total_loss: 45.740 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 5713 | tagging_loss_video: 6.491|tagging_loss_audio: 8.027|tagging_loss_text: 14.789|tagging_loss_image: 5.046|tagging_loss_fusion: 6.017|total_loss: 40.369 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 5714 | tagging_loss_video: 5.445|tagging_loss_audio: 8.983|tagging_loss_text: 15.767|tagging_loss_image: 4.115|tagging_loss_fusion: 2.697|total_loss: 37.007 | 67.60 Examples/sec\n",
      "INFO:tensorflow:training step 5715 | tagging_loss_video: 6.411|tagging_loss_audio: 9.503|tagging_loss_text: 17.087|tagging_loss_image: 5.682|tagging_loss_fusion: 5.877|total_loss: 44.560 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 5716 | tagging_loss_video: 5.778|tagging_loss_audio: 8.752|tagging_loss_text: 17.346|tagging_loss_image: 5.504|tagging_loss_fusion: 6.065|total_loss: 43.443 | 71.63 Examples/sec\n",
      "INFO:tensorflow:training step 5717 | tagging_loss_video: 5.710|tagging_loss_audio: 9.385|tagging_loss_text: 19.368|tagging_loss_image: 5.796|tagging_loss_fusion: 5.383|total_loss: 45.641 | 72.00 Examples/sec\n",
      "INFO:tensorflow:training step 5718 | tagging_loss_video: 5.603|tagging_loss_audio: 7.456|tagging_loss_text: 13.853|tagging_loss_image: 4.830|tagging_loss_fusion: 3.555|total_loss: 35.297 | 61.13 Examples/sec\n",
      "INFO:tensorflow:training step 5719 | tagging_loss_video: 6.200|tagging_loss_audio: 9.361|tagging_loss_text: 15.076|tagging_loss_image: 5.828|tagging_loss_fusion: 4.928|total_loss: 41.392 | 69.41 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5720 |tagging_loss_video: 6.865|tagging_loss_audio: 9.708|tagging_loss_text: 15.363|tagging_loss_image: 5.044|tagging_loss_fusion: 5.833|total_loss: 42.813 | Examples/sec: 66.88\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5721 | tagging_loss_video: 5.084|tagging_loss_audio: 7.481|tagging_loss_text: 16.569|tagging_loss_image: 4.509|tagging_loss_fusion: 4.630|total_loss: 38.272 | 65.31 Examples/sec\n",
      "INFO:tensorflow:training step 5722 | tagging_loss_video: 4.704|tagging_loss_audio: 8.946|tagging_loss_text: 13.524|tagging_loss_image: 5.241|tagging_loss_fusion: 3.562|total_loss: 35.976 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 5723 | tagging_loss_video: 5.440|tagging_loss_audio: 9.146|tagging_loss_text: 15.896|tagging_loss_image: 5.948|tagging_loss_fusion: 4.640|total_loss: 41.069 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 5724 | tagging_loss_video: 5.740|tagging_loss_audio: 8.712|tagging_loss_text: 13.511|tagging_loss_image: 3.601|tagging_loss_fusion: 3.700|total_loss: 35.263 | 61.82 Examples/sec\n",
      "INFO:tensorflow:training step 5725 | tagging_loss_video: 6.185|tagging_loss_audio: 8.803|tagging_loss_text: 14.804|tagging_loss_image: 5.911|tagging_loss_fusion: 4.835|total_loss: 40.537 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 5726 | tagging_loss_video: 5.425|tagging_loss_audio: 9.416|tagging_loss_text: 14.028|tagging_loss_image: 5.057|tagging_loss_fusion: 4.516|total_loss: 38.442 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 5727 | tagging_loss_video: 5.296|tagging_loss_audio: 8.551|tagging_loss_text: 16.332|tagging_loss_image: 5.937|tagging_loss_fusion: 3.962|total_loss: 40.078 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 5728 | tagging_loss_video: 5.636|tagging_loss_audio: 8.208|tagging_loss_text: 13.495|tagging_loss_image: 4.911|tagging_loss_fusion: 4.564|total_loss: 36.815 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 5729 | tagging_loss_video: 6.093|tagging_loss_audio: 9.275|tagging_loss_text: 15.461|tagging_loss_image: 6.018|tagging_loss_fusion: 4.556|total_loss: 41.403 | 64.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5730 |tagging_loss_video: 6.516|tagging_loss_audio: 8.415|tagging_loss_text: 12.479|tagging_loss_image: 5.531|tagging_loss_fusion: 3.498|total_loss: 36.439 | Examples/sec: 70.51\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5731 | tagging_loss_video: 5.441|tagging_loss_audio: 8.186|tagging_loss_text: 14.708|tagging_loss_image: 5.158|tagging_loss_fusion: 5.013|total_loss: 38.506 | 68.86 Examples/sec\n",
      "INFO:tensorflow:training step 5732 | tagging_loss_video: 4.688|tagging_loss_audio: 8.388|tagging_loss_text: 13.654|tagging_loss_image: 4.458|tagging_loss_fusion: 2.010|total_loss: 33.198 | 66.72 Examples/sec\n",
      "INFO:tensorflow:training step 5733 | tagging_loss_video: 5.038|tagging_loss_audio: 9.002|tagging_loss_text: 12.746|tagging_loss_image: 4.556|tagging_loss_fusion: 3.485|total_loss: 34.826 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 5734 | tagging_loss_video: 6.242|tagging_loss_audio: 7.917|tagging_loss_text: 17.006|tagging_loss_image: 5.804|tagging_loss_fusion: 4.243|total_loss: 41.211 | 72.08 Examples/sec\n",
      "INFO:tensorflow:training step 5735 | tagging_loss_video: 5.102|tagging_loss_audio: 7.685|tagging_loss_text: 17.567|tagging_loss_image: 4.637|tagging_loss_fusion: 3.699|total_loss: 38.690 | 62.01 Examples/sec\n",
      "INFO:tensorflow:training step 5736 | tagging_loss_video: 4.999|tagging_loss_audio: 8.742|tagging_loss_text: 15.952|tagging_loss_image: 4.369|tagging_loss_fusion: 4.501|total_loss: 38.564 | 66.59 Examples/sec\n",
      "INFO:tensorflow:training step 5737 | tagging_loss_video: 4.543|tagging_loss_audio: 8.457|tagging_loss_text: 13.088|tagging_loss_image: 3.602|tagging_loss_fusion: 2.744|total_loss: 32.433 | 69.30 Examples/sec\n",
      "INFO:tensorflow:training step 5738 | tagging_loss_video: 5.370|tagging_loss_audio: 7.900|tagging_loss_text: 17.443|tagging_loss_image: 3.659|tagging_loss_fusion: 3.222|total_loss: 37.594 | 66.11 Examples/sec\n",
      "INFO:tensorflow:training step 5739 | tagging_loss_video: 5.452|tagging_loss_audio: 8.746|tagging_loss_text: 17.000|tagging_loss_image: 5.089|tagging_loss_fusion: 2.680|total_loss: 38.968 | 67.99 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5740 |tagging_loss_video: 6.155|tagging_loss_audio: 8.520|tagging_loss_text: 13.928|tagging_loss_image: 5.563|tagging_loss_fusion: 6.641|total_loss: 40.806 | Examples/sec: 68.31\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.78 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 5741 | tagging_loss_video: 5.697|tagging_loss_audio: 8.162|tagging_loss_text: 14.760|tagging_loss_image: 4.821|tagging_loss_fusion: 5.267|total_loss: 38.707 | 63.27 Examples/sec\n",
      "INFO:tensorflow:training step 5742 | tagging_loss_video: 5.276|tagging_loss_audio: 9.485|tagging_loss_text: 16.927|tagging_loss_image: 4.722|tagging_loss_fusion: 3.581|total_loss: 39.992 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 5743 | tagging_loss_video: 5.750|tagging_loss_audio: 8.607|tagging_loss_text: 11.453|tagging_loss_image: 5.658|tagging_loss_fusion: 3.729|total_loss: 35.197 | 65.88 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5744 | tagging_loss_video: 6.048|tagging_loss_audio: 7.988|tagging_loss_text: 14.116|tagging_loss_image: 5.938|tagging_loss_fusion: 4.916|total_loss: 39.005 | 68.33 Examples/sec\n",
      "INFO:tensorflow:training step 5745 | tagging_loss_video: 6.015|tagging_loss_audio: 8.064|tagging_loss_text: 18.816|tagging_loss_image: 6.551|tagging_loss_fusion: 5.990|total_loss: 45.437 | 68.07 Examples/sec\n",
      "INFO:tensorflow:training step 5746 | tagging_loss_video: 4.086|tagging_loss_audio: 8.874|tagging_loss_text: 17.974|tagging_loss_image: 5.621|tagging_loss_fusion: 2.630|total_loss: 39.185 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 5747 | tagging_loss_video: 5.792|tagging_loss_audio: 8.667|tagging_loss_text: 14.599|tagging_loss_image: 5.953|tagging_loss_fusion: 4.640|total_loss: 39.651 | 68.20 Examples/sec\n",
      "INFO:tensorflow:training step 5748 | tagging_loss_video: 5.139|tagging_loss_audio: 8.569|tagging_loss_text: 15.387|tagging_loss_image: 5.482|tagging_loss_fusion: 3.763|total_loss: 38.341 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 5749 | tagging_loss_video: 6.779|tagging_loss_audio: 9.007|tagging_loss_text: 17.569|tagging_loss_image: 4.684|tagging_loss_fusion: 5.684|total_loss: 43.723 | 63.81 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5750 |tagging_loss_video: 5.794|tagging_loss_audio: 7.693|tagging_loss_text: 16.030|tagging_loss_image: 4.224|tagging_loss_fusion: 4.749|total_loss: 38.490 | Examples/sec: 70.27\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.92 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5751 | tagging_loss_video: 5.699|tagging_loss_audio: 7.614|tagging_loss_text: 15.774|tagging_loss_image: 5.024|tagging_loss_fusion: 4.019|total_loss: 38.130 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 5752 | tagging_loss_video: 5.831|tagging_loss_audio: 8.912|tagging_loss_text: 16.767|tagging_loss_image: 5.027|tagging_loss_fusion: 4.553|total_loss: 41.090 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 5753 | tagging_loss_video: 4.551|tagging_loss_audio: 8.380|tagging_loss_text: 18.176|tagging_loss_image: 5.337|tagging_loss_fusion: 2.828|total_loss: 39.273 | 67.09 Examples/sec\n",
      "INFO:tensorflow:training step 5754 | tagging_loss_video: 5.563|tagging_loss_audio: 7.886|tagging_loss_text: 11.062|tagging_loss_image: 5.525|tagging_loss_fusion: 3.853|total_loss: 33.889 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 5755 | tagging_loss_video: 6.111|tagging_loss_audio: 9.172|tagging_loss_text: 14.977|tagging_loss_image: 6.050|tagging_loss_fusion: 4.698|total_loss: 41.007 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 5756 | tagging_loss_video: 6.934|tagging_loss_audio: 8.684|tagging_loss_text: 16.322|tagging_loss_image: 6.289|tagging_loss_fusion: 5.994|total_loss: 44.223 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 5757 | tagging_loss_video: 6.129|tagging_loss_audio: 9.109|tagging_loss_text: 15.221|tagging_loss_image: 4.551|tagging_loss_fusion: 4.925|total_loss: 39.935 | 65.32 Examples/sec\n",
      "INFO:tensorflow:training step 5758 | tagging_loss_video: 4.534|tagging_loss_audio: 8.346|tagging_loss_text: 18.262|tagging_loss_image: 6.178|tagging_loss_fusion: 2.304|total_loss: 39.625 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 5759 | tagging_loss_video: 5.194|tagging_loss_audio: 9.694|tagging_loss_text: 12.925|tagging_loss_image: 6.522|tagging_loss_fusion: 3.996|total_loss: 38.329 | 71.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5760 |tagging_loss_video: 5.959|tagging_loss_audio: 7.057|tagging_loss_text: 14.439|tagging_loss_image: 4.285|tagging_loss_fusion: 5.770|total_loss: 37.509 | Examples/sec: 61.73\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.77 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5761 | tagging_loss_video: 4.976|tagging_loss_audio: 8.155|tagging_loss_text: 14.399|tagging_loss_image: 4.820|tagging_loss_fusion: 3.666|total_loss: 36.016 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 5762 | tagging_loss_video: 5.410|tagging_loss_audio: 7.910|tagging_loss_text: 14.779|tagging_loss_image: 5.052|tagging_loss_fusion: 3.952|total_loss: 37.104 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 5763 | tagging_loss_video: 3.052|tagging_loss_audio: 7.474|tagging_loss_text: 13.367|tagging_loss_image: 5.406|tagging_loss_fusion: 2.121|total_loss: 31.420 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 5764 | tagging_loss_video: 5.090|tagging_loss_audio: 7.396|tagging_loss_text: 15.421|tagging_loss_image: 5.429|tagging_loss_fusion: 3.839|total_loss: 37.175 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 5765 | tagging_loss_video: 3.842|tagging_loss_audio: 7.327|tagging_loss_text: 15.125|tagging_loss_image: 5.799|tagging_loss_fusion: 2.063|total_loss: 34.156 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 5766 | tagging_loss_video: 5.533|tagging_loss_audio: 8.535|tagging_loss_text: 13.297|tagging_loss_image: 4.961|tagging_loss_fusion: 4.828|total_loss: 37.155 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 5767 | tagging_loss_video: 5.812|tagging_loss_audio: 7.289|tagging_loss_text: 12.086|tagging_loss_image: 5.367|tagging_loss_fusion: 5.195|total_loss: 35.748 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 5768 | tagging_loss_video: 5.534|tagging_loss_audio: 8.187|tagging_loss_text: 14.760|tagging_loss_image: 5.392|tagging_loss_fusion: 3.900|total_loss: 37.772 | 62.02 Examples/sec\n",
      "INFO:tensorflow:training step 5769 | tagging_loss_video: 5.998|tagging_loss_audio: 7.472|tagging_loss_text: 14.370|tagging_loss_image: 4.464|tagging_loss_fusion: 3.610|total_loss: 35.914 | 71.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5770 |tagging_loss_video: 5.446|tagging_loss_audio: 7.992|tagging_loss_text: 14.256|tagging_loss_image: 4.650|tagging_loss_fusion: 4.371|total_loss: 36.714 | Examples/sec: 68.57\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.93\n",
      "INFO:tensorflow:Recording summary at step 5770.\n",
      "INFO:tensorflow:training step 5771 | tagging_loss_video: 5.555|tagging_loss_audio: 8.264|tagging_loss_text: 10.357|tagging_loss_image: 3.802|tagging_loss_fusion: 3.168|total_loss: 31.145 | 55.41 Examples/sec\n",
      "INFO:tensorflow:training step 5772 | tagging_loss_video: 6.254|tagging_loss_audio: 8.656|tagging_loss_text: 15.980|tagging_loss_image: 5.789|tagging_loss_fusion: 6.017|total_loss: 42.696 | 57.62 Examples/sec\n",
      "INFO:tensorflow:training step 5773 | tagging_loss_video: 5.800|tagging_loss_audio: 7.533|tagging_loss_text: 17.362|tagging_loss_image: 4.114|tagging_loss_fusion: 4.403|total_loss: 39.211 | 60.89 Examples/sec\n",
      "INFO:tensorflow:training step 5774 | tagging_loss_video: 5.402|tagging_loss_audio: 7.515|tagging_loss_text: 14.734|tagging_loss_image: 6.080|tagging_loss_fusion: 4.157|total_loss: 37.887 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 5775 | tagging_loss_video: 6.054|tagging_loss_audio: 8.568|tagging_loss_text: 18.598|tagging_loss_image: 4.085|tagging_loss_fusion: 5.258|total_loss: 42.564 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 5776 | tagging_loss_video: 5.416|tagging_loss_audio: 7.898|tagging_loss_text: 15.003|tagging_loss_image: 4.720|tagging_loss_fusion: 2.122|total_loss: 35.159 | 66.56 Examples/sec\n",
      "INFO:tensorflow:training step 5777 | tagging_loss_video: 4.166|tagging_loss_audio: 7.662|tagging_loss_text: 16.315|tagging_loss_image: 4.616|tagging_loss_fusion: 2.685|total_loss: 35.444 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 5778 | tagging_loss_video: 5.886|tagging_loss_audio: 8.286|tagging_loss_text: 11.223|tagging_loss_image: 5.309|tagging_loss_fusion: 5.462|total_loss: 36.167 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 5779 | tagging_loss_video: 6.065|tagging_loss_audio: 7.749|tagging_loss_text: 15.929|tagging_loss_image: 5.739|tagging_loss_fusion: 5.998|total_loss: 41.480 | 71.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5780 |tagging_loss_video: 5.514|tagging_loss_audio: 7.513|tagging_loss_text: 14.071|tagging_loss_image: 4.445|tagging_loss_fusion: 3.811|total_loss: 35.353 | Examples/sec: 68.06\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5781 | tagging_loss_video: 4.984|tagging_loss_audio: 9.446|tagging_loss_text: 13.901|tagging_loss_image: 6.044|tagging_loss_fusion: 2.967|total_loss: 37.342 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 5782 | tagging_loss_video: 6.283|tagging_loss_audio: 8.897|tagging_loss_text: 17.548|tagging_loss_image: 5.969|tagging_loss_fusion: 5.050|total_loss: 43.747 | 62.71 Examples/sec\n",
      "INFO:tensorflow:training step 5783 | tagging_loss_video: 5.864|tagging_loss_audio: 8.484|tagging_loss_text: 15.511|tagging_loss_image: 4.010|tagging_loss_fusion: 3.291|total_loss: 37.159 | 68.13 Examples/sec\n",
      "INFO:tensorflow:training step 5784 | tagging_loss_video: 5.796|tagging_loss_audio: 9.749|tagging_loss_text: 15.724|tagging_loss_image: 5.048|tagging_loss_fusion: 4.546|total_loss: 40.863 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 5785 | tagging_loss_video: 5.724|tagging_loss_audio: 8.415|tagging_loss_text: 15.220|tagging_loss_image: 4.073|tagging_loss_fusion: 3.761|total_loss: 37.194 | 66.86 Examples/sec\n",
      "INFO:tensorflow:training step 5786 | tagging_loss_video: 5.360|tagging_loss_audio: 6.978|tagging_loss_text: 15.435|tagging_loss_image: 3.025|tagging_loss_fusion: 2.616|total_loss: 33.413 | 67.65 Examples/sec\n",
      "INFO:tensorflow:training step 5787 | tagging_loss_video: 5.269|tagging_loss_audio: 7.945|tagging_loss_text: 16.668|tagging_loss_image: 4.751|tagging_loss_fusion: 3.758|total_loss: 38.393 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 5788 | tagging_loss_video: 5.576|tagging_loss_audio: 7.491|tagging_loss_text: 14.179|tagging_loss_image: 4.560|tagging_loss_fusion: 4.570|total_loss: 36.377 | 64.89 Examples/sec\n",
      "INFO:tensorflow:training step 5789 | tagging_loss_video: 5.450|tagging_loss_audio: 9.420|tagging_loss_text: 13.096|tagging_loss_image: 4.145|tagging_loss_fusion: 3.628|total_loss: 35.740 | 68.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5790 |tagging_loss_video: 4.947|tagging_loss_audio: 8.443|tagging_loss_text: 15.813|tagging_loss_image: 4.895|tagging_loss_fusion: 4.090|total_loss: 38.188 | Examples/sec: 69.85\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5791 | tagging_loss_video: 5.507|tagging_loss_audio: 8.232|tagging_loss_text: 14.108|tagging_loss_image: 4.769|tagging_loss_fusion: 3.175|total_loss: 35.792 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 5792 | tagging_loss_video: 5.517|tagging_loss_audio: 7.774|tagging_loss_text: 18.034|tagging_loss_image: 5.065|tagging_loss_fusion: 4.898|total_loss: 41.288 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 5793 | tagging_loss_video: 5.207|tagging_loss_audio: 8.350|tagging_loss_text: 17.637|tagging_loss_image: 3.450|tagging_loss_fusion: 3.206|total_loss: 37.850 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 5794 | tagging_loss_video: 6.020|tagging_loss_audio: 8.226|tagging_loss_text: 16.073|tagging_loss_image: 5.655|tagging_loss_fusion: 5.497|total_loss: 41.471 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 5795 | tagging_loss_video: 5.014|tagging_loss_audio: 8.255|tagging_loss_text: 14.804|tagging_loss_image: 4.602|tagging_loss_fusion: 4.010|total_loss: 36.686 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 5796 | tagging_loss_video: 5.640|tagging_loss_audio: 8.193|tagging_loss_text: 18.503|tagging_loss_image: 5.216|tagging_loss_fusion: 4.251|total_loss: 41.803 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 5797 | tagging_loss_video: 5.886|tagging_loss_audio: 8.920|tagging_loss_text: 13.993|tagging_loss_image: 4.513|tagging_loss_fusion: 4.533|total_loss: 37.844 | 67.80 Examples/sec\n",
      "INFO:tensorflow:training step 5798 | tagging_loss_video: 5.345|tagging_loss_audio: 7.277|tagging_loss_text: 14.963|tagging_loss_image: 4.597|tagging_loss_fusion: 4.095|total_loss: 36.276 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 5799 | tagging_loss_video: 4.242|tagging_loss_audio: 8.112|tagging_loss_text: 12.331|tagging_loss_image: 4.166|tagging_loss_fusion: 2.815|total_loss: 31.667 | 69.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5800 |tagging_loss_video: 4.265|tagging_loss_audio: 10.130|tagging_loss_text: 15.003|tagging_loss_image: 4.044|tagging_loss_fusion: 2.332|total_loss: 35.773 | Examples/sec: 68.50\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 5801 | tagging_loss_video: 4.479|tagging_loss_audio: 8.443|tagging_loss_text: 14.861|tagging_loss_image: 5.437|tagging_loss_fusion: 3.354|total_loss: 36.576 | 67.59 Examples/sec\n",
      "INFO:tensorflow:training step 5802 | tagging_loss_video: 4.707|tagging_loss_audio: 8.739|tagging_loss_text: 14.651|tagging_loss_image: 5.143|tagging_loss_fusion: 3.748|total_loss: 36.988 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 5803 | tagging_loss_video: 4.213|tagging_loss_audio: 7.867|tagging_loss_text: 14.789|tagging_loss_image: 3.636|tagging_loss_fusion: 2.409|total_loss: 32.914 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 5804 | tagging_loss_video: 6.809|tagging_loss_audio: 9.942|tagging_loss_text: 16.356|tagging_loss_image: 6.430|tagging_loss_fusion: 7.415|total_loss: 46.951 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 5805 | tagging_loss_video: 6.305|tagging_loss_audio: 7.676|tagging_loss_text: 15.034|tagging_loss_image: 4.633|tagging_loss_fusion: 5.759|total_loss: 39.407 | 68.15 Examples/sec\n",
      "INFO:tensorflow:training step 5806 | tagging_loss_video: 5.422|tagging_loss_audio: 8.533|tagging_loss_text: 18.115|tagging_loss_image: 6.183|tagging_loss_fusion: 4.388|total_loss: 42.639 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 5807 | tagging_loss_video: 5.272|tagging_loss_audio: 8.447|tagging_loss_text: 17.041|tagging_loss_image: 5.864|tagging_loss_fusion: 3.956|total_loss: 40.580 | 60.65 Examples/sec\n",
      "INFO:tensorflow:training step 5808 | tagging_loss_video: 6.055|tagging_loss_audio: 9.088|tagging_loss_text: 18.799|tagging_loss_image: 5.731|tagging_loss_fusion: 4.853|total_loss: 44.526 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 5809 | tagging_loss_video: 5.180|tagging_loss_audio: 8.407|tagging_loss_text: 15.177|tagging_loss_image: 4.969|tagging_loss_fusion: 4.796|total_loss: 38.530 | 69.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5810 |tagging_loss_video: 6.139|tagging_loss_audio: 8.453|tagging_loss_text: 17.880|tagging_loss_image: 5.685|tagging_loss_fusion: 5.438|total_loss: 43.594 | Examples/sec: 66.32\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5811 | tagging_loss_video: 6.757|tagging_loss_audio: 9.514|tagging_loss_text: 12.639|tagging_loss_image: 5.334|tagging_loss_fusion: 6.324|total_loss: 40.568 | 66.79 Examples/sec\n",
      "INFO:tensorflow:training step 5812 | tagging_loss_video: 5.235|tagging_loss_audio: 8.454|tagging_loss_text: 15.611|tagging_loss_image: 4.812|tagging_loss_fusion: 3.648|total_loss: 37.761 | 69.58 Examples/sec\n",
      "INFO:tensorflow:training step 5813 | tagging_loss_video: 5.792|tagging_loss_audio: 9.223|tagging_loss_text: 15.245|tagging_loss_image: 3.588|tagging_loss_fusion: 4.237|total_loss: 38.084 | 63.67 Examples/sec\n",
      "INFO:tensorflow:training step 5814 | tagging_loss_video: 5.318|tagging_loss_audio: 9.296|tagging_loss_text: 16.598|tagging_loss_image: 3.900|tagging_loss_fusion: 2.812|total_loss: 37.924 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 5815 | tagging_loss_video: 6.739|tagging_loss_audio: 9.025|tagging_loss_text: 16.679|tagging_loss_image: 6.243|tagging_loss_fusion: 6.363|total_loss: 45.049 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 5816 | tagging_loss_video: 5.219|tagging_loss_audio: 9.071|tagging_loss_text: 12.665|tagging_loss_image: 5.883|tagging_loss_fusion: 3.568|total_loss: 36.406 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 5817 | tagging_loss_video: 6.070|tagging_loss_audio: 10.354|tagging_loss_text: 17.147|tagging_loss_image: 5.640|tagging_loss_fusion: 3.814|total_loss: 43.027 | 67.74 Examples/sec\n",
      "INFO:tensorflow:training step 5818 | tagging_loss_video: 4.361|tagging_loss_audio: 9.093|tagging_loss_text: 15.754|tagging_loss_image: 5.327|tagging_loss_fusion: 3.268|total_loss: 37.803 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 5819 | tagging_loss_video: 5.691|tagging_loss_audio: 8.507|tagging_loss_text: 16.920|tagging_loss_image: 6.172|tagging_loss_fusion: 6.416|total_loss: 43.706 | 69.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5820 |tagging_loss_video: 5.209|tagging_loss_audio: 8.253|tagging_loss_text: 11.751|tagging_loss_image: 4.528|tagging_loss_fusion: 3.785|total_loss: 33.526 | Examples/sec: 68.88\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.80 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5821 | tagging_loss_video: 5.512|tagging_loss_audio: 9.584|tagging_loss_text: 15.166|tagging_loss_image: 5.009|tagging_loss_fusion: 4.088|total_loss: 39.359 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 5822 | tagging_loss_video: 6.121|tagging_loss_audio: 10.655|tagging_loss_text: 13.691|tagging_loss_image: 5.126|tagging_loss_fusion: 5.104|total_loss: 40.697 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 5823 | tagging_loss_video: 7.252|tagging_loss_audio: 8.891|tagging_loss_text: 19.657|tagging_loss_image: 5.275|tagging_loss_fusion: 5.187|total_loss: 46.262 | 66.49 Examples/sec\n",
      "INFO:tensorflow:training step 5824 | tagging_loss_video: 6.361|tagging_loss_audio: 9.514|tagging_loss_text: 16.028|tagging_loss_image: 5.967|tagging_loss_fusion: 5.761|total_loss: 43.632 | 63.13 Examples/sec\n",
      "INFO:tensorflow:training step 5825 | tagging_loss_video: 7.105|tagging_loss_audio: 10.571|tagging_loss_text: 16.029|tagging_loss_image: 5.904|tagging_loss_fusion: 5.092|total_loss: 44.700 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 5826 | tagging_loss_video: 6.243|tagging_loss_audio: 8.311|tagging_loss_text: 11.759|tagging_loss_image: 3.084|tagging_loss_fusion: 3.685|total_loss: 33.081 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 5827 | tagging_loss_video: 5.302|tagging_loss_audio: 9.503|tagging_loss_text: 15.984|tagging_loss_image: 5.867|tagging_loss_fusion: 3.755|total_loss: 40.410 | 64.58 Examples/sec\n",
      "INFO:tensorflow:training step 5828 | tagging_loss_video: 4.960|tagging_loss_audio: 8.202|tagging_loss_text: 15.418|tagging_loss_image: 5.155|tagging_loss_fusion: 4.479|total_loss: 38.215 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 5829 | tagging_loss_video: 5.965|tagging_loss_audio: 8.502|tagging_loss_text: 19.193|tagging_loss_image: 5.485|tagging_loss_fusion: 4.350|total_loss: 43.496 | 71.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5830 |tagging_loss_video: 4.870|tagging_loss_audio: 9.486|tagging_loss_text: 12.643|tagging_loss_image: 5.334|tagging_loss_fusion: 2.943|total_loss: 35.277 | Examples/sec: 67.72\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 5831 | tagging_loss_video: 6.248|tagging_loss_audio: 10.625|tagging_loss_text: 17.060|tagging_loss_image: 6.488|tagging_loss_fusion: 6.236|total_loss: 46.657 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 5832 | tagging_loss_video: 6.850|tagging_loss_audio: 9.311|tagging_loss_text: 14.781|tagging_loss_image: 5.350|tagging_loss_fusion: 6.317|total_loss: 42.609 | 59.99 Examples/sec\n",
      "INFO:tensorflow:training step 5833 | tagging_loss_video: 5.299|tagging_loss_audio: 8.999|tagging_loss_text: 15.075|tagging_loss_image: 5.615|tagging_loss_fusion: 5.572|total_loss: 40.560 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 5834 | tagging_loss_video: 4.657|tagging_loss_audio: 8.723|tagging_loss_text: 15.686|tagging_loss_image: 5.072|tagging_loss_fusion: 3.030|total_loss: 37.168 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 5835 | tagging_loss_video: 5.292|tagging_loss_audio: 7.578|tagging_loss_text: 13.486|tagging_loss_image: 4.214|tagging_loss_fusion: 2.984|total_loss: 33.554 | 64.77 Examples/sec\n",
      "INFO:tensorflow:training step 5836 | tagging_loss_video: 5.801|tagging_loss_audio: 10.904|tagging_loss_text: 16.135|tagging_loss_image: 5.254|tagging_loss_fusion: 4.169|total_loss: 42.263 | 67.82 Examples/sec\n",
      "INFO:tensorflow:training step 5837 | tagging_loss_video: 6.139|tagging_loss_audio: 7.862|tagging_loss_text: 13.519|tagging_loss_image: 4.575|tagging_loss_fusion: 5.778|total_loss: 37.874 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 5838 | tagging_loss_video: 5.811|tagging_loss_audio: 8.531|tagging_loss_text: 15.847|tagging_loss_image: 4.957|tagging_loss_fusion: 4.333|total_loss: 39.479 | 63.04 Examples/sec\n",
      "INFO:tensorflow:training step 5839 | tagging_loss_video: 6.392|tagging_loss_audio: 8.366|tagging_loss_text: 14.600|tagging_loss_image: 3.367|tagging_loss_fusion: 4.321|total_loss: 37.047 | 68.03 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5840 |tagging_loss_video: 4.081|tagging_loss_audio: 8.592|tagging_loss_text: 18.208|tagging_loss_image: 5.639|tagging_loss_fusion: 2.599|total_loss: 39.120 | Examples/sec: 72.28\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.96 |recall@0.1: 1.00 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5841 | tagging_loss_video: 4.845|tagging_loss_audio: 8.688|tagging_loss_text: 13.854|tagging_loss_image: 5.157|tagging_loss_fusion: 2.630|total_loss: 35.174 | 66.60 Examples/sec\n",
      "INFO:tensorflow:training step 5842 | tagging_loss_video: 5.482|tagging_loss_audio: 7.742|tagging_loss_text: 13.927|tagging_loss_image: 5.166|tagging_loss_fusion: 5.371|total_loss: 37.687 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 5843 | tagging_loss_video: 4.315|tagging_loss_audio: 8.384|tagging_loss_text: 17.441|tagging_loss_image: 4.544|tagging_loss_fusion: 2.289|total_loss: 36.973 | 65.03 Examples/sec\n",
      "INFO:tensorflow:training step 5844 | tagging_loss_video: 5.883|tagging_loss_audio: 8.359|tagging_loss_text: 19.392|tagging_loss_image: 4.332|tagging_loss_fusion: 4.852|total_loss: 42.818 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 5845 | tagging_loss_video: 5.985|tagging_loss_audio: 9.030|tagging_loss_text: 13.890|tagging_loss_image: 5.009|tagging_loss_fusion: 6.364|total_loss: 40.277 | 66.37 Examples/sec\n",
      "INFO:tensorflow:training step 5846 | tagging_loss_video: 4.774|tagging_loss_audio: 8.945|tagging_loss_text: 13.592|tagging_loss_image: 4.270|tagging_loss_fusion: 2.804|total_loss: 34.385 | 64.53 Examples/sec\n",
      "INFO:tensorflow:training step 5847 | tagging_loss_video: 5.523|tagging_loss_audio: 7.566|tagging_loss_text: 17.338|tagging_loss_image: 5.144|tagging_loss_fusion: 3.817|total_loss: 39.388 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 5848 | tagging_loss_video: 6.303|tagging_loss_audio: 7.755|tagging_loss_text: 14.609|tagging_loss_image: 5.750|tagging_loss_fusion: 5.446|total_loss: 39.863 | 72.25 Examples/sec\n",
      "INFO:tensorflow:training step 5849 | tagging_loss_video: 5.372|tagging_loss_audio: 8.337|tagging_loss_text: 15.560|tagging_loss_image: 3.987|tagging_loss_fusion: 3.998|total_loss: 37.254 | 62.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5850 |tagging_loss_video: 6.758|tagging_loss_audio: 8.301|tagging_loss_text: 13.930|tagging_loss_image: 5.849|tagging_loss_fusion: 6.675|total_loss: 41.513 | Examples/sec: 68.24\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.83 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5851 | tagging_loss_video: 6.639|tagging_loss_audio: 8.924|tagging_loss_text: 15.975|tagging_loss_image: 5.104|tagging_loss_fusion: 5.595|total_loss: 42.238 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 5852 | tagging_loss_video: 5.762|tagging_loss_audio: 7.985|tagging_loss_text: 14.127|tagging_loss_image: 5.069|tagging_loss_fusion: 4.100|total_loss: 37.042 | 62.64 Examples/sec\n",
      "INFO:tensorflow:training step 5853 | tagging_loss_video: 5.385|tagging_loss_audio: 9.188|tagging_loss_text: 15.081|tagging_loss_image: 5.285|tagging_loss_fusion: 4.317|total_loss: 39.256 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 5854 | tagging_loss_video: 6.680|tagging_loss_audio: 10.104|tagging_loss_text: 17.812|tagging_loss_image: 5.431|tagging_loss_fusion: 5.442|total_loss: 45.469 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 5855 | tagging_loss_video: 5.283|tagging_loss_audio: 8.508|tagging_loss_text: 14.008|tagging_loss_image: 4.510|tagging_loss_fusion: 3.513|total_loss: 35.822 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 5856 | tagging_loss_video: 6.100|tagging_loss_audio: 8.612|tagging_loss_text: 12.903|tagging_loss_image: 5.582|tagging_loss_fusion: 4.940|total_loss: 38.136 | 71.59 Examples/sec\n",
      "INFO:tensorflow:training step 5857 | tagging_loss_video: 5.422|tagging_loss_audio: 8.482|tagging_loss_text: 13.848|tagging_loss_image: 5.772|tagging_loss_fusion: 4.915|total_loss: 38.438 | 60.94 Examples/sec\n",
      "INFO:tensorflow:training step 5858 | tagging_loss_video: 5.073|tagging_loss_audio: 8.915|tagging_loss_text: 15.074|tagging_loss_image: 4.889|tagging_loss_fusion: 4.555|total_loss: 38.506 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 5859 | tagging_loss_video: 6.120|tagging_loss_audio: 8.375|tagging_loss_text: 13.943|tagging_loss_image: 6.157|tagging_loss_fusion: 5.915|total_loss: 40.510 | 69.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5860 |tagging_loss_video: 4.609|tagging_loss_audio: 6.775|tagging_loss_text: 14.724|tagging_loss_image: 5.261|tagging_loss_fusion: 4.000|total_loss: 35.369 | Examples/sec: 64.69\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.82 | precision@0.5: 0.97 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5861 | tagging_loss_video: 6.008|tagging_loss_audio: 8.456|tagging_loss_text: 17.583|tagging_loss_image: 5.524|tagging_loss_fusion: 5.788|total_loss: 43.359 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 5862 | tagging_loss_video: 5.877|tagging_loss_audio: 9.263|tagging_loss_text: 17.290|tagging_loss_image: 5.190|tagging_loss_fusion: 3.524|total_loss: 41.143 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 5863 | tagging_loss_video: 6.475|tagging_loss_audio: 7.679|tagging_loss_text: 13.521|tagging_loss_image: 4.919|tagging_loss_fusion: 5.857|total_loss: 38.451 | 66.06 Examples/sec\n",
      "INFO:tensorflow:training step 5864 | tagging_loss_video: 6.520|tagging_loss_audio: 8.618|tagging_loss_text: 18.493|tagging_loss_image: 6.364|tagging_loss_fusion: 5.906|total_loss: 45.902 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 5865 | tagging_loss_video: 5.543|tagging_loss_audio: 8.581|tagging_loss_text: 13.355|tagging_loss_image: 5.495|tagging_loss_fusion: 4.323|total_loss: 37.298 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 5866 | tagging_loss_video: 3.484|tagging_loss_audio: 8.848|tagging_loss_text: 16.329|tagging_loss_image: 5.061|tagging_loss_fusion: 1.712|total_loss: 35.434 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 5867 | tagging_loss_video: 4.109|tagging_loss_audio: 8.946|tagging_loss_text: 18.492|tagging_loss_image: 3.925|tagging_loss_fusion: 1.942|total_loss: 37.414 | 67.48 Examples/sec\n",
      "INFO:tensorflow:training step 5868 | tagging_loss_video: 5.227|tagging_loss_audio: 8.930|tagging_loss_text: 16.817|tagging_loss_image: 5.372|tagging_loss_fusion: 4.625|total_loss: 40.970 | 69.13 Examples/sec\n",
      "INFO:tensorflow:training step 5869 | tagging_loss_video: 5.119|tagging_loss_audio: 7.701|tagging_loss_text: 14.559|tagging_loss_image: 4.276|tagging_loss_fusion: 2.717|total_loss: 34.372 | 67.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5870 |tagging_loss_video: 6.133|tagging_loss_audio: 8.082|tagging_loss_text: 18.722|tagging_loss_image: 4.302|tagging_loss_fusion: 4.500|total_loss: 41.739 | Examples/sec: 68.23\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.81 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5871 | tagging_loss_video: 5.715|tagging_loss_audio: 8.706|tagging_loss_text: 17.066|tagging_loss_image: 5.580|tagging_loss_fusion: 4.427|total_loss: 41.494 | 71.34 Examples/sec\n",
      "INFO:tensorflow:training step 5872 | tagging_loss_video: 4.272|tagging_loss_audio: 8.931|tagging_loss_text: 16.084|tagging_loss_image: 4.932|tagging_loss_fusion: 3.766|total_loss: 37.985 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 5873 | tagging_loss_video: 3.055|tagging_loss_audio: 8.647|tagging_loss_text: 13.593|tagging_loss_image: 5.187|tagging_loss_fusion: 2.343|total_loss: 32.824 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 5874 | tagging_loss_video: 5.341|tagging_loss_audio: 7.724|tagging_loss_text: 13.654|tagging_loss_image: 4.927|tagging_loss_fusion: 4.752|total_loss: 36.397 | 62.22 Examples/sec\n",
      "INFO:tensorflow:training step 5875 | tagging_loss_video: 4.136|tagging_loss_audio: 8.237|tagging_loss_text: 18.731|tagging_loss_image: 6.451|tagging_loss_fusion: 2.965|total_loss: 40.519 | 72.19 Examples/sec\n",
      "INFO:tensorflow:training step 5876 | tagging_loss_video: 6.509|tagging_loss_audio: 8.101|tagging_loss_text: 15.148|tagging_loss_image: 4.014|tagging_loss_fusion: 4.724|total_loss: 38.496 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 5877 | tagging_loss_video: 2.701|tagging_loss_audio: 8.340|tagging_loss_text: 15.344|tagging_loss_image: 4.396|tagging_loss_fusion: 1.341|total_loss: 32.122 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 5878 | tagging_loss_video: 6.660|tagging_loss_audio: 8.939|tagging_loss_text: 15.523|tagging_loss_image: 5.756|tagging_loss_fusion: 5.606|total_loss: 42.484 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 5879 | tagging_loss_video: 5.442|tagging_loss_audio: 7.771|tagging_loss_text: 11.943|tagging_loss_image: 5.718|tagging_loss_fusion: 6.732|total_loss: 37.605 | 71.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5880 |tagging_loss_video: 5.674|tagging_loss_audio: 9.171|tagging_loss_text: 14.752|tagging_loss_image: 5.731|tagging_loss_fusion: 4.233|total_loss: 39.561 | Examples/sec: 69.28\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5881 | tagging_loss_video: 5.304|tagging_loss_audio: 9.342|tagging_loss_text: 15.507|tagging_loss_image: 5.978|tagging_loss_fusion: 4.295|total_loss: 40.425 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 5882 | tagging_loss_video: 3.683|tagging_loss_audio: 7.816|tagging_loss_text: 16.179|tagging_loss_image: 5.418|tagging_loss_fusion: 2.228|total_loss: 35.324 | 63.46 Examples/sec\n",
      "INFO:tensorflow:training step 5883 | tagging_loss_video: 5.192|tagging_loss_audio: 7.956|tagging_loss_text: 17.656|tagging_loss_image: 5.996|tagging_loss_fusion: 4.416|total_loss: 41.216 | 69.51 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 5884 | tagging_loss_video: 4.197|tagging_loss_audio: 9.505|tagging_loss_text: 18.110|tagging_loss_image: 6.369|tagging_loss_fusion: 2.789|total_loss: 40.971 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 5885 | tagging_loss_video: 6.342|tagging_loss_audio: 8.117|tagging_loss_text: 18.672|tagging_loss_image: 5.460|tagging_loss_fusion: 4.874|total_loss: 43.465 | 64.68 Examples/sec\n",
      "INFO:tensorflow:training step 5886 | tagging_loss_video: 5.934|tagging_loss_audio: 8.576|tagging_loss_text: 17.395|tagging_loss_image: 5.012|tagging_loss_fusion: 3.710|total_loss: 40.628 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 5887 | tagging_loss_video: 4.955|tagging_loss_audio: 10.133|tagging_loss_text: 16.068|tagging_loss_image: 4.267|tagging_loss_fusion: 3.497|total_loss: 38.920 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 5888 | tagging_loss_video: 5.459|tagging_loss_audio: 9.155|tagging_loss_text: 16.083|tagging_loss_image: 5.261|tagging_loss_fusion: 3.728|total_loss: 39.686 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 5889 | tagging_loss_video: 6.240|tagging_loss_audio: 7.910|tagging_loss_text: 14.105|tagging_loss_image: 3.476|tagging_loss_fusion: 3.620|total_loss: 35.351 | 68.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5890 |tagging_loss_video: 5.373|tagging_loss_audio: 9.940|tagging_loss_text: 16.465|tagging_loss_image: 5.136|tagging_loss_fusion: 4.861|total_loss: 41.776 | Examples/sec: 70.36\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5891 | tagging_loss_video: 6.160|tagging_loss_audio: 8.101|tagging_loss_text: 16.119|tagging_loss_image: 5.034|tagging_loss_fusion: 4.212|total_loss: 39.626 | 64.51 Examples/sec\n",
      "INFO:tensorflow:training step 5892 | tagging_loss_video: 5.665|tagging_loss_audio: 8.385|tagging_loss_text: 16.061|tagging_loss_image: 3.531|tagging_loss_fusion: 3.779|total_loss: 37.420 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 5893 | tagging_loss_video: 5.441|tagging_loss_audio: 7.747|tagging_loss_text: 15.616|tagging_loss_image: 4.290|tagging_loss_fusion: 2.974|total_loss: 36.068 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 5894 | tagging_loss_video: 5.624|tagging_loss_audio: 8.989|tagging_loss_text: 15.652|tagging_loss_image: 5.662|tagging_loss_fusion: 4.593|total_loss: 40.520 | 66.63 Examples/sec\n",
      "INFO:tensorflow:training step 5895 | tagging_loss_video: 4.821|tagging_loss_audio: 8.400|tagging_loss_text: 18.521|tagging_loss_image: 4.735|tagging_loss_fusion: 2.414|total_loss: 38.890 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 5896 | tagging_loss_video: 3.421|tagging_loss_audio: 8.831|tagging_loss_text: 15.224|tagging_loss_image: 7.109|tagging_loss_fusion: 2.152|total_loss: 36.738 | 63.30 Examples/sec\n",
      "INFO:tensorflow:training step 5897 | tagging_loss_video: 6.240|tagging_loss_audio: 8.789|tagging_loss_text: 16.148|tagging_loss_image: 5.342|tagging_loss_fusion: 5.040|total_loss: 41.558 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 5898 | tagging_loss_video: 5.775|tagging_loss_audio: 10.023|tagging_loss_text: 15.197|tagging_loss_image: 4.362|tagging_loss_fusion: 3.478|total_loss: 38.836 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 5899 | tagging_loss_video: 5.454|tagging_loss_audio: 7.566|tagging_loss_text: 14.446|tagging_loss_image: 5.225|tagging_loss_fusion: 4.447|total_loss: 37.138 | 58.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5900 |tagging_loss_video: 4.646|tagging_loss_audio: 7.452|tagging_loss_text: 17.514|tagging_loss_image: 5.044|tagging_loss_fusion: 3.174|total_loss: 37.829 | Examples/sec: 70.98\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 5901 | tagging_loss_video: 5.350|tagging_loss_audio: 7.594|tagging_loss_text: 17.473|tagging_loss_image: 5.353|tagging_loss_fusion: 3.933|total_loss: 39.703 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 5902 | tagging_loss_video: 4.528|tagging_loss_audio: 7.349|tagging_loss_text: 13.995|tagging_loss_image: 3.941|tagging_loss_fusion: 3.243|total_loss: 33.056 | 66.25 Examples/sec\n",
      "INFO:tensorflow:training step 5903 | tagging_loss_video: 5.482|tagging_loss_audio: 8.576|tagging_loss_text: 14.719|tagging_loss_image: 5.084|tagging_loss_fusion: 3.938|total_loss: 37.799 | 72.43 Examples/sec\n",
      "INFO:tensorflow:training step 5904 | tagging_loss_video: 4.529|tagging_loss_audio: 7.565|tagging_loss_text: 17.753|tagging_loss_image: 5.208|tagging_loss_fusion: 4.047|total_loss: 39.101 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 5905 | tagging_loss_video: 5.670|tagging_loss_audio: 7.411|tagging_loss_text: 15.302|tagging_loss_image: 3.283|tagging_loss_fusion: 4.134|total_loss: 35.800 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 5906 | tagging_loss_video: 5.256|tagging_loss_audio: 7.537|tagging_loss_text: 15.298|tagging_loss_image: 4.354|tagging_loss_fusion: 3.793|total_loss: 36.238 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 5907 | tagging_loss_video: 5.299|tagging_loss_audio: 9.397|tagging_loss_text: 18.423|tagging_loss_image: 4.616|tagging_loss_fusion: 3.912|total_loss: 41.647 | 71.05 Examples/sec\n",
      "INFO:tensorflow:training step 5908 | tagging_loss_video: 6.072|tagging_loss_audio: 8.804|tagging_loss_text: 15.739|tagging_loss_image: 5.507|tagging_loss_fusion: 4.688|total_loss: 40.811 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 5909 | tagging_loss_video: 4.989|tagging_loss_audio: 8.240|tagging_loss_text: 9.913|tagging_loss_image: 5.148|tagging_loss_fusion: 3.650|total_loss: 31.940 | 71.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5910 |tagging_loss_video: 5.664|tagging_loss_audio: 7.532|tagging_loss_text: 13.030|tagging_loss_image: 4.992|tagging_loss_fusion: 4.034|total_loss: 35.253 | Examples/sec: 59.45\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 5911 | tagging_loss_video: 5.879|tagging_loss_audio: 7.780|tagging_loss_text: 16.582|tagging_loss_image: 4.349|tagging_loss_fusion: 4.186|total_loss: 38.776 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 5912 | tagging_loss_video: 5.791|tagging_loss_audio: 8.880|tagging_loss_text: 17.452|tagging_loss_image: 5.052|tagging_loss_fusion: 4.787|total_loss: 41.960 | 67.62 Examples/sec\n",
      "INFO:tensorflow:training step 5913 | tagging_loss_video: 4.879|tagging_loss_audio: 7.253|tagging_loss_text: 11.566|tagging_loss_image: 5.235|tagging_loss_fusion: 4.262|total_loss: 33.196 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 5914 | tagging_loss_video: 6.002|tagging_loss_audio: 8.926|tagging_loss_text: 15.415|tagging_loss_image: 5.539|tagging_loss_fusion: 6.978|total_loss: 42.860 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 5915 | tagging_loss_video: 5.799|tagging_loss_audio: 8.570|tagging_loss_text: 13.771|tagging_loss_image: 5.600|tagging_loss_fusion: 4.835|total_loss: 38.575 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 5916 | tagging_loss_video: 4.947|tagging_loss_audio: 7.931|tagging_loss_text: 16.968|tagging_loss_image: 5.089|tagging_loss_fusion: 4.003|total_loss: 38.938 | 66.70 Examples/sec\n",
      "INFO:tensorflow:training step 5917 | tagging_loss_video: 5.817|tagging_loss_audio: 7.578|tagging_loss_text: 15.638|tagging_loss_image: 4.993|tagging_loss_fusion: 4.162|total_loss: 38.188 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 5918 | tagging_loss_video: 4.511|tagging_loss_audio: 8.162|tagging_loss_text: 16.506|tagging_loss_image: 4.261|tagging_loss_fusion: 3.108|total_loss: 36.548 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 5919 | tagging_loss_video: 5.111|tagging_loss_audio: 8.077|tagging_loss_text: 15.258|tagging_loss_image: 4.518|tagging_loss_fusion: 3.550|total_loss: 36.513 | 69.81 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5920 |tagging_loss_video: 5.160|tagging_loss_audio: 6.884|tagging_loss_text: 15.401|tagging_loss_image: 4.533|tagging_loss_fusion: 3.659|total_loss: 35.637 | Examples/sec: 68.41\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 5921 | tagging_loss_video: 3.722|tagging_loss_audio: 9.496|tagging_loss_text: 18.084|tagging_loss_image: 4.805|tagging_loss_fusion: 3.412|total_loss: 39.520 | 69.96 Examples/sec\n",
      "INFO:tensorflow:training step 5922 | tagging_loss_video: 5.813|tagging_loss_audio: 9.062|tagging_loss_text: 14.057|tagging_loss_image: 5.326|tagging_loss_fusion: 3.358|total_loss: 37.617 | 61.90 Examples/sec\n",
      "INFO:tensorflow:training step 5923 | tagging_loss_video: 5.428|tagging_loss_audio: 8.853|tagging_loss_text: 18.246|tagging_loss_image: 4.801|tagging_loss_fusion: 5.618|total_loss: 42.945 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 5924 | tagging_loss_video: 6.036|tagging_loss_audio: 8.763|tagging_loss_text: 16.401|tagging_loss_image: 4.670|tagging_loss_fusion: 5.158|total_loss: 41.028 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 5925 | tagging_loss_video: 3.809|tagging_loss_audio: 8.238|tagging_loss_text: 17.743|tagging_loss_image: 5.122|tagging_loss_fusion: 2.551|total_loss: 37.463 | 61.31 Examples/sec\n",
      "INFO:tensorflow:training step 5926 | tagging_loss_video: 3.372|tagging_loss_audio: 7.669|tagging_loss_text: 13.638|tagging_loss_image: 3.589|tagging_loss_fusion: 1.763|total_loss: 30.030 | 71.86 Examples/sec\n",
      "INFO:tensorflow:training step 5927 | tagging_loss_video: 4.665|tagging_loss_audio: 8.944|tagging_loss_text: 12.668|tagging_loss_image: 5.143|tagging_loss_fusion: 3.661|total_loss: 35.081 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 5928 | tagging_loss_video: 5.657|tagging_loss_audio: 7.588|tagging_loss_text: 15.477|tagging_loss_image: 3.198|tagging_loss_fusion: 3.549|total_loss: 35.470 | 64.90 Examples/sec\n",
      "INFO:tensorflow:training step 5929 | tagging_loss_video: 5.700|tagging_loss_audio: 8.660|tagging_loss_text: 13.792|tagging_loss_image: 6.040|tagging_loss_fusion: 4.999|total_loss: 39.192 | 69.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5930 |tagging_loss_video: 4.392|tagging_loss_audio: 6.491|tagging_loss_text: 14.473|tagging_loss_image: 5.999|tagging_loss_fusion: 4.350|total_loss: 35.705 | Examples/sec: 71.12\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 5931 | tagging_loss_video: 4.137|tagging_loss_audio: 8.215|tagging_loss_text: 15.374|tagging_loss_image: 5.273|tagging_loss_fusion: 2.321|total_loss: 35.321 | 65.08 Examples/sec\n",
      "INFO:tensorflow:training step 5932 | tagging_loss_video: 4.888|tagging_loss_audio: 9.303|tagging_loss_text: 13.186|tagging_loss_image: 5.420|tagging_loss_fusion: 2.794|total_loss: 35.591 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 5933 | tagging_loss_video: 5.327|tagging_loss_audio: 8.569|tagging_loss_text: 15.616|tagging_loss_image: 4.993|tagging_loss_fusion: 4.997|total_loss: 39.502 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 5934 | tagging_loss_video: 5.534|tagging_loss_audio: 7.339|tagging_loss_text: 16.781|tagging_loss_image: 4.815|tagging_loss_fusion: 3.082|total_loss: 37.550 | 68.09 Examples/sec\n",
      "INFO:tensorflow:training step 5935 | tagging_loss_video: 6.063|tagging_loss_audio: 9.744|tagging_loss_text: 17.047|tagging_loss_image: 5.826|tagging_loss_fusion: 5.552|total_loss: 44.232 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 5936 | tagging_loss_video: 5.025|tagging_loss_audio: 8.427|tagging_loss_text: 15.761|tagging_loss_image: 5.539|tagging_loss_fusion: 3.653|total_loss: 38.406 | 65.10 Examples/sec\n",
      "INFO:tensorflow:training step 5937 | tagging_loss_video: 4.952|tagging_loss_audio: 7.870|tagging_loss_text: 10.174|tagging_loss_image: 5.153|tagging_loss_fusion: 3.807|total_loss: 31.956 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 5938 | tagging_loss_video: 5.892|tagging_loss_audio: 7.508|tagging_loss_text: 16.192|tagging_loss_image: 3.897|tagging_loss_fusion: 4.197|total_loss: 37.686 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 5939 | tagging_loss_video: 5.544|tagging_loss_audio: 8.001|tagging_loss_text: 15.577|tagging_loss_image: 4.126|tagging_loss_fusion: 3.942|total_loss: 37.190 | 68.07 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5940 |tagging_loss_video: 6.262|tagging_loss_audio: 10.005|tagging_loss_text: 17.134|tagging_loss_image: 4.911|tagging_loss_fusion: 4.921|total_loss: 43.234 | Examples/sec: 70.16\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 5941 | tagging_loss_video: 4.926|tagging_loss_audio: 8.472|tagging_loss_text: 13.514|tagging_loss_image: 5.746|tagging_loss_fusion: 4.554|total_loss: 37.211 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 5942 | tagging_loss_video: 4.600|tagging_loss_audio: 8.346|tagging_loss_text: 11.850|tagging_loss_image: 5.885|tagging_loss_fusion: 3.208|total_loss: 33.888 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 5943 | tagging_loss_video: 4.917|tagging_loss_audio: 7.307|tagging_loss_text: 16.086|tagging_loss_image: 3.177|tagging_loss_fusion: 3.234|total_loss: 34.722 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 5944 | tagging_loss_video: 5.938|tagging_loss_audio: 9.791|tagging_loss_text: 17.061|tagging_loss_image: 5.539|tagging_loss_fusion: 5.474|total_loss: 43.803 | 64.75 Examples/sec\n",
      "INFO:tensorflow:training step 5945 | tagging_loss_video: 5.746|tagging_loss_audio: 8.562|tagging_loss_text: 15.739|tagging_loss_image: 6.612|tagging_loss_fusion: 5.796|total_loss: 42.455 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 5946 | tagging_loss_video: 6.352|tagging_loss_audio: 7.864|tagging_loss_text: 18.117|tagging_loss_image: 5.560|tagging_loss_fusion: 5.010|total_loss: 42.902 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 5947 | tagging_loss_video: 6.676|tagging_loss_audio: 9.588|tagging_loss_text: 16.357|tagging_loss_image: 6.473|tagging_loss_fusion: 7.584|total_loss: 46.678 | 63.29 Examples/sec\n",
      "INFO:tensorflow:training step 5948 | tagging_loss_video: 5.637|tagging_loss_audio: 9.000|tagging_loss_text: 14.466|tagging_loss_image: 4.697|tagging_loss_fusion: 3.649|total_loss: 37.449 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 5949 | tagging_loss_video: 5.034|tagging_loss_audio: 7.772|tagging_loss_text: 15.165|tagging_loss_image: 4.936|tagging_loss_fusion: 3.193|total_loss: 36.100 | 68.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5950 |tagging_loss_video: 5.618|tagging_loss_audio: 7.778|tagging_loss_text: 15.146|tagging_loss_image: 5.237|tagging_loss_fusion: 5.602|total_loss: 39.382 | Examples/sec: 64.91\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 5951 | tagging_loss_video: 5.800|tagging_loss_audio: 8.888|tagging_loss_text: 16.362|tagging_loss_image: 5.230|tagging_loss_fusion: 4.021|total_loss: 40.301 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 5952 | tagging_loss_video: 5.777|tagging_loss_audio: 8.613|tagging_loss_text: 15.606|tagging_loss_image: 4.422|tagging_loss_fusion: 4.647|total_loss: 39.066 | 67.81 Examples/sec\n",
      "INFO:tensorflow:training step 5953 | tagging_loss_video: 5.344|tagging_loss_audio: 8.739|tagging_loss_text: 14.549|tagging_loss_image: 3.892|tagging_loss_fusion: 3.184|total_loss: 35.709 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 5954 | tagging_loss_video: 4.676|tagging_loss_audio: 8.400|tagging_loss_text: 16.609|tagging_loss_image: 6.213|tagging_loss_fusion: 3.184|total_loss: 39.081 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 5955 | tagging_loss_video: 6.008|tagging_loss_audio: 9.176|tagging_loss_text: 15.957|tagging_loss_image: 5.746|tagging_loss_fusion: 3.955|total_loss: 40.842 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 5956 | tagging_loss_video: 6.113|tagging_loss_audio: 8.167|tagging_loss_text: 17.581|tagging_loss_image: 5.390|tagging_loss_fusion: 3.569|total_loss: 40.820 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 5957 | tagging_loss_video: 6.374|tagging_loss_audio: 9.152|tagging_loss_text: 18.643|tagging_loss_image: 6.771|tagging_loss_fusion: 5.699|total_loss: 46.639 | 67.20 Examples/sec\n",
      "INFO:tensorflow:training step 5958 | tagging_loss_video: 5.494|tagging_loss_audio: 9.151|tagging_loss_text: 16.413|tagging_loss_image: 4.662|tagging_loss_fusion: 4.295|total_loss: 40.016 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 5959 | tagging_loss_video: 5.915|tagging_loss_audio: 8.764|tagging_loss_text: 15.487|tagging_loss_image: 4.901|tagging_loss_fusion: 5.681|total_loss: 40.748 | 68.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5960 |tagging_loss_video: 5.474|tagging_loss_audio: 8.873|tagging_loss_text: 14.208|tagging_loss_image: 3.761|tagging_loss_fusion: 4.028|total_loss: 36.344 | Examples/sec: 69.30\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5961 | tagging_loss_video: 6.334|tagging_loss_audio: 8.520|tagging_loss_text: 17.350|tagging_loss_image: 3.766|tagging_loss_fusion: 5.379|total_loss: 41.348 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 5962 | tagging_loss_video: 5.914|tagging_loss_audio: 9.842|tagging_loss_text: 16.568|tagging_loss_image: 5.378|tagging_loss_fusion: 4.337|total_loss: 42.039 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 5963 | tagging_loss_video: 6.848|tagging_loss_audio: 9.668|tagging_loss_text: 17.480|tagging_loss_image: 5.297|tagging_loss_fusion: 4.933|total_loss: 44.225 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 5964 | tagging_loss_video: 6.413|tagging_loss_audio: 9.125|tagging_loss_text: 14.649|tagging_loss_image: 4.892|tagging_loss_fusion: 5.837|total_loss: 40.916 | 64.68 Examples/sec\n",
      "INFO:tensorflow:training step 5965 | tagging_loss_video: 6.287|tagging_loss_audio: 9.510|tagging_loss_text: 13.732|tagging_loss_image: 6.233|tagging_loss_fusion: 4.525|total_loss: 40.287 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 5966 | tagging_loss_video: 4.903|tagging_loss_audio: 9.032|tagging_loss_text: 15.909|tagging_loss_image: 4.526|tagging_loss_fusion: 2.901|total_loss: 37.272 | 70.35 Examples/sec\n",
      "INFO:tensorflow:training step 5967 | tagging_loss_video: 5.777|tagging_loss_audio: 8.431|tagging_loss_text: 14.644|tagging_loss_image: 4.706|tagging_loss_fusion: 5.088|total_loss: 38.646 | 63.58 Examples/sec\n",
      "INFO:tensorflow:training step 5968 | tagging_loss_video: 4.376|tagging_loss_audio: 9.201|tagging_loss_text: 14.755|tagging_loss_image: 5.504|tagging_loss_fusion: 3.053|total_loss: 36.889 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 5969 | tagging_loss_video: 6.208|tagging_loss_audio: 8.539|tagging_loss_text: 15.072|tagging_loss_image: 6.039|tagging_loss_fusion: 5.368|total_loss: 41.226 | 66.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5970 |tagging_loss_video: 6.362|tagging_loss_audio: 8.410|tagging_loss_text: 16.626|tagging_loss_image: 4.981|tagging_loss_fusion: 6.019|total_loss: 42.399 | Examples/sec: 70.58\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 5971 | tagging_loss_video: 7.212|tagging_loss_audio: 10.665|tagging_loss_text: 13.348|tagging_loss_image: 6.766|tagging_loss_fusion: 6.328|total_loss: 44.319 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 5972 | tagging_loss_video: 5.959|tagging_loss_audio: 9.048|tagging_loss_text: 19.225|tagging_loss_image: 6.770|tagging_loss_fusion: 4.830|total_loss: 45.832 | 64.56 Examples/sec\n",
      "INFO:tensorflow:training step 5973 | tagging_loss_video: 5.852|tagging_loss_audio: 9.191|tagging_loss_text: 16.365|tagging_loss_image: 5.789|tagging_loss_fusion: 4.882|total_loss: 42.080 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 5974 | tagging_loss_video: 5.991|tagging_loss_audio: 9.112|tagging_loss_text: 14.303|tagging_loss_image: 3.257|tagging_loss_fusion: 4.450|total_loss: 37.113 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 5975 | tagging_loss_video: 5.322|tagging_loss_audio: 7.919|tagging_loss_text: 16.056|tagging_loss_image: 4.931|tagging_loss_fusion: 3.800|total_loss: 38.028 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 5976 | tagging_loss_video: 5.234|tagging_loss_audio: 9.809|tagging_loss_text: 16.126|tagging_loss_image: 5.206|tagging_loss_fusion: 3.261|total_loss: 39.637 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 5977 | tagging_loss_video: 6.018|tagging_loss_audio: 9.132|tagging_loss_text: 17.561|tagging_loss_image: 4.397|tagging_loss_fusion: 6.245|total_loss: 43.354 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 5978 | tagging_loss_video: 5.748|tagging_loss_audio: 8.149|tagging_loss_text: 12.536|tagging_loss_image: 4.651|tagging_loss_fusion: 3.997|total_loss: 35.081 | 59.10 Examples/sec\n",
      "INFO:tensorflow:training step 5979 | tagging_loss_video: 6.139|tagging_loss_audio: 7.803|tagging_loss_text: 17.259|tagging_loss_image: 5.359|tagging_loss_fusion: 5.675|total_loss: 42.235 | 68.74 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5980 |tagging_loss_video: 5.808|tagging_loss_audio: 9.094|tagging_loss_text: 13.477|tagging_loss_image: 3.693|tagging_loss_fusion: 4.292|total_loss: 36.365 | Examples/sec: 70.19\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 5981 | tagging_loss_video: 3.739|tagging_loss_audio: 8.335|tagging_loss_text: 17.149|tagging_loss_image: 5.483|tagging_loss_fusion: 2.730|total_loss: 37.436 | 64.06 Examples/sec\n",
      "INFO:tensorflow:training step 5982 | tagging_loss_video: 4.666|tagging_loss_audio: 8.134|tagging_loss_text: 15.832|tagging_loss_image: 5.575|tagging_loss_fusion: 5.003|total_loss: 39.211 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 5983 | tagging_loss_video: 4.564|tagging_loss_audio: 7.749|tagging_loss_text: 18.142|tagging_loss_image: 4.030|tagging_loss_fusion: 2.951|total_loss: 37.436 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 5984 | tagging_loss_video: 5.729|tagging_loss_audio: 9.410|tagging_loss_text: 14.712|tagging_loss_image: 5.626|tagging_loss_fusion: 4.584|total_loss: 40.061 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 5985 | tagging_loss_video: 5.292|tagging_loss_audio: 9.059|tagging_loss_text: 15.855|tagging_loss_image: 5.772|tagging_loss_fusion: 4.049|total_loss: 40.027 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 5986 | tagging_loss_video: 5.807|tagging_loss_audio: 8.250|tagging_loss_text: 15.529|tagging_loss_image: 5.343|tagging_loss_fusion: 6.834|total_loss: 41.762 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 5987 | tagging_loss_video: 4.885|tagging_loss_audio: 7.618|tagging_loss_text: 17.342|tagging_loss_image: 4.227|tagging_loss_fusion: 2.538|total_loss: 36.610 | 67.35 Examples/sec\n",
      "INFO:tensorflow:training step 5988 | tagging_loss_video: 5.975|tagging_loss_audio: 7.832|tagging_loss_text: 13.945|tagging_loss_image: 5.826|tagging_loss_fusion: 6.031|total_loss: 39.609 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 5989 | tagging_loss_video: 5.764|tagging_loss_audio: 9.104|tagging_loss_text: 14.885|tagging_loss_image: 5.025|tagging_loss_fusion: 4.361|total_loss: 39.139 | 62.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 5990 |tagging_loss_video: 5.981|tagging_loss_audio: 9.319|tagging_loss_text: 15.325|tagging_loss_image: 5.558|tagging_loss_fusion: 7.206|total_loss: 43.389 | Examples/sec: 71.56\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.94 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 5991 | tagging_loss_video: 5.867|tagging_loss_audio: 8.846|tagging_loss_text: 20.514|tagging_loss_image: 6.473|tagging_loss_fusion: 5.378|total_loss: 47.079 | 67.77 Examples/sec\n",
      "INFO:tensorflow:training step 5992 | tagging_loss_video: 5.371|tagging_loss_audio: 9.011|tagging_loss_text: 15.462|tagging_loss_image: 4.652|tagging_loss_fusion: 4.173|total_loss: 38.670 | 66.37 Examples/sec\n",
      "INFO:tensorflow:training step 5993 | tagging_loss_video: 5.445|tagging_loss_audio: 9.207|tagging_loss_text: 15.075|tagging_loss_image: 6.052|tagging_loss_fusion: 4.010|total_loss: 39.789 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 5994 | tagging_loss_video: 6.501|tagging_loss_audio: 9.521|tagging_loss_text: 16.391|tagging_loss_image: 5.657|tagging_loss_fusion: 6.440|total_loss: 44.510 | 69.29 Examples/sec\n",
      "INFO:tensorflow:training step 5995 | tagging_loss_video: 5.911|tagging_loss_audio: 8.301|tagging_loss_text: 15.349|tagging_loss_image: 4.392|tagging_loss_fusion: 4.233|total_loss: 38.186 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 5996 | tagging_loss_video: 6.312|tagging_loss_audio: 9.292|tagging_loss_text: 15.059|tagging_loss_image: 5.167|tagging_loss_fusion: 6.103|total_loss: 41.934 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 5997 | tagging_loss_video: 4.962|tagging_loss_audio: 7.976|tagging_loss_text: 17.160|tagging_loss_image: 5.552|tagging_loss_fusion: 4.558|total_loss: 40.209 | 59.77 Examples/sec\n",
      "INFO:tensorflow:training step 5998 | tagging_loss_video: 4.657|tagging_loss_audio: 8.322|tagging_loss_text: 17.832|tagging_loss_image: 3.987|tagging_loss_fusion: 3.378|total_loss: 38.176 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 5999 | tagging_loss_video: 6.193|tagging_loss_audio: 8.223|tagging_loss_text: 17.454|tagging_loss_image: 5.411|tagging_loss_fusion: 5.634|total_loss: 42.915 | 72.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6000 |tagging_loss_video: 5.390|tagging_loss_audio: 7.089|tagging_loss_text: 13.500|tagging_loss_image: 5.345|tagging_loss_fusion: 5.362|total_loss: 36.685 | Examples/sec: 58.93\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.79 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.722|loss: 34.168|GAP: 0.733|examples_per_second: 93.608\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.736|loss: 30.688|GAP: 0.786|examples_per_second: 98.135\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 1.000|perr: 0.723|loss: 41.003|GAP: 0.716|examples_per_second: 89.079\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.775|loss: 34.455|GAP: 0.754|examples_per_second: 99.188\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.748|loss: 36.165|GAP: 0.749|examples_per_second: 91.412\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.695|loss: 34.576|GAP: 0.723|examples_per_second: 99.067\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.741|loss: 36.877|GAP: 0.735|examples_per_second: 93.155\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.762|loss: 33.028|GAP: 0.777|examples_per_second: 93.854\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 1.000|perr: 0.718|loss: 36.064|GAP: 0.736|examples_per_second: 99.888\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.699|loss: 36.989|GAP: 0.702|examples_per_second: 99.132\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.752|loss: 27.793|GAP: 0.790|examples_per_second: 88.703\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 0.969|perr: 0.770|loss: 31.732|GAP: 0.766|examples_per_second: 94.177\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.744|loss: 34.954|GAP: 0.738|examples_per_second: 91.953\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.754|loss: 34.533|GAP: 0.751|examples_per_second: 100.883\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.732|loss: 33.873|GAP: 0.747|examples_per_second: 100.941\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 6000 | MAP: 0.333 | GAP: 0.740 | p@0.1: 0.709 | p@0.5:0.805 | r@0.1:0.746 | r@0.5: 0.645 | Avg_Loss: 29.430782\n",
      "INFO:tensorflow:epoch/eval number 6000 | MAP: 0.271 | GAP: 0.666 | p@0.1: 0.549 | p@0.5:0.730 | r@0.1:0.801 | r@0.5: 0.611 | Avg_Loss: 24.312618\n",
      "INFO:tensorflow:epoch/eval number 6000 | MAP: 0.111 | GAP: 0.575 | p@0.1: 0.335 | p@0.5:0.784 | r@0.1:0.878 | r@0.5: 0.406 | Avg_Loss: 22.032285\n",
      "INFO:tensorflow:epoch/eval number 6000 | MAP: 0.271 | GAP: 0.649 | p@0.1: 0.629 | p@0.5:0.718 | r@0.1:0.717 | r@0.5: 0.643 | Avg_Loss: 43.301048\n",
      "INFO:tensorflow:epoch/eval number 6000 | MAP: 0.346 | GAP: 0.747 | p@0.1: 0.736 | p@0.5:0.806 | r@0.1:0.738 | r@0.5: 0.659 | Avg_Loss: 34.459962\n",
      "INFO:tensorflow:validation score on val799 is : 0.7465\n",
      "INFO:tensorflow:Recording summary at step 6000.\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-6000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_6000_0.7465/saved_model.pb\n",
      "INFO:tensorflow:training step 6001 | tagging_loss_video: 6.285|tagging_loss_audio: 7.767|tagging_loss_text: 18.154|tagging_loss_image: 5.413|tagging_loss_fusion: 5.440|total_loss: 43.059 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 6002 | tagging_loss_video: 5.748|tagging_loss_audio: 10.148|tagging_loss_text: 18.453|tagging_loss_image: 6.060|tagging_loss_fusion: 5.436|total_loss: 45.845 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 6003 | tagging_loss_video: 4.596|tagging_loss_audio: 8.634|tagging_loss_text: 13.943|tagging_loss_image: 5.324|tagging_loss_fusion: 3.571|total_loss: 36.068 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 6004 | tagging_loss_video: 6.604|tagging_loss_audio: 8.198|tagging_loss_text: 16.175|tagging_loss_image: 2.885|tagging_loss_fusion: 5.169|total_loss: 39.030 | 70.16 Examples/sec\n",
      "INFO:tensorflow:training step 6005 | tagging_loss_video: 6.039|tagging_loss_audio: 9.156|tagging_loss_text: 14.097|tagging_loss_image: 3.720|tagging_loss_fusion: 3.673|total_loss: 36.686 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 6006 | tagging_loss_video: 5.441|tagging_loss_audio: 8.539|tagging_loss_text: 14.824|tagging_loss_image: 5.327|tagging_loss_fusion: 3.567|total_loss: 37.698 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 6007 | tagging_loss_video: 5.283|tagging_loss_audio: 9.027|tagging_loss_text: 17.645|tagging_loss_image: 4.646|tagging_loss_fusion: 4.535|total_loss: 41.136 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 6008 | tagging_loss_video: 4.337|tagging_loss_audio: 8.279|tagging_loss_text: 11.850|tagging_loss_image: 5.446|tagging_loss_fusion: 2.618|total_loss: 32.530 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 6009 | tagging_loss_video: 5.069|tagging_loss_audio: 9.173|tagging_loss_text: 15.217|tagging_loss_image: 5.475|tagging_loss_fusion: 3.200|total_loss: 38.134 | 70.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6010 |tagging_loss_video: 5.709|tagging_loss_audio: 8.638|tagging_loss_text: 13.205|tagging_loss_image: 5.293|tagging_loss_fusion: 6.056|total_loss: 38.902 | Examples/sec: 68.80\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.80 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 6011 | tagging_loss_video: 4.217|tagging_loss_audio: 9.092|tagging_loss_text: 16.080|tagging_loss_image: 4.669|tagging_loss_fusion: 3.078|total_loss: 37.136 | 72.80 Examples/sec\n",
      "INFO:tensorflow:training step 6012 | tagging_loss_video: 7.075|tagging_loss_audio: 9.819|tagging_loss_text: 15.365|tagging_loss_image: 4.083|tagging_loss_fusion: 5.077|total_loss: 41.420 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 6013 | tagging_loss_video: 5.005|tagging_loss_audio: 7.841|tagging_loss_text: 15.372|tagging_loss_image: 4.444|tagging_loss_fusion: 4.125|total_loss: 36.788 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 6014 | tagging_loss_video: 5.293|tagging_loss_audio: 8.216|tagging_loss_text: 19.218|tagging_loss_image: 4.161|tagging_loss_fusion: 3.739|total_loss: 40.627 | 68.94 Examples/sec\n",
      "INFO:tensorflow:training step 6015 | tagging_loss_video: 5.998|tagging_loss_audio: 8.729|tagging_loss_text: 19.458|tagging_loss_image: 5.320|tagging_loss_fusion: 5.290|total_loss: 44.796 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 6016 | tagging_loss_video: 4.896|tagging_loss_audio: 8.835|tagging_loss_text: 16.078|tagging_loss_image: 4.684|tagging_loss_fusion: 2.771|total_loss: 37.263 | 71.62 Examples/sec\n",
      "INFO:tensorflow:training step 6017 | tagging_loss_video: 5.859|tagging_loss_audio: 8.006|tagging_loss_text: 15.614|tagging_loss_image: 5.051|tagging_loss_fusion: 5.542|total_loss: 40.072 | 71.64 Examples/sec\n",
      "INFO:tensorflow:training step 6018 | tagging_loss_video: 5.511|tagging_loss_audio: 8.160|tagging_loss_text: 17.714|tagging_loss_image: 6.085|tagging_loss_fusion: 4.681|total_loss: 42.151 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 6019 | tagging_loss_video: 5.607|tagging_loss_audio: 8.149|tagging_loss_text: 14.107|tagging_loss_image: 5.841|tagging_loss_fusion: 3.953|total_loss: 37.655 | 71.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6020 |tagging_loss_video: 6.010|tagging_loss_audio: 9.426|tagging_loss_text: 13.777|tagging_loss_image: 3.851|tagging_loss_fusion: 4.111|total_loss: 37.175 | Examples/sec: 70.78\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6021 | tagging_loss_video: 6.011|tagging_loss_audio: 9.087|tagging_loss_text: 15.456|tagging_loss_image: 5.264|tagging_loss_fusion: 4.715|total_loss: 40.533 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 6022 | tagging_loss_video: 5.498|tagging_loss_audio: 9.550|tagging_loss_text: 15.200|tagging_loss_image: 4.276|tagging_loss_fusion: 3.505|total_loss: 38.029 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 6023 | tagging_loss_video: 5.958|tagging_loss_audio: 7.848|tagging_loss_text: 15.716|tagging_loss_image: 4.467|tagging_loss_fusion: 3.975|total_loss: 37.964 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 6024 | tagging_loss_video: 5.001|tagging_loss_audio: 7.306|tagging_loss_text: 15.007|tagging_loss_image: 4.741|tagging_loss_fusion: 5.016|total_loss: 37.071 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 6025 | tagging_loss_video: 5.328|tagging_loss_audio: 7.312|tagging_loss_text: 16.119|tagging_loss_image: 4.946|tagging_loss_fusion: 3.591|total_loss: 37.296 | 71.95 Examples/sec\n",
      "INFO:tensorflow:training step 6026 | tagging_loss_video: 4.555|tagging_loss_audio: 7.639|tagging_loss_text: 14.572|tagging_loss_image: 4.205|tagging_loss_fusion: 3.412|total_loss: 34.383 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 6027 | tagging_loss_video: 5.580|tagging_loss_audio: 9.010|tagging_loss_text: 14.717|tagging_loss_image: 5.082|tagging_loss_fusion: 3.791|total_loss: 38.180 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 6028 | tagging_loss_video: 5.663|tagging_loss_audio: 7.991|tagging_loss_text: 14.485|tagging_loss_image: 4.808|tagging_loss_fusion: 4.343|total_loss: 37.290 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 6029 | tagging_loss_video: 4.549|tagging_loss_audio: 8.067|tagging_loss_text: 13.285|tagging_loss_image: 4.244|tagging_loss_fusion: 2.945|total_loss: 33.091 | 71.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6030 |tagging_loss_video: 5.268|tagging_loss_audio: 8.353|tagging_loss_text: 14.010|tagging_loss_image: 5.082|tagging_loss_fusion: 3.722|total_loss: 36.435 | Examples/sec: 68.15\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6031 | tagging_loss_video: 5.465|tagging_loss_audio: 9.105|tagging_loss_text: 15.474|tagging_loss_image: 2.890|tagging_loss_fusion: 3.200|total_loss: 36.133 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 6032 | tagging_loss_video: 5.573|tagging_loss_audio: 7.376|tagging_loss_text: 15.741|tagging_loss_image: 4.577|tagging_loss_fusion: 3.397|total_loss: 36.664 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 6033 | tagging_loss_video: 5.775|tagging_loss_audio: 7.748|tagging_loss_text: 12.399|tagging_loss_image: 3.960|tagging_loss_fusion: 4.678|total_loss: 34.560 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 6034 | tagging_loss_video: 4.708|tagging_loss_audio: 7.691|tagging_loss_text: 11.771|tagging_loss_image: 4.962|tagging_loss_fusion: 3.133|total_loss: 32.266 | 71.43 Examples/sec\n",
      "INFO:tensorflow:training step 6035 | tagging_loss_video: 6.463|tagging_loss_audio: 7.962|tagging_loss_text: 15.894|tagging_loss_image: 3.041|tagging_loss_fusion: 3.415|total_loss: 36.774 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 6036 | tagging_loss_video: 4.653|tagging_loss_audio: 8.369|tagging_loss_text: 16.008|tagging_loss_image: 6.072|tagging_loss_fusion: 3.512|total_loss: 38.614 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 6037 | tagging_loss_video: 5.642|tagging_loss_audio: 7.829|tagging_loss_text: 13.487|tagging_loss_image: 5.215|tagging_loss_fusion: 5.825|total_loss: 37.998 | 67.85 Examples/sec\n",
      "INFO:tensorflow:training step 6038 | tagging_loss_video: 5.365|tagging_loss_audio: 8.452|tagging_loss_text: 16.764|tagging_loss_image: 6.143|tagging_loss_fusion: 3.920|total_loss: 40.644 | 70.56 Examples/sec\n",
      "INFO:tensorflow:training step 6039 | tagging_loss_video: 4.554|tagging_loss_audio: 7.754|tagging_loss_text: 15.879|tagging_loss_image: 4.709|tagging_loss_fusion: 3.007|total_loss: 35.904 | 68.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6040 |tagging_loss_video: 5.807|tagging_loss_audio: 8.161|tagging_loss_text: 12.374|tagging_loss_image: 4.222|tagging_loss_fusion: 4.592|total_loss: 35.157 | Examples/sec: 70.60\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.82 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6041 | tagging_loss_video: 5.019|tagging_loss_audio: 8.798|tagging_loss_text: 16.960|tagging_loss_image: 5.849|tagging_loss_fusion: 3.774|total_loss: 40.401 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 6042 | tagging_loss_video: 5.343|tagging_loss_audio: 7.912|tagging_loss_text: 12.555|tagging_loss_image: 5.168|tagging_loss_fusion: 3.978|total_loss: 34.956 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 6043 | tagging_loss_video: 3.942|tagging_loss_audio: 8.310|tagging_loss_text: 17.914|tagging_loss_image: 5.409|tagging_loss_fusion: 3.179|total_loss: 38.752 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 6044 | tagging_loss_video: 5.529|tagging_loss_audio: 7.814|tagging_loss_text: 14.730|tagging_loss_image: 2.925|tagging_loss_fusion: 2.876|total_loss: 33.874 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 6045 | tagging_loss_video: 5.170|tagging_loss_audio: 8.980|tagging_loss_text: 15.276|tagging_loss_image: 3.702|tagging_loss_fusion: 2.727|total_loss: 35.855 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 6046 | tagging_loss_video: 6.301|tagging_loss_audio: 8.617|tagging_loss_text: 12.642|tagging_loss_image: 5.063|tagging_loss_fusion: 5.542|total_loss: 38.166 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 6047 | tagging_loss_video: 6.309|tagging_loss_audio: 9.269|tagging_loss_text: 16.216|tagging_loss_image: 5.292|tagging_loss_fusion: 5.722|total_loss: 42.808 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 6048 | tagging_loss_video: 6.434|tagging_loss_audio: 9.139|tagging_loss_text: 15.691|tagging_loss_image: 5.601|tagging_loss_fusion: 4.718|total_loss: 41.582 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 6049 | tagging_loss_video: 4.803|tagging_loss_audio: 7.928|tagging_loss_text: 15.219|tagging_loss_image: 4.780|tagging_loss_fusion: 3.650|total_loss: 36.381 | 70.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6050 |tagging_loss_video: 4.388|tagging_loss_audio: 7.523|tagging_loss_text: 10.679|tagging_loss_image: 4.027|tagging_loss_fusion: 2.624|total_loss: 29.241 | Examples/sec: 69.51\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6051 | tagging_loss_video: 5.204|tagging_loss_audio: 8.571|tagging_loss_text: 13.999|tagging_loss_image: 4.652|tagging_loss_fusion: 3.656|total_loss: 36.083 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 6052 | tagging_loss_video: 4.927|tagging_loss_audio: 7.966|tagging_loss_text: 14.823|tagging_loss_image: 5.863|tagging_loss_fusion: 4.598|total_loss: 38.178 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 6053 | tagging_loss_video: 5.106|tagging_loss_audio: 8.421|tagging_loss_text: 17.251|tagging_loss_image: 4.950|tagging_loss_fusion: 3.667|total_loss: 39.394 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 6054 | tagging_loss_video: 5.730|tagging_loss_audio: 8.403|tagging_loss_text: 16.456|tagging_loss_image: 4.968|tagging_loss_fusion: 6.616|total_loss: 42.173 | 68.35 Examples/sec\n",
      "INFO:tensorflow:training step 6055 | tagging_loss_video: 4.515|tagging_loss_audio: 7.749|tagging_loss_text: 14.752|tagging_loss_image: 5.581|tagging_loss_fusion: 3.306|total_loss: 35.902 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 6056 | tagging_loss_video: 4.995|tagging_loss_audio: 8.306|tagging_loss_text: 15.948|tagging_loss_image: 5.325|tagging_loss_fusion: 3.517|total_loss: 38.092 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 6057 | tagging_loss_video: 5.560|tagging_loss_audio: 9.036|tagging_loss_text: 16.290|tagging_loss_image: 3.582|tagging_loss_fusion: 3.935|total_loss: 38.404 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 6058 | tagging_loss_video: 6.200|tagging_loss_audio: 8.552|tagging_loss_text: 15.383|tagging_loss_image: 6.001|tagging_loss_fusion: 5.547|total_loss: 41.683 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 6059 | tagging_loss_video: 5.375|tagging_loss_audio: 9.036|tagging_loss_text: 20.013|tagging_loss_image: 5.754|tagging_loss_fusion: 3.644|total_loss: 43.823 | 69.90 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6060 |tagging_loss_video: 6.002|tagging_loss_audio: 7.901|tagging_loss_text: 17.807|tagging_loss_image: 4.925|tagging_loss_fusion: 5.929|total_loss: 42.564 | Examples/sec: 71.40\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.95 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 6061 | tagging_loss_video: 5.031|tagging_loss_audio: 8.549|tagging_loss_text: 14.626|tagging_loss_image: 4.911|tagging_loss_fusion: 3.855|total_loss: 36.972 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 6062 | tagging_loss_video: 4.593|tagging_loss_audio: 8.072|tagging_loss_text: 14.330|tagging_loss_image: 4.603|tagging_loss_fusion: 3.988|total_loss: 35.587 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 6063 | tagging_loss_video: 5.089|tagging_loss_audio: 7.575|tagging_loss_text: 15.577|tagging_loss_image: 4.357|tagging_loss_fusion: 2.568|total_loss: 35.165 | 70.96 Examples/sec\n",
      "INFO:tensorflow:training step 6064 | tagging_loss_video: 5.967|tagging_loss_audio: 8.530|tagging_loss_text: 17.844|tagging_loss_image: 5.229|tagging_loss_fusion: 4.341|total_loss: 41.911 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 6065 | tagging_loss_video: 5.701|tagging_loss_audio: 8.001|tagging_loss_text: 16.222|tagging_loss_image: 5.167|tagging_loss_fusion: 3.766|total_loss: 38.857 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 6066 | tagging_loss_video: 5.893|tagging_loss_audio: 9.140|tagging_loss_text: 16.019|tagging_loss_image: 5.061|tagging_loss_fusion: 4.358|total_loss: 40.472 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 6067 | tagging_loss_video: 5.998|tagging_loss_audio: 7.554|tagging_loss_text: 14.163|tagging_loss_image: 4.927|tagging_loss_fusion: 4.791|total_loss: 37.433 | 68.93 Examples/sec\n",
      "INFO:tensorflow:training step 6068 | tagging_loss_video: 4.476|tagging_loss_audio: 9.496|tagging_loss_text: 19.166|tagging_loss_image: 6.159|tagging_loss_fusion: 4.200|total_loss: 43.496 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 6069 | tagging_loss_video: 6.265|tagging_loss_audio: 9.234|tagging_loss_text: 13.594|tagging_loss_image: 6.328|tagging_loss_fusion: 6.342|total_loss: 41.762 | 71.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6070 |tagging_loss_video: 6.175|tagging_loss_audio: 8.614|tagging_loss_text: 14.617|tagging_loss_image: 5.233|tagging_loss_fusion: 5.037|total_loss: 39.676 | Examples/sec: 71.03\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6071 | tagging_loss_video: 6.012|tagging_loss_audio: 7.801|tagging_loss_text: 13.602|tagging_loss_image: 4.759|tagging_loss_fusion: 3.544|total_loss: 35.719 | 72.35 Examples/sec\n",
      "INFO:tensorflow:training step 6072 | tagging_loss_video: 5.926|tagging_loss_audio: 9.088|tagging_loss_text: 18.033|tagging_loss_image: 4.777|tagging_loss_fusion: 4.883|total_loss: 42.707 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 6073 | tagging_loss_video: 4.785|tagging_loss_audio: 7.938|tagging_loss_text: 12.537|tagging_loss_image: 4.663|tagging_loss_fusion: 3.975|total_loss: 33.898 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 6074 | tagging_loss_video: 5.108|tagging_loss_audio: 8.979|tagging_loss_text: 16.488|tagging_loss_image: 5.786|tagging_loss_fusion: 4.224|total_loss: 40.585 | 67.38 Examples/sec\n",
      "INFO:tensorflow:training step 6075 | tagging_loss_video: 5.134|tagging_loss_audio: 9.355|tagging_loss_text: 17.104|tagging_loss_image: 5.615|tagging_loss_fusion: 4.136|total_loss: 41.344 | 66.65 Examples/sec\n",
      "INFO:tensorflow:training step 6076 | tagging_loss_video: 4.422|tagging_loss_audio: 7.935|tagging_loss_text: 13.018|tagging_loss_image: 5.073|tagging_loss_fusion: 3.750|total_loss: 34.198 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 6077 | tagging_loss_video: 6.300|tagging_loss_audio: 8.450|tagging_loss_text: 13.856|tagging_loss_image: 5.056|tagging_loss_fusion: 4.975|total_loss: 38.636 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 6078 | tagging_loss_video: 4.304|tagging_loss_audio: 8.330|tagging_loss_text: 14.553|tagging_loss_image: 5.498|tagging_loss_fusion: 2.986|total_loss: 35.671 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 6079 | tagging_loss_video: 5.503|tagging_loss_audio: 9.394|tagging_loss_text: 15.222|tagging_loss_image: 5.235|tagging_loss_fusion: 4.626|total_loss: 39.980 | 69.11 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6080 |tagging_loss_video: 5.012|tagging_loss_audio: 8.714|tagging_loss_text: 16.198|tagging_loss_image: 5.480|tagging_loss_fusion: 3.815|total_loss: 39.218 | Examples/sec: 72.08\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6081 | tagging_loss_video: 5.991|tagging_loss_audio: 9.627|tagging_loss_text: 17.886|tagging_loss_image: 5.970|tagging_loss_fusion: 4.560|total_loss: 44.034 | 66.77 Examples/sec\n",
      "INFO:tensorflow:training step 6082 | tagging_loss_video: 4.986|tagging_loss_audio: 8.800|tagging_loss_text: 14.443|tagging_loss_image: 3.377|tagging_loss_fusion: 2.791|total_loss: 34.398 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 6083 | tagging_loss_video: 6.708|tagging_loss_audio: 8.854|tagging_loss_text: 15.498|tagging_loss_image: 4.322|tagging_loss_fusion: 4.793|total_loss: 40.174 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 6084 | tagging_loss_video: 4.364|tagging_loss_audio: 8.291|tagging_loss_text: 9.285|tagging_loss_image: 4.507|tagging_loss_fusion: 2.998|total_loss: 29.445 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 6085 | tagging_loss_video: 6.374|tagging_loss_audio: 10.111|tagging_loss_text: 18.799|tagging_loss_image: 6.543|tagging_loss_fusion: 7.972|total_loss: 49.800 | 70.68 Examples/sec\n",
      "INFO:tensorflow:training step 6086 | tagging_loss_video: 5.817|tagging_loss_audio: 9.281|tagging_loss_text: 18.015|tagging_loss_image: 4.845|tagging_loss_fusion: 3.881|total_loss: 41.838 | 71.15 Examples/sec\n",
      "INFO:tensorflow:training step 6087 | tagging_loss_video: 4.683|tagging_loss_audio: 8.487|tagging_loss_text: 14.546|tagging_loss_image: 4.385|tagging_loss_fusion: 2.354|total_loss: 34.454 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 6088 | tagging_loss_video: 5.655|tagging_loss_audio: 9.208|tagging_loss_text: 18.119|tagging_loss_image: 6.478|tagging_loss_fusion: 6.171|total_loss: 45.631 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 6089 | tagging_loss_video: 6.286|tagging_loss_audio: 9.586|tagging_loss_text: 16.016|tagging_loss_image: 4.744|tagging_loss_fusion: 4.262|total_loss: 40.894 | 67.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6090 |tagging_loss_video: 5.944|tagging_loss_audio: 8.322|tagging_loss_text: 15.893|tagging_loss_image: 5.286|tagging_loss_fusion: 6.277|total_loss: 41.723 | Examples/sec: 69.24\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.80 | precision@0.5: 0.96 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 6091 | tagging_loss_video: 5.332|tagging_loss_audio: 8.554|tagging_loss_text: 10.654|tagging_loss_image: 4.863|tagging_loss_fusion: 3.052|total_loss: 32.455 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 6092 | tagging_loss_video: 5.089|tagging_loss_audio: 9.289|tagging_loss_text: 13.402|tagging_loss_image: 5.723|tagging_loss_fusion: 3.491|total_loss: 36.994 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 6093 | tagging_loss_video: 4.690|tagging_loss_audio: 9.032|tagging_loss_text: 15.756|tagging_loss_image: 4.588|tagging_loss_fusion: 2.601|total_loss: 36.668 | 70.99 Examples/sec\n",
      "INFO:tensorflow:training step 6094 | tagging_loss_video: 5.619|tagging_loss_audio: 8.183|tagging_loss_text: 15.306|tagging_loss_image: 4.366|tagging_loss_fusion: 3.918|total_loss: 37.391 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 6095 | tagging_loss_video: 5.795|tagging_loss_audio: 9.168|tagging_loss_text: 17.046|tagging_loss_image: 7.173|tagging_loss_fusion: 6.159|total_loss: 45.342 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 6096 | tagging_loss_video: 6.315|tagging_loss_audio: 9.773|tagging_loss_text: 17.756|tagging_loss_image: 6.086|tagging_loss_fusion: 5.094|total_loss: 45.024 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 6097 | tagging_loss_video: 5.599|tagging_loss_audio: 8.623|tagging_loss_text: 13.763|tagging_loss_image: 5.211|tagging_loss_fusion: 5.544|total_loss: 38.740 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 6098 | tagging_loss_video: 4.501|tagging_loss_audio: 8.529|tagging_loss_text: 17.040|tagging_loss_image: 5.600|tagging_loss_fusion: 2.791|total_loss: 38.461 | 71.39 Examples/sec\n",
      "INFO:tensorflow:training step 6099 | tagging_loss_video: 5.771|tagging_loss_audio: 8.316|tagging_loss_text: 12.848|tagging_loss_image: 5.378|tagging_loss_fusion: 7.172|total_loss: 39.485 | 68.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6100 |tagging_loss_video: 6.721|tagging_loss_audio: 9.463|tagging_loss_text: 17.601|tagging_loss_image: 5.377|tagging_loss_fusion: 4.648|total_loss: 43.811 | Examples/sec: 70.66\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6101 | tagging_loss_video: 4.388|tagging_loss_audio: 8.479|tagging_loss_text: 16.213|tagging_loss_image: 5.573|tagging_loss_fusion: 3.886|total_loss: 38.538 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 6102 | tagging_loss_video: 6.192|tagging_loss_audio: 8.425|tagging_loss_text: 12.531|tagging_loss_image: 4.267|tagging_loss_fusion: 4.111|total_loss: 35.526 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 6103 | tagging_loss_video: 5.469|tagging_loss_audio: 8.119|tagging_loss_text: 13.923|tagging_loss_image: 3.967|tagging_loss_fusion: 3.434|total_loss: 34.912 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 6104 | tagging_loss_video: 5.429|tagging_loss_audio: 8.416|tagging_loss_text: 12.818|tagging_loss_image: 4.758|tagging_loss_fusion: 3.673|total_loss: 35.094 | 69.33 Examples/sec\n",
      "INFO:tensorflow:training step 6105 | tagging_loss_video: 5.845|tagging_loss_audio: 8.041|tagging_loss_text: 13.197|tagging_loss_image: 3.984|tagging_loss_fusion: 3.266|total_loss: 34.334 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 6106 | tagging_loss_video: 5.123|tagging_loss_audio: 8.188|tagging_loss_text: 15.835|tagging_loss_image: 5.146|tagging_loss_fusion: 3.234|total_loss: 37.526 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 6107 | tagging_loss_video: 6.088|tagging_loss_audio: 8.713|tagging_loss_text: 13.958|tagging_loss_image: 5.348|tagging_loss_fusion: 6.153|total_loss: 40.260 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 6108 | tagging_loss_video: 5.257|tagging_loss_audio: 8.540|tagging_loss_text: 12.706|tagging_loss_image: 4.659|tagging_loss_fusion: 4.115|total_loss: 35.276 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 6109 | tagging_loss_video: 5.028|tagging_loss_audio: 9.510|tagging_loss_text: 14.541|tagging_loss_image: 5.369|tagging_loss_fusion: 3.937|total_loss: 38.387 | 69.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6110 |tagging_loss_video: 5.517|tagging_loss_audio: 8.408|tagging_loss_text: 12.955|tagging_loss_image: 5.284|tagging_loss_fusion: 5.016|total_loss: 37.179 | Examples/sec: 66.17\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6111 | tagging_loss_video: 5.097|tagging_loss_audio: 7.507|tagging_loss_text: 16.025|tagging_loss_image: 5.350|tagging_loss_fusion: 3.897|total_loss: 37.875 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 6112 | tagging_loss_video: 5.540|tagging_loss_audio: 8.232|tagging_loss_text: 13.267|tagging_loss_image: 5.231|tagging_loss_fusion: 4.266|total_loss: 36.536 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 6113 | tagging_loss_video: 4.523|tagging_loss_audio: 8.820|tagging_loss_text: 17.561|tagging_loss_image: 4.376|tagging_loss_fusion: 2.665|total_loss: 37.946 | 62.68 Examples/sec\n",
      "INFO:tensorflow:training step 6114 | tagging_loss_video: 6.190|tagging_loss_audio: 9.795|tagging_loss_text: 14.631|tagging_loss_image: 4.622|tagging_loss_fusion: 4.956|total_loss: 40.194 | 67.04 Examples/sec\n",
      "INFO:tensorflow:training step 6115 | tagging_loss_video: 6.012|tagging_loss_audio: 8.621|tagging_loss_text: 16.717|tagging_loss_image: 5.719|tagging_loss_fusion: 4.800|total_loss: 41.869 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 6116 | tagging_loss_video: 4.851|tagging_loss_audio: 9.179|tagging_loss_text: 14.787|tagging_loss_image: 5.823|tagging_loss_fusion: 3.366|total_loss: 38.006 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 6117 | tagging_loss_video: 5.847|tagging_loss_audio: 8.873|tagging_loss_text: 11.657|tagging_loss_image: 4.049|tagging_loss_fusion: 3.768|total_loss: 34.194 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 6118 | tagging_loss_video: 5.654|tagging_loss_audio: 8.811|tagging_loss_text: 16.369|tagging_loss_image: 6.016|tagging_loss_fusion: 4.204|total_loss: 41.055 | 72.04 Examples/sec\n",
      "INFO:tensorflow:training step 6119 | tagging_loss_video: 5.373|tagging_loss_audio: 8.267|tagging_loss_text: 13.344|tagging_loss_image: 5.043|tagging_loss_fusion: 4.075|total_loss: 36.101 | 70.44 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6120 |tagging_loss_video: 5.885|tagging_loss_audio: 9.766|tagging_loss_text: 13.626|tagging_loss_image: 4.999|tagging_loss_fusion: 4.738|total_loss: 39.015 | Examples/sec: 71.80\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6121 | tagging_loss_video: 4.807|tagging_loss_audio: 7.627|tagging_loss_text: 10.553|tagging_loss_image: 5.153|tagging_loss_fusion: 2.888|total_loss: 31.028 | 59.59 Examples/sec\n",
      "INFO:tensorflow:training step 6122 | tagging_loss_video: 5.715|tagging_loss_audio: 8.962|tagging_loss_text: 16.439|tagging_loss_image: 5.182|tagging_loss_fusion: 4.340|total_loss: 40.637 | 67.47 Examples/sec\n",
      "INFO:tensorflow:training step 6123 | tagging_loss_video: 5.944|tagging_loss_audio: 8.969|tagging_loss_text: 17.450|tagging_loss_image: 5.526|tagging_loss_fusion: 4.416|total_loss: 42.305 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 6124 | tagging_loss_video: 5.530|tagging_loss_audio: 6.586|tagging_loss_text: 14.114|tagging_loss_image: 4.653|tagging_loss_fusion: 3.381|total_loss: 34.264 | 63.66 Examples/sec\n",
      "INFO:tensorflow:training step 6125 | tagging_loss_video: 5.824|tagging_loss_audio: 8.015|tagging_loss_text: 14.883|tagging_loss_image: 5.714|tagging_loss_fusion: 4.760|total_loss: 39.195 | 69.53 Examples/sec\n",
      "INFO:tensorflow:training step 6126 | tagging_loss_video: 4.918|tagging_loss_audio: 9.651|tagging_loss_text: 16.600|tagging_loss_image: 6.046|tagging_loss_fusion: 2.831|total_loss: 40.046 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 6127 | tagging_loss_video: 5.720|tagging_loss_audio: 7.935|tagging_loss_text: 12.836|tagging_loss_image: 6.047|tagging_loss_fusion: 4.661|total_loss: 37.200 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 6128 | tagging_loss_video: 5.820|tagging_loss_audio: 8.835|tagging_loss_text: 19.243|tagging_loss_image: 6.268|tagging_loss_fusion: 5.179|total_loss: 45.346 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 6129 | tagging_loss_video: 5.485|tagging_loss_audio: 9.272|tagging_loss_text: 14.699|tagging_loss_image: 3.647|tagging_loss_fusion: 3.867|total_loss: 36.970 | 69.20 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6130 |tagging_loss_video: 4.941|tagging_loss_audio: 7.695|tagging_loss_text: 13.606|tagging_loss_image: 4.573|tagging_loss_fusion: 2.868|total_loss: 33.683 | Examples/sec: 70.67\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.86 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6131 | tagging_loss_video: 6.472|tagging_loss_audio: 7.861|tagging_loss_text: 15.631|tagging_loss_image: 5.356|tagging_loss_fusion: 5.870|total_loss: 41.190 | 66.40 Examples/sec\n",
      "INFO:tensorflow:training step 6132 | tagging_loss_video: 5.002|tagging_loss_audio: 8.354|tagging_loss_text: 15.477|tagging_loss_image: 5.577|tagging_loss_fusion: 3.841|total_loss: 38.252 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 6133 | tagging_loss_video: 4.518|tagging_loss_audio: 8.502|tagging_loss_text: 13.882|tagging_loss_image: 4.488|tagging_loss_fusion: 3.042|total_loss: 34.432 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 6134 | tagging_loss_video: 4.908|tagging_loss_audio: 9.080|tagging_loss_text: 14.047|tagging_loss_image: 4.587|tagging_loss_fusion: 2.610|total_loss: 35.232 | 71.13 Examples/sec\n",
      "INFO:tensorflow:training step 6135 | tagging_loss_video: 5.391|tagging_loss_audio: 8.332|tagging_loss_text: 13.651|tagging_loss_image: 5.730|tagging_loss_fusion: 4.857|total_loss: 37.961 | 62.17 Examples/sec\n",
      "INFO:tensorflow:training step 6136 | tagging_loss_video: 5.976|tagging_loss_audio: 9.374|tagging_loss_text: 11.407|tagging_loss_image: 5.184|tagging_loss_fusion: 4.581|total_loss: 36.523 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 6137 | tagging_loss_video: 5.549|tagging_loss_audio: 7.687|tagging_loss_text: 16.331|tagging_loss_image: 4.529|tagging_loss_fusion: 4.990|total_loss: 39.085 | 69.00 Examples/sec\n",
      "INFO:tensorflow:training step 6138 | tagging_loss_video: 5.244|tagging_loss_audio: 7.931|tagging_loss_text: 13.653|tagging_loss_image: 5.095|tagging_loss_fusion: 3.880|total_loss: 35.803 | 66.44 Examples/sec\n",
      "INFO:tensorflow:training step 6139 | tagging_loss_video: 6.286|tagging_loss_audio: 8.730|tagging_loss_text: 17.347|tagging_loss_image: 4.173|tagging_loss_fusion: 5.592|total_loss: 42.128 | 70.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6140 |tagging_loss_video: 4.085|tagging_loss_audio: 8.581|tagging_loss_text: 15.164|tagging_loss_image: 5.976|tagging_loss_fusion: 2.695|total_loss: 36.500 | Examples/sec: 71.60\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 6141 | tagging_loss_video: 4.752|tagging_loss_audio: 7.911|tagging_loss_text: 16.047|tagging_loss_image: 4.854|tagging_loss_fusion: 2.725|total_loss: 36.290 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 6142 | tagging_loss_video: 6.060|tagging_loss_audio: 9.360|tagging_loss_text: 15.497|tagging_loss_image: 5.226|tagging_loss_fusion: 5.447|total_loss: 41.589 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 6143 | tagging_loss_video: 4.937|tagging_loss_audio: 8.360|tagging_loss_text: 14.593|tagging_loss_image: 5.701|tagging_loss_fusion: 3.254|total_loss: 36.845 | 67.94 Examples/sec\n",
      "INFO:tensorflow:training step 6144 | tagging_loss_video: 5.149|tagging_loss_audio: 8.235|tagging_loss_text: 18.275|tagging_loss_image: 6.202|tagging_loss_fusion: 4.564|total_loss: 42.426 | 69.52 Examples/sec\n",
      "INFO:tensorflow:training step 6145 | tagging_loss_video: 5.901|tagging_loss_audio: 8.835|tagging_loss_text: 16.909|tagging_loss_image: 4.755|tagging_loss_fusion: 4.736|total_loss: 41.137 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 6146 | tagging_loss_video: 4.622|tagging_loss_audio: 8.342|tagging_loss_text: 15.517|tagging_loss_image: 4.727|tagging_loss_fusion: 2.415|total_loss: 35.623 | 59.30 Examples/sec\n",
      "INFO:tensorflow:training step 6147 | tagging_loss_video: 6.281|tagging_loss_audio: 8.487|tagging_loss_text: 12.713|tagging_loss_image: 4.584|tagging_loss_fusion: 4.509|total_loss: 36.575 | 68.36 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6148 | tagging_loss_video: 5.282|tagging_loss_audio: 8.803|tagging_loss_text: 13.946|tagging_loss_image: 4.548|tagging_loss_fusion: 3.539|total_loss: 36.117 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 6149 | tagging_loss_video: 5.597|tagging_loss_audio: 8.402|tagging_loss_text: 13.819|tagging_loss_image: 5.377|tagging_loss_fusion: 4.030|total_loss: 37.226 | 62.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6150 |tagging_loss_video: 6.181|tagging_loss_audio: 9.421|tagging_loss_text: 17.375|tagging_loss_image: 5.669|tagging_loss_fusion: 5.724|total_loss: 44.370 | Examples/sec: 68.99\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6151 | tagging_loss_video: 5.063|tagging_loss_audio: 7.598|tagging_loss_text: 16.728|tagging_loss_image: 4.836|tagging_loss_fusion: 3.227|total_loss: 37.452 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 6152 | tagging_loss_video: 6.533|tagging_loss_audio: 8.734|tagging_loss_text: 16.817|tagging_loss_image: 6.015|tagging_loss_fusion: 3.573|total_loss: 41.672 | 65.18 Examples/sec\n",
      "INFO:tensorflow:training step 6153 | tagging_loss_video: 6.282|tagging_loss_audio: 7.880|tagging_loss_text: 12.173|tagging_loss_image: 3.653|tagging_loss_fusion: 5.870|total_loss: 35.858 | 68.79 Examples/sec\n",
      "INFO:tensorflow:training step 6154 | tagging_loss_video: 4.843|tagging_loss_audio: 9.239|tagging_loss_text: 14.410|tagging_loss_image: 5.189|tagging_loss_fusion: 3.753|total_loss: 37.434 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 6155 | tagging_loss_video: 5.139|tagging_loss_audio: 8.021|tagging_loss_text: 16.098|tagging_loss_image: 5.809|tagging_loss_fusion: 4.454|total_loss: 39.522 | 66.42 Examples/sec\n",
      "INFO:tensorflow:training step 6156 | tagging_loss_video: 6.107|tagging_loss_audio: 8.074|tagging_loss_text: 16.054|tagging_loss_image: 6.014|tagging_loss_fusion: 10.624|total_loss: 46.872 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 6157 | tagging_loss_video: 5.267|tagging_loss_audio: 8.539|tagging_loss_text: 14.972|tagging_loss_image: 4.960|tagging_loss_fusion: 4.639|total_loss: 38.377 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 6158 | tagging_loss_video: 4.584|tagging_loss_audio: 9.084|tagging_loss_text: 17.035|tagging_loss_image: 5.141|tagging_loss_fusion: 3.943|total_loss: 39.787 | 66.66 Examples/sec\n",
      "INFO:tensorflow:training step 6159 | tagging_loss_video: 6.069|tagging_loss_audio: 7.964|tagging_loss_text: 21.491|tagging_loss_image: 5.286|tagging_loss_fusion: 4.806|total_loss: 45.616 | 71.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6160 |tagging_loss_video: 6.361|tagging_loss_audio: 8.787|tagging_loss_text: 18.110|tagging_loss_image: 5.836|tagging_loss_fusion: 6.187|total_loss: 45.280 | Examples/sec: 62.23\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.78 | precision@0.5: 0.96 |recall@0.1: 0.95 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 6161 | tagging_loss_video: 5.663|tagging_loss_audio: 9.245|tagging_loss_text: 15.484|tagging_loss_image: 4.843|tagging_loss_fusion: 5.467|total_loss: 40.701 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 6162 | tagging_loss_video: 6.810|tagging_loss_audio: 9.426|tagging_loss_text: 15.198|tagging_loss_image: 6.210|tagging_loss_fusion: 6.993|total_loss: 44.637 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 6163 | tagging_loss_video: 5.564|tagging_loss_audio: 7.019|tagging_loss_text: 12.564|tagging_loss_image: 4.893|tagging_loss_fusion: 7.223|total_loss: 37.264 | 66.65 Examples/sec\n",
      "INFO:tensorflow:training step 6164 | tagging_loss_video: 4.137|tagging_loss_audio: 7.505|tagging_loss_text: 15.037|tagging_loss_image: 1.970|tagging_loss_fusion: 2.356|total_loss: 31.005 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 6165 | tagging_loss_video: 5.430|tagging_loss_audio: 7.731|tagging_loss_text: 16.799|tagging_loss_image: 4.100|tagging_loss_fusion: 3.727|total_loss: 37.788 | 68.81 Examples/sec\n",
      "INFO:tensorflow:training step 6166 | tagging_loss_video: 4.211|tagging_loss_audio: 7.218|tagging_loss_text: 16.426|tagging_loss_image: 5.162|tagging_loss_fusion: 3.702|total_loss: 36.720 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 6167 | tagging_loss_video: 6.105|tagging_loss_audio: 7.309|tagging_loss_text: 14.708|tagging_loss_image: 5.765|tagging_loss_fusion: 7.631|total_loss: 41.519 | 67.15 Examples/sec\n",
      "INFO:tensorflow:training step 6168 | tagging_loss_video: 5.453|tagging_loss_audio: 8.480|tagging_loss_text: 13.163|tagging_loss_image: 4.977|tagging_loss_fusion: 3.747|total_loss: 35.821 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 6169 | tagging_loss_video: 6.288|tagging_loss_audio: 7.862|tagging_loss_text: 14.624|tagging_loss_image: 4.849|tagging_loss_fusion: 4.017|total_loss: 37.640 | 69.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6170 |tagging_loss_video: 5.539|tagging_loss_audio: 7.832|tagging_loss_text: 12.749|tagging_loss_image: 3.329|tagging_loss_fusion: 3.883|total_loss: 33.332 | Examples/sec: 64.71\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6171 | tagging_loss_video: 5.730|tagging_loss_audio: 8.729|tagging_loss_text: 15.468|tagging_loss_image: 2.900|tagging_loss_fusion: 3.342|total_loss: 36.168 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 6172 | tagging_loss_video: 5.381|tagging_loss_audio: 8.244|tagging_loss_text: 15.733|tagging_loss_image: 4.395|tagging_loss_fusion: 3.143|total_loss: 36.896 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 6173 | tagging_loss_video: 4.460|tagging_loss_audio: 7.817|tagging_loss_text: 13.012|tagging_loss_image: 2.877|tagging_loss_fusion: 2.679|total_loss: 30.845 | 71.71 Examples/sec\n",
      "INFO:tensorflow:training step 6174 | tagging_loss_video: 5.029|tagging_loss_audio: 8.053|tagging_loss_text: 16.129|tagging_loss_image: 4.574|tagging_loss_fusion: 4.086|total_loss: 37.872 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 6175 | tagging_loss_video: 5.484|tagging_loss_audio: 8.521|tagging_loss_text: 13.807|tagging_loss_image: 5.164|tagging_loss_fusion: 3.658|total_loss: 36.633 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 6176 | tagging_loss_video: 4.744|tagging_loss_audio: 8.070|tagging_loss_text: 18.186|tagging_loss_image: 5.306|tagging_loss_fusion: 3.797|total_loss: 40.102 | 71.47 Examples/sec\n",
      "INFO:tensorflow:training step 6177 | tagging_loss_video: 4.428|tagging_loss_audio: 8.407|tagging_loss_text: 12.208|tagging_loss_image: 2.905|tagging_loss_fusion: 1.917|total_loss: 29.866 | 60.01 Examples/sec\n",
      "INFO:tensorflow:training step 6178 | tagging_loss_video: 4.966|tagging_loss_audio: 7.454|tagging_loss_text: 15.398|tagging_loss_image: 4.846|tagging_loss_fusion: 4.159|total_loss: 36.823 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 6179 | tagging_loss_video: 4.777|tagging_loss_audio: 9.162|tagging_loss_text: 17.900|tagging_loss_image: 5.436|tagging_loss_fusion: 2.938|total_loss: 40.213 | 70.94 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6180 |tagging_loss_video: 4.934|tagging_loss_audio: 7.413|tagging_loss_text: 12.382|tagging_loss_image: 4.813|tagging_loss_fusion: 3.681|total_loss: 33.223 | Examples/sec: 65.92\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6181 | tagging_loss_video: 3.918|tagging_loss_audio: 7.450|tagging_loss_text: 14.986|tagging_loss_image: 5.156|tagging_loss_fusion: 2.978|total_loss: 34.489 | 65.56 Examples/sec\n",
      "INFO:tensorflow:training step 6182 | tagging_loss_video: 5.440|tagging_loss_audio: 7.400|tagging_loss_text: 13.862|tagging_loss_image: 5.868|tagging_loss_fusion: 5.897|total_loss: 38.467 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 6183 | tagging_loss_video: 4.506|tagging_loss_audio: 7.231|tagging_loss_text: 13.271|tagging_loss_image: 5.076|tagging_loss_fusion: 3.396|total_loss: 33.481 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 6184 | tagging_loss_video: 5.537|tagging_loss_audio: 7.406|tagging_loss_text: 14.719|tagging_loss_image: 5.328|tagging_loss_fusion: 4.429|total_loss: 37.419 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 6185 | tagging_loss_video: 5.056|tagging_loss_audio: 7.344|tagging_loss_text: 14.611|tagging_loss_image: 4.523|tagging_loss_fusion: 3.744|total_loss: 35.278 | 67.00 Examples/sec\n",
      "INFO:tensorflow:training step 6186 | tagging_loss_video: 6.226|tagging_loss_audio: 8.205|tagging_loss_text: 14.741|tagging_loss_image: 5.395|tagging_loss_fusion: 6.802|total_loss: 41.369 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 6187 | tagging_loss_video: 5.267|tagging_loss_audio: 7.728|tagging_loss_text: 14.852|tagging_loss_image: 5.372|tagging_loss_fusion: 4.147|total_loss: 37.365 | 68.98 Examples/sec\n",
      "INFO:tensorflow:training step 6188 | tagging_loss_video: 5.865|tagging_loss_audio: 9.853|tagging_loss_text: 17.147|tagging_loss_image: 5.832|tagging_loss_fusion: 4.930|total_loss: 43.626 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 6189 | tagging_loss_video: 4.170|tagging_loss_audio: 7.500|tagging_loss_text: 17.740|tagging_loss_image: 4.041|tagging_loss_fusion: 3.127|total_loss: 36.577 | 67.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6190 |tagging_loss_video: 4.429|tagging_loss_audio: 7.468|tagging_loss_text: 15.431|tagging_loss_image: 4.935|tagging_loss_fusion: 2.985|total_loss: 35.248 | Examples/sec: 70.09\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.82 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6191 | tagging_loss_video: 5.773|tagging_loss_audio: 7.911|tagging_loss_text: 13.350|tagging_loss_image: 5.081|tagging_loss_fusion: 4.070|total_loss: 36.184 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 6192 | tagging_loss_video: 5.037|tagging_loss_audio: 7.604|tagging_loss_text: 14.833|tagging_loss_image: 5.404|tagging_loss_fusion: 3.841|total_loss: 36.719 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 6193 | tagging_loss_video: 4.431|tagging_loss_audio: 8.720|tagging_loss_text: 15.874|tagging_loss_image: 4.836|tagging_loss_fusion: 2.672|total_loss: 36.534 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 6194 | tagging_loss_video: 5.866|tagging_loss_audio: 8.268|tagging_loss_text: 14.487|tagging_loss_image: 5.114|tagging_loss_fusion: 5.177|total_loss: 38.913 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 6195 | tagging_loss_video: 4.770|tagging_loss_audio: 7.682|tagging_loss_text: 12.818|tagging_loss_image: 4.254|tagging_loss_fusion: 3.119|total_loss: 32.642 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 6196 | tagging_loss_video: 6.803|tagging_loss_audio: 9.085|tagging_loss_text: 15.962|tagging_loss_image: 6.258|tagging_loss_fusion: 7.820|total_loss: 45.929 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 6197 | tagging_loss_video: 6.152|tagging_loss_audio: 8.276|tagging_loss_text: 15.619|tagging_loss_image: 4.916|tagging_loss_fusion: 6.167|total_loss: 41.130 | 70.17 Examples/sec\n",
      "INFO:tensorflow:training step 6198 | tagging_loss_video: 5.254|tagging_loss_audio: 7.846|tagging_loss_text: 15.390|tagging_loss_image: 5.293|tagging_loss_fusion: 4.479|total_loss: 38.261 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 6199 | tagging_loss_video: 7.249|tagging_loss_audio: 9.127|tagging_loss_text: 15.553|tagging_loss_image: 6.466|tagging_loss_fusion: 9.308|total_loss: 47.703 | 69.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6200 |tagging_loss_video: 5.279|tagging_loss_audio: 8.520|tagging_loss_text: 15.074|tagging_loss_image: 5.780|tagging_loss_fusion: 4.340|total_loss: 38.992 | Examples/sec: 61.48\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6201 | tagging_loss_video: 4.167|tagging_loss_audio: 7.955|tagging_loss_text: 14.641|tagging_loss_image: 4.762|tagging_loss_fusion: 2.626|total_loss: 34.150 | 66.35 Examples/sec\n",
      "INFO:tensorflow:training step 6202 | tagging_loss_video: 5.170|tagging_loss_audio: 8.219|tagging_loss_text: 13.089|tagging_loss_image: 3.649|tagging_loss_fusion: 3.079|total_loss: 33.205 | 71.52 Examples/sec\n",
      "INFO:tensorflow:training step 6203 | tagging_loss_video: 4.299|tagging_loss_audio: 7.446|tagging_loss_text: 14.932|tagging_loss_image: 5.095|tagging_loss_fusion: 2.881|total_loss: 34.653 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 6204 | tagging_loss_video: 6.139|tagging_loss_audio: 8.971|tagging_loss_text: 15.003|tagging_loss_image: 4.425|tagging_loss_fusion: 4.379|total_loss: 38.917 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6205 | tagging_loss_video: 5.559|tagging_loss_audio: 7.916|tagging_loss_text: 14.847|tagging_loss_image: 4.744|tagging_loss_fusion: 4.004|total_loss: 37.071 | 69.62 Examples/sec\n",
      "INFO:tensorflow:training step 6206 | tagging_loss_video: 6.558|tagging_loss_audio: 9.350|tagging_loss_text: 15.352|tagging_loss_image: 4.721|tagging_loss_fusion: 5.577|total_loss: 41.558 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 6207 | tagging_loss_video: 5.212|tagging_loss_audio: 8.474|tagging_loss_text: 12.227|tagging_loss_image: 5.196|tagging_loss_fusion: 6.031|total_loss: 37.140 | 67.48 Examples/sec\n",
      "INFO:tensorflow:training step 6208 | tagging_loss_video: 6.777|tagging_loss_audio: 9.117|tagging_loss_text: 14.214|tagging_loss_image: 5.082|tagging_loss_fusion: 4.838|total_loss: 40.028 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 6209 | tagging_loss_video: 5.029|tagging_loss_audio: 8.271|tagging_loss_text: 13.600|tagging_loss_image: 5.975|tagging_loss_fusion: 4.369|total_loss: 37.245 | 69.43 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6210 |tagging_loss_video: 5.200|tagging_loss_audio: 8.859|tagging_loss_text: 17.412|tagging_loss_image: 4.648|tagging_loss_fusion: 4.227|total_loss: 40.346 | Examples/sec: 70.30\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6211 | tagging_loss_video: 4.440|tagging_loss_audio: 8.451|tagging_loss_text: 19.762|tagging_loss_image: 5.199|tagging_loss_fusion: 3.248|total_loss: 41.100 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 6212 | tagging_loss_video: 6.639|tagging_loss_audio: 8.690|tagging_loss_text: 16.634|tagging_loss_image: 5.215|tagging_loss_fusion: 6.131|total_loss: 43.310 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 6213 | tagging_loss_video: 5.620|tagging_loss_audio: 7.754|tagging_loss_text: 14.504|tagging_loss_image: 4.509|tagging_loss_fusion: 4.565|total_loss: 36.952 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 6214 | tagging_loss_video: 6.225|tagging_loss_audio: 8.767|tagging_loss_text: 16.501|tagging_loss_image: 5.199|tagging_loss_fusion: 5.885|total_loss: 42.577 | 63.68 Examples/sec\n",
      "INFO:tensorflow:training step 6215 | tagging_loss_video: 6.161|tagging_loss_audio: 9.779|tagging_loss_text: 22.327|tagging_loss_image: 6.202|tagging_loss_fusion: 4.538|total_loss: 49.006 | 67.31 Examples/sec\n",
      "INFO:tensorflow:training step 6216 | tagging_loss_video: 5.266|tagging_loss_audio: 8.073|tagging_loss_text: 14.309|tagging_loss_image: 5.264|tagging_loss_fusion: 5.274|total_loss: 38.186 | 72.49 Examples/sec\n",
      "INFO:tensorflow:training step 6217 | tagging_loss_video: 5.569|tagging_loss_audio: 8.784|tagging_loss_text: 15.238|tagging_loss_image: 5.124|tagging_loss_fusion: 3.321|total_loss: 38.035 | 59.62 Examples/sec\n",
      "INFO:tensorflow:training step 6218 | tagging_loss_video: 6.473|tagging_loss_audio: 8.827|tagging_loss_text: 13.843|tagging_loss_image: 4.159|tagging_loss_fusion: 4.349|total_loss: 37.652 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 6219 | tagging_loss_video: 5.410|tagging_loss_audio: 8.869|tagging_loss_text: 15.960|tagging_loss_image: 5.567|tagging_loss_fusion: 4.719|total_loss: 40.524 | 67.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6220 |tagging_loss_video: 6.377|tagging_loss_audio: 8.282|tagging_loss_text: 16.181|tagging_loss_image: 4.261|tagging_loss_fusion: 5.124|total_loss: 40.225 | Examples/sec: 61.86\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 6221 | tagging_loss_video: 4.360|tagging_loss_audio: 9.486|tagging_loss_text: 17.881|tagging_loss_image: 5.782|tagging_loss_fusion: 2.734|total_loss: 40.243 | 69.31 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 6222.\n",
      "INFO:tensorflow:training step 6222 | tagging_loss_video: 5.127|tagging_loss_audio: 9.265|tagging_loss_text: 14.791|tagging_loss_image: 5.372|tagging_loss_fusion: 2.533|total_loss: 37.087 | 52.92 Examples/sec\n",
      "INFO:tensorflow:training step 6223 | tagging_loss_video: 4.292|tagging_loss_audio: 8.494|tagging_loss_text: 16.700|tagging_loss_image: 2.966|tagging_loss_fusion: 2.490|total_loss: 34.942 | 63.87 Examples/sec\n",
      "INFO:tensorflow:training step 6224 | tagging_loss_video: 6.479|tagging_loss_audio: 9.338|tagging_loss_text: 18.063|tagging_loss_image: 5.795|tagging_loss_fusion: 5.508|total_loss: 45.182 | 59.86 Examples/sec\n",
      "INFO:tensorflow:training step 6225 | tagging_loss_video: 7.473|tagging_loss_audio: 9.221|tagging_loss_text: 15.132|tagging_loss_image: 3.924|tagging_loss_fusion: 6.613|total_loss: 42.363 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 6226 | tagging_loss_video: 5.747|tagging_loss_audio: 9.227|tagging_loss_text: 11.640|tagging_loss_image: 6.089|tagging_loss_fusion: 5.780|total_loss: 38.483 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6227 | tagging_loss_video: 5.772|tagging_loss_audio: 9.005|tagging_loss_text: 16.025|tagging_loss_image: 5.201|tagging_loss_fusion: 3.560|total_loss: 39.563 | 61.89 Examples/sec\n",
      "INFO:tensorflow:training step 6228 | tagging_loss_video: 6.463|tagging_loss_audio: 9.575|tagging_loss_text: 22.119|tagging_loss_image: 4.818|tagging_loss_fusion: 4.197|total_loss: 47.173 | 70.01 Examples/sec\n",
      "INFO:tensorflow:training step 6229 | tagging_loss_video: 6.369|tagging_loss_audio: 8.898|tagging_loss_text: 16.622|tagging_loss_image: 5.025|tagging_loss_fusion: 4.920|total_loss: 41.834 | 70.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6230 |tagging_loss_video: 5.847|tagging_loss_audio: 8.408|tagging_loss_text: 13.975|tagging_loss_image: 4.720|tagging_loss_fusion: 2.953|total_loss: 35.903 | Examples/sec: 64.27\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 6231 | tagging_loss_video: 6.146|tagging_loss_audio: 9.503|tagging_loss_text: 13.404|tagging_loss_image: 5.008|tagging_loss_fusion: 5.094|total_loss: 39.155 | 68.36 Examples/sec\n",
      "INFO:tensorflow:training step 6232 | tagging_loss_video: 6.119|tagging_loss_audio: 9.894|tagging_loss_text: 14.405|tagging_loss_image: 4.396|tagging_loss_fusion: 3.813|total_loss: 38.627 | 71.30 Examples/sec\n",
      "INFO:tensorflow:training step 6233 | tagging_loss_video: 5.692|tagging_loss_audio: 8.277|tagging_loss_text: 15.305|tagging_loss_image: 5.376|tagging_loss_fusion: 4.346|total_loss: 38.996 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 6234 | tagging_loss_video: 6.245|tagging_loss_audio: 9.658|tagging_loss_text: 17.064|tagging_loss_image: 5.802|tagging_loss_fusion: 5.161|total_loss: 43.930 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 6235 | tagging_loss_video: 6.421|tagging_loss_audio: 8.602|tagging_loss_text: 21.420|tagging_loss_image: 7.020|tagging_loss_fusion: 5.770|total_loss: 49.233 | 61.99 Examples/sec\n",
      "INFO:tensorflow:training step 6236 | tagging_loss_video: 5.246|tagging_loss_audio: 7.956|tagging_loss_text: 17.031|tagging_loss_image: 4.084|tagging_loss_fusion: 4.329|total_loss: 38.646 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 6237 | tagging_loss_video: 5.346|tagging_loss_audio: 8.512|tagging_loss_text: 14.972|tagging_loss_image: 4.983|tagging_loss_fusion: 3.279|total_loss: 37.092 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 6238 | tagging_loss_video: 5.776|tagging_loss_audio: 8.433|tagging_loss_text: 13.492|tagging_loss_image: 4.337|tagging_loss_fusion: 5.578|total_loss: 37.617 | 61.76 Examples/sec\n",
      "INFO:tensorflow:training step 6239 | tagging_loss_video: 5.906|tagging_loss_audio: 10.144|tagging_loss_text: 15.405|tagging_loss_image: 6.179|tagging_loss_fusion: 5.883|total_loss: 43.518 | 71.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6240 |tagging_loss_video: 5.647|tagging_loss_audio: 8.485|tagging_loss_text: 14.193|tagging_loss_image: 5.771|tagging_loss_fusion: 3.863|total_loss: 37.959 | Examples/sec: 70.48\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6241 | tagging_loss_video: 5.545|tagging_loss_audio: 7.799|tagging_loss_text: 15.187|tagging_loss_image: 4.008|tagging_loss_fusion: 2.974|total_loss: 35.514 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 6242 | tagging_loss_video: 6.348|tagging_loss_audio: 8.303|tagging_loss_text: 9.950|tagging_loss_image: 4.813|tagging_loss_fusion: 6.268|total_loss: 35.681 | 69.64 Examples/sec\n",
      "INFO:tensorflow:training step 6243 | tagging_loss_video: 5.391|tagging_loss_audio: 8.606|tagging_loss_text: 16.188|tagging_loss_image: 4.775|tagging_loss_fusion: 3.887|total_loss: 38.848 | 66.04 Examples/sec\n",
      "INFO:tensorflow:training step 6244 | tagging_loss_video: 5.820|tagging_loss_audio: 8.151|tagging_loss_text: 15.173|tagging_loss_image: 4.095|tagging_loss_fusion: 4.465|total_loss: 37.704 | 65.33 Examples/sec\n",
      "INFO:tensorflow:training step 6245 | tagging_loss_video: 5.941|tagging_loss_audio: 8.217|tagging_loss_text: 13.291|tagging_loss_image: 4.392|tagging_loss_fusion: 4.069|total_loss: 35.910 | 68.94 Examples/sec\n",
      "INFO:tensorflow:training step 6246 | tagging_loss_video: 4.789|tagging_loss_audio: 8.489|tagging_loss_text: 17.422|tagging_loss_image: 5.353|tagging_loss_fusion: 3.484|total_loss: 39.537 | 69.02 Examples/sec\n",
      "INFO:tensorflow:training step 6247 | tagging_loss_video: 6.212|tagging_loss_audio: 8.619|tagging_loss_text: 16.058|tagging_loss_image: 4.988|tagging_loss_fusion: 5.267|total_loss: 41.144 | 67.29 Examples/sec\n",
      "INFO:tensorflow:training step 6248 | tagging_loss_video: 5.524|tagging_loss_audio: 9.164|tagging_loss_text: 15.868|tagging_loss_image: 5.221|tagging_loss_fusion: 4.020|total_loss: 39.797 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 6249 | tagging_loss_video: 5.000|tagging_loss_audio: 8.494|tagging_loss_text: 14.896|tagging_loss_image: 4.281|tagging_loss_fusion: 2.766|total_loss: 35.439 | 62.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6250 |tagging_loss_video: 5.808|tagging_loss_audio: 8.072|tagging_loss_text: 16.015|tagging_loss_image: 3.718|tagging_loss_fusion: 3.990|total_loss: 37.602 | Examples/sec: 69.55\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6251 | tagging_loss_video: 5.947|tagging_loss_audio: 8.112|tagging_loss_text: 12.620|tagging_loss_image: 5.648|tagging_loss_fusion: 4.273|total_loss: 36.600 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 6252 | tagging_loss_video: 4.406|tagging_loss_audio: 9.544|tagging_loss_text: 15.558|tagging_loss_image: 3.615|tagging_loss_fusion: 2.647|total_loss: 35.770 | 66.41 Examples/sec\n",
      "INFO:tensorflow:training step 6253 | tagging_loss_video: 5.892|tagging_loss_audio: 9.519|tagging_loss_text: 11.842|tagging_loss_image: 4.825|tagging_loss_fusion: 5.592|total_loss: 37.671 | 65.28 Examples/sec\n",
      "INFO:tensorflow:training step 6254 | tagging_loss_video: 4.641|tagging_loss_audio: 8.955|tagging_loss_text: 17.484|tagging_loss_image: 6.027|tagging_loss_fusion: 4.834|total_loss: 41.941 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 6255 | tagging_loss_video: 5.327|tagging_loss_audio: 8.375|tagging_loss_text: 13.459|tagging_loss_image: 5.929|tagging_loss_fusion: 3.992|total_loss: 37.082 | 64.14 Examples/sec\n",
      "INFO:tensorflow:training step 6256 | tagging_loss_video: 6.321|tagging_loss_audio: 8.665|tagging_loss_text: 15.076|tagging_loss_image: 5.200|tagging_loss_fusion: 3.249|total_loss: 38.510 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 6257 | tagging_loss_video: 6.680|tagging_loss_audio: 9.285|tagging_loss_text: 14.968|tagging_loss_image: 6.074|tagging_loss_fusion: 8.363|total_loss: 45.370 | 64.82 Examples/sec\n",
      "INFO:tensorflow:training step 6258 | tagging_loss_video: 4.434|tagging_loss_audio: 8.789|tagging_loss_text: 14.020|tagging_loss_image: 5.451|tagging_loss_fusion: 2.346|total_loss: 35.041 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 6259 | tagging_loss_video: 5.034|tagging_loss_audio: 9.389|tagging_loss_text: 11.484|tagging_loss_image: 3.851|tagging_loss_fusion: 2.197|total_loss: 31.955 | 71.91 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6260 |tagging_loss_video: 3.468|tagging_loss_audio: 8.374|tagging_loss_text: 15.175|tagging_loss_image: 4.339|tagging_loss_fusion: 1.928|total_loss: 33.283 | Examples/sec: 60.55\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 1.00 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 6261 | tagging_loss_video: 6.087|tagging_loss_audio: 8.146|tagging_loss_text: 17.817|tagging_loss_image: 5.319|tagging_loss_fusion: 3.839|total_loss: 41.208 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 6262 | tagging_loss_video: 6.357|tagging_loss_audio: 9.005|tagging_loss_text: 16.060|tagging_loss_image: 5.064|tagging_loss_fusion: 5.705|total_loss: 42.191 | 68.22 Examples/sec\n",
      "INFO:tensorflow:training step 6263 | tagging_loss_video: 5.358|tagging_loss_audio: 6.962|tagging_loss_text: 14.112|tagging_loss_image: 3.825|tagging_loss_fusion: 4.133|total_loss: 34.390 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 6264 | tagging_loss_video: 5.490|tagging_loss_audio: 8.919|tagging_loss_text: 14.889|tagging_loss_image: 4.420|tagging_loss_fusion: 3.830|total_loss: 37.549 | 68.75 Examples/sec\n",
      "INFO:tensorflow:training step 6265 | tagging_loss_video: 5.767|tagging_loss_audio: 8.999|tagging_loss_text: 18.647|tagging_loss_image: 3.780|tagging_loss_fusion: 3.045|total_loss: 40.238 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 6266 | tagging_loss_video: 5.508|tagging_loss_audio: 8.287|tagging_loss_text: 14.863|tagging_loss_image: 5.693|tagging_loss_fusion: 3.723|total_loss: 38.073 | 65.56 Examples/sec\n",
      "INFO:tensorflow:training step 6267 | tagging_loss_video: 6.323|tagging_loss_audio: 9.707|tagging_loss_text: 16.249|tagging_loss_image: 6.160|tagging_loss_fusion: 5.501|total_loss: 43.940 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 6268 | tagging_loss_video: 5.166|tagging_loss_audio: 8.814|tagging_loss_text: 16.030|tagging_loss_image: 5.354|tagging_loss_fusion: 4.136|total_loss: 39.500 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 6269 | tagging_loss_video: 6.109|tagging_loss_audio: 9.013|tagging_loss_text: 11.565|tagging_loss_image: 4.198|tagging_loss_fusion: 5.119|total_loss: 36.005 | 67.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6270 |tagging_loss_video: 4.319|tagging_loss_audio: 8.517|tagging_loss_text: 14.941|tagging_loss_image: 5.076|tagging_loss_fusion: 2.415|total_loss: 35.268 | Examples/sec: 69.54\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.92 | precision@0.5: 0.99 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6271 | tagging_loss_video: 3.258|tagging_loss_audio: 8.479|tagging_loss_text: 12.099|tagging_loss_image: 4.758|tagging_loss_fusion: 2.181|total_loss: 30.775 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 6272 | tagging_loss_video: 6.032|tagging_loss_audio: 9.036|tagging_loss_text: 17.323|tagging_loss_image: 6.004|tagging_loss_fusion: 6.781|total_loss: 45.175 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 6273 | tagging_loss_video: 4.488|tagging_loss_audio: 7.927|tagging_loss_text: 15.356|tagging_loss_image: 4.676|tagging_loss_fusion: 4.217|total_loss: 36.664 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 6274 | tagging_loss_video: 5.554|tagging_loss_audio: 8.518|tagging_loss_text: 14.999|tagging_loss_image: 4.757|tagging_loss_fusion: 4.082|total_loss: 37.911 | 62.45 Examples/sec\n",
      "INFO:tensorflow:training step 6275 | tagging_loss_video: 3.904|tagging_loss_audio: 7.594|tagging_loss_text: 15.428|tagging_loss_image: 4.140|tagging_loss_fusion: 2.933|total_loss: 33.999 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 6276 | tagging_loss_video: 6.011|tagging_loss_audio: 7.992|tagging_loss_text: 14.974|tagging_loss_image: 5.659|tagging_loss_fusion: 6.986|total_loss: 41.622 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 6277 | tagging_loss_video: 5.542|tagging_loss_audio: 7.771|tagging_loss_text: 15.618|tagging_loss_image: 5.154|tagging_loss_fusion: 4.874|total_loss: 38.958 | 62.07 Examples/sec\n",
      "INFO:tensorflow:training step 6278 | tagging_loss_video: 4.988|tagging_loss_audio: 8.690|tagging_loss_text: 13.873|tagging_loss_image: 5.388|tagging_loss_fusion: 4.456|total_loss: 37.395 | 71.36 Examples/sec\n",
      "INFO:tensorflow:training step 6279 | tagging_loss_video: 4.347|tagging_loss_audio: 7.976|tagging_loss_text: 15.831|tagging_loss_image: 4.520|tagging_loss_fusion: 2.200|total_loss: 34.873 | 70.76 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6280 |tagging_loss_video: 3.920|tagging_loss_audio: 7.752|tagging_loss_text: 15.347|tagging_loss_image: 5.929|tagging_loss_fusion: 3.619|total_loss: 36.567 | Examples/sec: 66.80\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6281 | tagging_loss_video: 6.774|tagging_loss_audio: 8.292|tagging_loss_text: 14.041|tagging_loss_image: 4.870|tagging_loss_fusion: 5.487|total_loss: 39.463 | 69.31 Examples/sec\n",
      "INFO:tensorflow:training step 6282 | tagging_loss_video: 5.700|tagging_loss_audio: 8.145|tagging_loss_text: 16.587|tagging_loss_image: 3.417|tagging_loss_fusion: 3.006|total_loss: 36.855 | 72.58 Examples/sec\n",
      "INFO:tensorflow:training step 6283 | tagging_loss_video: 3.959|tagging_loss_audio: 8.041|tagging_loss_text: 18.965|tagging_loss_image: 6.158|tagging_loss_fusion: 2.209|total_loss: 39.332 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 6284 | tagging_loss_video: 7.475|tagging_loss_audio: 7.868|tagging_loss_text: 16.909|tagging_loss_image: 5.478|tagging_loss_fusion: 5.024|total_loss: 42.754 | 65.29 Examples/sec\n",
      "INFO:tensorflow:training step 6285 | tagging_loss_video: 5.230|tagging_loss_audio: 7.713|tagging_loss_text: 16.842|tagging_loss_image: 5.050|tagging_loss_fusion: 3.571|total_loss: 38.406 | 63.07 Examples/sec\n",
      "INFO:tensorflow:training step 6286 | tagging_loss_video: 6.482|tagging_loss_audio: 7.944|tagging_loss_text: 16.231|tagging_loss_image: 4.198|tagging_loss_fusion: 5.052|total_loss: 39.906 | 70.61 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6287 | tagging_loss_video: 5.671|tagging_loss_audio: 7.654|tagging_loss_text: 13.935|tagging_loss_image: 5.210|tagging_loss_fusion: 3.649|total_loss: 36.120 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 6288 | tagging_loss_video: 5.315|tagging_loss_audio: 8.499|tagging_loss_text: 15.885|tagging_loss_image: 4.033|tagging_loss_fusion: 3.878|total_loss: 37.610 | 61.25 Examples/sec\n",
      "INFO:tensorflow:training step 6289 | tagging_loss_video: 4.965|tagging_loss_audio: 8.859|tagging_loss_text: 18.094|tagging_loss_image: 6.345|tagging_loss_fusion: 2.447|total_loss: 40.710 | 70.77 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6290 |tagging_loss_video: 4.915|tagging_loss_audio: 7.464|tagging_loss_text: 16.084|tagging_loss_image: 4.060|tagging_loss_fusion: 3.117|total_loss: 35.640 | Examples/sec: 70.66\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.98 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6291 | tagging_loss_video: 6.081|tagging_loss_audio: 8.437|tagging_loss_text: 16.093|tagging_loss_image: 6.030|tagging_loss_fusion: 5.124|total_loss: 41.765 | 61.21 Examples/sec\n",
      "INFO:tensorflow:training step 6292 | tagging_loss_video: 5.312|tagging_loss_audio: 7.513|tagging_loss_text: 13.462|tagging_loss_image: 4.340|tagging_loss_fusion: 3.970|total_loss: 34.597 | 69.37 Examples/sec\n",
      "INFO:tensorflow:training step 6293 | tagging_loss_video: 6.207|tagging_loss_audio: 8.999|tagging_loss_text: 17.150|tagging_loss_image: 5.206|tagging_loss_fusion: 5.095|total_loss: 42.656 | 67.77 Examples/sec\n",
      "INFO:tensorflow:training step 6294 | tagging_loss_video: 5.834|tagging_loss_audio: 8.870|tagging_loss_text: 16.099|tagging_loss_image: 5.418|tagging_loss_fusion: 5.598|total_loss: 41.820 | 69.16 Examples/sec\n",
      "INFO:tensorflow:training step 6295 | tagging_loss_video: 5.618|tagging_loss_audio: 8.503|tagging_loss_text: 16.054|tagging_loss_image: 4.911|tagging_loss_fusion: 4.640|total_loss: 39.725 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 6296 | tagging_loss_video: 5.414|tagging_loss_audio: 7.734|tagging_loss_text: 16.922|tagging_loss_image: 4.468|tagging_loss_fusion: 4.098|total_loss: 38.636 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 6297 | tagging_loss_video: 5.578|tagging_loss_audio: 8.148|tagging_loss_text: 17.036|tagging_loss_image: 5.638|tagging_loss_fusion: 4.429|total_loss: 40.828 | 72.01 Examples/sec\n",
      "INFO:tensorflow:training step 6298 | tagging_loss_video: 5.014|tagging_loss_audio: 9.191|tagging_loss_text: 20.773|tagging_loss_image: 6.188|tagging_loss_fusion: 4.494|total_loss: 45.660 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 6299 | tagging_loss_video: 6.232|tagging_loss_audio: 8.291|tagging_loss_text: 15.237|tagging_loss_image: 5.433|tagging_loss_fusion: 4.787|total_loss: 39.980 | 69.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6300 |tagging_loss_video: 5.650|tagging_loss_audio: 8.860|tagging_loss_text: 15.455|tagging_loss_image: 4.677|tagging_loss_fusion: 4.251|total_loss: 38.893 | Examples/sec: 70.41\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6301 | tagging_loss_video: 5.466|tagging_loss_audio: 8.574|tagging_loss_text: 19.025|tagging_loss_image: 6.237|tagging_loss_fusion: 3.977|total_loss: 43.280 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 6302 | tagging_loss_video: 4.373|tagging_loss_audio: 7.255|tagging_loss_text: 12.583|tagging_loss_image: 4.830|tagging_loss_fusion: 3.168|total_loss: 32.210 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 6303 | tagging_loss_video: 4.577|tagging_loss_audio: 6.898|tagging_loss_text: 13.768|tagging_loss_image: 4.736|tagging_loss_fusion: 3.192|total_loss: 33.171 | 67.14 Examples/sec\n",
      "INFO:tensorflow:training step 6304 | tagging_loss_video: 5.418|tagging_loss_audio: 8.105|tagging_loss_text: 14.111|tagging_loss_image: 4.545|tagging_loss_fusion: 4.267|total_loss: 36.446 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 6305 | tagging_loss_video: 5.416|tagging_loss_audio: 6.732|tagging_loss_text: 13.980|tagging_loss_image: 4.148|tagging_loss_fusion: 4.335|total_loss: 34.610 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 6306 | tagging_loss_video: 6.422|tagging_loss_audio: 8.660|tagging_loss_text: 17.522|tagging_loss_image: 5.467|tagging_loss_fusion: 4.400|total_loss: 42.471 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 6307 | tagging_loss_video: 4.434|tagging_loss_audio: 8.047|tagging_loss_text: 14.468|tagging_loss_image: 3.960|tagging_loss_fusion: 2.711|total_loss: 33.620 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 6308 | tagging_loss_video: 5.584|tagging_loss_audio: 7.227|tagging_loss_text: 13.971|tagging_loss_image: 5.509|tagging_loss_fusion: 3.703|total_loss: 35.994 | 68.58 Examples/sec\n",
      "INFO:tensorflow:training step 6309 | tagging_loss_video: 4.990|tagging_loss_audio: 7.429|tagging_loss_text: 14.648|tagging_loss_image: 4.606|tagging_loss_fusion: 4.204|total_loss: 35.877 | 69.28 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6310 |tagging_loss_video: 4.736|tagging_loss_audio: 8.620|tagging_loss_text: 16.214|tagging_loss_image: 5.222|tagging_loss_fusion: 2.625|total_loss: 37.416 | Examples/sec: 64.85\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6311 | tagging_loss_video: 5.627|tagging_loss_audio: 7.798|tagging_loss_text: 16.413|tagging_loss_image: 3.649|tagging_loss_fusion: 3.661|total_loss: 37.148 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6312 | tagging_loss_video: 4.592|tagging_loss_audio: 8.152|tagging_loss_text: 12.390|tagging_loss_image: 3.868|tagging_loss_fusion: 2.729|total_loss: 31.732 | 67.54 Examples/sec\n",
      "INFO:tensorflow:training step 6313 | tagging_loss_video: 3.601|tagging_loss_audio: 8.050|tagging_loss_text: 13.632|tagging_loss_image: 5.236|tagging_loss_fusion: 2.065|total_loss: 32.585 | 70.48 Examples/sec\n",
      "INFO:tensorflow:training step 6314 | tagging_loss_video: 4.553|tagging_loss_audio: 7.895|tagging_loss_text: 16.566|tagging_loss_image: 5.804|tagging_loss_fusion: 3.606|total_loss: 38.424 | 68.23 Examples/sec\n",
      "INFO:tensorflow:training step 6315 | tagging_loss_video: 4.825|tagging_loss_audio: 7.919|tagging_loss_text: 17.438|tagging_loss_image: 5.084|tagging_loss_fusion: 3.405|total_loss: 38.672 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 6316 | tagging_loss_video: 5.192|tagging_loss_audio: 7.589|tagging_loss_text: 14.152|tagging_loss_image: 4.096|tagging_loss_fusion: 3.901|total_loss: 34.930 | 63.51 Examples/sec\n",
      "INFO:tensorflow:training step 6317 | tagging_loss_video: 3.721|tagging_loss_audio: 8.427|tagging_loss_text: 12.067|tagging_loss_image: 4.391|tagging_loss_fusion: 1.881|total_loss: 30.487 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 6318 | tagging_loss_video: 5.871|tagging_loss_audio: 7.494|tagging_loss_text: 16.529|tagging_loss_image: 6.164|tagging_loss_fusion: 3.984|total_loss: 40.043 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 6319 | tagging_loss_video: 5.416|tagging_loss_audio: 7.165|tagging_loss_text: 12.392|tagging_loss_image: 5.104|tagging_loss_fusion: 3.407|total_loss: 33.484 | 70.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6320 |tagging_loss_video: 5.717|tagging_loss_audio: 7.366|tagging_loss_text: 13.687|tagging_loss_image: 5.221|tagging_loss_fusion: 5.414|total_loss: 37.405 | Examples/sec: 69.79\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.80 | precision@0.5: 0.91 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6321 | tagging_loss_video: 4.314|tagging_loss_audio: 7.947|tagging_loss_text: 11.221|tagging_loss_image: 5.225|tagging_loss_fusion: 3.526|total_loss: 32.233 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 6322 | tagging_loss_video: 5.215|tagging_loss_audio: 8.059|tagging_loss_text: 14.603|tagging_loss_image: 5.457|tagging_loss_fusion: 3.613|total_loss: 36.947 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 6323 | tagging_loss_video: 5.073|tagging_loss_audio: 8.152|tagging_loss_text: 16.072|tagging_loss_image: 5.099|tagging_loss_fusion: 4.421|total_loss: 38.817 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 6324 | tagging_loss_video: 4.786|tagging_loss_audio: 8.777|tagging_loss_text: 15.296|tagging_loss_image: 3.805|tagging_loss_fusion: 3.387|total_loss: 36.050 | 67.75 Examples/sec\n",
      "INFO:tensorflow:training step 6325 | tagging_loss_video: 5.862|tagging_loss_audio: 8.677|tagging_loss_text: 14.032|tagging_loss_image: 5.640|tagging_loss_fusion: 5.492|total_loss: 39.703 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 6326 | tagging_loss_video: 4.955|tagging_loss_audio: 7.896|tagging_loss_text: 15.531|tagging_loss_image: 5.372|tagging_loss_fusion: 4.785|total_loss: 38.539 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 6327 | tagging_loss_video: 5.364|tagging_loss_audio: 9.011|tagging_loss_text: 16.410|tagging_loss_image: 5.320|tagging_loss_fusion: 4.340|total_loss: 40.446 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 6328 | tagging_loss_video: 5.467|tagging_loss_audio: 7.771|tagging_loss_text: 14.585|tagging_loss_image: 4.884|tagging_loss_fusion: 3.828|total_loss: 36.536 | 61.58 Examples/sec\n",
      "INFO:tensorflow:training step 6329 | tagging_loss_video: 5.308|tagging_loss_audio: 6.616|tagging_loss_text: 12.474|tagging_loss_image: 4.186|tagging_loss_fusion: 4.596|total_loss: 33.180 | 69.88 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6330 |tagging_loss_video: 5.717|tagging_loss_audio: 7.575|tagging_loss_text: 18.675|tagging_loss_image: 4.590|tagging_loss_fusion: 4.955|total_loss: 41.512 | Examples/sec: 70.24\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6331 | tagging_loss_video: 5.807|tagging_loss_audio: 7.843|tagging_loss_text: 12.256|tagging_loss_image: 5.587|tagging_loss_fusion: 5.058|total_loss: 36.551 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 6332 | tagging_loss_video: 5.294|tagging_loss_audio: 9.172|tagging_loss_text: 14.488|tagging_loss_image: 5.082|tagging_loss_fusion: 4.681|total_loss: 38.717 | 67.84 Examples/sec\n",
      "INFO:tensorflow:training step 6333 | tagging_loss_video: 5.116|tagging_loss_audio: 8.180|tagging_loss_text: 15.797|tagging_loss_image: 4.054|tagging_loss_fusion: 3.359|total_loss: 36.506 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 6334 | tagging_loss_video: 5.257|tagging_loss_audio: 7.836|tagging_loss_text: 14.096|tagging_loss_image: 4.690|tagging_loss_fusion: 2.855|total_loss: 34.735 | 64.19 Examples/sec\n",
      "INFO:tensorflow:training step 6335 | tagging_loss_video: 5.786|tagging_loss_audio: 8.254|tagging_loss_text: 14.573|tagging_loss_image: 5.160|tagging_loss_fusion: 4.935|total_loss: 38.709 | 68.99 Examples/sec\n",
      "INFO:tensorflow:training step 6336 | tagging_loss_video: 4.196|tagging_loss_audio: 8.491|tagging_loss_text: 14.932|tagging_loss_image: 5.695|tagging_loss_fusion: 2.085|total_loss: 35.399 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 6337 | tagging_loss_video: 4.865|tagging_loss_audio: 7.982|tagging_loss_text: 15.373|tagging_loss_image: 5.921|tagging_loss_fusion: 4.731|total_loss: 38.872 | 67.07 Examples/sec\n",
      "INFO:tensorflow:training step 6338 | tagging_loss_video: 5.798|tagging_loss_audio: 8.948|tagging_loss_text: 17.781|tagging_loss_image: 4.081|tagging_loss_fusion: 3.716|total_loss: 40.323 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 6339 | tagging_loss_video: 5.410|tagging_loss_audio: 7.938|tagging_loss_text: 16.462|tagging_loss_image: 5.064|tagging_loss_fusion: 2.341|total_loss: 37.215 | 62.86 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6340 |tagging_loss_video: 5.285|tagging_loss_audio: 8.531|tagging_loss_text: 13.357|tagging_loss_image: 4.978|tagging_loss_fusion: 3.894|total_loss: 36.045 | Examples/sec: 70.20\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6341 | tagging_loss_video: 3.982|tagging_loss_audio: 8.483|tagging_loss_text: 11.850|tagging_loss_image: 3.511|tagging_loss_fusion: 2.863|total_loss: 30.689 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 6342 | tagging_loss_video: 3.837|tagging_loss_audio: 7.635|tagging_loss_text: 13.617|tagging_loss_image: 4.920|tagging_loss_fusion: 2.425|total_loss: 32.434 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 6343 | tagging_loss_video: 5.902|tagging_loss_audio: 8.691|tagging_loss_text: 17.840|tagging_loss_image: 5.425|tagging_loss_fusion: 4.662|total_loss: 42.521 | 68.85 Examples/sec\n",
      "INFO:tensorflow:training step 6344 | tagging_loss_video: 5.524|tagging_loss_audio: 8.634|tagging_loss_text: 14.186|tagging_loss_image: 3.312|tagging_loss_fusion: 6.057|total_loss: 37.713 | 68.96 Examples/sec\n",
      "INFO:tensorflow:training step 6345 | tagging_loss_video: 4.579|tagging_loss_audio: 7.862|tagging_loss_text: 16.042|tagging_loss_image: 4.673|tagging_loss_fusion: 1.596|total_loss: 34.751 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 6346 | tagging_loss_video: 5.299|tagging_loss_audio: 7.508|tagging_loss_text: 16.099|tagging_loss_image: 5.054|tagging_loss_fusion: 4.411|total_loss: 38.371 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 6347 | tagging_loss_video: 6.199|tagging_loss_audio: 9.667|tagging_loss_text: 16.352|tagging_loss_image: 5.265|tagging_loss_fusion: 4.878|total_loss: 42.360 | 65.60 Examples/sec\n",
      "INFO:tensorflow:training step 6348 | tagging_loss_video: 4.531|tagging_loss_audio: 7.881|tagging_loss_text: 15.748|tagging_loss_image: 4.332|tagging_loss_fusion: 2.406|total_loss: 34.898 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 6349 | tagging_loss_video: 5.563|tagging_loss_audio: 7.859|tagging_loss_text: 15.318|tagging_loss_image: 5.622|tagging_loss_fusion: 5.416|total_loss: 39.778 | 69.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6350 |tagging_loss_video: 4.974|tagging_loss_audio: 8.371|tagging_loss_text: 15.656|tagging_loss_image: 5.578|tagging_loss_fusion: 3.649|total_loss: 38.228 | Examples/sec: 60.66\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.98 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6351 | tagging_loss_video: 6.059|tagging_loss_audio: 9.479|tagging_loss_text: 16.630|tagging_loss_image: 5.730|tagging_loss_fusion: 4.677|total_loss: 42.575 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 6352 | tagging_loss_video: 5.131|tagging_loss_audio: 8.413|tagging_loss_text: 14.502|tagging_loss_image: 5.852|tagging_loss_fusion: 4.701|total_loss: 38.599 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 6353 | tagging_loss_video: 4.077|tagging_loss_audio: 8.476|tagging_loss_text: 17.864|tagging_loss_image: 3.831|tagging_loss_fusion: 2.224|total_loss: 36.472 | 66.57 Examples/sec\n",
      "INFO:tensorflow:training step 6354 | tagging_loss_video: 5.437|tagging_loss_audio: 9.076|tagging_loss_text: 17.093|tagging_loss_image: 5.510|tagging_loss_fusion: 3.993|total_loss: 41.109 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 6355 | tagging_loss_video: 3.835|tagging_loss_audio: 8.201|tagging_loss_text: 16.252|tagging_loss_image: 2.973|tagging_loss_fusion: 2.696|total_loss: 33.956 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 6356 | tagging_loss_video: 4.767|tagging_loss_audio: 9.508|tagging_loss_text: 13.868|tagging_loss_image: 5.025|tagging_loss_fusion: 3.406|total_loss: 36.575 | 60.30 Examples/sec\n",
      "INFO:tensorflow:training step 6357 | tagging_loss_video: 6.091|tagging_loss_audio: 8.565|tagging_loss_text: 17.283|tagging_loss_image: 4.896|tagging_loss_fusion: 4.313|total_loss: 41.149 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 6358 | tagging_loss_video: 5.693|tagging_loss_audio: 8.360|tagging_loss_text: 17.385|tagging_loss_image: 6.103|tagging_loss_fusion: 4.641|total_loss: 42.181 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 6359 | tagging_loss_video: 4.662|tagging_loss_audio: 8.563|tagging_loss_text: 16.887|tagging_loss_image: 5.842|tagging_loss_fusion: 3.460|total_loss: 39.414 | 64.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6360 |tagging_loss_video: 7.069|tagging_loss_audio: 9.956|tagging_loss_text: 17.876|tagging_loss_image: 5.650|tagging_loss_fusion: 5.469|total_loss: 46.020 | Examples/sec: 67.96\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6361 | tagging_loss_video: 5.396|tagging_loss_audio: 8.821|tagging_loss_text: 17.722|tagging_loss_image: 4.030|tagging_loss_fusion: 3.527|total_loss: 39.496 | 71.98 Examples/sec\n",
      "INFO:tensorflow:training step 6362 | tagging_loss_video: 5.447|tagging_loss_audio: 8.834|tagging_loss_text: 17.617|tagging_loss_image: 4.175|tagging_loss_fusion: 3.704|total_loss: 39.776 | 65.85 Examples/sec\n",
      "INFO:tensorflow:training step 6363 | tagging_loss_video: 4.937|tagging_loss_audio: 8.204|tagging_loss_text: 15.464|tagging_loss_image: 4.892|tagging_loss_fusion: 2.050|total_loss: 35.547 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 6364 | tagging_loss_video: 5.886|tagging_loss_audio: 9.069|tagging_loss_text: 14.457|tagging_loss_image: 5.679|tagging_loss_fusion: 6.428|total_loss: 41.519 | 63.49 Examples/sec\n",
      "INFO:tensorflow:training step 6365 | tagging_loss_video: 6.366|tagging_loss_audio: 9.742|tagging_loss_text: 15.126|tagging_loss_image: 5.981|tagging_loss_fusion: 5.223|total_loss: 42.438 | 72.26 Examples/sec\n",
      "INFO:tensorflow:training step 6366 | tagging_loss_video: 4.412|tagging_loss_audio: 8.911|tagging_loss_text: 16.747|tagging_loss_image: 4.522|tagging_loss_fusion: 3.695|total_loss: 38.288 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 6367 | tagging_loss_video: 6.008|tagging_loss_audio: 9.438|tagging_loss_text: 14.640|tagging_loss_image: 3.967|tagging_loss_fusion: 3.645|total_loss: 37.698 | 65.14 Examples/sec\n",
      "INFO:tensorflow:training step 6368 | tagging_loss_video: 6.782|tagging_loss_audio: 10.651|tagging_loss_text: 19.077|tagging_loss_image: 5.095|tagging_loss_fusion: 6.103|total_loss: 47.708 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 6369 | tagging_loss_video: 4.913|tagging_loss_audio: 8.134|tagging_loss_text: 17.997|tagging_loss_image: 4.322|tagging_loss_fusion: 3.386|total_loss: 38.751 | 69.16 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6370 |tagging_loss_video: 5.441|tagging_loss_audio: 8.581|tagging_loss_text: 14.649|tagging_loss_image: 4.088|tagging_loss_fusion: 3.482|total_loss: 36.241 | Examples/sec: 71.50\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6371 | tagging_loss_video: 6.022|tagging_loss_audio: 8.602|tagging_loss_text: 10.731|tagging_loss_image: 4.684|tagging_loss_fusion: 4.658|total_loss: 34.698 | 67.22 Examples/sec\n",
      "INFO:tensorflow:training step 6372 | tagging_loss_video: 3.981|tagging_loss_audio: 9.453|tagging_loss_text: 15.760|tagging_loss_image: 5.594|tagging_loss_fusion: 2.829|total_loss: 37.617 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 6373 | tagging_loss_video: 5.206|tagging_loss_audio: 8.198|tagging_loss_text: 11.976|tagging_loss_image: 4.825|tagging_loss_fusion: 3.634|total_loss: 33.839 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 6374 | tagging_loss_video: 6.970|tagging_loss_audio: 9.742|tagging_loss_text: 15.581|tagging_loss_image: 6.454|tagging_loss_fusion: 6.686|total_loss: 45.432 | 71.21 Examples/sec\n",
      "INFO:tensorflow:training step 6375 | tagging_loss_video: 6.754|tagging_loss_audio: 8.760|tagging_loss_text: 16.980|tagging_loss_image: 6.204|tagging_loss_fusion: 5.486|total_loss: 44.185 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 6376 | tagging_loss_video: 5.443|tagging_loss_audio: 8.330|tagging_loss_text: 14.421|tagging_loss_image: 5.263|tagging_loss_fusion: 3.864|total_loss: 37.321 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 6377 | tagging_loss_video: 5.483|tagging_loss_audio: 9.180|tagging_loss_text: 15.667|tagging_loss_image: 5.064|tagging_loss_fusion: 4.338|total_loss: 39.732 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 6378 | tagging_loss_video: 2.990|tagging_loss_audio: 7.712|tagging_loss_text: 12.218|tagging_loss_image: 5.066|tagging_loss_fusion: 2.215|total_loss: 30.202 | 62.90 Examples/sec\n",
      "INFO:tensorflow:training step 6379 | tagging_loss_video: 5.236|tagging_loss_audio: 9.273|tagging_loss_text: 15.413|tagging_loss_image: 4.590|tagging_loss_fusion: 2.534|total_loss: 37.045 | 69.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6380 |tagging_loss_video: 6.046|tagging_loss_audio: 8.607|tagging_loss_text: 14.868|tagging_loss_image: 4.425|tagging_loss_fusion: 4.298|total_loss: 38.243 | Examples/sec: 72.12\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6381 | tagging_loss_video: 5.179|tagging_loss_audio: 7.819|tagging_loss_text: 13.203|tagging_loss_image: 5.302|tagging_loss_fusion: 4.849|total_loss: 36.352 | 67.34 Examples/sec\n",
      "INFO:tensorflow:training step 6382 | tagging_loss_video: 4.270|tagging_loss_audio: 9.078|tagging_loss_text: 12.612|tagging_loss_image: 4.744|tagging_loss_fusion: 2.887|total_loss: 33.591 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 6383 | tagging_loss_video: 4.223|tagging_loss_audio: 8.368|tagging_loss_text: 16.841|tagging_loss_image: 4.421|tagging_loss_fusion: 2.406|total_loss: 36.259 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 6384 | tagging_loss_video: 5.284|tagging_loss_audio: 8.716|tagging_loss_text: 15.846|tagging_loss_image: 3.842|tagging_loss_fusion: 4.158|total_loss: 37.846 | 64.74 Examples/sec\n",
      "INFO:tensorflow:training step 6385 | tagging_loss_video: 5.587|tagging_loss_audio: 7.587|tagging_loss_text: 13.918|tagging_loss_image: 3.857|tagging_loss_fusion: 5.065|total_loss: 36.014 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 6386 | tagging_loss_video: 5.143|tagging_loss_audio: 7.243|tagging_loss_text: 15.357|tagging_loss_image: 4.986|tagging_loss_fusion: 3.681|total_loss: 36.410 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 6387 | tagging_loss_video: 4.145|tagging_loss_audio: 8.536|tagging_loss_text: 12.723|tagging_loss_image: 4.693|tagging_loss_fusion: 2.954|total_loss: 33.051 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 6388 | tagging_loss_video: 5.998|tagging_loss_audio: 8.591|tagging_loss_text: 17.876|tagging_loss_image: 5.635|tagging_loss_fusion: 6.557|total_loss: 44.656 | 69.08 Examples/sec\n",
      "INFO:tensorflow:training step 6389 | tagging_loss_video: 6.072|tagging_loss_audio: 8.376|tagging_loss_text: 14.230|tagging_loss_image: 5.065|tagging_loss_fusion: 5.163|total_loss: 38.906 | 64.34 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6390 |tagging_loss_video: 6.360|tagging_loss_audio: 8.133|tagging_loss_text: 9.999|tagging_loss_image: 5.220|tagging_loss_fusion: 5.232|total_loss: 34.944 | Examples/sec: 68.90\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6391 | tagging_loss_video: 4.617|tagging_loss_audio: 7.509|tagging_loss_text: 13.943|tagging_loss_image: 4.792|tagging_loss_fusion: 2.920|total_loss: 33.780 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 6392 | tagging_loss_video: 5.230|tagging_loss_audio: 8.108|tagging_loss_text: 16.888|tagging_loss_image: 5.251|tagging_loss_fusion: 4.305|total_loss: 39.782 | 60.48 Examples/sec\n",
      "INFO:tensorflow:training step 6393 | tagging_loss_video: 4.836|tagging_loss_audio: 9.141|tagging_loss_text: 16.018|tagging_loss_image: 5.645|tagging_loss_fusion: 3.031|total_loss: 38.670 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 6394 | tagging_loss_video: 6.128|tagging_loss_audio: 8.268|tagging_loss_text: 14.426|tagging_loss_image: 6.430|tagging_loss_fusion: 5.309|total_loss: 40.561 | 67.87 Examples/sec\n",
      "INFO:tensorflow:training step 6395 | tagging_loss_video: 5.026|tagging_loss_audio: 8.404|tagging_loss_text: 16.813|tagging_loss_image: 4.007|tagging_loss_fusion: 3.403|total_loss: 37.654 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 6396 | tagging_loss_video: 6.018|tagging_loss_audio: 8.578|tagging_loss_text: 15.056|tagging_loss_image: 6.203|tagging_loss_fusion: 5.452|total_loss: 41.307 | 70.08 Examples/sec\n",
      "INFO:tensorflow:training step 6397 | tagging_loss_video: 5.484|tagging_loss_audio: 9.501|tagging_loss_text: 14.975|tagging_loss_image: 5.708|tagging_loss_fusion: 4.854|total_loss: 40.522 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 6398 | tagging_loss_video: 5.950|tagging_loss_audio: 8.139|tagging_loss_text: 17.367|tagging_loss_image: 5.156|tagging_loss_fusion: 4.396|total_loss: 41.008 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6399 | tagging_loss_video: 5.863|tagging_loss_audio: 9.111|tagging_loss_text: 15.790|tagging_loss_image: 4.521|tagging_loss_fusion: 2.855|total_loss: 38.141 | 67.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6400 |tagging_loss_video: 6.037|tagging_loss_audio: 8.761|tagging_loss_text: 13.196|tagging_loss_image: 4.811|tagging_loss_fusion: 5.158|total_loss: 37.963 | Examples/sec: 71.68\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.83 | precision@0.5: 0.93 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6401 | tagging_loss_video: 5.310|tagging_loss_audio: 9.014|tagging_loss_text: 16.428|tagging_loss_image: 5.809|tagging_loss_fusion: 5.292|total_loss: 41.854 | 70.29 Examples/sec\n",
      "INFO:tensorflow:training step 6402 | tagging_loss_video: 5.514|tagging_loss_audio: 9.084|tagging_loss_text: 13.966|tagging_loss_image: 5.584|tagging_loss_fusion: 4.842|total_loss: 38.990 | 69.80 Examples/sec\n",
      "INFO:tensorflow:training step 6403 | tagging_loss_video: 4.532|tagging_loss_audio: 6.838|tagging_loss_text: 14.113|tagging_loss_image: 4.568|tagging_loss_fusion: 2.731|total_loss: 32.782 | 63.12 Examples/sec\n",
      "INFO:tensorflow:training step 6404 | tagging_loss_video: 5.128|tagging_loss_audio: 8.687|tagging_loss_text: 15.559|tagging_loss_image: 4.830|tagging_loss_fusion: 3.198|total_loss: 37.402 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 6405 | tagging_loss_video: 4.422|tagging_loss_audio: 8.926|tagging_loss_text: 17.272|tagging_loss_image: 5.311|tagging_loss_fusion: 3.131|total_loss: 39.061 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 6406 | tagging_loss_video: 5.064|tagging_loss_audio: 8.612|tagging_loss_text: 14.168|tagging_loss_image: 5.123|tagging_loss_fusion: 3.573|total_loss: 36.541 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 6407 | tagging_loss_video: 5.749|tagging_loss_audio: 8.308|tagging_loss_text: 16.266|tagging_loss_image: 6.728|tagging_loss_fusion: 4.243|total_loss: 41.295 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 6408 | tagging_loss_video: 4.985|tagging_loss_audio: 8.895|tagging_loss_text: 13.349|tagging_loss_image: 4.891|tagging_loss_fusion: 3.826|total_loss: 35.946 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 6409 | tagging_loss_video: 5.870|tagging_loss_audio: 9.727|tagging_loss_text: 18.358|tagging_loss_image: 4.668|tagging_loss_fusion: 3.944|total_loss: 42.568 | 68.27 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6410 |tagging_loss_video: 6.436|tagging_loss_audio: 8.550|tagging_loss_text: 18.483|tagging_loss_image: 4.176|tagging_loss_fusion: 3.929|total_loss: 41.574 | Examples/sec: 68.97\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6411 | tagging_loss_video: 5.333|tagging_loss_audio: 8.325|tagging_loss_text: 16.147|tagging_loss_image: 5.124|tagging_loss_fusion: 3.146|total_loss: 38.075 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 6412 | tagging_loss_video: 5.729|tagging_loss_audio: 8.959|tagging_loss_text: 14.554|tagging_loss_image: 5.899|tagging_loss_fusion: 5.027|total_loss: 40.168 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 6413 | tagging_loss_video: 5.532|tagging_loss_audio: 8.425|tagging_loss_text: 12.034|tagging_loss_image: 4.276|tagging_loss_fusion: 2.970|total_loss: 33.237 | 72.27 Examples/sec\n",
      "INFO:tensorflow:training step 6414 | tagging_loss_video: 4.704|tagging_loss_audio: 8.203|tagging_loss_text: 14.994|tagging_loss_image: 5.445|tagging_loss_fusion: 3.933|total_loss: 37.279 | 62.53 Examples/sec\n",
      "INFO:tensorflow:training step 6415 | tagging_loss_video: 6.652|tagging_loss_audio: 8.571|tagging_loss_text: 13.411|tagging_loss_image: 3.712|tagging_loss_fusion: 4.096|total_loss: 36.442 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 6416 | tagging_loss_video: 5.804|tagging_loss_audio: 7.909|tagging_loss_text: 12.914|tagging_loss_image: 4.952|tagging_loss_fusion: 4.628|total_loss: 36.207 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6417 | tagging_loss_video: 4.467|tagging_loss_audio: 8.184|tagging_loss_text: 15.617|tagging_loss_image: 4.913|tagging_loss_fusion: 3.383|total_loss: 36.565 | 61.57 Examples/sec\n",
      "INFO:tensorflow:training step 6418 | tagging_loss_video: 6.201|tagging_loss_audio: 8.910|tagging_loss_text: 13.880|tagging_loss_image: 5.407|tagging_loss_fusion: 5.730|total_loss: 40.128 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 6419 | tagging_loss_video: 5.821|tagging_loss_audio: 8.170|tagging_loss_text: 17.215|tagging_loss_image: 4.461|tagging_loss_fusion: 4.732|total_loss: 40.399 | 70.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6420 |tagging_loss_video: 5.931|tagging_loss_audio: 7.601|tagging_loss_text: 14.656|tagging_loss_image: 5.399|tagging_loss_fusion: 5.522|total_loss: 39.110 | Examples/sec: 61.97\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.80 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6421 | tagging_loss_video: 5.304|tagging_loss_audio: 8.282|tagging_loss_text: 16.996|tagging_loss_image: 3.818|tagging_loss_fusion: 3.155|total_loss: 37.555 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 6422 | tagging_loss_video: 4.855|tagging_loss_audio: 7.930|tagging_loss_text: 13.936|tagging_loss_image: 5.646|tagging_loss_fusion: 4.110|total_loss: 36.477 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 6423 | tagging_loss_video: 5.168|tagging_loss_audio: 8.073|tagging_loss_text: 14.748|tagging_loss_image: 5.085|tagging_loss_fusion: 3.824|total_loss: 36.898 | 68.85 Examples/sec\n",
      "INFO:tensorflow:training step 6424 | tagging_loss_video: 5.916|tagging_loss_audio: 7.956|tagging_loss_text: 14.822|tagging_loss_image: 6.000|tagging_loss_fusion: 5.891|total_loss: 40.585 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 6425 | tagging_loss_video: 5.895|tagging_loss_audio: 8.202|tagging_loss_text: 17.511|tagging_loss_image: 5.270|tagging_loss_fusion: 5.067|total_loss: 41.945 | 69.27 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6426 | tagging_loss_video: 5.648|tagging_loss_audio: 7.718|tagging_loss_text: 14.823|tagging_loss_image: 4.741|tagging_loss_fusion: 4.377|total_loss: 37.307 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 6427 | tagging_loss_video: 6.033|tagging_loss_audio: 7.727|tagging_loss_text: 16.717|tagging_loss_image: 5.267|tagging_loss_fusion: 5.726|total_loss: 41.471 | 69.74 Examples/sec\n",
      "INFO:tensorflow:training step 6428 | tagging_loss_video: 6.521|tagging_loss_audio: 9.636|tagging_loss_text: 14.528|tagging_loss_image: 6.084|tagging_loss_fusion: 7.643|total_loss: 44.413 | 70.42 Examples/sec\n",
      "INFO:tensorflow:training step 6429 | tagging_loss_video: 4.912|tagging_loss_audio: 8.475|tagging_loss_text: 15.976|tagging_loss_image: 5.389|tagging_loss_fusion: 3.132|total_loss: 37.884 | 66.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6430 |tagging_loss_video: 4.479|tagging_loss_audio: 8.782|tagging_loss_text: 10.697|tagging_loss_image: 5.635|tagging_loss_fusion: 3.420|total_loss: 33.013 | Examples/sec: 66.27\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.90 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6431 | tagging_loss_video: 6.352|tagging_loss_audio: 7.580|tagging_loss_text: 19.749|tagging_loss_image: 6.353|tagging_loss_fusion: 5.546|total_loss: 45.581 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 6432 | tagging_loss_video: 4.552|tagging_loss_audio: 6.891|tagging_loss_text: 16.033|tagging_loss_image: 2.745|tagging_loss_fusion: 2.717|total_loss: 32.938 | 70.91 Examples/sec\n",
      "INFO:tensorflow:training step 6433 | tagging_loss_video: 6.745|tagging_loss_audio: 7.851|tagging_loss_text: 14.406|tagging_loss_image: 6.006|tagging_loss_fusion: 7.498|total_loss: 42.507 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 6434 | tagging_loss_video: 5.403|tagging_loss_audio: 7.707|tagging_loss_text: 14.763|tagging_loss_image: 4.312|tagging_loss_fusion: 3.447|total_loss: 35.632 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 6435 | tagging_loss_video: 5.007|tagging_loss_audio: 8.385|tagging_loss_text: 15.370|tagging_loss_image: 5.212|tagging_loss_fusion: 2.905|total_loss: 36.880 | 67.86 Examples/sec\n",
      "INFO:tensorflow:training step 6436 | tagging_loss_video: 4.485|tagging_loss_audio: 8.339|tagging_loss_text: 11.711|tagging_loss_image: 5.537|tagging_loss_fusion: 2.938|total_loss: 33.010 | 69.70 Examples/sec\n",
      "INFO:tensorflow:training step 6437 | tagging_loss_video: 6.106|tagging_loss_audio: 9.629|tagging_loss_text: 16.344|tagging_loss_image: 4.556|tagging_loss_fusion: 4.619|total_loss: 41.255 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 6438 | tagging_loss_video: 5.435|tagging_loss_audio: 7.924|tagging_loss_text: 17.806|tagging_loss_image: 6.322|tagging_loss_fusion: 4.135|total_loss: 41.622 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 6439 | tagging_loss_video: 5.833|tagging_loss_audio: 8.516|tagging_loss_text: 18.111|tagging_loss_image: 4.635|tagging_loss_fusion: 3.422|total_loss: 40.517 | 63.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6440 |tagging_loss_video: 6.040|tagging_loss_audio: 8.204|tagging_loss_text: 15.463|tagging_loss_image: 4.597|tagging_loss_fusion: 5.389|total_loss: 39.692 | Examples/sec: 70.10\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.80 | precision@0.5: 0.90 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6441 | tagging_loss_video: 6.766|tagging_loss_audio: 9.069|tagging_loss_text: 20.511|tagging_loss_image: 5.796|tagging_loss_fusion: 6.468|total_loss: 48.610 | 70.06 Examples/sec\n",
      "INFO:tensorflow:training step 6442 | tagging_loss_video: 4.893|tagging_loss_audio: 7.572|tagging_loss_text: 13.201|tagging_loss_image: 5.339|tagging_loss_fusion: 3.339|total_loss: 34.344 | 64.03 Examples/sec\n",
      "INFO:tensorflow:training step 6443 | tagging_loss_video: 4.740|tagging_loss_audio: 7.908|tagging_loss_text: 13.149|tagging_loss_image: 5.143|tagging_loss_fusion: 3.212|total_loss: 34.153 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 6444 | tagging_loss_video: 6.129|tagging_loss_audio: 8.345|tagging_loss_text: 17.460|tagging_loss_image: 4.895|tagging_loss_fusion: 4.609|total_loss: 41.438 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 6445 | tagging_loss_video: 5.372|tagging_loss_audio: 7.658|tagging_loss_text: 10.943|tagging_loss_image: 4.804|tagging_loss_fusion: 4.646|total_loss: 33.423 | 67.10 Examples/sec\n",
      "INFO:tensorflow:training step 6446 | tagging_loss_video: 5.409|tagging_loss_audio: 8.371|tagging_loss_text: 16.818|tagging_loss_image: 4.942|tagging_loss_fusion: 3.651|total_loss: 39.191 | 68.67 Examples/sec\n",
      "INFO:tensorflow:training step 6447 | tagging_loss_video: 5.366|tagging_loss_audio: 8.100|tagging_loss_text: 13.799|tagging_loss_image: 4.346|tagging_loss_fusion: 4.883|total_loss: 36.495 | 68.26 Examples/sec\n",
      "INFO:tensorflow:training step 6448 | tagging_loss_video: 3.921|tagging_loss_audio: 8.335|tagging_loss_text: 12.631|tagging_loss_image: 5.189|tagging_loss_fusion: 3.044|total_loss: 33.120 | 68.18 Examples/sec\n",
      "INFO:tensorflow:training step 6449 | tagging_loss_video: 5.380|tagging_loss_audio: 7.453|tagging_loss_text: 14.653|tagging_loss_image: 4.108|tagging_loss_fusion: 2.776|total_loss: 34.370 | 68.39 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6450 |tagging_loss_video: 4.778|tagging_loss_audio: 8.072|tagging_loss_text: 14.731|tagging_loss_image: 5.209|tagging_loss_fusion: 3.577|total_loss: 36.367 | Examples/sec: 65.23\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6451 | tagging_loss_video: 5.499|tagging_loss_audio: 8.154|tagging_loss_text: 15.746|tagging_loss_image: 5.679|tagging_loss_fusion: 5.647|total_loss: 40.726 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 6452 | tagging_loss_video: 3.250|tagging_loss_audio: 7.852|tagging_loss_text: 13.642|tagging_loss_image: 4.265|tagging_loss_fusion: 1.541|total_loss: 30.550 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 6453 | tagging_loss_video: 5.096|tagging_loss_audio: 7.394|tagging_loss_text: 14.882|tagging_loss_image: 4.726|tagging_loss_fusion: 3.839|total_loss: 35.937 | 59.23 Examples/sec\n",
      "INFO:tensorflow:training step 6454 | tagging_loss_video: 6.219|tagging_loss_audio: 7.487|tagging_loss_text: 17.262|tagging_loss_image: 5.530|tagging_loss_fusion: 4.973|total_loss: 41.471 | 66.31 Examples/sec\n",
      "INFO:tensorflow:training step 6455 | tagging_loss_video: 6.554|tagging_loss_audio: 8.312|tagging_loss_text: 18.882|tagging_loss_image: 4.993|tagging_loss_fusion: 5.638|total_loss: 44.379 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 6456 | tagging_loss_video: 4.834|tagging_loss_audio: 7.642|tagging_loss_text: 14.799|tagging_loss_image: 5.488|tagging_loss_fusion: 2.913|total_loss: 35.676 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 6457 | tagging_loss_video: 5.119|tagging_loss_audio: 7.890|tagging_loss_text: 16.756|tagging_loss_image: 5.135|tagging_loss_fusion: 3.532|total_loss: 38.432 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 6458 | tagging_loss_video: 5.246|tagging_loss_audio: 7.638|tagging_loss_text: 13.773|tagging_loss_image: 5.202|tagging_loss_fusion: 4.778|total_loss: 36.638 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 6459 | tagging_loss_video: 4.441|tagging_loss_audio: 7.256|tagging_loss_text: 18.246|tagging_loss_image: 4.191|tagging_loss_fusion: 2.411|total_loss: 36.545 | 66.59 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6460 |tagging_loss_video: 6.041|tagging_loss_audio: 7.273|tagging_loss_text: 14.999|tagging_loss_image: 5.547|tagging_loss_fusion: 5.279|total_loss: 39.139 | Examples/sec: 69.45\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.78 | precision@0.5: 0.90 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6461 | tagging_loss_video: 6.296|tagging_loss_audio: 7.809|tagging_loss_text: 15.862|tagging_loss_image: 5.670|tagging_loss_fusion: 7.892|total_loss: 43.529 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 6462 | tagging_loss_video: 5.156|tagging_loss_audio: 8.415|tagging_loss_text: 14.594|tagging_loss_image: 4.840|tagging_loss_fusion: 3.810|total_loss: 36.814 | 72.03 Examples/sec\n",
      "INFO:tensorflow:training step 6463 | tagging_loss_video: 4.920|tagging_loss_audio: 8.127|tagging_loss_text: 15.405|tagging_loss_image: 3.969|tagging_loss_fusion: 3.411|total_loss: 35.832 | 70.86 Examples/sec\n",
      "INFO:tensorflow:training step 6464 | tagging_loss_video: 5.178|tagging_loss_audio: 8.223|tagging_loss_text: 15.978|tagging_loss_image: 5.375|tagging_loss_fusion: 2.580|total_loss: 37.334 | 63.14 Examples/sec\n",
      "INFO:tensorflow:training step 6465 | tagging_loss_video: 6.159|tagging_loss_audio: 8.717|tagging_loss_text: 13.339|tagging_loss_image: 4.895|tagging_loss_fusion: 3.771|total_loss: 36.881 | 62.03 Examples/sec\n",
      "INFO:tensorflow:training step 6466 | tagging_loss_video: 6.397|tagging_loss_audio: 9.321|tagging_loss_text: 15.533|tagging_loss_image: 4.965|tagging_loss_fusion: 4.727|total_loss: 40.943 | 68.85 Examples/sec\n",
      "INFO:tensorflow:training step 6467 | tagging_loss_video: 6.320|tagging_loss_audio: 8.707|tagging_loss_text: 16.427|tagging_loss_image: 3.746|tagging_loss_fusion: 3.376|total_loss: 38.575 | 68.77 Examples/sec\n",
      "INFO:tensorflow:training step 6468 | tagging_loss_video: 4.769|tagging_loss_audio: 7.922|tagging_loss_text: 15.839|tagging_loss_image: 4.029|tagging_loss_fusion: 2.204|total_loss: 34.763 | 62.89 Examples/sec\n",
      "INFO:tensorflow:training step 6469 | tagging_loss_video: 5.069|tagging_loss_audio: 7.190|tagging_loss_text: 11.868|tagging_loss_image: 4.799|tagging_loss_fusion: 5.310|total_loss: 34.236 | 71.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6470 |tagging_loss_video: 4.294|tagging_loss_audio: 8.289|tagging_loss_text: 14.673|tagging_loss_image: 5.261|tagging_loss_fusion: 3.331|total_loss: 35.848 | Examples/sec: 67.56\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6471 | tagging_loss_video: 4.877|tagging_loss_audio: 7.912|tagging_loss_text: 14.202|tagging_loss_image: 3.244|tagging_loss_fusion: 3.485|total_loss: 33.721 | 68.87 Examples/sec\n",
      "INFO:tensorflow:training step 6472 | tagging_loss_video: 5.344|tagging_loss_audio: 8.569|tagging_loss_text: 15.177|tagging_loss_image: 3.784|tagging_loss_fusion: 3.551|total_loss: 36.427 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 6473 | tagging_loss_video: 5.067|tagging_loss_audio: 8.237|tagging_loss_text: 13.819|tagging_loss_image: 5.051|tagging_loss_fusion: 3.989|total_loss: 36.163 | 68.88 Examples/sec\n",
      "INFO:tensorflow:training step 6474 | tagging_loss_video: 5.805|tagging_loss_audio: 7.684|tagging_loss_text: 14.122|tagging_loss_image: 3.852|tagging_loss_fusion: 4.397|total_loss: 35.861 | 66.35 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 6475.\n",
      "INFO:tensorflow:training step 6475 | tagging_loss_video: 5.005|tagging_loss_audio: 8.329|tagging_loss_text: 14.576|tagging_loss_image: 5.417|tagging_loss_fusion: 3.951|total_loss: 37.278 | 50.79 Examples/sec\n",
      "INFO:tensorflow:training step 6476 | tagging_loss_video: 5.336|tagging_loss_audio: 7.525|tagging_loss_text: 16.786|tagging_loss_image: 4.981|tagging_loss_fusion: 3.013|total_loss: 37.639 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 6477 | tagging_loss_video: 5.908|tagging_loss_audio: 9.606|tagging_loss_text: 15.556|tagging_loss_image: 4.580|tagging_loss_fusion: 4.403|total_loss: 40.053 | 61.57 Examples/sec\n",
      "INFO:tensorflow:training step 6478 | tagging_loss_video: 5.384|tagging_loss_audio: 7.636|tagging_loss_text: 14.385|tagging_loss_image: 3.856|tagging_loss_fusion: 3.602|total_loss: 34.863 | 62.85 Examples/sec\n",
      "INFO:tensorflow:training step 6479 | tagging_loss_video: 5.628|tagging_loss_audio: 7.456|tagging_loss_text: 13.353|tagging_loss_image: 3.107|tagging_loss_fusion: 3.686|total_loss: 33.229 | 69.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6480 |tagging_loss_video: 4.548|tagging_loss_audio: 7.862|tagging_loss_text: 14.955|tagging_loss_image: 4.747|tagging_loss_fusion: 2.823|total_loss: 34.935 | Examples/sec: 67.68\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6481 | tagging_loss_video: 2.992|tagging_loss_audio: 7.923|tagging_loss_text: 13.633|tagging_loss_image: 2.542|tagging_loss_fusion: 1.614|total_loss: 28.704 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 6482 | tagging_loss_video: 4.802|tagging_loss_audio: 8.699|tagging_loss_text: 17.867|tagging_loss_image: 5.387|tagging_loss_fusion: 3.150|total_loss: 39.905 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 6483 | tagging_loss_video: 5.124|tagging_loss_audio: 8.180|tagging_loss_text: 15.554|tagging_loss_image: 3.759|tagging_loss_fusion: 3.917|total_loss: 36.533 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6484 | tagging_loss_video: 5.622|tagging_loss_audio: 9.186|tagging_loss_text: 13.254|tagging_loss_image: 5.658|tagging_loss_fusion: 5.942|total_loss: 39.662 | 63.24 Examples/sec\n",
      "INFO:tensorflow:training step 6485 | tagging_loss_video: 5.467|tagging_loss_audio: 7.189|tagging_loss_text: 15.445|tagging_loss_image: 5.639|tagging_loss_fusion: 4.284|total_loss: 38.023 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 6486 | tagging_loss_video: 4.156|tagging_loss_audio: 8.121|tagging_loss_text: 15.619|tagging_loss_image: 5.206|tagging_loss_fusion: 3.195|total_loss: 36.297 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 6487 | tagging_loss_video: 5.597|tagging_loss_audio: 8.667|tagging_loss_text: 17.174|tagging_loss_image: 5.980|tagging_loss_fusion: 3.935|total_loss: 41.353 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 6488 | tagging_loss_video: 5.171|tagging_loss_audio: 8.776|tagging_loss_text: 18.129|tagging_loss_image: 4.164|tagging_loss_fusion: 2.809|total_loss: 39.048 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 6489 | tagging_loss_video: 6.019|tagging_loss_audio: 7.731|tagging_loss_text: 14.972|tagging_loss_image: 4.306|tagging_loss_fusion: 3.545|total_loss: 36.572 | 65.12 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6490 |tagging_loss_video: 6.338|tagging_loss_audio: 9.785|tagging_loss_text: 16.632|tagging_loss_image: 4.800|tagging_loss_fusion: 5.546|total_loss: 43.102 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6491 | tagging_loss_video: 5.312|tagging_loss_audio: 7.531|tagging_loss_text: 11.870|tagging_loss_image: 5.376|tagging_loss_fusion: 4.279|total_loss: 34.369 | 71.08 Examples/sec\n",
      "INFO:tensorflow:training step 6492 | tagging_loss_video: 5.974|tagging_loss_audio: 7.809|tagging_loss_text: 15.816|tagging_loss_image: 6.453|tagging_loss_fusion: 7.063|total_loss: 43.115 | 62.07 Examples/sec\n",
      "INFO:tensorflow:training step 6493 | tagging_loss_video: 6.002|tagging_loss_audio: 9.213|tagging_loss_text: 18.591|tagging_loss_image: 6.768|tagging_loss_fusion: 5.887|total_loss: 46.460 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 6494 | tagging_loss_video: 4.558|tagging_loss_audio: 8.358|tagging_loss_text: 15.604|tagging_loss_image: 5.085|tagging_loss_fusion: 3.023|total_loss: 36.629 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 6495 | tagging_loss_video: 5.900|tagging_loss_audio: 8.852|tagging_loss_text: 16.635|tagging_loss_image: 5.560|tagging_loss_fusion: 4.856|total_loss: 41.803 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 6496 | tagging_loss_video: 6.058|tagging_loss_audio: 8.897|tagging_loss_text: 14.524|tagging_loss_image: 5.512|tagging_loss_fusion: 4.787|total_loss: 39.778 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 6497 | tagging_loss_video: 6.320|tagging_loss_audio: 9.927|tagging_loss_text: 14.500|tagging_loss_image: 4.473|tagging_loss_fusion: 4.771|total_loss: 39.990 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 6498 | tagging_loss_video: 5.706|tagging_loss_audio: 8.644|tagging_loss_text: 19.000|tagging_loss_image: 5.424|tagging_loss_fusion: 5.514|total_loss: 44.288 | 66.03 Examples/sec\n",
      "INFO:tensorflow:training step 6499 | tagging_loss_video: 6.784|tagging_loss_audio: 9.011|tagging_loss_text: 14.896|tagging_loss_image: 5.585|tagging_loss_fusion: 5.648|total_loss: 41.924 | 70.70 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6500 |tagging_loss_video: 5.626|tagging_loss_audio: 9.165|tagging_loss_text: 12.460|tagging_loss_image: 4.992|tagging_loss_fusion: 6.154|total_loss: 38.397 | Examples/sec: 71.88\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 6501 | tagging_loss_video: 6.803|tagging_loss_audio: 8.243|tagging_loss_text: 16.208|tagging_loss_image: 5.491|tagging_loss_fusion: 5.299|total_loss: 42.043 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 6502 | tagging_loss_video: 4.646|tagging_loss_audio: 7.589|tagging_loss_text: 12.982|tagging_loss_image: 4.805|tagging_loss_fusion: 3.548|total_loss: 33.570 | 67.12 Examples/sec\n",
      "INFO:tensorflow:training step 6503 | tagging_loss_video: 5.672|tagging_loss_audio: 10.037|tagging_loss_text: 16.629|tagging_loss_image: 5.934|tagging_loss_fusion: 4.928|total_loss: 43.200 | 63.02 Examples/sec\n",
      "INFO:tensorflow:training step 6504 | tagging_loss_video: 5.984|tagging_loss_audio: 9.469|tagging_loss_text: 16.560|tagging_loss_image: 5.649|tagging_loss_fusion: 4.887|total_loss: 42.549 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 6505 | tagging_loss_video: 6.858|tagging_loss_audio: 8.384|tagging_loss_text: 21.852|tagging_loss_image: 6.122|tagging_loss_fusion: 6.900|total_loss: 50.114 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 6506 | tagging_loss_video: 5.352|tagging_loss_audio: 9.555|tagging_loss_text: 16.710|tagging_loss_image: 6.585|tagging_loss_fusion: 4.130|total_loss: 42.332 | 64.48 Examples/sec\n",
      "INFO:tensorflow:training step 6507 | tagging_loss_video: 4.994|tagging_loss_audio: 10.114|tagging_loss_text: 12.214|tagging_loss_image: 6.162|tagging_loss_fusion: 3.229|total_loss: 36.714 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 6508 | tagging_loss_video: 6.279|tagging_loss_audio: 8.197|tagging_loss_text: 16.583|tagging_loss_image: 5.598|tagging_loss_fusion: 4.597|total_loss: 41.253 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 6509 | tagging_loss_video: 4.776|tagging_loss_audio: 8.707|tagging_loss_text: 13.300|tagging_loss_image: 4.402|tagging_loss_fusion: 2.867|total_loss: 34.052 | 64.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6510 |tagging_loss_video: 4.777|tagging_loss_audio: 9.072|tagging_loss_text: 10.727|tagging_loss_image: 3.738|tagging_loss_fusion: 3.389|total_loss: 31.703 | Examples/sec: 70.22\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.98 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6511 | tagging_loss_video: 5.934|tagging_loss_audio: 9.319|tagging_loss_text: 13.016|tagging_loss_image: 4.282|tagging_loss_fusion: 4.338|total_loss: 36.890 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 6512 | tagging_loss_video: 4.846|tagging_loss_audio: 8.427|tagging_loss_text: 15.303|tagging_loss_image: 5.163|tagging_loss_fusion: 3.696|total_loss: 37.435 | 72.20 Examples/sec\n",
      "INFO:tensorflow:training step 6513 | tagging_loss_video: 4.698|tagging_loss_audio: 10.342|tagging_loss_text: 15.569|tagging_loss_image: 6.653|tagging_loss_fusion: 3.663|total_loss: 40.926 | 66.19 Examples/sec\n",
      "INFO:tensorflow:training step 6514 | tagging_loss_video: 5.006|tagging_loss_audio: 9.762|tagging_loss_text: 17.749|tagging_loss_image: 6.747|tagging_loss_fusion: 6.126|total_loss: 45.392 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 6515 | tagging_loss_video: 6.191|tagging_loss_audio: 7.418|tagging_loss_text: 15.082|tagging_loss_image: 4.806|tagging_loss_fusion: 5.948|total_loss: 39.445 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 6516 | tagging_loss_video: 5.240|tagging_loss_audio: 8.532|tagging_loss_text: 14.971|tagging_loss_image: 4.368|tagging_loss_fusion: 4.146|total_loss: 37.257 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 6517 | tagging_loss_video: 6.115|tagging_loss_audio: 7.738|tagging_loss_text: 12.862|tagging_loss_image: 3.998|tagging_loss_fusion: 4.499|total_loss: 35.211 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 6518 | tagging_loss_video: 5.814|tagging_loss_audio: 9.547|tagging_loss_text: 16.855|tagging_loss_image: 6.579|tagging_loss_fusion: 4.472|total_loss: 43.268 | 67.90 Examples/sec\n",
      "INFO:tensorflow:training step 6519 | tagging_loss_video: 5.776|tagging_loss_audio: 7.876|tagging_loss_text: 16.241|tagging_loss_image: 5.614|tagging_loss_fusion: 5.277|total_loss: 40.784 | 69.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6520 |tagging_loss_video: 5.858|tagging_loss_audio: 9.377|tagging_loss_text: 13.868|tagging_loss_image: 4.702|tagging_loss_fusion: 3.425|total_loss: 37.230 | Examples/sec: 66.15\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6521 | tagging_loss_video: 4.762|tagging_loss_audio: 8.025|tagging_loss_text: 13.928|tagging_loss_image: 4.252|tagging_loss_fusion: 3.043|total_loss: 34.011 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 6522 | tagging_loss_video: 5.103|tagging_loss_audio: 9.284|tagging_loss_text: 15.508|tagging_loss_image: 5.122|tagging_loss_fusion: 3.398|total_loss: 38.414 | 66.95 Examples/sec\n",
      "INFO:tensorflow:training step 6523 | tagging_loss_video: 4.814|tagging_loss_audio: 9.101|tagging_loss_text: 12.539|tagging_loss_image: 5.734|tagging_loss_fusion: 2.727|total_loss: 34.916 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 6524 | tagging_loss_video: 5.791|tagging_loss_audio: 7.773|tagging_loss_text: 13.303|tagging_loss_image: 4.738|tagging_loss_fusion: 4.369|total_loss: 35.974 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6525 | tagging_loss_video: 5.955|tagging_loss_audio: 8.836|tagging_loss_text: 13.265|tagging_loss_image: 5.135|tagging_loss_fusion: 5.822|total_loss: 39.013 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 6526 | tagging_loss_video: 6.182|tagging_loss_audio: 9.186|tagging_loss_text: 16.054|tagging_loss_image: 3.191|tagging_loss_fusion: 3.735|total_loss: 38.348 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 6527 | tagging_loss_video: 5.457|tagging_loss_audio: 7.852|tagging_loss_text: 13.219|tagging_loss_image: 5.670|tagging_loss_fusion: 4.227|total_loss: 36.424 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 6528 | tagging_loss_video: 5.794|tagging_loss_audio: 7.848|tagging_loss_text: 16.181|tagging_loss_image: 5.301|tagging_loss_fusion: 4.906|total_loss: 40.030 | 60.50 Examples/sec\n",
      "INFO:tensorflow:training step 6529 | tagging_loss_video: 5.566|tagging_loss_audio: 8.666|tagging_loss_text: 16.030|tagging_loss_image: 4.753|tagging_loss_fusion: 5.720|total_loss: 40.736 | 72.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6530 |tagging_loss_video: 4.867|tagging_loss_audio: 7.543|tagging_loss_text: 15.923|tagging_loss_image: 4.238|tagging_loss_fusion: 2.781|total_loss: 35.351 | Examples/sec: 69.04\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6531 | tagging_loss_video: 5.875|tagging_loss_audio: 8.451|tagging_loss_text: 12.855|tagging_loss_image: 4.295|tagging_loss_fusion: 4.464|total_loss: 35.940 | 66.61 Examples/sec\n",
      "INFO:tensorflow:training step 6532 | tagging_loss_video: 6.031|tagging_loss_audio: 9.136|tagging_loss_text: 18.101|tagging_loss_image: 5.189|tagging_loss_fusion: 3.773|total_loss: 42.231 | 72.32 Examples/sec\n",
      "INFO:tensorflow:training step 6533 | tagging_loss_video: 6.332|tagging_loss_audio: 8.050|tagging_loss_text: 18.248|tagging_loss_image: 5.212|tagging_loss_fusion: 3.964|total_loss: 41.806 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 6534 | tagging_loss_video: 5.722|tagging_loss_audio: 9.080|tagging_loss_text: 13.436|tagging_loss_image: 4.126|tagging_loss_fusion: 4.730|total_loss: 37.094 | 67.79 Examples/sec\n",
      "INFO:tensorflow:training step 6535 | tagging_loss_video: 4.868|tagging_loss_audio: 7.937|tagging_loss_text: 17.115|tagging_loss_image: 5.716|tagging_loss_fusion: 2.603|total_loss: 38.239 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 6536 | tagging_loss_video: 4.446|tagging_loss_audio: 10.415|tagging_loss_text: 18.545|tagging_loss_image: 6.038|tagging_loss_fusion: 2.985|total_loss: 42.429 | 66.14 Examples/sec\n",
      "INFO:tensorflow:training step 6537 | tagging_loss_video: 5.805|tagging_loss_audio: 7.938|tagging_loss_text: 16.017|tagging_loss_image: 5.375|tagging_loss_fusion: 4.897|total_loss: 40.032 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 6538 | tagging_loss_video: 6.225|tagging_loss_audio: 8.688|tagging_loss_text: 16.484|tagging_loss_image: 5.812|tagging_loss_fusion: 3.788|total_loss: 40.997 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6539 | tagging_loss_video: 4.968|tagging_loss_audio: 7.588|tagging_loss_text: 15.837|tagging_loss_image: 3.374|tagging_loss_fusion: 1.875|total_loss: 33.642 | 69.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6540 |tagging_loss_video: 5.316|tagging_loss_audio: 8.853|tagging_loss_text: 13.033|tagging_loss_image: 5.291|tagging_loss_fusion: 3.755|total_loss: 36.247 | Examples/sec: 69.08\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6541 | tagging_loss_video: 4.399|tagging_loss_audio: 7.878|tagging_loss_text: 16.045|tagging_loss_image: 4.180|tagging_loss_fusion: 2.726|total_loss: 35.228 | 67.62 Examples/sec\n",
      "INFO:tensorflow:training step 6542 | tagging_loss_video: 4.279|tagging_loss_audio: 7.509|tagging_loss_text: 13.503|tagging_loss_image: 3.941|tagging_loss_fusion: 2.540|total_loss: 31.772 | 72.01 Examples/sec\n",
      "INFO:tensorflow:training step 6543 | tagging_loss_video: 4.421|tagging_loss_audio: 8.348|tagging_loss_text: 16.225|tagging_loss_image: 4.526|tagging_loss_fusion: 1.710|total_loss: 35.230 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 6544 | tagging_loss_video: 5.379|tagging_loss_audio: 8.207|tagging_loss_text: 12.444|tagging_loss_image: 3.657|tagging_loss_fusion: 3.409|total_loss: 33.095 | 67.26 Examples/sec\n",
      "INFO:tensorflow:training step 6545 | tagging_loss_video: 5.837|tagging_loss_audio: 8.684|tagging_loss_text: 16.208|tagging_loss_image: 5.448|tagging_loss_fusion: 6.307|total_loss: 42.484 | 67.07 Examples/sec\n",
      "INFO:tensorflow:training step 6546 | tagging_loss_video: 5.003|tagging_loss_audio: 8.948|tagging_loss_text: 19.236|tagging_loss_image: 5.943|tagging_loss_fusion: 4.209|total_loss: 43.340 | 68.12 Examples/sec\n",
      "INFO:tensorflow:training step 6547 | tagging_loss_video: 5.318|tagging_loss_audio: 9.970|tagging_loss_text: 14.025|tagging_loss_image: 3.847|tagging_loss_fusion: 3.320|total_loss: 36.481 | 71.32 Examples/sec\n",
      "INFO:tensorflow:training step 6548 | tagging_loss_video: 6.017|tagging_loss_audio: 8.951|tagging_loss_text: 16.340|tagging_loss_image: 3.442|tagging_loss_fusion: 2.781|total_loss: 37.531 | 61.81 Examples/sec\n",
      "INFO:tensorflow:training step 6549 | tagging_loss_video: 6.036|tagging_loss_audio: 8.250|tagging_loss_text: 17.074|tagging_loss_image: 3.952|tagging_loss_fusion: 3.825|total_loss: 39.137 | 70.02 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6550 |tagging_loss_video: 5.693|tagging_loss_audio: 9.165|tagging_loss_text: 15.484|tagging_loss_image: 5.570|tagging_loss_fusion: 3.689|total_loss: 39.600 | Examples/sec: 67.33\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6551 | tagging_loss_video: 4.524|tagging_loss_audio: 8.746|tagging_loss_text: 16.630|tagging_loss_image: 5.396|tagging_loss_fusion: 3.388|total_loss: 38.685 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 6552 | tagging_loss_video: 5.686|tagging_loss_audio: 7.438|tagging_loss_text: 18.046|tagging_loss_image: 4.083|tagging_loss_fusion: 4.715|total_loss: 39.969 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 6553 | tagging_loss_video: 5.427|tagging_loss_audio: 8.327|tagging_loss_text: 17.046|tagging_loss_image: 5.434|tagging_loss_fusion: 3.768|total_loss: 40.002 | 65.61 Examples/sec\n",
      "INFO:tensorflow:training step 6554 | tagging_loss_video: 4.827|tagging_loss_audio: 8.696|tagging_loss_text: 12.732|tagging_loss_image: 4.672|tagging_loss_fusion: 3.135|total_loss: 34.063 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 6555 | tagging_loss_video: 6.385|tagging_loss_audio: 8.091|tagging_loss_text: 14.967|tagging_loss_image: 4.770|tagging_loss_fusion: 5.047|total_loss: 39.260 | 70.34 Examples/sec\n",
      "INFO:tensorflow:training step 6556 | tagging_loss_video: 5.100|tagging_loss_audio: 7.714|tagging_loss_text: 12.352|tagging_loss_image: 5.134|tagging_loss_fusion: 3.302|total_loss: 33.601 | 60.09 Examples/sec\n",
      "INFO:tensorflow:training step 6557 | tagging_loss_video: 5.788|tagging_loss_audio: 9.562|tagging_loss_text: 15.950|tagging_loss_image: 4.677|tagging_loss_fusion: 3.348|total_loss: 39.327 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 6558 | tagging_loss_video: 5.764|tagging_loss_audio: 8.388|tagging_loss_text: 13.764|tagging_loss_image: 3.012|tagging_loss_fusion: 4.470|total_loss: 35.398 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 6559 | tagging_loss_video: 5.794|tagging_loss_audio: 7.751|tagging_loss_text: 17.445|tagging_loss_image: 3.720|tagging_loss_fusion: 3.512|total_loss: 38.222 | 71.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6560 |tagging_loss_video: 5.720|tagging_loss_audio: 8.695|tagging_loss_text: 14.794|tagging_loss_image: 3.989|tagging_loss_fusion: 3.748|total_loss: 36.945 | Examples/sec: 68.05\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.89 | precision@0.5: 0.99 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6561 | tagging_loss_video: 5.563|tagging_loss_audio: 8.137|tagging_loss_text: 15.257|tagging_loss_image: 5.733|tagging_loss_fusion: 4.931|total_loss: 39.621 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 6562 | tagging_loss_video: 5.448|tagging_loss_audio: 7.930|tagging_loss_text: 16.173|tagging_loss_image: 6.107|tagging_loss_fusion: 5.075|total_loss: 40.734 | 69.68 Examples/sec\n",
      "INFO:tensorflow:training step 6563 | tagging_loss_video: 6.271|tagging_loss_audio: 8.390|tagging_loss_text: 17.645|tagging_loss_image: 4.178|tagging_loss_fusion: 4.786|total_loss: 41.269 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 6564 | tagging_loss_video: 5.604|tagging_loss_audio: 8.341|tagging_loss_text: 16.854|tagging_loss_image: 4.568|tagging_loss_fusion: 4.609|total_loss: 39.976 | 62.06 Examples/sec\n",
      "INFO:tensorflow:training step 6565 | tagging_loss_video: 5.321|tagging_loss_audio: 7.759|tagging_loss_text: 14.826|tagging_loss_image: 5.937|tagging_loss_fusion: 5.569|total_loss: 39.412 | 70.34 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6566 | tagging_loss_video: 5.763|tagging_loss_audio: 8.184|tagging_loss_text: 15.340|tagging_loss_image: 4.345|tagging_loss_fusion: 4.811|total_loss: 38.444 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6567 | tagging_loss_video: 6.179|tagging_loss_audio: 7.966|tagging_loss_text: 14.511|tagging_loss_image: 4.418|tagging_loss_fusion: 4.221|total_loss: 37.294 | 64.89 Examples/sec\n",
      "INFO:tensorflow:training step 6568 | tagging_loss_video: 5.689|tagging_loss_audio: 8.549|tagging_loss_text: 11.129|tagging_loss_image: 5.171|tagging_loss_fusion: 4.315|total_loss: 34.852 | 67.37 Examples/sec\n",
      "INFO:tensorflow:training step 6569 | tagging_loss_video: 5.614|tagging_loss_audio: 8.337|tagging_loss_text: 16.767|tagging_loss_image: 5.435|tagging_loss_fusion: 4.159|total_loss: 40.312 | 67.71 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6570 |tagging_loss_video: 5.765|tagging_loss_audio: 8.975|tagging_loss_text: 16.102|tagging_loss_image: 6.633|tagging_loss_fusion: 5.935|total_loss: 43.410 | Examples/sec: 61.99\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.86 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6571 | tagging_loss_video: 4.870|tagging_loss_audio: 8.265|tagging_loss_text: 13.462|tagging_loss_image: 4.144|tagging_loss_fusion: 3.481|total_loss: 34.223 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 6572 | tagging_loss_video: 5.432|tagging_loss_audio: 8.372|tagging_loss_text: 17.160|tagging_loss_image: 5.330|tagging_loss_fusion: 4.242|total_loss: 40.537 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 6573 | tagging_loss_video: 6.156|tagging_loss_audio: 7.471|tagging_loss_text: 15.424|tagging_loss_image: 4.926|tagging_loss_fusion: 5.608|total_loss: 39.585 | 71.41 Examples/sec\n",
      "INFO:tensorflow:training step 6574 | tagging_loss_video: 5.826|tagging_loss_audio: 8.461|tagging_loss_text: 14.665|tagging_loss_image: 4.813|tagging_loss_fusion: 4.541|total_loss: 38.306 | 68.56 Examples/sec\n",
      "INFO:tensorflow:training step 6575 | tagging_loss_video: 4.739|tagging_loss_audio: 7.422|tagging_loss_text: 13.668|tagging_loss_image: 4.834|tagging_loss_fusion: 4.077|total_loss: 34.740 | 69.91 Examples/sec\n",
      "INFO:tensorflow:training step 6576 | tagging_loss_video: 5.858|tagging_loss_audio: 9.240|tagging_loss_text: 16.359|tagging_loss_image: 4.993|tagging_loss_fusion: 4.857|total_loss: 41.306 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 6577 | tagging_loss_video: 5.652|tagging_loss_audio: 8.982|tagging_loss_text: 14.840|tagging_loss_image: 6.237|tagging_loss_fusion: 2.672|total_loss: 38.384 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 6578 | tagging_loss_video: 5.655|tagging_loss_audio: 7.974|tagging_loss_text: 15.227|tagging_loss_image: 5.302|tagging_loss_fusion: 3.609|total_loss: 37.766 | 63.83 Examples/sec\n",
      "INFO:tensorflow:training step 6579 | tagging_loss_video: 5.363|tagging_loss_audio: 7.950|tagging_loss_text: 12.637|tagging_loss_image: 5.116|tagging_loss_fusion: 4.043|total_loss: 35.109 | 69.15 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6580 |tagging_loss_video: 7.047|tagging_loss_audio: 10.108|tagging_loss_text: 19.757|tagging_loss_image: 6.031|tagging_loss_fusion: 6.624|total_loss: 49.567 | Examples/sec: 71.02\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 6581 | tagging_loss_video: 5.278|tagging_loss_audio: 7.234|tagging_loss_text: 15.708|tagging_loss_image: 4.374|tagging_loss_fusion: 3.793|total_loss: 36.387 | 67.25 Examples/sec\n",
      "INFO:tensorflow:training step 6582 | tagging_loss_video: 5.589|tagging_loss_audio: 8.830|tagging_loss_text: 15.023|tagging_loss_image: 5.378|tagging_loss_fusion: 6.414|total_loss: 41.235 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 6583 | tagging_loss_video: 3.744|tagging_loss_audio: 8.295|tagging_loss_text: 17.467|tagging_loss_image: 5.655|tagging_loss_fusion: 2.850|total_loss: 38.010 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 6584 | tagging_loss_video: 4.839|tagging_loss_audio: 6.795|tagging_loss_text: 15.216|tagging_loss_image: 4.598|tagging_loss_fusion: 3.249|total_loss: 34.698 | 69.77 Examples/sec\n",
      "INFO:tensorflow:training step 6585 | tagging_loss_video: 5.847|tagging_loss_audio: 7.573|tagging_loss_text: 16.102|tagging_loss_image: 5.418|tagging_loss_fusion: 5.836|total_loss: 40.777 | 67.32 Examples/sec\n",
      "INFO:tensorflow:training step 6586 | tagging_loss_video: 4.325|tagging_loss_audio: 7.470|tagging_loss_text: 14.464|tagging_loss_image: 4.833|tagging_loss_fusion: 2.296|total_loss: 33.389 | 67.16 Examples/sec\n",
      "INFO:tensorflow:training step 6587 | tagging_loss_video: 5.692|tagging_loss_audio: 8.540|tagging_loss_text: 11.975|tagging_loss_image: 4.713|tagging_loss_fusion: 4.257|total_loss: 35.177 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 6588 | tagging_loss_video: 4.604|tagging_loss_audio: 6.583|tagging_loss_text: 15.928|tagging_loss_image: 4.615|tagging_loss_fusion: 3.924|total_loss: 35.653 | 67.46 Examples/sec\n",
      "INFO:tensorflow:training step 6589 | tagging_loss_video: 5.131|tagging_loss_audio: 8.177|tagging_loss_text: 16.215|tagging_loss_image: 4.622|tagging_loss_fusion: 2.950|total_loss: 37.095 | 71.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6590 |tagging_loss_video: 5.668|tagging_loss_audio: 8.632|tagging_loss_text: 14.352|tagging_loss_image: 5.399|tagging_loss_fusion: 4.406|total_loss: 38.457 | Examples/sec: 71.50\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6591 | tagging_loss_video: 5.074|tagging_loss_audio: 7.746|tagging_loss_text: 13.626|tagging_loss_image: 3.551|tagging_loss_fusion: 2.636|total_loss: 32.634 | 66.69 Examples/sec\n",
      "INFO:tensorflow:training step 6592 | tagging_loss_video: 4.742|tagging_loss_audio: 7.478|tagging_loss_text: 16.125|tagging_loss_image: 4.432|tagging_loss_fusion: 3.137|total_loss: 35.914 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 6593 | tagging_loss_video: 5.863|tagging_loss_audio: 8.101|tagging_loss_text: 13.124|tagging_loss_image: 4.995|tagging_loss_fusion: 4.156|total_loss: 36.238 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 6594 | tagging_loss_video: 5.426|tagging_loss_audio: 8.187|tagging_loss_text: 16.726|tagging_loss_image: 5.782|tagging_loss_fusion: 4.829|total_loss: 40.951 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 6595 | tagging_loss_video: 4.973|tagging_loss_audio: 7.800|tagging_loss_text: 14.779|tagging_loss_image: 5.073|tagging_loss_fusion: 4.048|total_loss: 36.673 | 65.29 Examples/sec\n",
      "INFO:tensorflow:training step 6596 | tagging_loss_video: 4.931|tagging_loss_audio: 7.960|tagging_loss_text: 15.394|tagging_loss_image: 5.401|tagging_loss_fusion: 3.900|total_loss: 37.586 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 6597 | tagging_loss_video: 5.296|tagging_loss_audio: 8.248|tagging_loss_text: 16.530|tagging_loss_image: 4.072|tagging_loss_fusion: 3.828|total_loss: 37.973 | 65.34 Examples/sec\n",
      "INFO:tensorflow:training step 6598 | tagging_loss_video: 5.127|tagging_loss_audio: 7.760|tagging_loss_text: 14.999|tagging_loss_image: 3.432|tagging_loss_fusion: 3.128|total_loss: 34.447 | 64.96 Examples/sec\n",
      "INFO:tensorflow:training step 6599 | tagging_loss_video: 5.340|tagging_loss_audio: 8.025|tagging_loss_text: 12.369|tagging_loss_image: 4.894|tagging_loss_fusion: 3.786|total_loss: 34.415 | 68.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6600 |tagging_loss_video: 5.794|tagging_loss_audio: 7.661|tagging_loss_text: 17.166|tagging_loss_image: 4.162|tagging_loss_fusion: 3.973|total_loss: 38.756 | Examples/sec: 67.82\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6601 | tagging_loss_video: 5.045|tagging_loss_audio: 7.626|tagging_loss_text: 15.927|tagging_loss_image: 4.064|tagging_loss_fusion: 2.558|total_loss: 35.220 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 6602 | tagging_loss_video: 3.862|tagging_loss_audio: 7.721|tagging_loss_text: 16.077|tagging_loss_image: 4.497|tagging_loss_fusion: 2.202|total_loss: 34.359 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 6603 | tagging_loss_video: 5.767|tagging_loss_audio: 7.917|tagging_loss_text: 16.004|tagging_loss_image: 4.995|tagging_loss_fusion: 3.965|total_loss: 38.648 | 66.67 Examples/sec\n",
      "INFO:tensorflow:training step 6604 | tagging_loss_video: 6.206|tagging_loss_audio: 9.032|tagging_loss_text: 14.743|tagging_loss_image: 4.204|tagging_loss_fusion: 4.811|total_loss: 38.996 | 59.76 Examples/sec\n",
      "INFO:tensorflow:training step 6605 | tagging_loss_video: 5.467|tagging_loss_audio: 7.556|tagging_loss_text: 14.844|tagging_loss_image: 5.404|tagging_loss_fusion: 4.089|total_loss: 37.361 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 6606 | tagging_loss_video: 5.686|tagging_loss_audio: 9.030|tagging_loss_text: 19.264|tagging_loss_image: 6.318|tagging_loss_fusion: 5.392|total_loss: 45.689 | 70.88 Examples/sec\n",
      "INFO:tensorflow:training step 6607 | tagging_loss_video: 5.267|tagging_loss_audio: 7.688|tagging_loss_text: 15.210|tagging_loss_image: 3.888|tagging_loss_fusion: 2.613|total_loss: 34.666 | 65.22 Examples/sec\n",
      "INFO:tensorflow:training step 6608 | tagging_loss_video: 4.799|tagging_loss_audio: 6.594|tagging_loss_text: 11.859|tagging_loss_image: 4.514|tagging_loss_fusion: 2.773|total_loss: 30.538 | 67.39 Examples/sec\n",
      "INFO:tensorflow:training step 6609 | tagging_loss_video: 5.780|tagging_loss_audio: 7.827|tagging_loss_text: 17.361|tagging_loss_image: 4.371|tagging_loss_fusion: 3.592|total_loss: 38.932 | 67.81 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6610 |tagging_loss_video: 4.680|tagging_loss_audio: 8.600|tagging_loss_text: 12.885|tagging_loss_image: 4.800|tagging_loss_fusion: 2.922|total_loss: 33.887 | Examples/sec: 66.14\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6611 | tagging_loss_video: 4.948|tagging_loss_audio: 8.148|tagging_loss_text: 15.188|tagging_loss_image: 4.995|tagging_loss_fusion: 2.509|total_loss: 35.788 | 70.02 Examples/sec\n",
      "INFO:tensorflow:training step 6612 | tagging_loss_video: 5.693|tagging_loss_audio: 7.730|tagging_loss_text: 15.819|tagging_loss_image: 4.149|tagging_loss_fusion: 3.271|total_loss: 36.662 | 69.26 Examples/sec\n",
      "INFO:tensorflow:training step 6613 | tagging_loss_video: 5.464|tagging_loss_audio: 7.713|tagging_loss_text: 17.298|tagging_loss_image: 5.752|tagging_loss_fusion: 4.857|total_loss: 41.084 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 6614 | tagging_loss_video: 5.536|tagging_loss_audio: 8.841|tagging_loss_text: 14.560|tagging_loss_image: 3.841|tagging_loss_fusion: 4.253|total_loss: 37.030 | 69.11 Examples/sec\n",
      "INFO:tensorflow:training step 6615 | tagging_loss_video: 3.501|tagging_loss_audio: 8.240|tagging_loss_text: 17.645|tagging_loss_image: 5.297|tagging_loss_fusion: 1.851|total_loss: 36.535 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 6616 | tagging_loss_video: 5.400|tagging_loss_audio: 8.128|tagging_loss_text: 20.293|tagging_loss_image: 4.194|tagging_loss_fusion: 4.240|total_loss: 42.254 | 68.80 Examples/sec\n",
      "INFO:tensorflow:training step 6617 | tagging_loss_video: 5.655|tagging_loss_audio: 8.737|tagging_loss_text: 14.831|tagging_loss_image: 5.037|tagging_loss_fusion: 4.104|total_loss: 38.364 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 6618 | tagging_loss_video: 4.754|tagging_loss_audio: 7.583|tagging_loss_text: 15.073|tagging_loss_image: 4.986|tagging_loss_fusion: 3.427|total_loss: 35.824 | 62.04 Examples/sec\n",
      "INFO:tensorflow:training step 6619 | tagging_loss_video: 5.912|tagging_loss_audio: 8.063|tagging_loss_text: 12.082|tagging_loss_image: 4.343|tagging_loss_fusion: 4.303|total_loss: 34.702 | 70.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6620 |tagging_loss_video: 4.452|tagging_loss_audio: 7.651|tagging_loss_text: 11.834|tagging_loss_image: 4.007|tagging_loss_fusion: 2.490|total_loss: 30.433 | Examples/sec: 71.52\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6621 | tagging_loss_video: 4.934|tagging_loss_audio: 8.357|tagging_loss_text: 16.233|tagging_loss_image: 4.460|tagging_loss_fusion: 2.639|total_loss: 36.624 | 65.48 Examples/sec\n",
      "INFO:tensorflow:training step 6622 | tagging_loss_video: 6.361|tagging_loss_audio: 8.324|tagging_loss_text: 17.137|tagging_loss_image: 4.744|tagging_loss_fusion: 5.128|total_loss: 41.693 | 69.46 Examples/sec\n",
      "INFO:tensorflow:training step 6623 | tagging_loss_video: 4.321|tagging_loss_audio: 7.898|tagging_loss_text: 12.854|tagging_loss_image: 5.529|tagging_loss_fusion: 2.524|total_loss: 33.126 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 6624 | tagging_loss_video: 3.617|tagging_loss_audio: 8.530|tagging_loss_text: 14.658|tagging_loss_image: 5.689|tagging_loss_fusion: 3.684|total_loss: 36.178 | 63.25 Examples/sec\n",
      "INFO:tensorflow:training step 6625 | tagging_loss_video: 4.506|tagging_loss_audio: 7.907|tagging_loss_text: 14.166|tagging_loss_image: 4.420|tagging_loss_fusion: 2.865|total_loss: 33.864 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 6626 | tagging_loss_video: 5.442|tagging_loss_audio: 8.149|tagging_loss_text: 14.928|tagging_loss_image: 4.196|tagging_loss_fusion: 5.950|total_loss: 38.664 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 6627 | tagging_loss_video: 5.628|tagging_loss_audio: 7.934|tagging_loss_text: 17.879|tagging_loss_image: 5.491|tagging_loss_fusion: 5.981|total_loss: 42.912 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 6628 | tagging_loss_video: 5.260|tagging_loss_audio: 7.952|tagging_loss_text: 15.316|tagging_loss_image: 5.053|tagging_loss_fusion: 4.325|total_loss: 37.906 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 6629 | tagging_loss_video: 5.798|tagging_loss_audio: 8.772|tagging_loss_text: 16.333|tagging_loss_image: 4.852|tagging_loss_fusion: 4.133|total_loss: 39.889 | 60.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6630 |tagging_loss_video: 3.984|tagging_loss_audio: 9.498|tagging_loss_text: 15.897|tagging_loss_image: 5.221|tagging_loss_fusion: 2.633|total_loss: 37.232 | Examples/sec: 70.08\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.92 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6631 | tagging_loss_video: 5.450|tagging_loss_audio: 8.492|tagging_loss_text: 15.834|tagging_loss_image: 3.985|tagging_loss_fusion: 4.707|total_loss: 38.467 | 71.59 Examples/sec\n",
      "INFO:tensorflow:training step 6632 | tagging_loss_video: 5.032|tagging_loss_audio: 8.302|tagging_loss_text: 16.495|tagging_loss_image: 6.360|tagging_loss_fusion: 3.283|total_loss: 39.473 | 60.52 Examples/sec\n",
      "INFO:tensorflow:training step 6633 | tagging_loss_video: 4.774|tagging_loss_audio: 8.816|tagging_loss_text: 17.104|tagging_loss_image: 3.772|tagging_loss_fusion: 3.157|total_loss: 37.623 | 67.01 Examples/sec\n",
      "INFO:tensorflow:training step 6634 | tagging_loss_video: 5.989|tagging_loss_audio: 7.961|tagging_loss_text: 14.948|tagging_loss_image: 4.255|tagging_loss_fusion: 4.985|total_loss: 38.139 | 69.03 Examples/sec\n",
      "INFO:tensorflow:training step 6635 | tagging_loss_video: 5.623|tagging_loss_audio: 8.573|tagging_loss_text: 14.538|tagging_loss_image: 3.246|tagging_loss_fusion: 3.130|total_loss: 35.110 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 6636 | tagging_loss_video: 5.956|tagging_loss_audio: 9.355|tagging_loss_text: 17.281|tagging_loss_image: 4.799|tagging_loss_fusion: 3.447|total_loss: 40.838 | 69.39 Examples/sec\n",
      "INFO:tensorflow:training step 6637 | tagging_loss_video: 4.634|tagging_loss_audio: 8.028|tagging_loss_text: 17.388|tagging_loss_image: 3.801|tagging_loss_fusion: 3.263|total_loss: 37.113 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 6638 | tagging_loss_video: 5.290|tagging_loss_audio: 8.049|tagging_loss_text: 16.166|tagging_loss_image: 5.962|tagging_loss_fusion: 3.529|total_loss: 38.996 | 62.48 Examples/sec\n",
      "INFO:tensorflow:training step 6639 | tagging_loss_video: 6.614|tagging_loss_audio: 10.850|tagging_loss_text: 15.658|tagging_loss_image: 5.184|tagging_loss_fusion: 5.496|total_loss: 43.803 | 68.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6640 |tagging_loss_video: 5.425|tagging_loss_audio: 8.448|tagging_loss_text: 13.773|tagging_loss_image: 2.598|tagging_loss_fusion: 2.986|total_loss: 33.230 | Examples/sec: 70.61\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 6641 | tagging_loss_video: 6.109|tagging_loss_audio: 9.261|tagging_loss_text: 12.669|tagging_loss_image: 4.222|tagging_loss_fusion: 3.562|total_loss: 35.823 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 6642 | tagging_loss_video: 3.860|tagging_loss_audio: 9.108|tagging_loss_text: 15.470|tagging_loss_image: 4.303|tagging_loss_fusion: 1.896|total_loss: 34.636 | 65.65 Examples/sec\n",
      "INFO:tensorflow:training step 6643 | tagging_loss_video: 6.453|tagging_loss_audio: 9.142|tagging_loss_text: 15.186|tagging_loss_image: 5.551|tagging_loss_fusion: 5.644|total_loss: 41.977 | 71.22 Examples/sec\n",
      "INFO:tensorflow:training step 6644 | tagging_loss_video: 4.861|tagging_loss_audio: 8.437|tagging_loss_text: 17.993|tagging_loss_image: 4.662|tagging_loss_fusion: 2.761|total_loss: 38.715 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 6645 | tagging_loss_video: 6.414|tagging_loss_audio: 8.726|tagging_loss_text: 17.475|tagging_loss_image: 4.486|tagging_loss_fusion: 4.498|total_loss: 41.598 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 6646 | tagging_loss_video: 5.760|tagging_loss_audio: 9.421|tagging_loss_text: 16.019|tagging_loss_image: 5.893|tagging_loss_fusion: 5.634|total_loss: 42.727 | 63.53 Examples/sec\n",
      "INFO:tensorflow:training step 6647 | tagging_loss_video: 7.079|tagging_loss_audio: 11.402|tagging_loss_text: 15.267|tagging_loss_image: 5.547|tagging_loss_fusion: 5.937|total_loss: 45.231 | 67.15 Examples/sec\n",
      "INFO:tensorflow:training step 6648 | tagging_loss_video: 5.483|tagging_loss_audio: 8.543|tagging_loss_text: 15.920|tagging_loss_image: 4.173|tagging_loss_fusion: 3.340|total_loss: 37.459 | 71.97 Examples/sec\n",
      "INFO:tensorflow:training step 6649 | tagging_loss_video: 5.429|tagging_loss_audio: 9.405|tagging_loss_text: 14.649|tagging_loss_image: 4.305|tagging_loss_fusion: 2.499|total_loss: 36.288 | 62.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6650 |tagging_loss_video: 6.074|tagging_loss_audio: 8.673|tagging_loss_text: 11.393|tagging_loss_image: 5.258|tagging_loss_fusion: 6.487|total_loss: 37.885 | Examples/sec: 70.55\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.90 |recall@0.1: 0.95 | recall@0.5: 0.84\n",
      "INFO:tensorflow:training step 6651 | tagging_loss_video: 4.509|tagging_loss_audio: 8.949|tagging_loss_text: 15.074|tagging_loss_image: 5.873|tagging_loss_fusion: 2.981|total_loss: 37.387 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 6652 | tagging_loss_video: 6.411|tagging_loss_audio: 8.407|tagging_loss_text: 13.978|tagging_loss_image: 4.305|tagging_loss_fusion: 4.734|total_loss: 37.835 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 6653 | tagging_loss_video: 6.456|tagging_loss_audio: 9.710|tagging_loss_text: 17.798|tagging_loss_image: 4.605|tagging_loss_fusion: 5.685|total_loss: 44.255 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 6654 | tagging_loss_video: 5.799|tagging_loss_audio: 9.325|tagging_loss_text: 12.551|tagging_loss_image: 5.411|tagging_loss_fusion: 4.434|total_loss: 37.520 | 64.38 Examples/sec\n",
      "INFO:tensorflow:training step 6655 | tagging_loss_video: 5.178|tagging_loss_audio: 8.566|tagging_loss_text: 10.496|tagging_loss_image: 4.739|tagging_loss_fusion: 4.199|total_loss: 33.178 | 68.41 Examples/sec\n",
      "INFO:tensorflow:training step 6656 | tagging_loss_video: 3.661|tagging_loss_audio: 7.697|tagging_loss_text: 14.299|tagging_loss_image: 4.533|tagging_loss_fusion: 2.122|total_loss: 32.313 | 69.21 Examples/sec\n",
      "INFO:tensorflow:training step 6657 | tagging_loss_video: 5.806|tagging_loss_audio: 8.218|tagging_loss_text: 14.778|tagging_loss_image: 5.082|tagging_loss_fusion: 5.699|total_loss: 39.582 | 64.17 Examples/sec\n",
      "INFO:tensorflow:training step 6658 | tagging_loss_video: 6.112|tagging_loss_audio: 9.690|tagging_loss_text: 18.318|tagging_loss_image: 6.490|tagging_loss_fusion: 6.197|total_loss: 46.806 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 6659 | tagging_loss_video: 4.935|tagging_loss_audio: 9.408|tagging_loss_text: 16.199|tagging_loss_image: 6.001|tagging_loss_fusion: 3.168|total_loss: 39.710 | 69.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6660 |tagging_loss_video: 6.080|tagging_loss_audio: 7.689|tagging_loss_text: 10.568|tagging_loss_image: 5.160|tagging_loss_fusion: 5.491|total_loss: 34.987 | Examples/sec: 65.96\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6661 | tagging_loss_video: 4.288|tagging_loss_audio: 9.100|tagging_loss_text: 15.265|tagging_loss_image: 4.906|tagging_loss_fusion: 2.551|total_loss: 36.109 | 72.03 Examples/sec\n",
      "INFO:tensorflow:training step 6662 | tagging_loss_video: 4.183|tagging_loss_audio: 8.889|tagging_loss_text: 14.827|tagging_loss_image: 4.064|tagging_loss_fusion: 3.862|total_loss: 35.825 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 6663 | tagging_loss_video: 3.982|tagging_loss_audio: 7.460|tagging_loss_text: 15.179|tagging_loss_image: 5.306|tagging_loss_fusion: 2.833|total_loss: 34.759 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 6664 | tagging_loss_video: 4.603|tagging_loss_audio: 7.711|tagging_loss_text: 12.018|tagging_loss_image: 4.178|tagging_loss_fusion: 2.467|total_loss: 30.978 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 6665 | tagging_loss_video: 5.426|tagging_loss_audio: 8.914|tagging_loss_text: 13.948|tagging_loss_image: 4.973|tagging_loss_fusion: 3.267|total_loss: 36.529 | 66.16 Examples/sec\n",
      "INFO:tensorflow:training step 6666 | tagging_loss_video: 5.358|tagging_loss_audio: 9.144|tagging_loss_text: 14.065|tagging_loss_image: 4.071|tagging_loss_fusion: 4.823|total_loss: 37.460 | 67.35 Examples/sec\n",
      "INFO:tensorflow:training step 6667 | tagging_loss_video: 5.188|tagging_loss_audio: 8.435|tagging_loss_text: 15.849|tagging_loss_image: 5.727|tagging_loss_fusion: 3.550|total_loss: 38.748 | 69.55 Examples/sec\n",
      "INFO:tensorflow:training step 6668 | tagging_loss_video: 5.519|tagging_loss_audio: 8.075|tagging_loss_text: 16.824|tagging_loss_image: 3.931|tagging_loss_fusion: 3.960|total_loss: 38.309 | 67.50 Examples/sec\n",
      "INFO:tensorflow:training step 6669 | tagging_loss_video: 5.426|tagging_loss_audio: 7.619|tagging_loss_text: 18.692|tagging_loss_image: 4.727|tagging_loss_fusion: 3.756|total_loss: 40.220 | 66.61 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6670 |tagging_loss_video: 5.756|tagging_loss_audio: 8.396|tagging_loss_text: 14.604|tagging_loss_image: 5.619|tagging_loss_fusion: 4.802|total_loss: 39.177 | Examples/sec: 68.23\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6671 | tagging_loss_video: 5.226|tagging_loss_audio: 8.298|tagging_loss_text: 14.887|tagging_loss_image: 4.365|tagging_loss_fusion: 3.082|total_loss: 35.858 | 67.35 Examples/sec\n",
      "INFO:tensorflow:training step 6672 | tagging_loss_video: 5.928|tagging_loss_audio: 9.221|tagging_loss_text: 18.103|tagging_loss_image: 5.608|tagging_loss_fusion: 5.120|total_loss: 43.980 | 71.48 Examples/sec\n",
      "INFO:tensorflow:training step 6673 | tagging_loss_video: 7.316|tagging_loss_audio: 8.130|tagging_loss_text: 18.224|tagging_loss_image: 6.728|tagging_loss_fusion: 8.111|total_loss: 48.509 | 68.57 Examples/sec\n",
      "INFO:tensorflow:training step 6674 | tagging_loss_video: 6.018|tagging_loss_audio: 8.431|tagging_loss_text: 14.126|tagging_loss_image: 6.057|tagging_loss_fusion: 4.754|total_loss: 39.386 | 63.67 Examples/sec\n",
      "INFO:tensorflow:training step 6675 | tagging_loss_video: 5.850|tagging_loss_audio: 8.153|tagging_loss_text: 15.056|tagging_loss_image: 5.177|tagging_loss_fusion: 3.804|total_loss: 38.041 | 68.03 Examples/sec\n",
      "INFO:tensorflow:training step 6676 | tagging_loss_video: 5.445|tagging_loss_audio: 9.928|tagging_loss_text: 15.680|tagging_loss_image: 5.662|tagging_loss_fusion: 4.427|total_loss: 41.142 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 6677 | tagging_loss_video: 5.682|tagging_loss_audio: 7.747|tagging_loss_text: 13.343|tagging_loss_image: 4.369|tagging_loss_fusion: 3.390|total_loss: 34.531 | 70.22 Examples/sec\n",
      "INFO:tensorflow:training step 6678 | tagging_loss_video: 5.620|tagging_loss_audio: 10.088|tagging_loss_text: 16.516|tagging_loss_image: 5.031|tagging_loss_fusion: 3.540|total_loss: 40.795 | 67.99 Examples/sec\n",
      "INFO:tensorflow:training step 6679 | tagging_loss_video: 5.437|tagging_loss_audio: 7.463|tagging_loss_text: 14.517|tagging_loss_image: 5.271|tagging_loss_fusion: 5.392|total_loss: 38.080 | 63.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6680 |tagging_loss_video: 4.838|tagging_loss_audio: 8.974|tagging_loss_text: 9.605|tagging_loss_image: 3.783|tagging_loss_fusion: 2.727|total_loss: 29.927 | Examples/sec: 70.72\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6681 | tagging_loss_video: 5.758|tagging_loss_audio: 8.332|tagging_loss_text: 18.839|tagging_loss_image: 5.019|tagging_loss_fusion: 4.991|total_loss: 42.939 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 6682 | tagging_loss_video: 5.208|tagging_loss_audio: 6.780|tagging_loss_text: 13.508|tagging_loss_image: 5.038|tagging_loss_fusion: 4.847|total_loss: 35.382 | 65.39 Examples/sec\n",
      "INFO:tensorflow:training step 6683 | tagging_loss_video: 5.869|tagging_loss_audio: 7.840|tagging_loss_text: 12.170|tagging_loss_image: 4.769|tagging_loss_fusion: 5.611|total_loss: 36.260 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 6684 | tagging_loss_video: 5.049|tagging_loss_audio: 8.913|tagging_loss_text: 17.289|tagging_loss_image: 4.837|tagging_loss_fusion: 2.655|total_loss: 38.742 | 72.10 Examples/sec\n",
      "INFO:tensorflow:training step 6685 | tagging_loss_video: 5.683|tagging_loss_audio: 8.365|tagging_loss_text: 12.158|tagging_loss_image: 4.620|tagging_loss_fusion: 4.201|total_loss: 35.027 | 62.16 Examples/sec\n",
      "INFO:tensorflow:training step 6686 | tagging_loss_video: 6.534|tagging_loss_audio: 8.566|tagging_loss_text: 17.748|tagging_loss_image: 4.879|tagging_loss_fusion: 4.306|total_loss: 42.033 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 6687 | tagging_loss_video: 3.686|tagging_loss_audio: 9.138|tagging_loss_text: 12.675|tagging_loss_image: 5.134|tagging_loss_fusion: 1.816|total_loss: 32.449 | 71.88 Examples/sec\n",
      "INFO:tensorflow:training step 6688 | tagging_loss_video: 5.461|tagging_loss_audio: 7.968|tagging_loss_text: 15.641|tagging_loss_image: 4.585|tagging_loss_fusion: 4.169|total_loss: 37.825 | 63.73 Examples/sec\n",
      "INFO:tensorflow:training step 6689 | tagging_loss_video: 5.814|tagging_loss_audio: 8.229|tagging_loss_text: 16.371|tagging_loss_image: 6.075|tagging_loss_fusion: 5.681|total_loss: 42.169 | 68.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6690 |tagging_loss_video: 5.818|tagging_loss_audio: 8.195|tagging_loss_text: 16.143|tagging_loss_image: 5.219|tagging_loss_fusion: 3.288|total_loss: 38.663 | Examples/sec: 69.87\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6691 | tagging_loss_video: 5.480|tagging_loss_audio: 8.380|tagging_loss_text: 15.943|tagging_loss_image: 4.874|tagging_loss_fusion: 2.986|total_loss: 37.663 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 6692 | tagging_loss_video: 4.999|tagging_loss_audio: 7.693|tagging_loss_text: 15.390|tagging_loss_image: 5.382|tagging_loss_fusion: 3.935|total_loss: 37.399 | 67.30 Examples/sec\n",
      "INFO:tensorflow:training step 6693 | tagging_loss_video: 5.370|tagging_loss_audio: 8.302|tagging_loss_text: 16.366|tagging_loss_image: 5.415|tagging_loss_fusion: 3.922|total_loss: 39.375 | 71.20 Examples/sec\n",
      "INFO:tensorflow:training step 6694 | tagging_loss_video: 5.589|tagging_loss_audio: 8.773|tagging_loss_text: 16.754|tagging_loss_image: 4.395|tagging_loss_fusion: 3.122|total_loss: 38.634 | 67.77 Examples/sec\n",
      "INFO:tensorflow:training step 6695 | tagging_loss_video: 6.238|tagging_loss_audio: 7.665|tagging_loss_text: 11.559|tagging_loss_image: 4.504|tagging_loss_fusion: 4.720|total_loss: 34.686 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 6696 | tagging_loss_video: 4.757|tagging_loss_audio: 8.010|tagging_loss_text: 14.338|tagging_loss_image: 3.703|tagging_loss_fusion: 2.547|total_loss: 33.356 | 61.77 Examples/sec\n",
      "INFO:tensorflow:training step 6697 | tagging_loss_video: 5.377|tagging_loss_audio: 9.863|tagging_loss_text: 13.873|tagging_loss_image: 5.077|tagging_loss_fusion: 3.946|total_loss: 38.135 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 6698 | tagging_loss_video: 5.151|tagging_loss_audio: 7.927|tagging_loss_text: 15.165|tagging_loss_image: 4.742|tagging_loss_fusion: 3.422|total_loss: 36.407 | 68.04 Examples/sec\n",
      "INFO:tensorflow:training step 6699 | tagging_loss_video: 6.301|tagging_loss_audio: 7.801|tagging_loss_text: 16.741|tagging_loss_image: 3.747|tagging_loss_fusion: 4.504|total_loss: 39.095 | 68.31 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6700 |tagging_loss_video: 4.273|tagging_loss_audio: 7.968|tagging_loss_text: 16.996|tagging_loss_image: 5.849|tagging_loss_fusion: 4.051|total_loss: 39.137 | Examples/sec: 71.81\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6701 | tagging_loss_video: 5.085|tagging_loss_audio: 7.347|tagging_loss_text: 13.922|tagging_loss_image: 5.422|tagging_loss_fusion: 3.531|total_loss: 35.307 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 6702 | tagging_loss_video: 5.401|tagging_loss_audio: 7.871|tagging_loss_text: 16.180|tagging_loss_image: 5.296|tagging_loss_fusion: 5.165|total_loss: 39.912 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 6703 | tagging_loss_video: 5.953|tagging_loss_audio: 7.950|tagging_loss_text: 15.506|tagging_loss_image: 4.544|tagging_loss_fusion: 3.652|total_loss: 37.605 | 67.38 Examples/sec\n",
      "INFO:tensorflow:training step 6704 | tagging_loss_video: 5.510|tagging_loss_audio: 8.376|tagging_loss_text: 16.166|tagging_loss_image: 5.333|tagging_loss_fusion: 3.661|total_loss: 39.045 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6705 | tagging_loss_video: 5.257|tagging_loss_audio: 7.280|tagging_loss_text: 16.229|tagging_loss_image: 5.567|tagging_loss_fusion: 3.693|total_loss: 38.025 | 70.31 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6706 | tagging_loss_video: 6.149|tagging_loss_audio: 8.493|tagging_loss_text: 16.733|tagging_loss_image: 5.218|tagging_loss_fusion: 4.291|total_loss: 40.884 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 6707 | tagging_loss_video: 5.251|tagging_loss_audio: 8.457|tagging_loss_text: 15.893|tagging_loss_image: 5.766|tagging_loss_fusion: 4.329|total_loss: 39.696 | 62.58 Examples/sec\n",
      "INFO:tensorflow:training step 6708 | tagging_loss_video: 5.557|tagging_loss_audio: 8.003|tagging_loss_text: 16.000|tagging_loss_image: 5.761|tagging_loss_fusion: 3.748|total_loss: 39.068 | 70.18 Examples/sec\n",
      "INFO:tensorflow:training step 6709 | tagging_loss_video: 4.903|tagging_loss_audio: 9.355|tagging_loss_text: 12.734|tagging_loss_image: 5.490|tagging_loss_fusion: 3.968|total_loss: 36.450 | 70.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6710 |tagging_loss_video: 6.876|tagging_loss_audio: 8.931|tagging_loss_text: 15.361|tagging_loss_image: 6.284|tagging_loss_fusion: 6.928|total_loss: 44.380 | Examples/sec: 62.19\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.83 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6711 | tagging_loss_video: 5.246|tagging_loss_audio: 7.659|tagging_loss_text: 13.460|tagging_loss_image: 2.812|tagging_loss_fusion: 2.349|total_loss: 31.527 | 64.87 Examples/sec\n",
      "INFO:tensorflow:training step 6712 | tagging_loss_video: 4.197|tagging_loss_audio: 8.471|tagging_loss_text: 18.524|tagging_loss_image: 4.520|tagging_loss_fusion: 2.175|total_loss: 37.886 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 6713 | tagging_loss_video: 5.542|tagging_loss_audio: 7.282|tagging_loss_text: 14.094|tagging_loss_image: 4.491|tagging_loss_fusion: 3.687|total_loss: 35.096 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 6714 | tagging_loss_video: 5.738|tagging_loss_audio: 8.860|tagging_loss_text: 15.368|tagging_loss_image: 5.111|tagging_loss_fusion: 3.621|total_loss: 38.697 | 69.95 Examples/sec\n",
      "INFO:tensorflow:training step 6715 | tagging_loss_video: 5.359|tagging_loss_audio: 7.720|tagging_loss_text: 14.316|tagging_loss_image: 5.401|tagging_loss_fusion: 4.700|total_loss: 37.495 | 64.10 Examples/sec\n",
      "INFO:tensorflow:training step 6716 | tagging_loss_video: 4.429|tagging_loss_audio: 8.204|tagging_loss_text: 14.308|tagging_loss_image: 5.582|tagging_loss_fusion: 3.205|total_loss: 35.728 | 69.35 Examples/sec\n",
      "INFO:tensorflow:training step 6717 | tagging_loss_video: 5.606|tagging_loss_audio: 8.252|tagging_loss_text: 14.114|tagging_loss_image: 6.400|tagging_loss_fusion: 4.748|total_loss: 39.119 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 6718 | tagging_loss_video: 4.045|tagging_loss_audio: 7.516|tagging_loss_text: 18.840|tagging_loss_image: 4.865|tagging_loss_fusion: 3.171|total_loss: 38.437 | 64.60 Examples/sec\n",
      "INFO:tensorflow:training step 6719 | tagging_loss_video: 4.374|tagging_loss_audio: 8.160|tagging_loss_text: 14.763|tagging_loss_image: 4.962|tagging_loss_fusion: 3.031|total_loss: 35.289 | 68.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6720 |tagging_loss_video: 5.936|tagging_loss_audio: 9.253|tagging_loss_text: 19.755|tagging_loss_image: 6.033|tagging_loss_fusion: 3.412|total_loss: 44.389 | Examples/sec: 67.38\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6721 | tagging_loss_video: 5.251|tagging_loss_audio: 6.894|tagging_loss_text: 17.610|tagging_loss_image: 4.275|tagging_loss_fusion: 4.138|total_loss: 38.169 | 68.91 Examples/sec\n",
      "INFO:tensorflow:training step 6722 | tagging_loss_video: 4.425|tagging_loss_audio: 7.210|tagging_loss_text: 15.004|tagging_loss_image: 5.025|tagging_loss_fusion: 2.698|total_loss: 34.362 | 68.42 Examples/sec\n",
      "INFO:tensorflow:training step 6723 | tagging_loss_video: 5.400|tagging_loss_audio: 8.087|tagging_loss_text: 14.775|tagging_loss_image: 5.060|tagging_loss_fusion: 3.504|total_loss: 36.827 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 6724 | tagging_loss_video: 5.392|tagging_loss_audio: 6.958|tagging_loss_text: 14.565|tagging_loss_image: 4.760|tagging_loss_fusion: 4.272|total_loss: 35.948 | 61.10 Examples/sec\n",
      "INFO:tensorflow:training step 6725 | tagging_loss_video: 5.961|tagging_loss_audio: 7.704|tagging_loss_text: 14.003|tagging_loss_image: 3.326|tagging_loss_fusion: 4.191|total_loss: 35.186 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 6726 | tagging_loss_video: 5.667|tagging_loss_audio: 7.684|tagging_loss_text: 16.433|tagging_loss_image: 5.015|tagging_loss_fusion: 4.400|total_loss: 39.200 | 67.10 Examples/sec\n",
      "INFO:tensorflow:training step 6727 | tagging_loss_video: 5.388|tagging_loss_audio: 7.159|tagging_loss_text: 13.298|tagging_loss_image: 3.791|tagging_loss_fusion: 3.669|total_loss: 33.305 | 59.44 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 6727.\n",
      "INFO:tensorflow:training step 6728 | tagging_loss_video: 7.232|tagging_loss_audio: 8.269|tagging_loss_text: 14.739|tagging_loss_image: 5.645|tagging_loss_fusion: 4.914|total_loss: 40.799 | 47.90 Examples/sec\n",
      "INFO:tensorflow:training step 6729 | tagging_loss_video: 4.998|tagging_loss_audio: 7.920|tagging_loss_text: 17.757|tagging_loss_image: 5.780|tagging_loss_fusion: 4.695|total_loss: 41.150 | 71.78 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6730 |tagging_loss_video: 5.422|tagging_loss_audio: 8.243|tagging_loss_text: 14.871|tagging_loss_image: 4.748|tagging_loss_fusion: 4.400|total_loss: 37.683 | Examples/sec: 70.04\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.82 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6731 | tagging_loss_video: 4.577|tagging_loss_audio: 6.994|tagging_loss_text: 13.647|tagging_loss_image: 4.634|tagging_loss_fusion: 3.531|total_loss: 33.383 | 68.00 Examples/sec\n",
      "INFO:tensorflow:training step 6732 | tagging_loss_video: 5.531|tagging_loss_audio: 8.024|tagging_loss_text: 16.552|tagging_loss_image: 4.394|tagging_loss_fusion: 2.889|total_loss: 37.390 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 6733 | tagging_loss_video: 5.579|tagging_loss_audio: 8.456|tagging_loss_text: 16.682|tagging_loss_image: 5.015|tagging_loss_fusion: 3.490|total_loss: 39.220 | 67.13 Examples/sec\n",
      "INFO:tensorflow:training step 6734 | tagging_loss_video: 3.779|tagging_loss_audio: 7.956|tagging_loss_text: 14.813|tagging_loss_image: 4.133|tagging_loss_fusion: 1.987|total_loss: 32.668 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 6735 | tagging_loss_video: 6.472|tagging_loss_audio: 7.446|tagging_loss_text: 16.099|tagging_loss_image: 5.045|tagging_loss_fusion: 4.787|total_loss: 39.851 | 68.67 Examples/sec\n",
      "INFO:tensorflow:training step 6736 | tagging_loss_video: 6.077|tagging_loss_audio: 7.701|tagging_loss_text: 15.853|tagging_loss_image: 4.954|tagging_loss_fusion: 3.969|total_loss: 38.555 | 69.76 Examples/sec\n",
      "INFO:tensorflow:training step 6737 | tagging_loss_video: 4.459|tagging_loss_audio: 7.532|tagging_loss_text: 16.305|tagging_loss_image: 3.441|tagging_loss_fusion: 2.581|total_loss: 34.317 | 64.31 Examples/sec\n",
      "INFO:tensorflow:training step 6738 | tagging_loss_video: 5.580|tagging_loss_audio: 7.436|tagging_loss_text: 11.743|tagging_loss_image: 4.085|tagging_loss_fusion: 4.140|total_loss: 32.984 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 6739 | tagging_loss_video: 4.142|tagging_loss_audio: 7.866|tagging_loss_text: 12.557|tagging_loss_image: 5.303|tagging_loss_fusion: 2.784|total_loss: 32.652 | 72.30 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6740 |tagging_loss_video: 4.964|tagging_loss_audio: 7.620|tagging_loss_text: 13.952|tagging_loss_image: 5.187|tagging_loss_fusion: 3.373|total_loss: 35.096 | Examples/sec: 72.42\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6741 | tagging_loss_video: 5.597|tagging_loss_audio: 7.153|tagging_loss_text: 18.772|tagging_loss_image: 4.209|tagging_loss_fusion: 3.785|total_loss: 39.516 | 70.89 Examples/sec\n",
      "INFO:tensorflow:training step 6742 | tagging_loss_video: 5.802|tagging_loss_audio: 8.638|tagging_loss_text: 15.961|tagging_loss_image: 4.874|tagging_loss_fusion: 3.258|total_loss: 38.533 | 63.60 Examples/sec\n",
      "INFO:tensorflow:training step 6743 | tagging_loss_video: 4.734|tagging_loss_audio: 8.726|tagging_loss_text: 15.456|tagging_loss_image: 4.513|tagging_loss_fusion: 3.006|total_loss: 36.435 | 61.39 Examples/sec\n",
      "INFO:tensorflow:training step 6744 | tagging_loss_video: 5.283|tagging_loss_audio: 6.787|tagging_loss_text: 14.844|tagging_loss_image: 3.522|tagging_loss_fusion: 4.681|total_loss: 35.116 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 6745 | tagging_loss_video: 5.749|tagging_loss_audio: 9.002|tagging_loss_text: 14.999|tagging_loss_image: 4.796|tagging_loss_fusion: 4.034|total_loss: 38.580 | 71.25 Examples/sec\n",
      "INFO:tensorflow:training step 6746 | tagging_loss_video: 5.300|tagging_loss_audio: 7.531|tagging_loss_text: 15.233|tagging_loss_image: 4.570|tagging_loss_fusion: 3.958|total_loss: 36.592 | 62.84 Examples/sec\n",
      "INFO:tensorflow:training step 6747 | tagging_loss_video: 4.419|tagging_loss_audio: 7.311|tagging_loss_text: 11.886|tagging_loss_image: 4.596|tagging_loss_fusion: 2.675|total_loss: 30.887 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 6748 | tagging_loss_video: 4.398|tagging_loss_audio: 8.223|tagging_loss_text: 12.013|tagging_loss_image: 4.727|tagging_loss_fusion: 3.520|total_loss: 32.881 | 70.56 Examples/sec\n",
      "INFO:tensorflow:training step 6749 | tagging_loss_video: 4.498|tagging_loss_audio: 7.070|tagging_loss_text: 10.964|tagging_loss_image: 4.955|tagging_loss_fusion: 3.495|total_loss: 30.983 | 62.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6750 |tagging_loss_video: 5.203|tagging_loss_audio: 9.085|tagging_loss_text: 17.931|tagging_loss_image: 5.528|tagging_loss_fusion: 4.282|total_loss: 42.029 | Examples/sec: 70.21\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6751 | tagging_loss_video: 5.382|tagging_loss_audio: 7.250|tagging_loss_text: 17.790|tagging_loss_image: 5.068|tagging_loss_fusion: 4.352|total_loss: 39.843 | 70.72 Examples/sec\n",
      "INFO:tensorflow:training step 6752 | tagging_loss_video: 5.211|tagging_loss_audio: 7.894|tagging_loss_text: 14.756|tagging_loss_image: 4.211|tagging_loss_fusion: 3.206|total_loss: 35.278 | 65.22 Examples/sec\n",
      "INFO:tensorflow:training step 6753 | tagging_loss_video: 5.681|tagging_loss_audio: 7.404|tagging_loss_text: 15.938|tagging_loss_image: 4.960|tagging_loss_fusion: 4.875|total_loss: 38.858 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 6754 | tagging_loss_video: 4.925|tagging_loss_audio: 8.059|tagging_loss_text: 14.919|tagging_loss_image: 3.355|tagging_loss_fusion: 1.904|total_loss: 33.162 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 6755 | tagging_loss_video: 5.996|tagging_loss_audio: 8.242|tagging_loss_text: 17.484|tagging_loss_image: 4.163|tagging_loss_fusion: 3.198|total_loss: 39.083 | 68.37 Examples/sec\n",
      "INFO:tensorflow:training step 6756 | tagging_loss_video: 6.607|tagging_loss_audio: 9.219|tagging_loss_text: 16.987|tagging_loss_image: 4.815|tagging_loss_fusion: 4.884|total_loss: 42.511 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 6757 | tagging_loss_video: 6.740|tagging_loss_audio: 8.693|tagging_loss_text: 15.771|tagging_loss_image: 6.318|tagging_loss_fusion: 6.706|total_loss: 44.228 | 63.04 Examples/sec\n",
      "INFO:tensorflow:training step 6758 | tagging_loss_video: 5.102|tagging_loss_audio: 7.655|tagging_loss_text: 13.382|tagging_loss_image: 5.058|tagging_loss_fusion: 3.364|total_loss: 34.561 | 68.27 Examples/sec\n",
      "INFO:tensorflow:training step 6759 | tagging_loss_video: 5.246|tagging_loss_audio: 7.406|tagging_loss_text: 10.005|tagging_loss_image: 3.681|tagging_loss_fusion: 3.507|total_loss: 29.846 | 70.72 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6760 |tagging_loss_video: 3.806|tagging_loss_audio: 7.877|tagging_loss_text: 15.592|tagging_loss_image: 3.168|tagging_loss_fusion: 1.608|total_loss: 32.050 | Examples/sec: 61.54\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.90 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.97\n",
      "INFO:tensorflow:training step 6761 | tagging_loss_video: 5.736|tagging_loss_audio: 8.636|tagging_loss_text: 19.246|tagging_loss_image: 5.104|tagging_loss_fusion: 4.436|total_loss: 43.157 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 6762 | tagging_loss_video: 5.127|tagging_loss_audio: 7.678|tagging_loss_text: 17.581|tagging_loss_image: 5.627|tagging_loss_fusion: 4.509|total_loss: 40.521 | 70.40 Examples/sec\n",
      "INFO:tensorflow:training step 6763 | tagging_loss_video: 4.815|tagging_loss_audio: 8.977|tagging_loss_text: 17.408|tagging_loss_image: 5.141|tagging_loss_fusion: 4.287|total_loss: 40.629 | 70.45 Examples/sec\n",
      "INFO:tensorflow:training step 6764 | tagging_loss_video: 5.388|tagging_loss_audio: 7.445|tagging_loss_text: 16.728|tagging_loss_image: 4.944|tagging_loss_fusion: 4.032|total_loss: 38.536 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 6765 | tagging_loss_video: 5.757|tagging_loss_audio: 8.710|tagging_loss_text: 17.041|tagging_loss_image: 4.970|tagging_loss_fusion: 4.592|total_loss: 41.070 | 71.55 Examples/sec\n",
      "INFO:tensorflow:training step 6766 | tagging_loss_video: 4.142|tagging_loss_audio: 8.211|tagging_loss_text: 17.146|tagging_loss_image: 5.885|tagging_loss_fusion: 2.833|total_loss: 38.216 | 68.07 Examples/sec\n",
      "INFO:tensorflow:training step 6767 | tagging_loss_video: 5.740|tagging_loss_audio: 8.125|tagging_loss_text: 16.721|tagging_loss_image: 5.256|tagging_loss_fusion: 4.609|total_loss: 40.451 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 6768 | tagging_loss_video: 5.491|tagging_loss_audio: 7.919|tagging_loss_text: 14.996|tagging_loss_image: 5.412|tagging_loss_fusion: 4.618|total_loss: 38.436 | 62.62 Examples/sec\n",
      "INFO:tensorflow:training step 6769 | tagging_loss_video: 5.607|tagging_loss_audio: 8.422|tagging_loss_text: 17.295|tagging_loss_image: 5.772|tagging_loss_fusion: 4.983|total_loss: 42.078 | 69.60 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6770 |tagging_loss_video: 5.824|tagging_loss_audio: 8.190|tagging_loss_text: 16.473|tagging_loss_image: 3.954|tagging_loss_fusion: 5.664|total_loss: 40.104 | Examples/sec: 72.35\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.79 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6771 | tagging_loss_video: 5.835|tagging_loss_audio: 7.601|tagging_loss_text: 16.512|tagging_loss_image: 5.191|tagging_loss_fusion: 3.876|total_loss: 39.014 | 62.90 Examples/sec\n",
      "INFO:tensorflow:training step 6772 | tagging_loss_video: 5.107|tagging_loss_audio: 9.251|tagging_loss_text: 18.537|tagging_loss_image: 5.725|tagging_loss_fusion: 4.030|total_loss: 42.650 | 66.84 Examples/sec\n",
      "INFO:tensorflow:training step 6773 | tagging_loss_video: 4.822|tagging_loss_audio: 8.374|tagging_loss_text: 14.956|tagging_loss_image: 5.912|tagging_loss_fusion: 4.216|total_loss: 38.280 | 70.24 Examples/sec\n",
      "INFO:tensorflow:training step 6774 | tagging_loss_video: 5.514|tagging_loss_audio: 7.617|tagging_loss_text: 11.766|tagging_loss_image: 5.359|tagging_loss_fusion: 3.998|total_loss: 34.255 | 60.10 Examples/sec\n",
      "INFO:tensorflow:training step 6775 | tagging_loss_video: 5.746|tagging_loss_audio: 8.616|tagging_loss_text: 15.213|tagging_loss_image: 5.601|tagging_loss_fusion: 5.559|total_loss: 40.735 | 69.22 Examples/sec\n",
      "INFO:tensorflow:training step 6776 | tagging_loss_video: 4.604|tagging_loss_audio: 8.650|tagging_loss_text: 10.869|tagging_loss_image: 5.634|tagging_loss_fusion: 4.504|total_loss: 34.261 | 69.43 Examples/sec\n",
      "INFO:tensorflow:training step 6777 | tagging_loss_video: 5.799|tagging_loss_audio: 7.603|tagging_loss_text: 12.633|tagging_loss_image: 5.627|tagging_loss_fusion: 4.414|total_loss: 36.076 | 66.09 Examples/sec\n",
      "INFO:tensorflow:training step 6778 | tagging_loss_video: 6.789|tagging_loss_audio: 9.680|tagging_loss_text: 18.609|tagging_loss_image: 4.646|tagging_loss_fusion: 5.330|total_loss: 45.054 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 6779 | tagging_loss_video: 4.974|tagging_loss_audio: 8.566|tagging_loss_text: 14.445|tagging_loss_image: 4.137|tagging_loss_fusion: 2.858|total_loss: 34.981 | 69.13 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6780 |tagging_loss_video: 5.505|tagging_loss_audio: 8.150|tagging_loss_text: 18.297|tagging_loss_image: 5.551|tagging_loss_fusion: 4.390|total_loss: 41.894 | Examples/sec: 70.22\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.86 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6781 | tagging_loss_video: 5.083|tagging_loss_audio: 7.471|tagging_loss_text: 13.612|tagging_loss_image: 5.511|tagging_loss_fusion: 3.845|total_loss: 35.522 | 70.58 Examples/sec\n",
      "INFO:tensorflow:training step 6782 | tagging_loss_video: 5.686|tagging_loss_audio: 8.282|tagging_loss_text: 17.318|tagging_loss_image: 3.611|tagging_loss_fusion: 2.993|total_loss: 37.890 | 62.46 Examples/sec\n",
      "INFO:tensorflow:training step 6783 | tagging_loss_video: 4.996|tagging_loss_audio: 10.159|tagging_loss_text: 14.377|tagging_loss_image: 6.317|tagging_loss_fusion: 5.149|total_loss: 40.998 | 71.14 Examples/sec\n",
      "INFO:tensorflow:training step 6784 | tagging_loss_video: 6.544|tagging_loss_audio: 8.057|tagging_loss_text: 17.462|tagging_loss_image: 5.590|tagging_loss_fusion: 4.746|total_loss: 42.399 | 67.95 Examples/sec\n",
      "INFO:tensorflow:training step 6785 | tagging_loss_video: 5.284|tagging_loss_audio: 8.574|tagging_loss_text: 16.011|tagging_loss_image: 3.592|tagging_loss_fusion: 2.837|total_loss: 36.299 | 66.42 Examples/sec\n",
      "INFO:tensorflow:training step 6786 | tagging_loss_video: 6.621|tagging_loss_audio: 9.929|tagging_loss_text: 19.056|tagging_loss_image: 5.948|tagging_loss_fusion: 7.774|total_loss: 49.328 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 6787 | tagging_loss_video: 5.131|tagging_loss_audio: 8.101|tagging_loss_text: 15.905|tagging_loss_image: 4.843|tagging_loss_fusion: 3.405|total_loss: 37.386 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 6788 | tagging_loss_video: 4.989|tagging_loss_audio: 7.861|tagging_loss_text: 14.640|tagging_loss_image: 4.886|tagging_loss_fusion: 3.333|total_loss: 35.709 | 68.38 Examples/sec\n",
      "INFO:tensorflow:training step 6789 | tagging_loss_video: 4.027|tagging_loss_audio: 8.234|tagging_loss_text: 15.408|tagging_loss_image: 5.180|tagging_loss_fusion: 2.131|total_loss: 34.980 | 69.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6790 |tagging_loss_video: 5.820|tagging_loss_audio: 8.901|tagging_loss_text: 16.454|tagging_loss_image: 4.920|tagging_loss_fusion: 4.784|total_loss: 40.880 | Examples/sec: 72.25\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 6791 | tagging_loss_video: 5.721|tagging_loss_audio: 7.730|tagging_loss_text: 15.321|tagging_loss_image: 5.063|tagging_loss_fusion: 3.853|total_loss: 37.688 | 68.14 Examples/sec\n",
      "INFO:tensorflow:training step 6792 | tagging_loss_video: 3.925|tagging_loss_audio: 10.078|tagging_loss_text: 14.825|tagging_loss_image: 6.069|tagging_loss_fusion: 2.262|total_loss: 37.160 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 6793 | tagging_loss_video: 6.619|tagging_loss_audio: 9.181|tagging_loss_text: 19.956|tagging_loss_image: 4.962|tagging_loss_fusion: 4.606|total_loss: 45.324 | 61.41 Examples/sec\n",
      "INFO:tensorflow:training step 6794 | tagging_loss_video: 5.111|tagging_loss_audio: 8.310|tagging_loss_text: 16.379|tagging_loss_image: 5.079|tagging_loss_fusion: 3.791|total_loss: 38.670 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 6795 | tagging_loss_video: 4.868|tagging_loss_audio: 8.909|tagging_loss_text: 15.002|tagging_loss_image: 6.167|tagging_loss_fusion: 4.847|total_loss: 39.792 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 6796 | tagging_loss_video: 5.762|tagging_loss_audio: 8.252|tagging_loss_text: 16.671|tagging_loss_image: 4.323|tagging_loss_fusion: 3.983|total_loss: 38.991 | 64.92 Examples/sec\n",
      "INFO:tensorflow:training step 6797 | tagging_loss_video: 5.759|tagging_loss_audio: 9.263|tagging_loss_text: 15.402|tagging_loss_image: 5.583|tagging_loss_fusion: 4.583|total_loss: 40.591 | 70.15 Examples/sec\n",
      "INFO:tensorflow:training step 6798 | tagging_loss_video: 5.719|tagging_loss_audio: 8.197|tagging_loss_text: 18.262|tagging_loss_image: 4.935|tagging_loss_fusion: 5.577|total_loss: 42.690 | 70.38 Examples/sec\n",
      "INFO:tensorflow:training step 6799 | tagging_loss_video: 4.807|tagging_loss_audio: 7.065|tagging_loss_text: 16.478|tagging_loss_image: 4.712|tagging_loss_fusion: 2.822|total_loss: 35.884 | 62.66 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6800 |tagging_loss_video: 4.965|tagging_loss_audio: 8.084|tagging_loss_text: 13.927|tagging_loss_image: 4.398|tagging_loss_fusion: 2.904|total_loss: 34.279 | Examples/sec: 66.65\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 6801 | tagging_loss_video: 5.236|tagging_loss_audio: 8.320|tagging_loss_text: 14.816|tagging_loss_image: 4.638|tagging_loss_fusion: 3.359|total_loss: 36.369 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 6802 | tagging_loss_video: 4.947|tagging_loss_audio: 7.777|tagging_loss_text: 12.523|tagging_loss_image: 5.326|tagging_loss_fusion: 3.061|total_loss: 33.635 | 68.20 Examples/sec\n",
      "INFO:tensorflow:training step 6803 | tagging_loss_video: 3.810|tagging_loss_audio: 7.355|tagging_loss_text: 13.905|tagging_loss_image: 4.493|tagging_loss_fusion: 2.630|total_loss: 32.192 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 6804 | tagging_loss_video: 5.563|tagging_loss_audio: 7.777|tagging_loss_text: 20.208|tagging_loss_image: 5.874|tagging_loss_fusion: 3.734|total_loss: 43.156 | 64.80 Examples/sec\n",
      "INFO:tensorflow:training step 6805 | tagging_loss_video: 5.913|tagging_loss_audio: 9.242|tagging_loss_text: 12.715|tagging_loss_image: 4.046|tagging_loss_fusion: 3.653|total_loss: 35.569 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 6806 | tagging_loss_video: 5.649|tagging_loss_audio: 8.804|tagging_loss_text: 15.193|tagging_loss_image: 5.473|tagging_loss_fusion: 4.542|total_loss: 39.662 | 69.28 Examples/sec\n",
      "INFO:tensorflow:training step 6807 | tagging_loss_video: 5.944|tagging_loss_audio: 8.065|tagging_loss_text: 15.529|tagging_loss_image: 5.462|tagging_loss_fusion: 4.134|total_loss: 39.133 | 62.84 Examples/sec\n",
      "INFO:tensorflow:training step 6808 | tagging_loss_video: 3.336|tagging_loss_audio: 8.193|tagging_loss_text: 12.676|tagging_loss_image: 5.693|tagging_loss_fusion: 2.704|total_loss: 32.602 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 6809 | tagging_loss_video: 5.591|tagging_loss_audio: 7.906|tagging_loss_text: 15.273|tagging_loss_image: 3.460|tagging_loss_fusion: 4.072|total_loss: 36.303 | 70.84 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6810 |tagging_loss_video: 5.090|tagging_loss_audio: 8.556|tagging_loss_text: 16.225|tagging_loss_image: 5.055|tagging_loss_fusion: 3.969|total_loss: 38.896 | Examples/sec: 63.02\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6811 | tagging_loss_video: 5.712|tagging_loss_audio: 8.543|tagging_loss_text: 17.409|tagging_loss_image: 3.593|tagging_loss_fusion: 5.523|total_loss: 40.781 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 6812 | tagging_loss_video: 6.189|tagging_loss_audio: 9.096|tagging_loss_text: 16.744|tagging_loss_image: 4.892|tagging_loss_fusion: 4.916|total_loss: 41.836 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 6813 | tagging_loss_video: 4.747|tagging_loss_audio: 8.219|tagging_loss_text: 14.102|tagging_loss_image: 4.696|tagging_loss_fusion: 3.055|total_loss: 34.819 | 71.18 Examples/sec\n",
      "INFO:tensorflow:training step 6814 | tagging_loss_video: 5.793|tagging_loss_audio: 8.447|tagging_loss_text: 15.755|tagging_loss_image: 5.719|tagging_loss_fusion: 4.244|total_loss: 39.958 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 6815 | tagging_loss_video: 5.298|tagging_loss_audio: 9.691|tagging_loss_text: 18.518|tagging_loss_image: 5.844|tagging_loss_fusion: 4.220|total_loss: 43.571 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 6816 | tagging_loss_video: 5.225|tagging_loss_audio: 7.701|tagging_loss_text: 14.668|tagging_loss_image: 5.769|tagging_loss_fusion: 3.273|total_loss: 36.636 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 6817 | tagging_loss_video: 5.007|tagging_loss_audio: 8.917|tagging_loss_text: 18.652|tagging_loss_image: 4.250|tagging_loss_fusion: 3.260|total_loss: 40.087 | 71.42 Examples/sec\n",
      "INFO:tensorflow:training step 6818 | tagging_loss_video: 4.772|tagging_loss_audio: 7.282|tagging_loss_text: 14.525|tagging_loss_image: 3.983|tagging_loss_fusion: 3.394|total_loss: 33.955 | 63.48 Examples/sec\n",
      "INFO:tensorflow:training step 6819 | tagging_loss_video: 5.876|tagging_loss_audio: 9.066|tagging_loss_text: 17.135|tagging_loss_image: 5.172|tagging_loss_fusion: 5.477|total_loss: 42.726 | 68.10 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6820 |tagging_loss_video: 6.148|tagging_loss_audio: 8.025|tagging_loss_text: 18.139|tagging_loss_image: 4.430|tagging_loss_fusion: 4.609|total_loss: 41.351 | Examples/sec: 71.08\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.86 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 6821 | tagging_loss_video: 4.976|tagging_loss_audio: 6.512|tagging_loss_text: 15.346|tagging_loss_image: 5.270|tagging_loss_fusion: 4.983|total_loss: 37.086 | 66.86 Examples/sec\n",
      "INFO:tensorflow:training step 6822 | tagging_loss_video: 5.863|tagging_loss_audio: 8.784|tagging_loss_text: 14.200|tagging_loss_image: 4.130|tagging_loss_fusion: 4.422|total_loss: 37.399 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 6823 | tagging_loss_video: 4.981|tagging_loss_audio: 8.567|tagging_loss_text: 16.577|tagging_loss_image: 5.289|tagging_loss_fusion: 3.477|total_loss: 38.891 | 68.30 Examples/sec\n",
      "INFO:tensorflow:training step 6824 | tagging_loss_video: 6.672|tagging_loss_audio: 8.995|tagging_loss_text: 14.875|tagging_loss_image: 5.901|tagging_loss_fusion: 5.742|total_loss: 42.186 | 61.68 Examples/sec\n",
      "INFO:tensorflow:training step 6825 | tagging_loss_video: 4.979|tagging_loss_audio: 8.691|tagging_loss_text: 15.529|tagging_loss_image: 5.165|tagging_loss_fusion: 4.044|total_loss: 38.408 | 68.95 Examples/sec\n",
      "INFO:tensorflow:training step 6826 | tagging_loss_video: 5.130|tagging_loss_audio: 8.670|tagging_loss_text: 15.366|tagging_loss_image: 3.315|tagging_loss_fusion: 3.280|total_loss: 35.761 | 71.80 Examples/sec\n",
      "INFO:tensorflow:training step 6827 | tagging_loss_video: 5.005|tagging_loss_audio: 8.761|tagging_loss_text: 17.003|tagging_loss_image: 4.160|tagging_loss_fusion: 3.491|total_loss: 38.420 | 65.51 Examples/sec\n",
      "INFO:tensorflow:training step 6828 | tagging_loss_video: 4.377|tagging_loss_audio: 7.956|tagging_loss_text: 16.373|tagging_loss_image: 5.797|tagging_loss_fusion: 2.781|total_loss: 37.284 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 6829 | tagging_loss_video: 5.436|tagging_loss_audio: 8.934|tagging_loss_text: 16.791|tagging_loss_image: 4.684|tagging_loss_fusion: 2.399|total_loss: 38.243 | 62.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6830 |tagging_loss_video: 5.604|tagging_loss_audio: 7.953|tagging_loss_text: 15.252|tagging_loss_image: 4.398|tagging_loss_fusion: 3.450|total_loss: 36.657 | Examples/sec: 71.20\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6831 | tagging_loss_video: 4.770|tagging_loss_audio: 7.902|tagging_loss_text: 18.725|tagging_loss_image: 5.099|tagging_loss_fusion: 3.088|total_loss: 39.584 | 70.05 Examples/sec\n",
      "INFO:tensorflow:training step 6832 | tagging_loss_video: 6.481|tagging_loss_audio: 6.881|tagging_loss_text: 15.004|tagging_loss_image: 5.035|tagging_loss_fusion: 4.254|total_loss: 37.655 | 64.91 Examples/sec\n",
      "INFO:tensorflow:training step 6833 | tagging_loss_video: 5.878|tagging_loss_audio: 8.137|tagging_loss_text: 13.411|tagging_loss_image: 4.915|tagging_loss_fusion: 6.161|total_loss: 38.501 | 70.33 Examples/sec\n",
      "INFO:tensorflow:training step 6834 | tagging_loss_video: 5.241|tagging_loss_audio: 8.266|tagging_loss_text: 15.644|tagging_loss_image: 5.301|tagging_loss_fusion: 3.206|total_loss: 37.658 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 6835 | tagging_loss_video: 5.270|tagging_loss_audio: 7.861|tagging_loss_text: 16.262|tagging_loss_image: 5.274|tagging_loss_fusion: 3.746|total_loss: 38.414 | 61.05 Examples/sec\n",
      "INFO:tensorflow:training step 6836 | tagging_loss_video: 4.191|tagging_loss_audio: 7.842|tagging_loss_text: 13.887|tagging_loss_image: 4.731|tagging_loss_fusion: 2.657|total_loss: 33.308 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 6837 | tagging_loss_video: 5.184|tagging_loss_audio: 7.951|tagging_loss_text: 11.031|tagging_loss_image: 5.542|tagging_loss_fusion: 3.547|total_loss: 33.254 | 71.06 Examples/sec\n",
      "INFO:tensorflow:training step 6838 | tagging_loss_video: 5.020|tagging_loss_audio: 7.094|tagging_loss_text: 13.951|tagging_loss_image: 4.839|tagging_loss_fusion: 3.776|total_loss: 34.681 | 61.27 Examples/sec\n",
      "INFO:tensorflow:training step 6839 | tagging_loss_video: 6.580|tagging_loss_audio: 9.380|tagging_loss_text: 16.267|tagging_loss_image: 4.932|tagging_loss_fusion: 5.850|total_loss: 43.008 | 68.87 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6840 |tagging_loss_video: 4.060|tagging_loss_audio: 7.768|tagging_loss_text: 11.938|tagging_loss_image: 5.550|tagging_loss_fusion: 3.722|total_loss: 33.037 | Examples/sec: 70.32\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 6841 | tagging_loss_video: 5.349|tagging_loss_audio: 8.090|tagging_loss_text: 14.763|tagging_loss_image: 4.181|tagging_loss_fusion: 2.747|total_loss: 35.131 | 69.72 Examples/sec\n",
      "INFO:tensorflow:training step 6842 | tagging_loss_video: 5.830|tagging_loss_audio: 8.560|tagging_loss_text: 14.820|tagging_loss_image: 5.369|tagging_loss_fusion: 5.875|total_loss: 40.454 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 6843 | tagging_loss_video: 4.902|tagging_loss_audio: 7.365|tagging_loss_text: 18.865|tagging_loss_image: 5.613|tagging_loss_fusion: 3.013|total_loss: 39.758 | 60.55 Examples/sec\n",
      "INFO:tensorflow:training step 6844 | tagging_loss_video: 5.070|tagging_loss_audio: 7.628|tagging_loss_text: 13.406|tagging_loss_image: 4.120|tagging_loss_fusion: 3.422|total_loss: 33.647 | 70.66 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6845 | tagging_loss_video: 4.684|tagging_loss_audio: 8.116|tagging_loss_text: 13.932|tagging_loss_image: 3.645|tagging_loss_fusion: 3.381|total_loss: 33.758 | 71.72 Examples/sec\n",
      "INFO:tensorflow:training step 6846 | tagging_loss_video: 6.040|tagging_loss_audio: 8.762|tagging_loss_text: 17.273|tagging_loss_image: 4.095|tagging_loss_fusion: 3.372|total_loss: 39.541 | 64.83 Examples/sec\n",
      "INFO:tensorflow:training step 6847 | tagging_loss_video: 5.971|tagging_loss_audio: 9.699|tagging_loss_text: 13.896|tagging_loss_image: 5.538|tagging_loss_fusion: 6.161|total_loss: 41.264 | 69.47 Examples/sec\n",
      "INFO:tensorflow:training step 6848 | tagging_loss_video: 4.591|tagging_loss_audio: 7.127|tagging_loss_text: 15.394|tagging_loss_image: 3.836|tagging_loss_fusion: 3.157|total_loss: 34.105 | 72.80 Examples/sec\n",
      "INFO:tensorflow:training step 6849 | tagging_loss_video: 5.824|tagging_loss_audio: 8.860|tagging_loss_text: 16.820|tagging_loss_image: 4.894|tagging_loss_fusion: 3.411|total_loss: 39.809 | 61.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6850 |tagging_loss_video: 4.351|tagging_loss_audio: 8.420|tagging_loss_text: 13.447|tagging_loss_image: 4.731|tagging_loss_fusion: 3.431|total_loss: 34.380 | Examples/sec: 69.99\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.85 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6851 | tagging_loss_video: 5.788|tagging_loss_audio: 8.319|tagging_loss_text: 15.086|tagging_loss_image: 3.991|tagging_loss_fusion: 3.488|total_loss: 36.673 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 6852 | tagging_loss_video: 5.439|tagging_loss_audio: 7.481|tagging_loss_text: 16.799|tagging_loss_image: 4.017|tagging_loss_fusion: 3.266|total_loss: 37.002 | 66.65 Examples/sec\n",
      "INFO:tensorflow:training step 6853 | tagging_loss_video: 5.983|tagging_loss_audio: 8.215|tagging_loss_text: 13.277|tagging_loss_image: 5.554|tagging_loss_fusion: 5.728|total_loss: 38.756 | 68.92 Examples/sec\n",
      "INFO:tensorflow:training step 6854 | tagging_loss_video: 4.680|tagging_loss_audio: 7.468|tagging_loss_text: 11.713|tagging_loss_image: 4.817|tagging_loss_fusion: 3.762|total_loss: 32.440 | 67.02 Examples/sec\n",
      "INFO:tensorflow:training step 6855 | tagging_loss_video: 4.635|tagging_loss_audio: 8.428|tagging_loss_text: 17.706|tagging_loss_image: 4.649|tagging_loss_fusion: 3.044|total_loss: 38.461 | 69.97 Examples/sec\n",
      "INFO:tensorflow:training step 6856 | tagging_loss_video: 5.766|tagging_loss_audio: 8.329|tagging_loss_text: 11.879|tagging_loss_image: 4.257|tagging_loss_fusion: 3.623|total_loss: 33.855 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 6857 | tagging_loss_video: 4.255|tagging_loss_audio: 8.225|tagging_loss_text: 15.219|tagging_loss_image: 4.288|tagging_loss_fusion: 2.961|total_loss: 34.947 | 61.48 Examples/sec\n",
      "INFO:tensorflow:training step 6858 | tagging_loss_video: 6.903|tagging_loss_audio: 8.529|tagging_loss_text: 18.996|tagging_loss_image: 5.602|tagging_loss_fusion: 7.205|total_loss: 47.234 | 67.20 Examples/sec\n",
      "INFO:tensorflow:training step 6859 | tagging_loss_video: 6.152|tagging_loss_audio: 9.290|tagging_loss_text: 15.959|tagging_loss_image: 6.466|tagging_loss_fusion: 4.587|total_loss: 42.454 | 71.75 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6860 |tagging_loss_video: 5.773|tagging_loss_audio: 7.549|tagging_loss_text: 11.956|tagging_loss_image: 4.570|tagging_loss_fusion: 4.630|total_loss: 34.478 | Examples/sec: 59.70\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.95 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 6861 | tagging_loss_video: 4.273|tagging_loss_audio: 6.665|tagging_loss_text: 14.995|tagging_loss_image: 4.453|tagging_loss_fusion: 2.954|total_loss: 33.339 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 6862 | tagging_loss_video: 6.073|tagging_loss_audio: 7.865|tagging_loss_text: 15.434|tagging_loss_image: 4.699|tagging_loss_fusion: 5.410|total_loss: 39.480 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 6863 | tagging_loss_video: 4.235|tagging_loss_audio: 7.438|tagging_loss_text: 11.526|tagging_loss_image: 4.557|tagging_loss_fusion: 2.857|total_loss: 30.613 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 6864 | tagging_loss_video: 5.048|tagging_loss_audio: 8.562|tagging_loss_text: 14.710|tagging_loss_image: 3.991|tagging_loss_fusion: 4.150|total_loss: 36.461 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 6865 | tagging_loss_video: 5.384|tagging_loss_audio: 8.536|tagging_loss_text: 15.787|tagging_loss_image: 4.917|tagging_loss_fusion: 5.086|total_loss: 39.710 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 6866 | tagging_loss_video: 5.967|tagging_loss_audio: 7.284|tagging_loss_text: 16.626|tagging_loss_image: 5.182|tagging_loss_fusion: 4.492|total_loss: 39.551 | 72.06 Examples/sec\n",
      "INFO:tensorflow:training step 6867 | tagging_loss_video: 4.785|tagging_loss_audio: 7.272|tagging_loss_text: 15.261|tagging_loss_image: 5.176|tagging_loss_fusion: 3.033|total_loss: 35.526 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 6868 | tagging_loss_video: 7.283|tagging_loss_audio: 8.404|tagging_loss_text: 17.691|tagging_loss_image: 4.638|tagging_loss_fusion: 5.092|total_loss: 43.108 | 60.52 Examples/sec\n",
      "INFO:tensorflow:training step 6869 | tagging_loss_video: 5.872|tagging_loss_audio: 7.605|tagging_loss_text: 16.418|tagging_loss_image: 4.642|tagging_loss_fusion: 4.914|total_loss: 39.451 | 69.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6870 |tagging_loss_video: 5.527|tagging_loss_audio: 7.454|tagging_loss_text: 15.465|tagging_loss_image: 4.640|tagging_loss_fusion: 4.172|total_loss: 37.257 | Examples/sec: 70.45\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6871 | tagging_loss_video: 4.799|tagging_loss_audio: 7.272|tagging_loss_text: 11.170|tagging_loss_image: 4.532|tagging_loss_fusion: 3.690|total_loss: 31.464 | 61.98 Examples/sec\n",
      "INFO:tensorflow:training step 6872 | tagging_loss_video: 5.035|tagging_loss_audio: 8.819|tagging_loss_text: 12.429|tagging_loss_image: 4.225|tagging_loss_fusion: 2.713|total_loss: 33.221 | 71.69 Examples/sec\n",
      "INFO:tensorflow:training step 6873 | tagging_loss_video: 4.958|tagging_loss_audio: 8.065|tagging_loss_text: 13.796|tagging_loss_image: 6.030|tagging_loss_fusion: 4.014|total_loss: 36.862 | 70.47 Examples/sec\n",
      "INFO:tensorflow:training step 6874 | tagging_loss_video: 4.672|tagging_loss_audio: 7.924|tagging_loss_text: 14.159|tagging_loss_image: 4.613|tagging_loss_fusion: 3.600|total_loss: 34.967 | 61.20 Examples/sec\n",
      "INFO:tensorflow:training step 6875 | tagging_loss_video: 4.611|tagging_loss_audio: 7.872|tagging_loss_text: 16.118|tagging_loss_image: 5.053|tagging_loss_fusion: 3.524|total_loss: 37.178 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 6876 | tagging_loss_video: 5.492|tagging_loss_audio: 8.245|tagging_loss_text: 13.083|tagging_loss_image: 5.374|tagging_loss_fusion: 4.830|total_loss: 37.023 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 6877 | tagging_loss_video: 4.337|tagging_loss_audio: 8.144|tagging_loss_text: 14.346|tagging_loss_image: 3.919|tagging_loss_fusion: 3.488|total_loss: 34.234 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 6878 | tagging_loss_video: 5.578|tagging_loss_audio: 7.436|tagging_loss_text: 16.949|tagging_loss_image: 4.832|tagging_loss_fusion: 5.727|total_loss: 40.521 | 68.72 Examples/sec\n",
      "INFO:tensorflow:training step 6879 | tagging_loss_video: 5.793|tagging_loss_audio: 6.808|tagging_loss_text: 15.842|tagging_loss_image: 4.947|tagging_loss_fusion: 4.056|total_loss: 37.446 | 68.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6880 |tagging_loss_video: 4.744|tagging_loss_audio: 8.149|tagging_loss_text: 17.228|tagging_loss_image: 4.606|tagging_loss_fusion: 2.417|total_loss: 37.143 | Examples/sec: 71.24\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.88 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.97\n",
      "INFO:tensorflow:training step 6881 | tagging_loss_video: 5.934|tagging_loss_audio: 8.072|tagging_loss_text: 14.071|tagging_loss_image: 4.538|tagging_loss_fusion: 4.352|total_loss: 36.966 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 6882 | tagging_loss_video: 5.607|tagging_loss_audio: 7.816|tagging_loss_text: 17.356|tagging_loss_image: 6.035|tagging_loss_fusion: 4.572|total_loss: 41.385 | 71.53 Examples/sec\n",
      "INFO:tensorflow:training step 6883 | tagging_loss_video: 5.535|tagging_loss_audio: 8.432|tagging_loss_text: 15.439|tagging_loss_image: 5.255|tagging_loss_fusion: 4.540|total_loss: 39.202 | 59.65 Examples/sec\n",
      "INFO:tensorflow:training step 6884 | tagging_loss_video: 5.470|tagging_loss_audio: 8.106|tagging_loss_text: 15.516|tagging_loss_image: 5.082|tagging_loss_fusion: 2.997|total_loss: 37.170 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 6885 | tagging_loss_video: 5.566|tagging_loss_audio: 8.461|tagging_loss_text: 16.421|tagging_loss_image: 3.574|tagging_loss_fusion: 3.151|total_loss: 37.173 | 71.59 Examples/sec\n",
      "INFO:tensorflow:training step 6886 | tagging_loss_video: 4.422|tagging_loss_audio: 7.236|tagging_loss_text: 13.947|tagging_loss_image: 5.372|tagging_loss_fusion: 2.484|total_loss: 33.460 | 63.51 Examples/sec\n",
      "INFO:tensorflow:training step 6887 | tagging_loss_video: 4.994|tagging_loss_audio: 7.100|tagging_loss_text: 13.059|tagging_loss_image: 4.558|tagging_loss_fusion: 4.235|total_loss: 33.946 | 67.70 Examples/sec\n",
      "INFO:tensorflow:training step 6888 | tagging_loss_video: 5.537|tagging_loss_audio: 8.103|tagging_loss_text: 15.349|tagging_loss_image: 4.872|tagging_loss_fusion: 3.859|total_loss: 37.719 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 6889 | tagging_loss_video: 4.414|tagging_loss_audio: 7.743|tagging_loss_text: 15.467|tagging_loss_image: 4.425|tagging_loss_fusion: 3.182|total_loss: 35.230 | 62.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6890 |tagging_loss_video: 4.519|tagging_loss_audio: 7.738|tagging_loss_text: 13.793|tagging_loss_image: 5.046|tagging_loss_fusion: 2.608|total_loss: 33.704 | Examples/sec: 67.14\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6891 | tagging_loss_video: 4.427|tagging_loss_audio: 7.430|tagging_loss_text: 16.467|tagging_loss_image: 4.614|tagging_loss_fusion: 2.464|total_loss: 35.402 | 68.07 Examples/sec\n",
      "INFO:tensorflow:training step 6892 | tagging_loss_video: 5.093|tagging_loss_audio: 7.779|tagging_loss_text: 13.484|tagging_loss_image: 5.156|tagging_loss_fusion: 3.960|total_loss: 35.471 | 70.59 Examples/sec\n",
      "INFO:tensorflow:training step 6893 | tagging_loss_video: 6.212|tagging_loss_audio: 8.384|tagging_loss_text: 15.942|tagging_loss_image: 4.873|tagging_loss_fusion: 4.751|total_loss: 40.161 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 6894 | tagging_loss_video: 5.939|tagging_loss_audio: 8.770|tagging_loss_text: 14.259|tagging_loss_image: 5.269|tagging_loss_fusion: 4.538|total_loss: 38.775 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 6895 | tagging_loss_video: 4.957|tagging_loss_audio: 7.750|tagging_loss_text: 13.280|tagging_loss_image: 4.883|tagging_loss_fusion: 3.384|total_loss: 34.254 | 68.55 Examples/sec\n",
      "INFO:tensorflow:training step 6896 | tagging_loss_video: 6.148|tagging_loss_audio: 8.573|tagging_loss_text: 16.261|tagging_loss_image: 6.298|tagging_loss_fusion: 6.142|total_loss: 43.423 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 6897 | tagging_loss_video: 6.692|tagging_loss_audio: 9.599|tagging_loss_text: 14.399|tagging_loss_image: 5.210|tagging_loss_fusion: 4.045|total_loss: 39.946 | 63.46 Examples/sec\n",
      "INFO:tensorflow:training step 6898 | tagging_loss_video: 5.226|tagging_loss_audio: 7.695|tagging_loss_text: 14.648|tagging_loss_image: 4.694|tagging_loss_fusion: 3.898|total_loss: 36.161 | 69.87 Examples/sec\n",
      "INFO:tensorflow:training step 6899 | tagging_loss_video: 5.143|tagging_loss_audio: 7.730|tagging_loss_text: 12.480|tagging_loss_image: 4.320|tagging_loss_fusion: 2.161|total_loss: 31.834 | 67.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6900 |tagging_loss_video: 4.879|tagging_loss_audio: 7.774|tagging_loss_text: 11.039|tagging_loss_image: 4.245|tagging_loss_fusion: 2.888|total_loss: 30.825 | Examples/sec: 72.16\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6901 | tagging_loss_video: 5.211|tagging_loss_audio: 8.617|tagging_loss_text: 16.410|tagging_loss_image: 4.315|tagging_loss_fusion: 2.596|total_loss: 37.149 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 6902 | tagging_loss_video: 5.260|tagging_loss_audio: 6.950|tagging_loss_text: 12.828|tagging_loss_image: 3.688|tagging_loss_fusion: 4.272|total_loss: 32.998 | 72.51 Examples/sec\n",
      "INFO:tensorflow:training step 6903 | tagging_loss_video: 5.464|tagging_loss_audio: 8.355|tagging_loss_text: 15.332|tagging_loss_image: 5.120|tagging_loss_fusion: 4.290|total_loss: 38.560 | 60.30 Examples/sec\n",
      "INFO:tensorflow:training step 6904 | tagging_loss_video: 4.899|tagging_loss_audio: 7.727|tagging_loss_text: 14.805|tagging_loss_image: 4.983|tagging_loss_fusion: 2.900|total_loss: 35.314 | 68.08 Examples/sec\n",
      "INFO:tensorflow:training step 6905 | tagging_loss_video: 5.623|tagging_loss_audio: 8.640|tagging_loss_text: 17.750|tagging_loss_image: 5.345|tagging_loss_fusion: 4.587|total_loss: 41.946 | 66.99 Examples/sec\n",
      "INFO:tensorflow:training step 6906 | tagging_loss_video: 5.930|tagging_loss_audio: 8.257|tagging_loss_text: 17.138|tagging_loss_image: 4.322|tagging_loss_fusion: 4.806|total_loss: 40.452 | 66.88 Examples/sec\n",
      "INFO:tensorflow:training step 6907 | tagging_loss_video: 5.703|tagging_loss_audio: 8.481|tagging_loss_text: 14.608|tagging_loss_image: 5.403|tagging_loss_fusion: 5.111|total_loss: 39.307 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 6908 | tagging_loss_video: 5.938|tagging_loss_audio: 8.625|tagging_loss_text: 10.219|tagging_loss_image: 4.149|tagging_loss_fusion: 4.012|total_loss: 32.943 | 65.84 Examples/sec\n",
      "INFO:tensorflow:training step 6909 | tagging_loss_video: 4.416|tagging_loss_audio: 8.741|tagging_loss_text: 18.761|tagging_loss_image: 5.390|tagging_loss_fusion: 3.236|total_loss: 40.545 | 68.22 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6910 |tagging_loss_video: 4.701|tagging_loss_audio: 8.396|tagging_loss_text: 15.833|tagging_loss_image: 5.072|tagging_loss_fusion: 2.602|total_loss: 36.604 | Examples/sec: 69.90\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6911 | tagging_loss_video: 5.870|tagging_loss_audio: 9.049|tagging_loss_text: 15.786|tagging_loss_image: 5.862|tagging_loss_fusion: 6.112|total_loss: 42.680 | 66.49 Examples/sec\n",
      "INFO:tensorflow:training step 6912 | tagging_loss_video: 6.520|tagging_loss_audio: 8.710|tagging_loss_text: 20.028|tagging_loss_image: 5.765|tagging_loss_fusion: 6.052|total_loss: 47.076 | 67.40 Examples/sec\n",
      "INFO:tensorflow:training step 6913 | tagging_loss_video: 5.837|tagging_loss_audio: 7.927|tagging_loss_text: 17.557|tagging_loss_image: 5.526|tagging_loss_fusion: 6.235|total_loss: 43.082 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 6914 | tagging_loss_video: 5.088|tagging_loss_audio: 8.910|tagging_loss_text: 18.015|tagging_loss_image: 5.636|tagging_loss_fusion: 4.171|total_loss: 41.818 | 65.46 Examples/sec\n",
      "INFO:tensorflow:training step 6915 | tagging_loss_video: 5.465|tagging_loss_audio: 8.577|tagging_loss_text: 15.211|tagging_loss_image: 4.824|tagging_loss_fusion: 4.044|total_loss: 38.121 | 67.34 Examples/sec\n",
      "INFO:tensorflow:training step 6916 | tagging_loss_video: 5.681|tagging_loss_audio: 8.394|tagging_loss_text: 14.463|tagging_loss_image: 4.744|tagging_loss_fusion: 3.948|total_loss: 37.230 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 6917 | tagging_loss_video: 5.482|tagging_loss_audio: 7.983|tagging_loss_text: 15.455|tagging_loss_image: 4.848|tagging_loss_fusion: 2.971|total_loss: 36.739 | 70.53 Examples/sec\n",
      "INFO:tensorflow:training step 6918 | tagging_loss_video: 6.708|tagging_loss_audio: 9.135|tagging_loss_text: 18.622|tagging_loss_image: 4.891|tagging_loss_fusion: 4.800|total_loss: 44.156 | 69.32 Examples/sec\n",
      "INFO:tensorflow:training step 6919 | tagging_loss_video: 5.315|tagging_loss_audio: 8.502|tagging_loss_text: 14.436|tagging_loss_image: 5.721|tagging_loss_fusion: 5.888|total_loss: 39.863 | 70.18 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6920 |tagging_loss_video: 6.445|tagging_loss_audio: 8.315|tagging_loss_text: 16.198|tagging_loss_image: 6.144|tagging_loss_fusion: 7.120|total_loss: 44.222 | Examples/sec: 70.45\n",
      "INFO:tensorflow:GAP: 0.91 | precision@0.1: 0.80 | precision@0.5: 0.92 |recall@0.1: 0.96 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 6921 | tagging_loss_video: 5.327|tagging_loss_audio: 7.563|tagging_loss_text: 12.987|tagging_loss_image: 4.497|tagging_loss_fusion: 3.928|total_loss: 34.303 | 71.94 Examples/sec\n",
      "INFO:tensorflow:training step 6922 | tagging_loss_video: 5.211|tagging_loss_audio: 9.214|tagging_loss_text: 15.164|tagging_loss_image: 5.464|tagging_loss_fusion: 4.156|total_loss: 39.209 | 64.42 Examples/sec\n",
      "INFO:tensorflow:training step 6923 | tagging_loss_video: 5.898|tagging_loss_audio: 8.816|tagging_loss_text: 17.272|tagging_loss_image: 5.538|tagging_loss_fusion: 4.358|total_loss: 41.883 | 66.60 Examples/sec\n",
      "INFO:tensorflow:training step 6924 | tagging_loss_video: 5.874|tagging_loss_audio: 8.317|tagging_loss_text: 17.464|tagging_loss_image: 5.914|tagging_loss_fusion: 4.795|total_loss: 42.365 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 6925 | tagging_loss_video: 4.798|tagging_loss_audio: 8.307|tagging_loss_text: 15.319|tagging_loss_image: 5.806|tagging_loss_fusion: 3.989|total_loss: 38.219 | 60.17 Examples/sec\n",
      "INFO:tensorflow:training step 6926 | tagging_loss_video: 4.889|tagging_loss_audio: 9.272|tagging_loss_text: 16.778|tagging_loss_image: 6.353|tagging_loss_fusion: 3.140|total_loss: 40.431 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 6927 | tagging_loss_video: 6.200|tagging_loss_audio: 8.091|tagging_loss_text: 17.268|tagging_loss_image: 4.676|tagging_loss_fusion: 5.010|total_loss: 41.245 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 6928 | tagging_loss_video: 5.146|tagging_loss_audio: 8.781|tagging_loss_text: 14.629|tagging_loss_image: 4.933|tagging_loss_fusion: 3.158|total_loss: 36.647 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 6929 | tagging_loss_video: 5.544|tagging_loss_audio: 9.094|tagging_loss_text: 16.737|tagging_loss_image: 4.552|tagging_loss_fusion: 3.544|total_loss: 39.470 | 68.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6930 |tagging_loss_video: 5.085|tagging_loss_audio: 7.889|tagging_loss_text: 13.696|tagging_loss_image: 4.179|tagging_loss_fusion: 2.970|total_loss: 33.819 | Examples/sec: 71.33\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.89 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6931 | tagging_loss_video: 5.530|tagging_loss_audio: 7.404|tagging_loss_text: 15.936|tagging_loss_image: 4.227|tagging_loss_fusion: 4.032|total_loss: 37.129 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 6932 | tagging_loss_video: 5.900|tagging_loss_audio: 10.560|tagging_loss_text: 15.587|tagging_loss_image: 4.438|tagging_loss_fusion: 3.734|total_loss: 40.218 | 67.80 Examples/sec\n",
      "INFO:tensorflow:training step 6933 | tagging_loss_video: 4.196|tagging_loss_audio: 9.377|tagging_loss_text: 17.733|tagging_loss_image: 4.752|tagging_loss_fusion: 3.255|total_loss: 39.313 | 63.37 Examples/sec\n",
      "INFO:tensorflow:training step 6934 | tagging_loss_video: 5.463|tagging_loss_audio: 7.927|tagging_loss_text: 14.395|tagging_loss_image: 5.453|tagging_loss_fusion: 4.411|total_loss: 37.649 | 67.32 Examples/sec\n",
      "INFO:tensorflow:training step 6935 | tagging_loss_video: 5.186|tagging_loss_audio: 8.473|tagging_loss_text: 15.677|tagging_loss_image: 5.136|tagging_loss_fusion: 5.220|total_loss: 39.692 | 68.78 Examples/sec\n",
      "INFO:tensorflow:training step 6936 | tagging_loss_video: 3.950|tagging_loss_audio: 7.989|tagging_loss_text: 16.687|tagging_loss_image: 5.119|tagging_loss_fusion: 3.542|total_loss: 37.288 | 65.54 Examples/sec\n",
      "INFO:tensorflow:training step 6937 | tagging_loss_video: 4.857|tagging_loss_audio: 9.468|tagging_loss_text: 15.418|tagging_loss_image: 5.213|tagging_loss_fusion: 2.621|total_loss: 37.577 | 70.46 Examples/sec\n",
      "INFO:tensorflow:training step 6938 | tagging_loss_video: 5.760|tagging_loss_audio: 8.205|tagging_loss_text: 12.144|tagging_loss_image: 4.895|tagging_loss_fusion: 3.901|total_loss: 34.906 | 71.37 Examples/sec\n",
      "INFO:tensorflow:training step 6939 | tagging_loss_video: 3.920|tagging_loss_audio: 8.647|tagging_loss_text: 14.508|tagging_loss_image: 5.243|tagging_loss_fusion: 1.671|total_loss: 33.990 | 63.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6940 |tagging_loss_video: 5.391|tagging_loss_audio: 7.492|tagging_loss_text: 13.911|tagging_loss_image: 3.478|tagging_loss_fusion: 3.486|total_loss: 33.758 | Examples/sec: 67.39\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6941 | tagging_loss_video: 6.014|tagging_loss_audio: 7.784|tagging_loss_text: 16.844|tagging_loss_image: 5.584|tagging_loss_fusion: 4.040|total_loss: 40.266 | 69.84 Examples/sec\n",
      "INFO:tensorflow:training step 6942 | tagging_loss_video: 6.009|tagging_loss_audio: 7.774|tagging_loss_text: 15.821|tagging_loss_image: 5.271|tagging_loss_fusion: 5.418|total_loss: 40.292 | 63.66 Examples/sec\n",
      "INFO:tensorflow:training step 6943 | tagging_loss_video: 3.923|tagging_loss_audio: 7.908|tagging_loss_text: 16.434|tagging_loss_image: 4.685|tagging_loss_fusion: 2.677|total_loss: 35.626 | 68.02 Examples/sec\n",
      "INFO:tensorflow:training step 6944 | tagging_loss_video: 5.599|tagging_loss_audio: 7.671|tagging_loss_text: 16.723|tagging_loss_image: 5.861|tagging_loss_fusion: 4.084|total_loss: 39.939 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 6945 | tagging_loss_video: 5.876|tagging_loss_audio: 7.725|tagging_loss_text: 10.711|tagging_loss_image: 5.558|tagging_loss_fusion: 4.599|total_loss: 34.470 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 6946 | tagging_loss_video: 4.740|tagging_loss_audio: 8.832|tagging_loss_text: 18.494|tagging_loss_image: 4.994|tagging_loss_fusion: 2.774|total_loss: 39.834 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 6947 | tagging_loss_video: 5.293|tagging_loss_audio: 7.734|tagging_loss_text: 16.813|tagging_loss_image: 5.547|tagging_loss_fusion: 3.387|total_loss: 38.774 | 60.08 Examples/sec\n",
      "INFO:tensorflow:training step 6948 | tagging_loss_video: 5.441|tagging_loss_audio: 8.709|tagging_loss_text: 17.342|tagging_loss_image: 4.558|tagging_loss_fusion: 4.583|total_loss: 40.633 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 6949 | tagging_loss_video: 5.333|tagging_loss_audio: 7.993|tagging_loss_text: 14.604|tagging_loss_image: 5.121|tagging_loss_fusion: 4.482|total_loss: 37.532 | 69.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6950 |tagging_loss_video: 4.241|tagging_loss_audio: 8.361|tagging_loss_text: 15.546|tagging_loss_image: 5.476|tagging_loss_fusion: 2.833|total_loss: 36.456 | Examples/sec: 65.30\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.89 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 6951 | tagging_loss_video: 5.507|tagging_loss_audio: 9.156|tagging_loss_text: 13.223|tagging_loss_image: 5.016|tagging_loss_fusion: 3.090|total_loss: 35.993 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 6952 | tagging_loss_video: 5.741|tagging_loss_audio: 8.831|tagging_loss_text: 17.519|tagging_loss_image: 6.195|tagging_loss_fusion: 4.203|total_loss: 42.489 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 6953 | tagging_loss_video: 5.839|tagging_loss_audio: 8.443|tagging_loss_text: 12.078|tagging_loss_image: 4.347|tagging_loss_fusion: 4.387|total_loss: 35.095 | 62.29 Examples/sec\n",
      "INFO:tensorflow:training step 6954 | tagging_loss_video: 6.381|tagging_loss_audio: 8.039|tagging_loss_text: 11.636|tagging_loss_image: 5.323|tagging_loss_fusion: 6.629|total_loss: 38.009 | 66.16 Examples/sec\n",
      "INFO:tensorflow:training step 6955 | tagging_loss_video: 4.592|tagging_loss_audio: 9.935|tagging_loss_text: 16.393|tagging_loss_image: 4.756|tagging_loss_fusion: 2.468|total_loss: 38.144 | 70.10 Examples/sec\n",
      "INFO:tensorflow:training step 6956 | tagging_loss_video: 5.951|tagging_loss_audio: 8.373|tagging_loss_text: 14.665|tagging_loss_image: 4.733|tagging_loss_fusion: 4.255|total_loss: 37.977 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 6957 | tagging_loss_video: 6.228|tagging_loss_audio: 9.864|tagging_loss_text: 17.938|tagging_loss_image: 5.801|tagging_loss_fusion: 5.781|total_loss: 45.611 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 6958 | tagging_loss_video: 5.364|tagging_loss_audio: 7.947|tagging_loss_text: 13.858|tagging_loss_image: 5.274|tagging_loss_fusion: 4.785|total_loss: 37.228 | 59.73 Examples/sec\n",
      "INFO:tensorflow:training step 6959 | tagging_loss_video: 5.118|tagging_loss_audio: 8.705|tagging_loss_text: 15.061|tagging_loss_image: 4.821|tagging_loss_fusion: 3.379|total_loss: 37.083 | 72.51 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6960 |tagging_loss_video: 5.909|tagging_loss_audio: 9.234|tagging_loss_text: 16.056|tagging_loss_image: 6.153|tagging_loss_fusion: 6.463|total_loss: 43.816 | Examples/sec: 70.89\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.81 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 6961 | tagging_loss_video: 5.063|tagging_loss_audio: 6.896|tagging_loss_text: 12.891|tagging_loss_image: 3.896|tagging_loss_fusion: 2.336|total_loss: 31.082 | 65.17 Examples/sec\n",
      "INFO:tensorflow:training step 6962 | tagging_loss_video: 4.110|tagging_loss_audio: 8.413|tagging_loss_text: 16.885|tagging_loss_image: 5.458|tagging_loss_fusion: 2.811|total_loss: 37.678 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 6963 | tagging_loss_video: 4.947|tagging_loss_audio: 8.772|tagging_loss_text: 13.127|tagging_loss_image: 3.601|tagging_loss_fusion: 3.328|total_loss: 33.775 | 70.74 Examples/sec\n",
      "INFO:tensorflow:training step 6964 | tagging_loss_video: 5.447|tagging_loss_audio: 8.653|tagging_loss_text: 18.223|tagging_loss_image: 4.642|tagging_loss_fusion: 4.705|total_loss: 41.669 | 63.42 Examples/sec\n",
      "INFO:tensorflow:training step 6965 | tagging_loss_video: 5.442|tagging_loss_audio: 8.796|tagging_loss_text: 17.742|tagging_loss_image: 6.082|tagging_loss_fusion: 4.102|total_loss: 42.163 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 6966 | tagging_loss_video: 4.578|tagging_loss_audio: 8.559|tagging_loss_text: 14.690|tagging_loss_image: 4.873|tagging_loss_fusion: 3.283|total_loss: 35.983 | 71.40 Examples/sec\n",
      "INFO:tensorflow:training step 6967 | tagging_loss_video: 4.748|tagging_loss_audio: 7.433|tagging_loss_text: 17.001|tagging_loss_image: 5.109|tagging_loss_fusion: 3.972|total_loss: 38.263 | 65.50 Examples/sec\n",
      "INFO:tensorflow:training step 6968 | tagging_loss_video: 5.762|tagging_loss_audio: 8.244|tagging_loss_text: 15.652|tagging_loss_image: 5.999|tagging_loss_fusion: 5.396|total_loss: 41.053 | 69.51 Examples/sec\n",
      "INFO:tensorflow:training step 6969 | tagging_loss_video: 5.176|tagging_loss_audio: 8.792|tagging_loss_text: 13.443|tagging_loss_image: 5.381|tagging_loss_fusion: 3.329|total_loss: 36.120 | 72.00 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6970 |tagging_loss_video: 4.872|tagging_loss_audio: 7.752|tagging_loss_text: 16.626|tagging_loss_image: 5.221|tagging_loss_fusion: 3.112|total_loss: 37.583 | Examples/sec: 70.10\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 6971 | tagging_loss_video: 4.510|tagging_loss_audio: 7.623|tagging_loss_text: 16.699|tagging_loss_image: 4.221|tagging_loss_fusion: 1.892|total_loss: 34.945 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 6972 | tagging_loss_video: 5.758|tagging_loss_audio: 7.255|tagging_loss_text: 17.726|tagging_loss_image: 4.714|tagging_loss_fusion: 4.672|total_loss: 40.125 | 59.17 Examples/sec\n",
      "INFO:tensorflow:training step 6973 | tagging_loss_video: 4.978|tagging_loss_audio: 8.221|tagging_loss_text: 14.749|tagging_loss_image: 4.943|tagging_loss_fusion: 2.394|total_loss: 35.285 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 6974 | tagging_loss_video: 3.792|tagging_loss_audio: 7.817|tagging_loss_text: 12.239|tagging_loss_image: 4.614|tagging_loss_fusion: 3.027|total_loss: 31.489 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 6975 | tagging_loss_video: 4.123|tagging_loss_audio: 7.731|tagging_loss_text: 15.625|tagging_loss_image: 5.097|tagging_loss_fusion: 2.687|total_loss: 35.264 | 61.98 Examples/sec\n",
      "INFO:tensorflow:training step 6976 | tagging_loss_video: 6.289|tagging_loss_audio: 8.647|tagging_loss_text: 18.031|tagging_loss_image: 5.648|tagging_loss_fusion: 4.112|total_loss: 42.727 | 67.52 Examples/sec\n",
      "INFO:tensorflow:training step 6977 | tagging_loss_video: 5.542|tagging_loss_audio: 7.664|tagging_loss_text: 14.471|tagging_loss_image: 3.552|tagging_loss_fusion: 4.268|total_loss: 35.497 | 68.47 Examples/sec\n",
      "INFO:tensorflow:training step 6978 | tagging_loss_video: 5.607|tagging_loss_audio: 7.457|tagging_loss_text: 16.045|tagging_loss_image: 5.686|tagging_loss_fusion: 5.216|total_loss: 40.012 | 69.86 Examples/sec\n",
      "INFO:tensorflow:training step 6979 | tagging_loss_video: 5.248|tagging_loss_audio: 8.948|tagging_loss_text: 18.486|tagging_loss_image: 6.171|tagging_loss_fusion: 3.853|total_loss: 42.706 | 65.77 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 6979.\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6980 |tagging_loss_video: 5.856|tagging_loss_audio: 7.693|tagging_loss_text: 15.461|tagging_loss_image: 4.608|tagging_loss_fusion: 3.861|total_loss: 37.479 | Examples/sec: 52.60\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 6981 | tagging_loss_video: 5.971|tagging_loss_audio: 8.942|tagging_loss_text: 14.799|tagging_loss_image: 6.507|tagging_loss_fusion: 4.960|total_loss: 41.179 | 65.23 Examples/sec\n",
      "INFO:tensorflow:training step 6982 | tagging_loss_video: 3.711|tagging_loss_audio: 8.318|tagging_loss_text: 17.502|tagging_loss_image: 5.306|tagging_loss_fusion: 2.310|total_loss: 37.147 | 61.08 Examples/sec\n",
      "INFO:tensorflow:training step 6983 | tagging_loss_video: 5.788|tagging_loss_audio: 7.572|tagging_loss_text: 15.536|tagging_loss_image: 4.962|tagging_loss_fusion: 4.199|total_loss: 38.057 | 68.46 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 6984 | tagging_loss_video: 5.637|tagging_loss_audio: 7.983|tagging_loss_text: 15.343|tagging_loss_image: 4.984|tagging_loss_fusion: 5.738|total_loss: 39.686 | 71.28 Examples/sec\n",
      "INFO:tensorflow:training step 6985 | tagging_loss_video: 5.380|tagging_loss_audio: 7.379|tagging_loss_text: 16.584|tagging_loss_image: 4.811|tagging_loss_fusion: 4.679|total_loss: 38.833 | 60.39 Examples/sec\n",
      "INFO:tensorflow:training step 6986 | tagging_loss_video: 6.254|tagging_loss_audio: 8.465|tagging_loss_text: 15.982|tagging_loss_image: 4.555|tagging_loss_fusion: 4.567|total_loss: 39.824 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 6987 | tagging_loss_video: 5.680|tagging_loss_audio: 8.766|tagging_loss_text: 15.385|tagging_loss_image: 5.386|tagging_loss_fusion: 4.397|total_loss: 39.613 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 6988 | tagging_loss_video: 6.168|tagging_loss_audio: 9.384|tagging_loss_text: 17.560|tagging_loss_image: 5.151|tagging_loss_fusion: 3.792|total_loss: 42.055 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 6989 | tagging_loss_video: 5.337|tagging_loss_audio: 7.491|tagging_loss_text: 14.752|tagging_loss_image: 4.025|tagging_loss_fusion: 2.856|total_loss: 34.460 | 67.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 6990 |tagging_loss_video: 5.694|tagging_loss_audio: 8.869|tagging_loss_text: 14.413|tagging_loss_image: 5.083|tagging_loss_fusion: 4.248|total_loss: 38.307 | Examples/sec: 69.48\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.84 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 6991 | tagging_loss_video: 5.436|tagging_loss_audio: 7.819|tagging_loss_text: 14.083|tagging_loss_image: 4.743|tagging_loss_fusion: 3.313|total_loss: 35.395 | 71.86 Examples/sec\n",
      "INFO:tensorflow:training step 6992 | tagging_loss_video: 5.212|tagging_loss_audio: 7.596|tagging_loss_text: 13.962|tagging_loss_image: 4.566|tagging_loss_fusion: 3.344|total_loss: 34.679 | 68.69 Examples/sec\n",
      "INFO:tensorflow:training step 6993 | tagging_loss_video: 5.430|tagging_loss_audio: 7.283|tagging_loss_text: 16.295|tagging_loss_image: 3.905|tagging_loss_fusion: 2.284|total_loss: 35.198 | 67.36 Examples/sec\n",
      "INFO:tensorflow:training step 6994 | tagging_loss_video: 5.975|tagging_loss_audio: 8.590|tagging_loss_text: 14.289|tagging_loss_image: 5.069|tagging_loss_fusion: 4.575|total_loss: 38.499 | 67.33 Examples/sec\n",
      "INFO:tensorflow:training step 6995 | tagging_loss_video: 5.159|tagging_loss_audio: 7.761|tagging_loss_text: 20.055|tagging_loss_image: 5.920|tagging_loss_fusion: 3.110|total_loss: 42.004 | 69.78 Examples/sec\n",
      "INFO:tensorflow:training step 6996 | tagging_loss_video: 6.155|tagging_loss_audio: 8.173|tagging_loss_text: 17.402|tagging_loss_image: 6.676|tagging_loss_fusion: 5.339|total_loss: 43.745 | 69.61 Examples/sec\n",
      "INFO:tensorflow:training step 6997 | tagging_loss_video: 4.249|tagging_loss_audio: 8.059|tagging_loss_text: 16.157|tagging_loss_image: 5.941|tagging_loss_fusion: 3.246|total_loss: 37.653 | 69.83 Examples/sec\n",
      "INFO:tensorflow:training step 6998 | tagging_loss_video: 5.988|tagging_loss_audio: 9.473|tagging_loss_text: 19.021|tagging_loss_image: 6.568|tagging_loss_fusion: 4.089|total_loss: 45.139 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 6999 | tagging_loss_video: 5.028|tagging_loss_audio: 7.298|tagging_loss_text: 13.819|tagging_loss_image: 4.533|tagging_loss_fusion: 3.776|total_loss: 34.454 | 62.54 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7000 |tagging_loss_video: 5.400|tagging_loss_audio: 6.928|tagging_loss_text: 13.759|tagging_loss_image: 5.060|tagging_loss_fusion: 4.878|total_loss: 36.024 | Examples/sec: 68.10\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.96 | recall@0.5: 0.88\n",
      "INFO:tensorflow:examples_processed: 32 | hit_at_one: 1.000|perr: 0.716|loss: 35.576|GAP: 0.736|examples_per_second: 87.311\n",
      "INFO:tensorflow:examples_processed: 64 | hit_at_one: 1.000|perr: 0.730|loss: 32.036|GAP: 0.786|examples_per_second: 98.219\n",
      "INFO:tensorflow:examples_processed: 96 | hit_at_one: 1.000|perr: 0.726|loss: 42.576|GAP: 0.710|examples_per_second: 99.418\n",
      "INFO:tensorflow:examples_processed: 128 | hit_at_one: 1.000|perr: 0.769|loss: 36.099|GAP: 0.756|examples_per_second: 99.460\n",
      "INFO:tensorflow:examples_processed: 160 | hit_at_one: 1.000|perr: 0.749|loss: 37.988|GAP: 0.743|examples_per_second: 101.092\n",
      "INFO:tensorflow:examples_processed: 192 | hit_at_one: 1.000|perr: 0.696|loss: 35.557|GAP: 0.724|examples_per_second: 96.561\n",
      "INFO:tensorflow:examples_processed: 224 | hit_at_one: 1.000|perr: 0.734|loss: 38.488|GAP: 0.737|examples_per_second: 99.356\n",
      "INFO:tensorflow:examples_processed: 256 | hit_at_one: 1.000|perr: 0.764|loss: 34.749|GAP: 0.769|examples_per_second: 98.112\n",
      "INFO:tensorflow:examples_processed: 288 | hit_at_one: 1.000|perr: 0.720|loss: 37.671|GAP: 0.739|examples_per_second: 97.223\n",
      "INFO:tensorflow:examples_processed: 320 | hit_at_one: 1.000|perr: 0.702|loss: 38.326|GAP: 0.707|examples_per_second: 99.016\n",
      "INFO:tensorflow:examples_processed: 352 | hit_at_one: 1.000|perr: 0.743|loss: 28.895|GAP: 0.789|examples_per_second: 95.753\n",
      "INFO:tensorflow:examples_processed: 384 | hit_at_one: 0.969|perr: 0.766|loss: 33.425|GAP: 0.768|examples_per_second: 99.338\n",
      "INFO:tensorflow:examples_processed: 416 | hit_at_one: 1.000|perr: 0.744|loss: 36.363|GAP: 0.739|examples_per_second: 97.331\n",
      "INFO:tensorflow:examples_processed: 448 | hit_at_one: 1.000|perr: 0.756|loss: 35.750|GAP: 0.756|examples_per_second: 94.881\n",
      "INFO:tensorflow:examples_processed: 480 | hit_at_one: 1.000|perr: 0.732|loss: 35.783|GAP: 0.747|examples_per_second: 100.746\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 7000 | MAP: 0.336 | GAP: 0.740 | p@0.1: 0.719 | p@0.5:0.804 | r@0.1:0.742 | r@0.5: 0.649 | Avg_Loss: 31.119726\n",
      "INFO:tensorflow:epoch/eval number 7000 | MAP: 0.271 | GAP: 0.665 | p@0.1: 0.557 | p@0.5:0.732 | r@0.1:0.788 | r@0.5: 0.610 | Avg_Loss: 25.291189\n",
      "INFO:tensorflow:epoch/eval number 7000 | MAP: 0.111 | GAP: 0.574 | p@0.1: 0.335 | p@0.5:0.784 | r@0.1:0.878 | r@0.5: 0.406 | Avg_Loss: 22.037116\n",
      "INFO:tensorflow:epoch/eval number 7000 | MAP: 0.273 | GAP: 0.649 | p@0.1: 0.644 | p@0.5:0.719 | r@0.1:0.705 | r@0.5: 0.633 | Avg_Loss: 46.499480\n",
      "INFO:tensorflow:epoch/eval number 7000 | MAP: 0.351 | GAP: 0.747 | p@0.1: 0.741 | p@0.5:0.802 | r@0.1:0.734 | r@0.5: 0.662 | Avg_Loss: 35.952192\n",
      "INFO:tensorflow:validation score on val799 is : 0.7468\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/tagging5k_temp/model.ckpt-7000\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./checkpoints/tagging5k_temp/export/step_7000_0.7468/saved_model.pb\n",
      "INFO:tensorflow:training step 7001 | tagging_loss_video: 4.664|tagging_loss_audio: 7.704|tagging_loss_text: 14.344|tagging_loss_image: 4.907|tagging_loss_fusion: 3.194|total_loss: 34.813 | 66.98 Examples/sec\n",
      "INFO:tensorflow:training step 7002 | tagging_loss_video: 4.661|tagging_loss_audio: 7.849|tagging_loss_text: 13.668|tagging_loss_image: 4.165|tagging_loss_fusion: 2.399|total_loss: 32.742 | 71.70 Examples/sec\n",
      "INFO:tensorflow:training step 7003 | tagging_loss_video: 5.217|tagging_loss_audio: 7.307|tagging_loss_text: 14.517|tagging_loss_image: 4.828|tagging_loss_fusion: 2.852|total_loss: 34.722 | 72.30 Examples/sec\n",
      "INFO:tensorflow:training step 7004 | tagging_loss_video: 6.375|tagging_loss_audio: 8.136|tagging_loss_text: 12.601|tagging_loss_image: 3.637|tagging_loss_fusion: 3.925|total_loss: 34.673 | 65.94 Examples/sec\n",
      "INFO:tensorflow:training step 7005 | tagging_loss_video: 6.061|tagging_loss_audio: 7.329|tagging_loss_text: 17.420|tagging_loss_image: 5.316|tagging_loss_fusion: 5.806|total_loss: 41.932 | 71.89 Examples/sec\n",
      "INFO:tensorflow:training step 7006 | tagging_loss_video: 5.720|tagging_loss_audio: 8.442|tagging_loss_text: 15.978|tagging_loss_image: 5.235|tagging_loss_fusion: 4.773|total_loss: 40.148 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 7007 | tagging_loss_video: 5.093|tagging_loss_audio: 7.945|tagging_loss_text: 14.048|tagging_loss_image: 5.605|tagging_loss_fusion: 3.617|total_loss: 36.308 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 7008 | tagging_loss_video: 4.740|tagging_loss_audio: 8.304|tagging_loss_text: 14.164|tagging_loss_image: 4.664|tagging_loss_fusion: 3.165|total_loss: 35.037 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 7009 | tagging_loss_video: 4.945|tagging_loss_audio: 8.466|tagging_loss_text: 14.269|tagging_loss_image: 5.311|tagging_loss_fusion: 3.344|total_loss: 36.334 | 70.92 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7010 |tagging_loss_video: 5.449|tagging_loss_audio: 7.449|tagging_loss_text: 12.683|tagging_loss_image: 4.706|tagging_loss_fusion: 5.202|total_loss: 35.490 | Examples/sec: 69.11\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.77 | precision@0.5: 0.92 |recall@0.1: 0.98 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 7011 | tagging_loss_video: 4.766|tagging_loss_audio: 6.782|tagging_loss_text: 11.289|tagging_loss_image: 4.066|tagging_loss_fusion: 2.641|total_loss: 29.544 | 71.56 Examples/sec\n",
      "INFO:tensorflow:training step 7012 | tagging_loss_video: 6.281|tagging_loss_audio: 7.880|tagging_loss_text: 14.672|tagging_loss_image: 5.127|tagging_loss_fusion: 4.244|total_loss: 38.204 | 65.16 Examples/sec\n",
      "INFO:tensorflow:training step 7013 | tagging_loss_video: 3.928|tagging_loss_audio: 7.333|tagging_loss_text: 12.901|tagging_loss_image: 5.117|tagging_loss_fusion: 2.180|total_loss: 31.458 | 70.93 Examples/sec\n",
      "INFO:tensorflow:training step 7014 | tagging_loss_video: 5.874|tagging_loss_audio: 7.956|tagging_loss_text: 17.248|tagging_loss_image: 5.490|tagging_loss_fusion: 5.056|total_loss: 41.624 | 71.24 Examples/sec\n",
      "INFO:tensorflow:training step 7015 | tagging_loss_video: 4.654|tagging_loss_audio: 8.342|tagging_loss_text: 14.474|tagging_loss_image: 5.621|tagging_loss_fusion: 5.054|total_loss: 38.145 | 69.18 Examples/sec\n",
      "INFO:tensorflow:training step 7016 | tagging_loss_video: 5.649|tagging_loss_audio: 6.933|tagging_loss_text: 13.476|tagging_loss_image: 4.946|tagging_loss_fusion: 4.958|total_loss: 35.963 | 71.93 Examples/sec\n",
      "INFO:tensorflow:training step 7017 | tagging_loss_video: 5.857|tagging_loss_audio: 8.192|tagging_loss_text: 16.647|tagging_loss_image: 4.959|tagging_loss_fusion: 6.051|total_loss: 41.706 | 67.01 Examples/sec\n",
      "INFO:tensorflow:training step 7018 | tagging_loss_video: 3.900|tagging_loss_audio: 9.060|tagging_loss_text: 15.605|tagging_loss_image: 5.148|tagging_loss_fusion: 2.289|total_loss: 36.002 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 7019 | tagging_loss_video: 5.927|tagging_loss_audio: 8.074|tagging_loss_text: 14.684|tagging_loss_image: 5.538|tagging_loss_fusion: 5.033|total_loss: 39.256 | 69.46 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7020 |tagging_loss_video: 6.597|tagging_loss_audio: 8.459|tagging_loss_text: 14.789|tagging_loss_image: 5.319|tagging_loss_fusion: 4.019|total_loss: 39.183 | Examples/sec: 70.94\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.87 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7021 | tagging_loss_video: 6.501|tagging_loss_audio: 8.041|tagging_loss_text: 17.795|tagging_loss_image: 4.044|tagging_loss_fusion: 4.732|total_loss: 41.113 | 70.81 Examples/sec\n",
      "INFO:tensorflow:training step 7022 | tagging_loss_video: 5.011|tagging_loss_audio: 7.869|tagging_loss_text: 15.291|tagging_loss_image: 4.821|tagging_loss_fusion: 2.674|total_loss: 35.666 | 71.78 Examples/sec\n",
      "INFO:tensorflow:training step 7023 | tagging_loss_video: 3.953|tagging_loss_audio: 7.439|tagging_loss_text: 14.986|tagging_loss_image: 4.285|tagging_loss_fusion: 2.095|total_loss: 32.758 | 67.43 Examples/sec\n",
      "INFO:tensorflow:training step 7024 | tagging_loss_video: 5.666|tagging_loss_audio: 7.736|tagging_loss_text: 12.344|tagging_loss_image: 4.336|tagging_loss_fusion: 3.302|total_loss: 33.383 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 7025 | tagging_loss_video: 6.383|tagging_loss_audio: 9.294|tagging_loss_text: 17.838|tagging_loss_image: 4.490|tagging_loss_fusion: 4.433|total_loss: 42.438 | 67.89 Examples/sec\n",
      "INFO:tensorflow:training step 7026 | tagging_loss_video: 5.073|tagging_loss_audio: 7.957|tagging_loss_text: 16.233|tagging_loss_image: 4.174|tagging_loss_fusion: 2.357|total_loss: 35.795 | 72.39 Examples/sec\n",
      "INFO:tensorflow:training step 7027 | tagging_loss_video: 6.281|tagging_loss_audio: 8.574|tagging_loss_text: 16.746|tagging_loss_image: 5.547|tagging_loss_fusion: 5.817|total_loss: 42.965 | 70.51 Examples/sec\n",
      "INFO:tensorflow:training step 7028 | tagging_loss_video: 4.251|tagging_loss_audio: 7.754|tagging_loss_text: 13.517|tagging_loss_image: 5.450|tagging_loss_fusion: 3.816|total_loss: 34.788 | 72.07 Examples/sec\n",
      "INFO:tensorflow:training step 7029 | tagging_loss_video: 5.597|tagging_loss_audio: 8.813|tagging_loss_text: 18.453|tagging_loss_image: 5.664|tagging_loss_fusion: 5.747|total_loss: 44.275 | 68.01 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7030 |tagging_loss_video: 5.333|tagging_loss_audio: 8.479|tagging_loss_text: 19.277|tagging_loss_image: 4.037|tagging_loss_fusion: 3.241|total_loss: 40.367 | Examples/sec: 72.81\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 7031 | tagging_loss_video: 4.894|tagging_loss_audio: 7.987|tagging_loss_text: 17.397|tagging_loss_image: 4.592|tagging_loss_fusion: 3.121|total_loss: 37.991 | 69.03 Examples/sec\n",
      "INFO:tensorflow:training step 7032 | tagging_loss_video: 5.012|tagging_loss_audio: 9.070|tagging_loss_text: 14.978|tagging_loss_image: 4.143|tagging_loss_fusion: 3.325|total_loss: 36.529 | 70.83 Examples/sec\n",
      "INFO:tensorflow:training step 7033 | tagging_loss_video: 6.140|tagging_loss_audio: 8.952|tagging_loss_text: 15.889|tagging_loss_image: 5.136|tagging_loss_fusion: 4.850|total_loss: 40.967 | 70.50 Examples/sec\n",
      "INFO:tensorflow:training step 7034 | tagging_loss_video: 5.587|tagging_loss_audio: 7.963|tagging_loss_text: 11.857|tagging_loss_image: 4.724|tagging_loss_fusion: 4.013|total_loss: 34.144 | 71.45 Examples/sec\n",
      "INFO:tensorflow:training step 7035 | tagging_loss_video: 4.831|tagging_loss_audio: 8.752|tagging_loss_text: 10.993|tagging_loss_image: 4.559|tagging_loss_fusion: 2.145|total_loss: 31.280 | 69.57 Examples/sec\n",
      "INFO:tensorflow:training step 7036 | tagging_loss_video: 4.958|tagging_loss_audio: 8.813|tagging_loss_text: 14.111|tagging_loss_image: 6.031|tagging_loss_fusion: 4.114|total_loss: 38.027 | 70.00 Examples/sec\n",
      "INFO:tensorflow:training step 7037 | tagging_loss_video: 5.724|tagging_loss_audio: 8.003|tagging_loss_text: 15.597|tagging_loss_image: 4.949|tagging_loss_fusion: 4.401|total_loss: 38.674 | 68.21 Examples/sec\n",
      "INFO:tensorflow:training step 7038 | tagging_loss_video: 5.810|tagging_loss_audio: 8.707|tagging_loss_text: 17.301|tagging_loss_image: 5.610|tagging_loss_fusion: 4.087|total_loss: 41.516 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 7039 | tagging_loss_video: 3.370|tagging_loss_audio: 7.599|tagging_loss_text: 18.645|tagging_loss_image: 5.877|tagging_loss_fusion: 2.825|total_loss: 38.315 | 68.38 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7040 |tagging_loss_video: 5.062|tagging_loss_audio: 9.224|tagging_loss_text: 18.087|tagging_loss_image: 4.775|tagging_loss_fusion: 3.111|total_loss: 40.258 | Examples/sec: 71.40\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.90 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7041 | tagging_loss_video: 4.498|tagging_loss_audio: 8.061|tagging_loss_text: 16.866|tagging_loss_image: 5.399|tagging_loss_fusion: 3.001|total_loss: 37.825 | 68.62 Examples/sec\n",
      "INFO:tensorflow:training step 7042 | tagging_loss_video: 5.561|tagging_loss_audio: 9.421|tagging_loss_text: 17.892|tagging_loss_image: 5.960|tagging_loss_fusion: 4.480|total_loss: 43.313 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 7043 | tagging_loss_video: 5.592|tagging_loss_audio: 8.539|tagging_loss_text: 13.125|tagging_loss_image: 4.513|tagging_loss_fusion: 4.019|total_loss: 35.789 | 68.16 Examples/sec\n",
      "INFO:tensorflow:training step 7044 | tagging_loss_video: 5.302|tagging_loss_audio: 8.434|tagging_loss_text: 16.188|tagging_loss_image: 5.357|tagging_loss_fusion: 3.262|total_loss: 38.543 | 70.31 Examples/sec\n",
      "INFO:tensorflow:training step 7045 | tagging_loss_video: 5.046|tagging_loss_audio: 7.775|tagging_loss_text: 17.944|tagging_loss_image: 3.300|tagging_loss_fusion: 3.217|total_loss: 37.283 | 66.93 Examples/sec\n",
      "INFO:tensorflow:training step 7046 | tagging_loss_video: 5.588|tagging_loss_audio: 8.495|tagging_loss_text: 13.726|tagging_loss_image: 4.491|tagging_loss_fusion: 4.693|total_loss: 36.994 | 70.69 Examples/sec\n",
      "INFO:tensorflow:training step 7047 | tagging_loss_video: 6.755|tagging_loss_audio: 9.083|tagging_loss_text: 16.571|tagging_loss_image: 5.052|tagging_loss_fusion: 5.170|total_loss: 42.631 | 67.97 Examples/sec\n",
      "INFO:tensorflow:training step 7048 | tagging_loss_video: 5.860|tagging_loss_audio: 8.790|tagging_loss_text: 16.752|tagging_loss_image: 3.890|tagging_loss_fusion: 3.414|total_loss: 38.706 | 72.12 Examples/sec\n",
      "INFO:tensorflow:training step 7049 | tagging_loss_video: 5.046|tagging_loss_audio: 8.472|tagging_loss_text: 16.705|tagging_loss_image: 5.817|tagging_loss_fusion: 4.099|total_loss: 40.138 | 70.42 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7050 |tagging_loss_video: 5.238|tagging_loss_audio: 9.696|tagging_loss_text: 16.776|tagging_loss_image: 3.558|tagging_loss_fusion: 2.713|total_loss: 37.982 | Examples/sec: 71.63\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.91 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 7051 | tagging_loss_video: 4.416|tagging_loss_audio: 7.922|tagging_loss_text: 16.586|tagging_loss_image: 4.306|tagging_loss_fusion: 2.480|total_loss: 35.710 | 68.50 Examples/sec\n",
      "INFO:tensorflow:training step 7052 | tagging_loss_video: 5.586|tagging_loss_audio: 8.934|tagging_loss_text: 12.649|tagging_loss_image: 4.184|tagging_loss_fusion: 3.890|total_loss: 35.243 | 72.13 Examples/sec\n",
      "INFO:tensorflow:training step 7053 | tagging_loss_video: 5.022|tagging_loss_audio: 8.177|tagging_loss_text: 13.400|tagging_loss_image: 4.662|tagging_loss_fusion: 3.633|total_loss: 34.893 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 7054 | tagging_loss_video: 4.374|tagging_loss_audio: 8.587|tagging_loss_text: 13.007|tagging_loss_image: 4.935|tagging_loss_fusion: 2.536|total_loss: 33.439 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 7055 | tagging_loss_video: 5.106|tagging_loss_audio: 8.368|tagging_loss_text: 12.632|tagging_loss_image: 4.246|tagging_loss_fusion: 3.892|total_loss: 34.244 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 7056 | tagging_loss_video: 5.618|tagging_loss_audio: 10.720|tagging_loss_text: 14.833|tagging_loss_image: 4.293|tagging_loss_fusion: 3.798|total_loss: 39.262 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 7057 | tagging_loss_video: 4.531|tagging_loss_audio: 8.775|tagging_loss_text: 13.318|tagging_loss_image: 6.376|tagging_loss_fusion: 2.752|total_loss: 35.752 | 69.63 Examples/sec\n",
      "INFO:tensorflow:training step 7058 | tagging_loss_video: 5.324|tagging_loss_audio: 8.268|tagging_loss_text: 15.074|tagging_loss_image: 5.330|tagging_loss_fusion: 4.343|total_loss: 38.339 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 7059 | tagging_loss_video: 5.441|tagging_loss_audio: 8.198|tagging_loss_text: 14.310|tagging_loss_image: 4.943|tagging_loss_fusion: 3.673|total_loss: 36.565 | 68.91 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7060 |tagging_loss_video: 5.716|tagging_loss_audio: 7.820|tagging_loss_text: 17.322|tagging_loss_image: 4.198|tagging_loss_fusion: 4.133|total_loss: 39.189 | Examples/sec: 71.71\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 7061 | tagging_loss_video: 6.395|tagging_loss_audio: 9.515|tagging_loss_text: 16.855|tagging_loss_image: 5.260|tagging_loss_fusion: 4.202|total_loss: 42.228 | 65.74 Examples/sec\n",
      "INFO:tensorflow:training step 7062 | tagging_loss_video: 6.206|tagging_loss_audio: 7.916|tagging_loss_text: 17.552|tagging_loss_image: 5.135|tagging_loss_fusion: 4.508|total_loss: 41.317 | 69.82 Examples/sec\n",
      "INFO:tensorflow:training step 7063 | tagging_loss_video: 5.256|tagging_loss_audio: 7.777|tagging_loss_text: 13.183|tagging_loss_image: 4.520|tagging_loss_fusion: 4.593|total_loss: 35.328 | 72.15 Examples/sec\n",
      "INFO:tensorflow:training step 7064 | tagging_loss_video: 5.389|tagging_loss_audio: 7.991|tagging_loss_text: 15.910|tagging_loss_image: 4.814|tagging_loss_fusion: 3.197|total_loss: 37.301 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 7065 | tagging_loss_video: 5.935|tagging_loss_audio: 8.408|tagging_loss_text: 15.505|tagging_loss_image: 5.416|tagging_loss_fusion: 4.864|total_loss: 40.129 | 70.95 Examples/sec\n",
      "INFO:tensorflow:training step 7066 | tagging_loss_video: 5.321|tagging_loss_audio: 7.212|tagging_loss_text: 16.474|tagging_loss_image: 4.486|tagging_loss_fusion: 4.360|total_loss: 37.854 | 69.23 Examples/sec\n",
      "INFO:tensorflow:training step 7067 | tagging_loss_video: 5.432|tagging_loss_audio: 7.403|tagging_loss_text: 13.287|tagging_loss_image: 5.223|tagging_loss_fusion: 4.390|total_loss: 35.736 | 68.52 Examples/sec\n",
      "INFO:tensorflow:training step 7068 | tagging_loss_video: 5.583|tagging_loss_audio: 8.041|tagging_loss_text: 16.740|tagging_loss_image: 3.907|tagging_loss_fusion: 3.555|total_loss: 37.826 | 68.60 Examples/sec\n",
      "INFO:tensorflow:training step 7069 | tagging_loss_video: 5.993|tagging_loss_audio: 8.857|tagging_loss_text: 12.704|tagging_loss_image: 4.031|tagging_loss_fusion: 4.064|total_loss: 35.649 | 68.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7070 |tagging_loss_video: 6.110|tagging_loss_audio: 7.952|tagging_loss_text: 13.878|tagging_loss_image: 4.975|tagging_loss_fusion: 4.986|total_loss: 37.901 | Examples/sec: 72.06\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.81 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.88\n",
      "INFO:tensorflow:training step 7071 | tagging_loss_video: 4.088|tagging_loss_audio: 7.950|tagging_loss_text: 17.484|tagging_loss_image: 4.655|tagging_loss_fusion: 2.959|total_loss: 37.135 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 7072 | tagging_loss_video: 5.452|tagging_loss_audio: 8.646|tagging_loss_text: 14.662|tagging_loss_image: 5.252|tagging_loss_fusion: 4.846|total_loss: 38.859 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 7073 | tagging_loss_video: 5.173|tagging_loss_audio: 8.329|tagging_loss_text: 15.272|tagging_loss_image: 6.118|tagging_loss_fusion: 3.640|total_loss: 38.532 | 67.67 Examples/sec\n",
      "INFO:tensorflow:training step 7074 | tagging_loss_video: 4.807|tagging_loss_audio: 8.822|tagging_loss_text: 14.191|tagging_loss_image: 5.637|tagging_loss_fusion: 4.273|total_loss: 37.731 | 71.57 Examples/sec\n",
      "INFO:tensorflow:training step 7075 | tagging_loss_video: 5.898|tagging_loss_audio: 8.632|tagging_loss_text: 16.035|tagging_loss_image: 4.783|tagging_loss_fusion: 3.761|total_loss: 39.109 | 69.59 Examples/sec\n",
      "INFO:tensorflow:training step 7076 | tagging_loss_video: 5.590|tagging_loss_audio: 8.397|tagging_loss_text: 19.770|tagging_loss_image: 5.546|tagging_loss_fusion: 3.628|total_loss: 42.931 | 71.50 Examples/sec\n",
      "INFO:tensorflow:training step 7077 | tagging_loss_video: 5.602|tagging_loss_audio: 8.478|tagging_loss_text: 15.429|tagging_loss_image: 5.068|tagging_loss_fusion: 5.042|total_loss: 39.619 | 70.23 Examples/sec\n",
      "INFO:tensorflow:training step 7078 | tagging_loss_video: 5.898|tagging_loss_audio: 7.822|tagging_loss_text: 18.479|tagging_loss_image: 4.890|tagging_loss_fusion: 5.308|total_loss: 42.398 | 72.21 Examples/sec\n",
      "INFO:tensorflow:training step 7079 | tagging_loss_video: 6.664|tagging_loss_audio: 9.976|tagging_loss_text: 14.269|tagging_loss_image: 5.548|tagging_loss_fusion: 6.359|total_loss: 42.818 | 66.96 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7080 |tagging_loss_video: 4.449|tagging_loss_audio: 7.447|tagging_loss_text: 14.685|tagging_loss_image: 4.770|tagging_loss_fusion: 2.739|total_loss: 34.089 | Examples/sec: 69.45\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.89 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 7081 | tagging_loss_video: 6.273|tagging_loss_audio: 9.534|tagging_loss_text: 12.909|tagging_loss_image: 5.685|tagging_loss_fusion: 4.167|total_loss: 38.569 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 7082 | tagging_loss_video: 4.226|tagging_loss_audio: 7.900|tagging_loss_text: 19.129|tagging_loss_image: 5.111|tagging_loss_fusion: 1.506|total_loss: 37.872 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 7083 | tagging_loss_video: 5.623|tagging_loss_audio: 8.183|tagging_loss_text: 15.068|tagging_loss_image: 5.709|tagging_loss_fusion: 5.411|total_loss: 39.993 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 7084 | tagging_loss_video: 5.024|tagging_loss_audio: 7.697|tagging_loss_text: 16.043|tagging_loss_image: 4.438|tagging_loss_fusion: 2.792|total_loss: 35.994 | 69.24 Examples/sec\n",
      "INFO:tensorflow:training step 7085 | tagging_loss_video: 4.669|tagging_loss_audio: 7.035|tagging_loss_text: 13.475|tagging_loss_image: 3.265|tagging_loss_fusion: 3.839|total_loss: 32.284 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 7086 | tagging_loss_video: 5.288|tagging_loss_audio: 7.636|tagging_loss_text: 13.532|tagging_loss_image: 4.700|tagging_loss_fusion: 4.075|total_loss: 35.232 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 7087 | tagging_loss_video: 5.896|tagging_loss_audio: 8.629|tagging_loss_text: 15.209|tagging_loss_image: 4.966|tagging_loss_fusion: 4.730|total_loss: 39.430 | 68.45 Examples/sec\n",
      "INFO:tensorflow:training step 7088 | tagging_loss_video: 4.953|tagging_loss_audio: 8.293|tagging_loss_text: 16.210|tagging_loss_image: 4.897|tagging_loss_fusion: 2.539|total_loss: 36.891 | 71.09 Examples/sec\n",
      "INFO:tensorflow:training step 7089 | tagging_loss_video: 6.802|tagging_loss_audio: 8.456|tagging_loss_text: 18.474|tagging_loss_image: 4.869|tagging_loss_fusion: 4.782|total_loss: 43.383 | 69.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7090 |tagging_loss_video: 6.001|tagging_loss_audio: 9.151|tagging_loss_text: 14.022|tagging_loss_image: 4.688|tagging_loss_fusion: 5.274|total_loss: 39.137 | Examples/sec: 72.54\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.80 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 7091 | tagging_loss_video: 6.169|tagging_loss_audio: 8.066|tagging_loss_text: 15.658|tagging_loss_image: 4.545|tagging_loss_fusion: 4.798|total_loss: 39.236 | 69.56 Examples/sec\n",
      "INFO:tensorflow:training step 7092 | tagging_loss_video: 6.255|tagging_loss_audio: 8.701|tagging_loss_text: 17.070|tagging_loss_image: 4.725|tagging_loss_fusion: 5.588|total_loss: 42.339 | 71.82 Examples/sec\n",
      "INFO:tensorflow:training step 7093 | tagging_loss_video: 4.932|tagging_loss_audio: 8.446|tagging_loss_text: 15.430|tagging_loss_image: 5.875|tagging_loss_fusion: 3.349|total_loss: 38.032 | 65.24 Examples/sec\n",
      "INFO:tensorflow:training step 7094 | tagging_loss_video: 6.258|tagging_loss_audio: 8.266|tagging_loss_text: 16.641|tagging_loss_image: 4.058|tagging_loss_fusion: 4.388|total_loss: 39.611 | 68.43 Examples/sec\n",
      "INFO:tensorflow:training step 7095 | tagging_loss_video: 4.936|tagging_loss_audio: 7.625|tagging_loss_text: 12.696|tagging_loss_image: 4.756|tagging_loss_fusion: 3.211|total_loss: 33.223 | 72.34 Examples/sec\n",
      "INFO:tensorflow:training step 7096 | tagging_loss_video: 4.793|tagging_loss_audio: 8.699|tagging_loss_text: 17.732|tagging_loss_image: 3.742|tagging_loss_fusion: 2.577|total_loss: 37.544 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 7097 | tagging_loss_video: 5.718|tagging_loss_audio: 9.324|tagging_loss_text: 14.058|tagging_loss_image: 4.348|tagging_loss_fusion: 3.730|total_loss: 37.179 | 70.84 Examples/sec\n",
      "INFO:tensorflow:training step 7098 | tagging_loss_video: 5.261|tagging_loss_audio: 8.137|tagging_loss_text: 17.680|tagging_loss_image: 4.603|tagging_loss_fusion: 2.353|total_loss: 38.035 | 70.82 Examples/sec\n",
      "INFO:tensorflow:training step 7099 | tagging_loss_video: 4.936|tagging_loss_audio: 7.039|tagging_loss_text: 14.318|tagging_loss_image: 5.424|tagging_loss_fusion: 3.367|total_loss: 35.084 | 63.40 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7100 |tagging_loss_video: 4.040|tagging_loss_audio: 8.365|tagging_loss_text: 15.279|tagging_loss_image: 3.111|tagging_loss_fusion: 2.687|total_loss: 33.483 | Examples/sec: 70.36\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.91 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 7101 | tagging_loss_video: 5.630|tagging_loss_audio: 7.958|tagging_loss_text: 13.762|tagging_loss_image: 6.134|tagging_loss_fusion: 4.711|total_loss: 38.195 | 70.97 Examples/sec\n",
      "INFO:tensorflow:training step 7102 | tagging_loss_video: 6.010|tagging_loss_audio: 7.361|tagging_loss_text: 15.344|tagging_loss_image: 3.642|tagging_loss_fusion: 4.566|total_loss: 36.922 | 62.89 Examples/sec\n",
      "INFO:tensorflow:training step 7103 | tagging_loss_video: 6.019|tagging_loss_audio: 8.508|tagging_loss_text: 16.254|tagging_loss_image: 6.354|tagging_loss_fusion: 5.926|total_loss: 43.061 | 68.01 Examples/sec\n",
      "INFO:tensorflow:training step 7104 | tagging_loss_video: 3.892|tagging_loss_audio: 7.304|tagging_loss_text: 13.931|tagging_loss_image: 5.584|tagging_loss_fusion: 2.239|total_loss: 32.951 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 7105 | tagging_loss_video: 4.275|tagging_loss_audio: 7.379|tagging_loss_text: 16.160|tagging_loss_image: 5.018|tagging_loss_fusion: 3.336|total_loss: 36.168 | 70.03 Examples/sec\n",
      "INFO:tensorflow:training step 7106 | tagging_loss_video: 4.246|tagging_loss_audio: 9.429|tagging_loss_text: 18.328|tagging_loss_image: 3.887|tagging_loss_fusion: 2.350|total_loss: 38.239 | 71.11 Examples/sec\n",
      "INFO:tensorflow:training step 7107 | tagging_loss_video: 4.398|tagging_loss_audio: 7.839|tagging_loss_text: 16.864|tagging_loss_image: 5.844|tagging_loss_fusion: 2.874|total_loss: 37.820 | 63.37 Examples/sec\n",
      "INFO:tensorflow:training step 7108 | tagging_loss_video: 5.831|tagging_loss_audio: 7.325|tagging_loss_text: 16.248|tagging_loss_image: 5.711|tagging_loss_fusion: 4.160|total_loss: 39.274 | 68.40 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 7109 | tagging_loss_video: 6.074|tagging_loss_audio: 8.585|tagging_loss_text: 13.944|tagging_loss_image: 4.583|tagging_loss_fusion: 3.564|total_loss: 36.750 | 70.26 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7110 |tagging_loss_video: 5.617|tagging_loss_audio: 7.257|tagging_loss_text: 16.589|tagging_loss_image: 5.179|tagging_loss_fusion: 4.442|total_loss: 39.083 | Examples/sec: 62.35\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 7111 | tagging_loss_video: 4.921|tagging_loss_audio: 8.102|tagging_loss_text: 16.677|tagging_loss_image: 5.041|tagging_loss_fusion: 4.439|total_loss: 39.180 | 69.50 Examples/sec\n",
      "INFO:tensorflow:training step 7112 | tagging_loss_video: 4.863|tagging_loss_audio: 7.681|tagging_loss_text: 14.723|tagging_loss_image: 4.772|tagging_loss_fusion: 3.893|total_loss: 35.932 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 7113 | tagging_loss_video: 5.163|tagging_loss_audio: 8.811|tagging_loss_text: 16.087|tagging_loss_image: 5.717|tagging_loss_fusion: 4.456|total_loss: 40.233 | 61.53 Examples/sec\n",
      "INFO:tensorflow:training step 7114 | tagging_loss_video: 4.801|tagging_loss_audio: 7.341|tagging_loss_text: 12.176|tagging_loss_image: 5.051|tagging_loss_fusion: 3.515|total_loss: 32.884 | 70.49 Examples/sec\n",
      "INFO:tensorflow:training step 7115 | tagging_loss_video: 6.182|tagging_loss_audio: 7.999|tagging_loss_text: 16.469|tagging_loss_image: 5.171|tagging_loss_fusion: 4.794|total_loss: 40.614 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 7116 | tagging_loss_video: 5.748|tagging_loss_audio: 7.440|tagging_loss_text: 14.763|tagging_loss_image: 5.577|tagging_loss_fusion: 4.862|total_loss: 38.391 | 69.36 Examples/sec\n",
      "INFO:tensorflow:training step 7117 | tagging_loss_video: 5.320|tagging_loss_audio: 8.086|tagging_loss_text: 16.773|tagging_loss_image: 5.345|tagging_loss_fusion: 3.794|total_loss: 39.318 | 69.49 Examples/sec\n",
      "INFO:tensorflow:training step 7118 | tagging_loss_video: 3.449|tagging_loss_audio: 7.769|tagging_loss_text: 12.354|tagging_loss_image: 4.773|tagging_loss_fusion: 2.038|total_loss: 30.384 | 70.87 Examples/sec\n",
      "INFO:tensorflow:training step 7119 | tagging_loss_video: 3.783|tagging_loss_audio: 7.744|tagging_loss_text: 10.898|tagging_loss_image: 5.403|tagging_loss_fusion: 2.617|total_loss: 30.446 | 68.83 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7120 |tagging_loss_video: 6.492|tagging_loss_audio: 8.592|tagging_loss_text: 15.581|tagging_loss_image: 4.546|tagging_loss_fusion: 4.992|total_loss: 40.203 | Examples/sec: 71.33\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 7121 | tagging_loss_video: 5.222|tagging_loss_audio: 8.295|tagging_loss_text: 19.577|tagging_loss_image: 5.587|tagging_loss_fusion: 2.706|total_loss: 41.387 | 63.25 Examples/sec\n",
      "INFO:tensorflow:training step 7122 | tagging_loss_video: 3.861|tagging_loss_audio: 8.431|tagging_loss_text: 11.950|tagging_loss_image: 4.990|tagging_loss_fusion: 2.909|total_loss: 32.140 | 69.67 Examples/sec\n",
      "INFO:tensorflow:training step 7123 | tagging_loss_video: 6.566|tagging_loss_audio: 8.849|tagging_loss_text: 19.012|tagging_loss_image: 4.577|tagging_loss_fusion: 5.347|total_loss: 44.352 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 7124 | tagging_loss_video: 5.489|tagging_loss_audio: 7.088|tagging_loss_text: 11.935|tagging_loss_image: 4.347|tagging_loss_fusion: 3.506|total_loss: 32.365 | 64.64 Examples/sec\n",
      "INFO:tensorflow:training step 7125 | tagging_loss_video: 4.453|tagging_loss_audio: 7.871|tagging_loss_text: 11.871|tagging_loss_image: 4.064|tagging_loss_fusion: 2.641|total_loss: 30.900 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 7126 | tagging_loss_video: 4.745|tagging_loss_audio: 7.388|tagging_loss_text: 16.109|tagging_loss_image: 4.715|tagging_loss_fusion: 4.126|total_loss: 37.083 | 67.71 Examples/sec\n",
      "INFO:tensorflow:training step 7127 | tagging_loss_video: 4.699|tagging_loss_audio: 6.820|tagging_loss_text: 13.345|tagging_loss_image: 4.221|tagging_loss_fusion: 2.576|total_loss: 31.660 | 61.02 Examples/sec\n",
      "INFO:tensorflow:training step 7128 | tagging_loss_video: 5.965|tagging_loss_audio: 8.522|tagging_loss_text: 17.525|tagging_loss_image: 4.484|tagging_loss_fusion: 5.814|total_loss: 42.311 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 7129 | tagging_loss_video: 5.051|tagging_loss_audio: 7.627|tagging_loss_text: 15.779|tagging_loss_image: 4.293|tagging_loss_fusion: 3.651|total_loss: 36.400 | 71.58 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7130 |tagging_loss_video: 5.121|tagging_loss_audio: 8.109|tagging_loss_text: 14.638|tagging_loss_image: 5.994|tagging_loss_fusion: 4.259|total_loss: 38.120 | Examples/sec: 69.49\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 7131 | tagging_loss_video: 4.957|tagging_loss_audio: 7.541|tagging_loss_text: 16.518|tagging_loss_image: 3.572|tagging_loss_fusion: 2.606|total_loss: 35.193 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 7132 | tagging_loss_video: 6.202|tagging_loss_audio: 7.879|tagging_loss_text: 15.474|tagging_loss_image: 5.806|tagging_loss_fusion: 5.878|total_loss: 41.239 | 62.42 Examples/sec\n",
      "INFO:tensorflow:training step 7133 | tagging_loss_video: 4.386|tagging_loss_audio: 7.331|tagging_loss_text: 15.039|tagging_loss_image: 6.099|tagging_loss_fusion: 3.914|total_loss: 36.769 | 68.68 Examples/sec\n",
      "INFO:tensorflow:training step 7134 | tagging_loss_video: 5.006|tagging_loss_audio: 7.904|tagging_loss_text: 13.599|tagging_loss_image: 4.358|tagging_loss_fusion: 2.622|total_loss: 33.490 | 68.71 Examples/sec\n",
      "INFO:tensorflow:training step 7135 | tagging_loss_video: 5.397|tagging_loss_audio: 7.382|tagging_loss_text: 14.255|tagging_loss_image: 4.995|tagging_loss_fusion: 6.201|total_loss: 38.230 | 67.93 Examples/sec\n",
      "INFO:tensorflow:training step 7136 | tagging_loss_video: 5.328|tagging_loss_audio: 7.653|tagging_loss_text: 14.489|tagging_loss_image: 4.610|tagging_loss_fusion: 2.317|total_loss: 34.397 | 68.53 Examples/sec\n",
      "INFO:tensorflow:training step 7137 | tagging_loss_video: 6.305|tagging_loss_audio: 8.235|tagging_loss_text: 15.961|tagging_loss_image: 4.986|tagging_loss_fusion: 4.760|total_loss: 40.245 | 68.84 Examples/sec\n",
      "INFO:tensorflow:training step 7138 | tagging_loss_video: 4.708|tagging_loss_audio: 8.608|tagging_loss_text: 16.081|tagging_loss_image: 3.796|tagging_loss_fusion: 3.471|total_loss: 36.664 | 66.32 Examples/sec\n",
      "INFO:tensorflow:training step 7139 | tagging_loss_video: 5.862|tagging_loss_audio: 7.611|tagging_loss_text: 16.765|tagging_loss_image: 4.098|tagging_loss_fusion: 4.204|total_loss: 38.541 | 68.67 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7140 |tagging_loss_video: 5.598|tagging_loss_audio: 7.873|tagging_loss_text: 13.106|tagging_loss_image: 5.934|tagging_loss_fusion: 3.863|total_loss: 36.375 | Examples/sec: 68.70\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.80 | precision@0.5: 0.95 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7141 | tagging_loss_video: 4.721|tagging_loss_audio: 7.461|tagging_loss_text: 14.989|tagging_loss_image: 4.384|tagging_loss_fusion: 3.126|total_loss: 34.682 | 70.12 Examples/sec\n",
      "INFO:tensorflow:training step 7142 | tagging_loss_video: 3.666|tagging_loss_audio: 8.362|tagging_loss_text: 16.297|tagging_loss_image: 3.985|tagging_loss_fusion: 2.854|total_loss: 35.164 | 69.75 Examples/sec\n",
      "INFO:tensorflow:training step 7143 | tagging_loss_video: 4.163|tagging_loss_audio: 7.556|tagging_loss_text: 11.237|tagging_loss_image: 5.152|tagging_loss_fusion: 3.557|total_loss: 31.665 | 72.00 Examples/sec\n",
      "INFO:tensorflow:training step 7144 | tagging_loss_video: 5.037|tagging_loss_audio: 7.700|tagging_loss_text: 13.932|tagging_loss_image: 4.013|tagging_loss_fusion: 3.252|total_loss: 33.933 | 72.23 Examples/sec\n",
      "INFO:tensorflow:training step 7145 | tagging_loss_video: 5.581|tagging_loss_audio: 7.706|tagging_loss_text: 17.422|tagging_loss_image: 4.167|tagging_loss_fusion: 3.618|total_loss: 38.494 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 7146 | tagging_loss_video: 5.129|tagging_loss_audio: 8.448|tagging_loss_text: 13.899|tagging_loss_image: 4.344|tagging_loss_fusion: 3.630|total_loss: 35.449 | 59.18 Examples/sec\n",
      "INFO:tensorflow:training step 7147 | tagging_loss_video: 4.322|tagging_loss_audio: 8.750|tagging_loss_text: 14.752|tagging_loss_image: 5.055|tagging_loss_fusion: 2.378|total_loss: 35.256 | 62.37 Examples/sec\n",
      "INFO:tensorflow:training step 7148 | tagging_loss_video: 5.153|tagging_loss_audio: 7.754|tagging_loss_text: 14.834|tagging_loss_image: 5.837|tagging_loss_fusion: 4.805|total_loss: 38.383 | 70.77 Examples/sec\n",
      "INFO:tensorflow:training step 7149 | tagging_loss_video: 5.204|tagging_loss_audio: 8.622|tagging_loss_text: 15.683|tagging_loss_image: 5.144|tagging_loss_fusion: 3.080|total_loss: 37.733 | 71.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7150 |tagging_loss_video: 5.018|tagging_loss_audio: 7.410|tagging_loss_text: 14.591|tagging_loss_image: 4.931|tagging_loss_fusion: 4.112|total_loss: 36.063 | Examples/sec: 63.43\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.81 | precision@0.5: 0.93 |recall@0.1: 0.97 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 7151 | tagging_loss_video: 4.488|tagging_loss_audio: 7.044|tagging_loss_text: 12.477|tagging_loss_image: 4.856|tagging_loss_fusion: 4.003|total_loss: 32.869 | 71.02 Examples/sec\n",
      "INFO:tensorflow:training step 7152 | tagging_loss_video: 5.913|tagging_loss_audio: 9.072|tagging_loss_text: 14.671|tagging_loss_image: 4.355|tagging_loss_fusion: 4.644|total_loss: 38.655 | 70.55 Examples/sec\n",
      "INFO:tensorflow:training step 7153 | tagging_loss_video: 6.130|tagging_loss_audio: 8.201|tagging_loss_text: 11.609|tagging_loss_image: 4.962|tagging_loss_fusion: 5.908|total_loss: 36.810 | 68.54 Examples/sec\n",
      "INFO:tensorflow:training step 7154 | tagging_loss_video: 4.499|tagging_loss_audio: 8.115|tagging_loss_text: 15.176|tagging_loss_image: 4.580|tagging_loss_fusion: 2.168|total_loss: 34.538 | 68.17 Examples/sec\n",
      "INFO:tensorflow:training step 7155 | tagging_loss_video: 5.130|tagging_loss_audio: 8.065|tagging_loss_text: 13.156|tagging_loss_image: 4.930|tagging_loss_fusion: 3.053|total_loss: 34.333 | 70.60 Examples/sec\n",
      "INFO:tensorflow:training step 7156 | tagging_loss_video: 5.410|tagging_loss_audio: 6.882|tagging_loss_text: 14.105|tagging_loss_image: 4.759|tagging_loss_fusion: 2.087|total_loss: 33.243 | 67.18 Examples/sec\n",
      "INFO:tensorflow:training step 7157 | tagging_loss_video: 5.540|tagging_loss_audio: 8.586|tagging_loss_text: 13.175|tagging_loss_image: 6.445|tagging_loss_fusion: 5.543|total_loss: 39.289 | 67.55 Examples/sec\n",
      "INFO:tensorflow:training step 7158 | tagging_loss_video: 4.116|tagging_loss_audio: 7.974|tagging_loss_text: 12.217|tagging_loss_image: 5.786|tagging_loss_fusion: 2.804|total_loss: 32.896 | 71.35 Examples/sec\n",
      "INFO:tensorflow:training step 7159 | tagging_loss_video: 4.992|tagging_loss_audio: 7.190|tagging_loss_text: 16.763|tagging_loss_image: 4.250|tagging_loss_fusion: 2.754|total_loss: 35.948 | 67.71 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7160 |tagging_loss_video: 6.523|tagging_loss_audio: 8.879|tagging_loss_text: 14.775|tagging_loss_image: 5.668|tagging_loss_fusion: 5.285|total_loss: 41.130 | Examples/sec: 70.32\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 7161 | tagging_loss_video: 4.640|tagging_loss_audio: 8.867|tagging_loss_text: 16.450|tagging_loss_image: 4.912|tagging_loss_fusion: 2.751|total_loss: 37.620 | 60.86 Examples/sec\n",
      "INFO:tensorflow:training step 7162 | tagging_loss_video: 4.160|tagging_loss_audio: 7.688|tagging_loss_text: 15.277|tagging_loss_image: 3.746|tagging_loss_fusion: 2.130|total_loss: 33.002 | 69.40 Examples/sec\n",
      "INFO:tensorflow:training step 7163 | tagging_loss_video: 5.085|tagging_loss_audio: 7.934|tagging_loss_text: 14.354|tagging_loss_image: 2.228|tagging_loss_fusion: 2.304|total_loss: 31.905 | 70.14 Examples/sec\n",
      "INFO:tensorflow:training step 7164 | tagging_loss_video: 5.172|tagging_loss_audio: 7.609|tagging_loss_text: 13.629|tagging_loss_image: 4.563|tagging_loss_fusion: 3.962|total_loss: 34.936 | 63.38 Examples/sec\n",
      "INFO:tensorflow:training step 7165 | tagging_loss_video: 5.631|tagging_loss_audio: 8.471|tagging_loss_text: 16.400|tagging_loss_image: 4.009|tagging_loss_fusion: 3.286|total_loss: 37.798 | 72.22 Examples/sec\n",
      "INFO:tensorflow:training step 7166 | tagging_loss_video: 3.656|tagging_loss_audio: 7.161|tagging_loss_text: 14.860|tagging_loss_image: 5.259|tagging_loss_fusion: 2.160|total_loss: 33.096 | 69.94 Examples/sec\n",
      "INFO:tensorflow:training step 7167 | tagging_loss_video: 5.333|tagging_loss_audio: 8.783|tagging_loss_text: 13.943|tagging_loss_image: 4.701|tagging_loss_fusion: 3.238|total_loss: 35.998 | 69.42 Examples/sec\n",
      "INFO:tensorflow:training step 7168 | tagging_loss_video: 6.075|tagging_loss_audio: 7.097|tagging_loss_text: 15.424|tagging_loss_image: 5.711|tagging_loss_fusion: 5.253|total_loss: 39.559 | 70.19 Examples/sec\n",
      "INFO:tensorflow:training step 7169 | tagging_loss_video: 5.702|tagging_loss_audio: 8.351|tagging_loss_text: 17.047|tagging_loss_image: 3.789|tagging_loss_fusion: 4.218|total_loss: 39.107 | 67.44 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 7170.\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7170 |tagging_loss_video: 5.175|tagging_loss_audio: 9.259|tagging_loss_text: 16.428|tagging_loss_image: 5.522|tagging_loss_fusion: 4.936|total_loss: 41.320 | Examples/sec: 52.38\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.83 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 7171 | tagging_loss_video: 4.521|tagging_loss_audio: 8.026|tagging_loss_text: 15.661|tagging_loss_image: 5.495|tagging_loss_fusion: 2.932|total_loss: 36.635 | 65.06 Examples/sec\n",
      "INFO:tensorflow:training step 7172 | tagging_loss_video: 5.952|tagging_loss_audio: 8.774|tagging_loss_text: 17.318|tagging_loss_image: 5.308|tagging_loss_fusion: 4.731|total_loss: 42.083 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 7173 | tagging_loss_video: 5.356|tagging_loss_audio: 7.861|tagging_loss_text: 15.163|tagging_loss_image: 5.604|tagging_loss_fusion: 4.467|total_loss: 38.450 | 70.94 Examples/sec\n",
      "INFO:tensorflow:training step 7174 | tagging_loss_video: 5.338|tagging_loss_audio: 7.933|tagging_loss_text: 13.756|tagging_loss_image: 5.700|tagging_loss_fusion: 5.072|total_loss: 37.799 | 59.92 Examples/sec\n",
      "INFO:tensorflow:training step 7175 | tagging_loss_video: 6.675|tagging_loss_audio: 8.204|tagging_loss_text: 11.863|tagging_loss_image: 6.588|tagging_loss_fusion: 5.590|total_loss: 38.921 | 71.21 Examples/sec\n",
      "INFO:tensorflow:training step 7176 | tagging_loss_video: 5.581|tagging_loss_audio: 7.885|tagging_loss_text: 17.536|tagging_loss_image: 5.541|tagging_loss_fusion: 5.319|total_loss: 41.862 | 70.37 Examples/sec\n",
      "INFO:tensorflow:training step 7177 | tagging_loss_video: 6.428|tagging_loss_audio: 8.477|tagging_loss_text: 15.232|tagging_loss_image: 5.470|tagging_loss_fusion: 5.442|total_loss: 41.049 | 60.63 Examples/sec\n",
      "INFO:tensorflow:training step 7178 | tagging_loss_video: 6.173|tagging_loss_audio: 8.127|tagging_loss_text: 17.288|tagging_loss_image: 5.538|tagging_loss_fusion: 4.566|total_loss: 41.692 | 70.73 Examples/sec\n",
      "INFO:tensorflow:training step 7179 | tagging_loss_video: 3.303|tagging_loss_audio: 9.046|tagging_loss_text: 14.498|tagging_loss_image: 4.278|tagging_loss_fusion: 2.207|total_loss: 33.332 | 71.64 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7180 |tagging_loss_video: 6.299|tagging_loss_audio: 7.748|tagging_loss_text: 16.878|tagging_loss_image: 5.975|tagging_loss_fusion: 6.966|total_loss: 43.867 | Examples/sec: 64.72\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.79 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 7181 | tagging_loss_video: 6.470|tagging_loss_audio: 8.986|tagging_loss_text: 22.346|tagging_loss_image: 4.797|tagging_loss_fusion: 6.028|total_loss: 48.628 | 70.39 Examples/sec\n",
      "INFO:tensorflow:training step 7182 | tagging_loss_video: 3.938|tagging_loss_audio: 9.048|tagging_loss_text: 15.759|tagging_loss_image: 3.119|tagging_loss_fusion: 2.422|total_loss: 34.286 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 7183 | tagging_loss_video: 4.755|tagging_loss_audio: 8.273|tagging_loss_text: 14.787|tagging_loss_image: 5.324|tagging_loss_fusion: 2.904|total_loss: 36.043 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 7184 | tagging_loss_video: 4.434|tagging_loss_audio: 7.655|tagging_loss_text: 14.219|tagging_loss_image: 4.706|tagging_loss_fusion: 2.088|total_loss: 33.102 | 70.32 Examples/sec\n",
      "INFO:tensorflow:training step 7185 | tagging_loss_video: 6.017|tagging_loss_audio: 9.533|tagging_loss_text: 15.881|tagging_loss_image: 3.823|tagging_loss_fusion: 3.363|total_loss: 38.615 | 59.25 Examples/sec\n",
      "INFO:tensorflow:training step 7186 | tagging_loss_video: 4.312|tagging_loss_audio: 8.813|tagging_loss_text: 18.008|tagging_loss_image: 5.881|tagging_loss_fusion: 2.354|total_loss: 39.368 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 7187 | tagging_loss_video: 5.625|tagging_loss_audio: 8.143|tagging_loss_text: 18.222|tagging_loss_image: 4.983|tagging_loss_fusion: 3.385|total_loss: 40.357 | 71.34 Examples/sec\n",
      "INFO:tensorflow:training step 7188 | tagging_loss_video: 6.129|tagging_loss_audio: 8.697|tagging_loss_text: 14.616|tagging_loss_image: 4.876|tagging_loss_fusion: 5.447|total_loss: 39.766 | 63.27 Examples/sec\n",
      "INFO:tensorflow:training step 7189 | tagging_loss_video: 5.595|tagging_loss_audio: 11.016|tagging_loss_text: 21.350|tagging_loss_image: 6.341|tagging_loss_fusion: 4.341|total_loss: 48.642 | 67.06 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7190 |tagging_loss_video: 4.210|tagging_loss_audio: 8.443|tagging_loss_text: 14.525|tagging_loss_image: 4.509|tagging_loss_fusion: 2.702|total_loss: 34.389 | Examples/sec: 69.49\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.88 | precision@0.5: 0.96 |recall@0.1: 1.00 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7191 | tagging_loss_video: 6.186|tagging_loss_audio: 8.661|tagging_loss_text: 13.965|tagging_loss_image: 4.450|tagging_loss_fusion: 5.310|total_loss: 38.571 | 68.29 Examples/sec\n",
      "INFO:tensorflow:training step 7192 | tagging_loss_video: 5.299|tagging_loss_audio: 7.984|tagging_loss_text: 16.085|tagging_loss_image: 5.628|tagging_loss_fusion: 3.923|total_loss: 38.918 | 69.17 Examples/sec\n",
      "INFO:tensorflow:training step 7193 | tagging_loss_video: 5.621|tagging_loss_audio: 8.413|tagging_loss_text: 13.024|tagging_loss_image: 4.227|tagging_loss_fusion: 4.018|total_loss: 35.304 | 66.57 Examples/sec\n",
      "INFO:tensorflow:training step 7194 | tagging_loss_video: 4.794|tagging_loss_audio: 7.832|tagging_loss_text: 15.944|tagging_loss_image: 4.963|tagging_loss_fusion: 2.235|total_loss: 35.769 | 67.11 Examples/sec\n",
      "INFO:tensorflow:training step 7195 | tagging_loss_video: 5.694|tagging_loss_audio: 10.042|tagging_loss_text: 19.294|tagging_loss_image: 5.622|tagging_loss_fusion: 3.034|total_loss: 43.686 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 7196 | tagging_loss_video: 5.559|tagging_loss_audio: 9.054|tagging_loss_text: 14.784|tagging_loss_image: 4.971|tagging_loss_fusion: 4.720|total_loss: 39.088 | 63.97 Examples/sec\n",
      "INFO:tensorflow:training step 7197 | tagging_loss_video: 5.942|tagging_loss_audio: 8.061|tagging_loss_text: 12.441|tagging_loss_image: 3.947|tagging_loss_fusion: 4.736|total_loss: 35.128 | 69.88 Examples/sec\n",
      "INFO:tensorflow:training step 7198 | tagging_loss_video: 5.077|tagging_loss_audio: 7.966|tagging_loss_text: 14.983|tagging_loss_image: 4.511|tagging_loss_fusion: 2.551|total_loss: 35.087 | 69.99 Examples/sec\n",
      "INFO:tensorflow:training step 7199 | tagging_loss_video: 4.953|tagging_loss_audio: 7.971|tagging_loss_text: 14.752|tagging_loss_image: 3.887|tagging_loss_fusion: 2.678|total_loss: 34.241 | 58.69 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7200 |tagging_loss_video: 6.077|tagging_loss_audio: 8.585|tagging_loss_text: 18.356|tagging_loss_image: 5.331|tagging_loss_fusion: 5.317|total_loss: 43.666 | Examples/sec: 68.80\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.85 | precision@0.5: 0.95 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 7201 | tagging_loss_video: 4.582|tagging_loss_audio: 8.495|tagging_loss_text: 14.861|tagging_loss_image: 5.789|tagging_loss_fusion: 3.472|total_loss: 37.198 | 69.90 Examples/sec\n",
      "INFO:tensorflow:training step 7202 | tagging_loss_video: 6.058|tagging_loss_audio: 8.676|tagging_loss_text: 14.508|tagging_loss_image: 3.189|tagging_loss_fusion: 4.385|total_loss: 36.817 | 71.00 Examples/sec\n",
      "INFO:tensorflow:training step 7203 | tagging_loss_video: 5.268|tagging_loss_audio: 8.500|tagging_loss_text: 16.579|tagging_loss_image: 4.793|tagging_loss_fusion: 4.490|total_loss: 39.629 | 69.01 Examples/sec\n",
      "INFO:tensorflow:training step 7204 | tagging_loss_video: 5.123|tagging_loss_audio: 7.677|tagging_loss_text: 14.814|tagging_loss_image: 3.394|tagging_loss_fusion: 3.546|total_loss: 34.555 | 68.89 Examples/sec\n",
      "INFO:tensorflow:training step 7205 | tagging_loss_video: 4.951|tagging_loss_audio: 7.486|tagging_loss_text: 13.192|tagging_loss_image: 5.667|tagging_loss_fusion: 4.971|total_loss: 36.266 | 71.34 Examples/sec\n",
      "INFO:tensorflow:training step 7206 | tagging_loss_video: 4.381|tagging_loss_audio: 7.383|tagging_loss_text: 13.287|tagging_loss_image: 4.497|tagging_loss_fusion: 3.621|total_loss: 33.168 | 70.41 Examples/sec\n",
      "INFO:tensorflow:training step 7207 | tagging_loss_video: 5.837|tagging_loss_audio: 8.465|tagging_loss_text: 15.345|tagging_loss_image: 4.533|tagging_loss_fusion: 4.285|total_loss: 38.466 | 71.94 Examples/sec\n",
      "INFO:tensorflow:training step 7208 | tagging_loss_video: 5.652|tagging_loss_audio: 9.473|tagging_loss_text: 13.381|tagging_loss_image: 5.513|tagging_loss_fusion: 4.032|total_loss: 38.051 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 7209 | tagging_loss_video: 4.814|tagging_loss_audio: 8.698|tagging_loss_text: 10.572|tagging_loss_image: 5.118|tagging_loss_fusion: 3.407|total_loss: 32.609 | 69.04 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7210 |tagging_loss_video: 4.991|tagging_loss_audio: 8.677|tagging_loss_text: 14.878|tagging_loss_image: 5.528|tagging_loss_fusion: 4.021|total_loss: 38.093 | Examples/sec: 64.11\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.84 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 7211 | tagging_loss_video: 5.107|tagging_loss_audio: 7.886|tagging_loss_text: 15.326|tagging_loss_image: 4.889|tagging_loss_fusion: 4.156|total_loss: 37.363 | 69.19 Examples/sec\n",
      "INFO:tensorflow:training step 7212 | tagging_loss_video: 2.821|tagging_loss_audio: 8.258|tagging_loss_text: 12.602|tagging_loss_image: 4.893|tagging_loss_fusion: 1.322|total_loss: 29.896 | 67.49 Examples/sec\n",
      "INFO:tensorflow:training step 7213 | tagging_loss_video: 5.742|tagging_loss_audio: 8.519|tagging_loss_text: 14.189|tagging_loss_image: 4.768|tagging_loss_fusion: 4.387|total_loss: 37.605 | 72.02 Examples/sec\n",
      "INFO:tensorflow:training step 7214 | tagging_loss_video: 3.605|tagging_loss_audio: 8.359|tagging_loss_text: 13.929|tagging_loss_image: 5.399|tagging_loss_fusion: 2.715|total_loss: 34.008 | 67.57 Examples/sec\n",
      "INFO:tensorflow:training step 7215 | tagging_loss_video: 4.655|tagging_loss_audio: 9.152|tagging_loss_text: 12.163|tagging_loss_image: 5.322|tagging_loss_fusion: 3.355|total_loss: 34.648 | 70.76 Examples/sec\n",
      "INFO:tensorflow:training step 7216 | tagging_loss_video: 5.636|tagging_loss_audio: 9.146|tagging_loss_text: 15.447|tagging_loss_image: 5.791|tagging_loss_fusion: 7.819|total_loss: 43.837 | 66.60 Examples/sec\n",
      "INFO:tensorflow:training step 7217 | tagging_loss_video: 6.549|tagging_loss_audio: 8.693|tagging_loss_text: 17.112|tagging_loss_image: 5.857|tagging_loss_fusion: 8.326|total_loss: 46.538 | 67.73 Examples/sec\n",
      "INFO:tensorflow:training step 7218 | tagging_loss_video: 5.055|tagging_loss_audio: 8.789|tagging_loss_text: 16.385|tagging_loss_image: 5.118|tagging_loss_fusion: 4.105|total_loss: 39.452 | 65.49 Examples/sec\n",
      "INFO:tensorflow:training step 7219 | tagging_loss_video: 5.522|tagging_loss_audio: 8.081|tagging_loss_text: 18.682|tagging_loss_image: 5.490|tagging_loss_fusion: 4.091|total_loss: 41.867 | 70.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7220 |tagging_loss_video: 5.932|tagging_loss_audio: 8.840|tagging_loss_text: 15.769|tagging_loss_image: 4.859|tagging_loss_fusion: 3.755|total_loss: 39.155 | Examples/sec: 70.15\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7221 | tagging_loss_video: 5.523|tagging_loss_audio: 7.388|tagging_loss_text: 15.195|tagging_loss_image: 4.695|tagging_loss_fusion: 4.366|total_loss: 37.167 | 61.30 Examples/sec\n",
      "INFO:tensorflow:training step 7222 | tagging_loss_video: 4.496|tagging_loss_audio: 9.364|tagging_loss_text: 15.083|tagging_loss_image: 3.227|tagging_loss_fusion: 2.696|total_loss: 34.866 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 7223 | tagging_loss_video: 5.966|tagging_loss_audio: 9.010|tagging_loss_text: 12.547|tagging_loss_image: 3.863|tagging_loss_fusion: 4.654|total_loss: 36.039 | 71.26 Examples/sec\n",
      "INFO:tensorflow:training step 7224 | tagging_loss_video: 4.698|tagging_loss_audio: 6.713|tagging_loss_text: 12.265|tagging_loss_image: 5.314|tagging_loss_fusion: 4.383|total_loss: 33.373 | 62.65 Examples/sec\n",
      "INFO:tensorflow:training step 7225 | tagging_loss_video: 4.597|tagging_loss_audio: 7.792|tagging_loss_text: 12.157|tagging_loss_image: 5.233|tagging_loss_fusion: 4.433|total_loss: 34.211 | 69.04 Examples/sec\n",
      "INFO:tensorflow:training step 7226 | tagging_loss_video: 4.512|tagging_loss_audio: 9.140|tagging_loss_text: 14.492|tagging_loss_image: 3.190|tagging_loss_fusion: 2.256|total_loss: 33.591 | 71.61 Examples/sec\n",
      "INFO:tensorflow:training step 7227 | tagging_loss_video: 4.536|tagging_loss_audio: 8.867|tagging_loss_text: 16.197|tagging_loss_image: 5.888|tagging_loss_fusion: 3.486|total_loss: 38.974 | 61.43 Examples/sec\n",
      "INFO:tensorflow:training step 7228 | tagging_loss_video: 4.536|tagging_loss_audio: 8.174|tagging_loss_text: 18.475|tagging_loss_image: 4.599|tagging_loss_fusion: 2.523|total_loss: 38.307 | 69.54 Examples/sec\n",
      "INFO:tensorflow:training step 7229 | tagging_loss_video: 5.079|tagging_loss_audio: 9.327|tagging_loss_text: 16.694|tagging_loss_image: 4.630|tagging_loss_fusion: 3.504|total_loss: 39.234 | 71.85 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7230 |tagging_loss_video: 5.418|tagging_loss_audio: 8.895|tagging_loss_text: 15.656|tagging_loss_image: 5.607|tagging_loss_fusion: 3.877|total_loss: 39.452 | Examples/sec: 64.07\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.86 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 7231 | tagging_loss_video: 5.333|tagging_loss_audio: 9.014|tagging_loss_text: 19.196|tagging_loss_image: 4.308|tagging_loss_fusion: 3.305|total_loss: 41.157 | 65.64 Examples/sec\n",
      "INFO:tensorflow:training step 7232 | tagging_loss_video: 4.966|tagging_loss_audio: 8.840|tagging_loss_text: 16.803|tagging_loss_image: 4.337|tagging_loss_fusion: 2.682|total_loss: 37.628 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 7233 | tagging_loss_video: 5.159|tagging_loss_audio: 8.230|tagging_loss_text: 15.248|tagging_loss_image: 3.778|tagging_loss_fusion: 3.922|total_loss: 36.338 | 70.65 Examples/sec\n",
      "INFO:tensorflow:training step 7234 | tagging_loss_video: 5.425|tagging_loss_audio: 7.202|tagging_loss_text: 14.043|tagging_loss_image: 5.471|tagging_loss_fusion: 4.039|total_loss: 36.181 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 7235 | tagging_loss_video: 2.710|tagging_loss_audio: 8.289|tagging_loss_text: 9.562|tagging_loss_image: 3.883|tagging_loss_fusion: 1.691|total_loss: 26.136 | 63.79 Examples/sec\n",
      "INFO:tensorflow:training step 7236 | tagging_loss_video: 5.152|tagging_loss_audio: 8.539|tagging_loss_text: 12.735|tagging_loss_image: 3.059|tagging_loss_fusion: 3.100|total_loss: 32.583 | 70.07 Examples/sec\n",
      "INFO:tensorflow:training step 7237 | tagging_loss_video: 5.148|tagging_loss_audio: 8.344|tagging_loss_text: 14.954|tagging_loss_image: 5.423|tagging_loss_fusion: 4.953|total_loss: 38.823 | 70.78 Examples/sec\n",
      "INFO:tensorflow:training step 7238 | tagging_loss_video: 4.216|tagging_loss_audio: 8.286|tagging_loss_text: 14.326|tagging_loss_image: 5.037|tagging_loss_fusion: 2.636|total_loss: 34.501 | 64.03 Examples/sec\n",
      "INFO:tensorflow:training step 7239 | tagging_loss_video: 4.016|tagging_loss_audio: 8.170|tagging_loss_text: 15.254|tagging_loss_image: 5.141|tagging_loss_fusion: 2.175|total_loss: 34.757 | 68.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7240 |tagging_loss_video: 4.532|tagging_loss_audio: 7.553|tagging_loss_text: 16.532|tagging_loss_image: 5.434|tagging_loss_fusion: 2.716|total_loss: 36.767 | Examples/sec: 71.87\n",
      "INFO:tensorflow:GAP: 0.99 | precision@0.1: 0.85 | precision@0.5: 0.94 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7241 | tagging_loss_video: 6.142|tagging_loss_audio: 7.339|tagging_loss_text: 13.952|tagging_loss_image: 4.481|tagging_loss_fusion: 4.046|total_loss: 35.960 | 61.47 Examples/sec\n",
      "INFO:tensorflow:training step 7242 | tagging_loss_video: 4.808|tagging_loss_audio: 8.942|tagging_loss_text: 14.762|tagging_loss_image: 6.000|tagging_loss_fusion: 3.881|total_loss: 38.392 | 69.27 Examples/sec\n",
      "INFO:tensorflow:training step 7243 | tagging_loss_video: 5.006|tagging_loss_audio: 7.204|tagging_loss_text: 16.564|tagging_loss_image: 4.253|tagging_loss_fusion: 3.797|total_loss: 36.825 | 67.96 Examples/sec\n",
      "INFO:tensorflow:training step 7244 | tagging_loss_video: 4.903|tagging_loss_audio: 7.880|tagging_loss_text: 14.038|tagging_loss_image: 5.715|tagging_loss_fusion: 5.280|total_loss: 37.816 | 70.21 Examples/sec\n",
      "INFO:tensorflow:training step 7245 | tagging_loss_video: 6.071|tagging_loss_audio: 9.247|tagging_loss_text: 15.534|tagging_loss_image: 5.825|tagging_loss_fusion: 4.632|total_loss: 41.308 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 7246 | tagging_loss_video: 4.763|tagging_loss_audio: 7.863|tagging_loss_text: 14.143|tagging_loss_image: 5.383|tagging_loss_fusion: 4.205|total_loss: 36.357 | 65.83 Examples/sec\n",
      "INFO:tensorflow:training step 7247 | tagging_loss_video: 4.602|tagging_loss_audio: 7.377|tagging_loss_text: 14.822|tagging_loss_image: 5.541|tagging_loss_fusion: 3.368|total_loss: 35.710 | 69.07 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 7248 | tagging_loss_video: 4.350|tagging_loss_audio: 8.182|tagging_loss_text: 16.738|tagging_loss_image: 5.012|tagging_loss_fusion: 2.105|total_loss: 36.387 | 69.85 Examples/sec\n",
      "INFO:tensorflow:training step 7249 | tagging_loss_video: 4.936|tagging_loss_audio: 8.278|tagging_loss_text: 15.213|tagging_loss_image: 3.904|tagging_loss_fusion: 3.318|total_loss: 35.649 | 61.82 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7250 |tagging_loss_video: 5.102|tagging_loss_audio: 8.228|tagging_loss_text: 14.596|tagging_loss_image: 5.137|tagging_loss_fusion: 3.094|total_loss: 36.157 | Examples/sec: 69.88\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.86 | precision@0.5: 0.95 |recall@0.1: 0.98 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 7251 | tagging_loss_video: 4.854|tagging_loss_audio: 8.074|tagging_loss_text: 14.059|tagging_loss_image: 5.448|tagging_loss_fusion: 3.040|total_loss: 35.475 | 69.81 Examples/sec\n",
      "INFO:tensorflow:training step 7252 | tagging_loss_video: 6.377|tagging_loss_audio: 8.432|tagging_loss_text: 16.079|tagging_loss_image: 5.821|tagging_loss_fusion: 5.409|total_loss: 42.117 | 64.57 Examples/sec\n",
      "INFO:tensorflow:training step 7253 | tagging_loss_video: 5.040|tagging_loss_audio: 7.747|tagging_loss_text: 14.751|tagging_loss_image: 5.696|tagging_loss_fusion: 5.712|total_loss: 38.945 | 70.75 Examples/sec\n",
      "INFO:tensorflow:training step 7254 | tagging_loss_video: 5.108|tagging_loss_audio: 7.849|tagging_loss_text: 14.409|tagging_loss_image: 5.154|tagging_loss_fusion: 3.659|total_loss: 36.178 | 67.28 Examples/sec\n",
      "INFO:tensorflow:training step 7255 | tagging_loss_video: 5.885|tagging_loss_audio: 7.869|tagging_loss_text: 14.086|tagging_loss_image: 4.814|tagging_loss_fusion: 4.282|total_loss: 36.936 | 69.34 Examples/sec\n",
      "INFO:tensorflow:training step 7256 | tagging_loss_video: 4.539|tagging_loss_audio: 9.096|tagging_loss_text: 14.657|tagging_loss_image: 4.548|tagging_loss_fusion: 3.277|total_loss: 36.118 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 7257 | tagging_loss_video: 3.941|tagging_loss_audio: 7.511|tagging_loss_text: 15.596|tagging_loss_image: 4.209|tagging_loss_fusion: 2.893|total_loss: 34.151 | 64.33 Examples/sec\n",
      "INFO:tensorflow:training step 7258 | tagging_loss_video: 6.024|tagging_loss_audio: 7.972|tagging_loss_text: 14.976|tagging_loss_image: 4.611|tagging_loss_fusion: 4.874|total_loss: 38.457 | 68.70 Examples/sec\n",
      "INFO:tensorflow:training step 7259 | tagging_loss_video: 6.686|tagging_loss_audio: 7.908|tagging_loss_text: 15.564|tagging_loss_image: 4.636|tagging_loss_fusion: 5.167|total_loss: 39.961 | 70.79 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7260 |tagging_loss_video: 6.504|tagging_loss_audio: 8.665|tagging_loss_text: 19.566|tagging_loss_image: 5.237|tagging_loss_fusion: 6.922|total_loss: 46.895 | Examples/sec: 62.01\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.76 | precision@0.5: 0.91 |recall@0.1: 0.96 | recall@0.5: 0.85\n",
      "INFO:tensorflow:training step 7261 | tagging_loss_video: 5.947|tagging_loss_audio: 8.990|tagging_loss_text: 18.289|tagging_loss_image: 6.153|tagging_loss_fusion: 5.002|total_loss: 44.382 | 71.17 Examples/sec\n",
      "INFO:tensorflow:training step 7262 | tagging_loss_video: 6.010|tagging_loss_audio: 9.775|tagging_loss_text: 16.728|tagging_loss_image: 7.094|tagging_loss_fusion: 5.721|total_loss: 45.327 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 7263 | tagging_loss_video: 5.003|tagging_loss_audio: 7.555|tagging_loss_text: 13.814|tagging_loss_image: 5.351|tagging_loss_fusion: 5.123|total_loss: 36.846 | 71.91 Examples/sec\n",
      "INFO:tensorflow:training step 7264 | tagging_loss_video: 4.558|tagging_loss_audio: 7.452|tagging_loss_text: 13.743|tagging_loss_image: 4.066|tagging_loss_fusion: 2.755|total_loss: 32.575 | 70.67 Examples/sec\n",
      "INFO:tensorflow:training step 7265 | tagging_loss_video: 4.895|tagging_loss_audio: 7.494|tagging_loss_text: 18.799|tagging_loss_image: 5.933|tagging_loss_fusion: 3.777|total_loss: 40.897 | 64.42 Examples/sec\n",
      "INFO:tensorflow:training step 7266 | tagging_loss_video: 5.275|tagging_loss_audio: 7.323|tagging_loss_text: 13.952|tagging_loss_image: 4.575|tagging_loss_fusion: 5.129|total_loss: 36.254 | 65.00 Examples/sec\n",
      "INFO:tensorflow:training step 7267 | tagging_loss_video: 5.305|tagging_loss_audio: 7.711|tagging_loss_text: 16.117|tagging_loss_image: 4.496|tagging_loss_fusion: 4.118|total_loss: 37.747 | 70.52 Examples/sec\n",
      "INFO:tensorflow:training step 7268 | tagging_loss_video: 5.591|tagging_loss_audio: 7.188|tagging_loss_text: 12.492|tagging_loss_image: 4.751|tagging_loss_fusion: 4.630|total_loss: 34.652 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 7269 | tagging_loss_video: 5.350|tagging_loss_audio: 7.519|tagging_loss_text: 13.961|tagging_loss_image: 4.212|tagging_loss_fusion: 3.806|total_loss: 34.848 | 69.24 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7270 |tagging_loss_video: 5.528|tagging_loss_audio: 7.958|tagging_loss_text: 14.630|tagging_loss_image: 3.764|tagging_loss_fusion: 4.086|total_loss: 35.966 | Examples/sec: 69.45\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.97 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7271 | tagging_loss_video: 4.285|tagging_loss_audio: 7.526|tagging_loss_text: 16.216|tagging_loss_image: 6.089|tagging_loss_fusion: 2.879|total_loss: 36.995 | 67.33 Examples/sec\n",
      "INFO:tensorflow:training step 7272 | tagging_loss_video: 4.527|tagging_loss_audio: 7.682|tagging_loss_text: 14.357|tagging_loss_image: 4.667|tagging_loss_fusion: 2.979|total_loss: 34.212 | 70.28 Examples/sec\n",
      "INFO:tensorflow:training step 7273 | tagging_loss_video: 4.573|tagging_loss_audio: 7.206|tagging_loss_text: 10.508|tagging_loss_image: 3.657|tagging_loss_fusion: 3.361|total_loss: 29.304 | 66.21 Examples/sec\n",
      "INFO:tensorflow:training step 7274 | tagging_loss_video: 5.293|tagging_loss_audio: 7.224|tagging_loss_text: 13.046|tagging_loss_image: 3.962|tagging_loss_fusion: 3.146|total_loss: 32.673 | 62.75 Examples/sec\n",
      "INFO:tensorflow:training step 7275 | tagging_loss_video: 5.206|tagging_loss_audio: 7.851|tagging_loss_text: 16.567|tagging_loss_image: 5.999|tagging_loss_fusion: 2.967|total_loss: 38.589 | 67.98 Examples/sec\n",
      "INFO:tensorflow:training step 7276 | tagging_loss_video: 5.305|tagging_loss_audio: 8.080|tagging_loss_text: 15.230|tagging_loss_image: 5.388|tagging_loss_fusion: 3.480|total_loss: 37.482 | 67.05 Examples/sec\n",
      "INFO:tensorflow:training step 7277 | tagging_loss_video: 5.515|tagging_loss_audio: 7.640|tagging_loss_text: 16.072|tagging_loss_image: 5.274|tagging_loss_fusion: 4.865|total_loss: 39.365 | 60.95 Examples/sec\n",
      "INFO:tensorflow:training step 7278 | tagging_loss_video: 5.239|tagging_loss_audio: 7.601|tagging_loss_text: 14.075|tagging_loss_image: 5.070|tagging_loss_fusion: 4.728|total_loss: 36.714 | 71.23 Examples/sec\n",
      "INFO:tensorflow:training step 7279 | tagging_loss_video: 3.860|tagging_loss_audio: 7.990|tagging_loss_text: 16.533|tagging_loss_image: 4.961|tagging_loss_fusion: 2.572|total_loss: 35.916 | 67.35 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7280 |tagging_loss_video: 5.248|tagging_loss_audio: 7.131|tagging_loss_text: 11.078|tagging_loss_image: 4.999|tagging_loss_fusion: 3.538|total_loss: 31.993 | Examples/sec: 71.43\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.85 | precision@0.5: 0.93 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7281 | tagging_loss_video: 5.229|tagging_loss_audio: 7.629|tagging_loss_text: 14.984|tagging_loss_image: 4.655|tagging_loss_fusion: 3.630|total_loss: 36.127 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 7282 | tagging_loss_video: 4.175|tagging_loss_audio: 7.276|tagging_loss_text: 13.860|tagging_loss_image: 5.031|tagging_loss_fusion: 2.514|total_loss: 32.856 | 70.09 Examples/sec\n",
      "INFO:tensorflow:training step 7283 | tagging_loss_video: 4.803|tagging_loss_audio: 7.757|tagging_loss_text: 13.928|tagging_loss_image: 4.218|tagging_loss_fusion: 3.027|total_loss: 33.734 | 70.27 Examples/sec\n",
      "INFO:tensorflow:training step 7284 | tagging_loss_video: 5.367|tagging_loss_audio: 7.257|tagging_loss_text: 16.070|tagging_loss_image: 5.106|tagging_loss_fusion: 4.688|total_loss: 38.487 | 69.73 Examples/sec\n",
      "INFO:tensorflow:training step 7285 | tagging_loss_video: 3.359|tagging_loss_audio: 9.014|tagging_loss_text: 15.287|tagging_loss_image: 5.520|tagging_loss_fusion: 2.289|total_loss: 35.469 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 7286 | tagging_loss_video: 5.859|tagging_loss_audio: 8.280|tagging_loss_text: 16.844|tagging_loss_image: 4.790|tagging_loss_fusion: 4.408|total_loss: 40.182 | 71.16 Examples/sec\n",
      "INFO:tensorflow:training step 7287 | tagging_loss_video: 4.756|tagging_loss_audio: 8.073|tagging_loss_text: 15.515|tagging_loss_image: 5.236|tagging_loss_fusion: 2.602|total_loss: 36.180 | 70.43 Examples/sec\n",
      "INFO:tensorflow:training step 7288 | tagging_loss_video: 5.386|tagging_loss_audio: 8.691|tagging_loss_text: 17.109|tagging_loss_image: 4.935|tagging_loss_fusion: 4.167|total_loss: 40.289 | 68.66 Examples/sec\n",
      "INFO:tensorflow:training step 7289 | tagging_loss_video: 4.840|tagging_loss_audio: 7.258|tagging_loss_text: 12.685|tagging_loss_image: 3.328|tagging_loss_fusion: 3.007|total_loss: 31.117 | 67.93 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7290 |tagging_loss_video: 4.970|tagging_loss_audio: 6.930|tagging_loss_text: 14.840|tagging_loss_image: 3.345|tagging_loss_fusion: 3.041|total_loss: 33.125 | Examples/sec: 70.00\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.81 | precision@0.5: 0.96 |recall@0.1: 0.99 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7291 | tagging_loss_video: 4.985|tagging_loss_audio: 8.234|tagging_loss_text: 14.687|tagging_loss_image: 4.374|tagging_loss_fusion: 3.002|total_loss: 35.281 | 71.66 Examples/sec\n",
      "INFO:tensorflow:training step 7292 | tagging_loss_video: 4.073|tagging_loss_audio: 8.190|tagging_loss_text: 16.753|tagging_loss_image: 5.264|tagging_loss_fusion: 3.759|total_loss: 38.038 | 64.45 Examples/sec\n",
      "INFO:tensorflow:training step 7293 | tagging_loss_video: 4.276|tagging_loss_audio: 7.818|tagging_loss_text: 15.865|tagging_loss_image: 4.862|tagging_loss_fusion: 2.770|total_loss: 35.592 | 71.12 Examples/sec\n",
      "INFO:tensorflow:training step 7294 | tagging_loss_video: 5.739|tagging_loss_audio: 7.175|tagging_loss_text: 14.448|tagging_loss_image: 4.111|tagging_loss_fusion: 4.307|total_loss: 35.780 | 68.34 Examples/sec\n",
      "INFO:tensorflow:training step 7295 | tagging_loss_video: 3.832|tagging_loss_audio: 7.688|tagging_loss_text: 15.393|tagging_loss_image: 5.359|tagging_loss_fusion: 2.861|total_loss: 35.133 | 63.85 Examples/sec\n",
      "INFO:tensorflow:training step 7296 | tagging_loss_video: 4.114|tagging_loss_audio: 8.183|tagging_loss_text: 14.570|tagging_loss_image: 5.939|tagging_loss_fusion: 2.757|total_loss: 35.563 | 67.72 Examples/sec\n",
      "INFO:tensorflow:training step 7297 | tagging_loss_video: 4.514|tagging_loss_audio: 7.660|tagging_loss_text: 13.570|tagging_loss_image: 3.447|tagging_loss_fusion: 2.211|total_loss: 31.402 | 68.51 Examples/sec\n",
      "INFO:tensorflow:training step 7298 | tagging_loss_video: 5.649|tagging_loss_audio: 7.378|tagging_loss_text: 13.272|tagging_loss_image: 4.957|tagging_loss_fusion: 4.810|total_loss: 36.065 | 69.92 Examples/sec\n",
      "INFO:tensorflow:training step 7299 | tagging_loss_video: 5.035|tagging_loss_audio: 8.739|tagging_loss_text: 16.992|tagging_loss_image: 5.781|tagging_loss_fusion: 3.428|total_loss: 39.975 | 69.89 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7300 |tagging_loss_video: 5.870|tagging_loss_audio: 8.033|tagging_loss_text: 13.017|tagging_loss_image: 4.649|tagging_loss_fusion: 4.284|total_loss: 35.852 | Examples/sec: 61.43\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.83 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7301 | tagging_loss_video: 4.761|tagging_loss_audio: 7.597|tagging_loss_text: 13.363|tagging_loss_image: 3.753|tagging_loss_fusion: 2.804|total_loss: 32.279 | 70.30 Examples/sec\n",
      "INFO:tensorflow:training step 7302 | tagging_loss_video: 4.097|tagging_loss_audio: 7.729|tagging_loss_text: 15.613|tagging_loss_image: 3.558|tagging_loss_fusion: 1.840|total_loss: 32.836 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 7303 | tagging_loss_video: 4.709|tagging_loss_audio: 7.583|tagging_loss_text: 16.878|tagging_loss_image: 5.156|tagging_loss_fusion: 5.808|total_loss: 40.135 | 68.59 Examples/sec\n",
      "INFO:tensorflow:training step 7304 | tagging_loss_video: 5.805|tagging_loss_audio: 8.913|tagging_loss_text: 17.846|tagging_loss_image: 5.437|tagging_loss_fusion: 5.780|total_loss: 43.781 | 70.11 Examples/sec\n",
      "INFO:tensorflow:training step 7305 | tagging_loss_video: 2.940|tagging_loss_audio: 7.725|tagging_loss_text: 12.841|tagging_loss_image: 5.486|tagging_loss_fusion: 1.066|total_loss: 30.057 | 69.89 Examples/sec\n",
      "INFO:tensorflow:training step 7306 | tagging_loss_video: 4.519|tagging_loss_audio: 8.069|tagging_loss_text: 16.007|tagging_loss_image: 6.124|tagging_loss_fusion: 3.007|total_loss: 37.726 | 67.09 Examples/sec\n",
      "INFO:tensorflow:training step 7307 | tagging_loss_video: 3.724|tagging_loss_audio: 8.310|tagging_loss_text: 16.711|tagging_loss_image: 4.753|tagging_loss_fusion: 2.477|total_loss: 35.976 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 7308 | tagging_loss_video: 4.455|tagging_loss_audio: 8.440|tagging_loss_text: 15.620|tagging_loss_image: 4.046|tagging_loss_fusion: 2.411|total_loss: 34.972 | 66.19 Examples/sec\n",
      "INFO:tensorflow:training step 7309 | tagging_loss_video: 6.158|tagging_loss_audio: 8.538|tagging_loss_text: 17.859|tagging_loss_image: 5.374|tagging_loss_fusion: 5.649|total_loss: 43.579 | 71.19 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7310 |tagging_loss_video: 6.017|tagging_loss_audio: 8.891|tagging_loss_text: 13.914|tagging_loss_image: 6.043|tagging_loss_fusion: 4.963|total_loss: 39.827 | Examples/sec: 69.15\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.85 | precision@0.5: 0.96 |recall@0.1: 0.96 | recall@0.5: 0.91\n",
      "INFO:tensorflow:training step 7311 | tagging_loss_video: 5.686|tagging_loss_audio: 8.386|tagging_loss_text: 17.035|tagging_loss_image: 4.927|tagging_loss_fusion: 2.963|total_loss: 38.997 | 69.45 Examples/sec\n",
      "INFO:tensorflow:training step 7312 | tagging_loss_video: 4.788|tagging_loss_audio: 8.516|tagging_loss_text: 17.327|tagging_loss_image: 5.687|tagging_loss_fusion: 3.831|total_loss: 40.148 | 68.24 Examples/sec\n",
      "INFO:tensorflow:training step 7313 | tagging_loss_video: 5.094|tagging_loss_audio: 8.495|tagging_loss_text: 15.836|tagging_loss_image: 5.234|tagging_loss_fusion: 4.379|total_loss: 39.038 | 72.79 Examples/sec\n",
      "INFO:tensorflow:training step 7314 | tagging_loss_video: 5.609|tagging_loss_audio: 8.373|tagging_loss_text: 16.469|tagging_loss_image: 3.969|tagging_loss_fusion: 5.585|total_loss: 40.005 | 58.77 Examples/sec\n",
      "INFO:tensorflow:training step 7315 | tagging_loss_video: 6.178|tagging_loss_audio: 8.358|tagging_loss_text: 16.317|tagging_loss_image: 6.113|tagging_loss_fusion: 5.118|total_loss: 42.084 | 70.25 Examples/sec\n",
      "INFO:tensorflow:training step 7316 | tagging_loss_video: 4.193|tagging_loss_audio: 7.687|tagging_loss_text: 14.288|tagging_loss_image: 5.109|tagging_loss_fusion: 3.143|total_loss: 34.420 | 70.26 Examples/sec\n",
      "INFO:tensorflow:training step 7317 | tagging_loss_video: 6.347|tagging_loss_audio: 8.584|tagging_loss_text: 14.542|tagging_loss_image: 3.980|tagging_loss_fusion: 5.379|total_loss: 38.831 | 63.48 Examples/sec\n",
      "INFO:tensorflow:training step 7318 | tagging_loss_video: 5.671|tagging_loss_audio: 8.086|tagging_loss_text: 17.276|tagging_loss_image: 4.638|tagging_loss_fusion: 4.011|total_loss: 39.682 | 69.71 Examples/sec\n",
      "INFO:tensorflow:training step 7319 | tagging_loss_video: 5.061|tagging_loss_audio: 8.445|tagging_loss_text: 15.931|tagging_loss_image: 3.832|tagging_loss_fusion: 3.762|total_loss: 37.031 | 68.14 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7320 |tagging_loss_video: 5.201|tagging_loss_audio: 8.631|tagging_loss_text: 14.062|tagging_loss_image: 4.324|tagging_loss_fusion: 2.509|total_loss: 34.727 | Examples/sec: 70.21\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.92 | precision@0.5: 0.98 |recall@0.1: 0.99 | recall@0.5: 0.96\n",
      "INFO:tensorflow:training step 7321 | tagging_loss_video: 5.791|tagging_loss_audio: 9.411|tagging_loss_text: 14.892|tagging_loss_image: 6.953|tagging_loss_fusion: 5.959|total_loss: 43.006 | 70.57 Examples/sec\n",
      "INFO:tensorflow:training step 7322 | tagging_loss_video: 4.511|tagging_loss_audio: 8.468|tagging_loss_text: 13.120|tagging_loss_image: 3.256|tagging_loss_fusion: 2.998|total_loss: 32.353 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 7323 | tagging_loss_video: 5.974|tagging_loss_audio: 7.632|tagging_loss_text: 14.071|tagging_loss_image: 6.654|tagging_loss_fusion: 7.334|total_loss: 41.664 | 69.69 Examples/sec\n",
      "INFO:tensorflow:training step 7324 | tagging_loss_video: 4.822|tagging_loss_audio: 7.888|tagging_loss_text: 12.978|tagging_loss_image: 5.447|tagging_loss_fusion: 3.907|total_loss: 35.043 | 72.28 Examples/sec\n",
      "INFO:tensorflow:training step 7325 | tagging_loss_video: 5.631|tagging_loss_audio: 9.613|tagging_loss_text: 13.718|tagging_loss_image: 6.422|tagging_loss_fusion: 4.126|total_loss: 39.510 | 61.55 Examples/sec\n",
      "INFO:tensorflow:training step 7326 | tagging_loss_video: 5.835|tagging_loss_audio: 9.403|tagging_loss_text: 12.951|tagging_loss_image: 2.680|tagging_loss_fusion: 3.727|total_loss: 34.597 | 65.91 Examples/sec\n",
      "INFO:tensorflow:training step 7327 | tagging_loss_video: 6.278|tagging_loss_audio: 8.827|tagging_loss_text: 14.553|tagging_loss_image: 5.621|tagging_loss_fusion: 4.112|total_loss: 39.391 | 70.92 Examples/sec\n",
      "INFO:tensorflow:training step 7328 | tagging_loss_video: 6.460|tagging_loss_audio: 8.687|tagging_loss_text: 13.233|tagging_loss_image: 3.776|tagging_loss_fusion: 4.698|total_loss: 36.853 | 67.35 Examples/sec\n",
      "INFO:tensorflow:training step 7329 | tagging_loss_video: 5.634|tagging_loss_audio: 9.023|tagging_loss_text: 20.603|tagging_loss_image: 6.095|tagging_loss_fusion: 5.106|total_loss: 46.460 | 70.80 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7330 |tagging_loss_video: 6.325|tagging_loss_audio: 8.571|tagging_loss_text: 17.962|tagging_loss_image: 6.087|tagging_loss_fusion: 8.633|total_loss: 47.579 | Examples/sec: 68.45\n",
      "INFO:tensorflow:GAP: 0.89 | precision@0.1: 0.78 | precision@0.5: 0.91 |recall@0.1: 0.94 | recall@0.5: 0.83\n",
      "INFO:tensorflow:training step 7331 | tagging_loss_video: 5.198|tagging_loss_audio: 8.608|tagging_loss_text: 14.652|tagging_loss_image: 4.599|tagging_loss_fusion: 3.447|total_loss: 36.505 | 65.73 Examples/sec\n",
      "INFO:tensorflow:training step 7332 | tagging_loss_video: 5.736|tagging_loss_audio: 8.669|tagging_loss_text: 14.065|tagging_loss_image: 4.810|tagging_loss_fusion: 6.432|total_loss: 39.711 | 70.71 Examples/sec\n",
      "INFO:tensorflow:training step 7333 | tagging_loss_video: 5.365|tagging_loss_audio: 8.305|tagging_loss_text: 11.654|tagging_loss_image: 6.070|tagging_loss_fusion: 3.805|total_loss: 35.199 | 68.73 Examples/sec\n",
      "INFO:tensorflow:training step 7334 | tagging_loss_video: 4.290|tagging_loss_audio: 7.713|tagging_loss_text: 17.951|tagging_loss_image: 4.898|tagging_loss_fusion: 3.225|total_loss: 38.077 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 7335 | tagging_loss_video: 5.539|tagging_loss_audio: 10.134|tagging_loss_text: 17.057|tagging_loss_image: 4.985|tagging_loss_fusion: 4.330|total_loss: 42.046 | 68.49 Examples/sec\n",
      "INFO:tensorflow:training step 7336 | tagging_loss_video: 6.114|tagging_loss_audio: 8.025|tagging_loss_text: 15.515|tagging_loss_image: 6.209|tagging_loss_fusion: 4.773|total_loss: 40.636 | 64.49 Examples/sec\n",
      "INFO:tensorflow:training step 7337 | tagging_loss_video: 5.075|tagging_loss_audio: 7.939|tagging_loss_text: 18.340|tagging_loss_image: 5.435|tagging_loss_fusion: 4.952|total_loss: 41.740 | 69.66 Examples/sec\n",
      "INFO:tensorflow:training step 7338 | tagging_loss_video: 5.499|tagging_loss_audio: 8.209|tagging_loss_text: 13.618|tagging_loss_image: 5.627|tagging_loss_fusion: 4.497|total_loss: 37.449 | 70.66 Examples/sec\n",
      "INFO:tensorflow:training step 7339 | tagging_loss_video: 4.844|tagging_loss_audio: 8.017|tagging_loss_text: 14.107|tagging_loss_image: 5.300|tagging_loss_fusion: 2.994|total_loss: 35.263 | 60.56 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7340 |tagging_loss_video: 6.295|tagging_loss_audio: 8.419|tagging_loss_text: 16.153|tagging_loss_image: 5.257|tagging_loss_fusion: 5.403|total_loss: 41.527 | Examples/sec: 71.65\n",
      "INFO:tensorflow:GAP: 0.93 | precision@0.1: 0.84 | precision@0.5: 0.95 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 7341 | tagging_loss_video: 5.950|tagging_loss_audio: 8.304|tagging_loss_text: 16.907|tagging_loss_image: 4.250|tagging_loss_fusion: 5.112|total_loss: 40.523 | 68.93 Examples/sec\n",
      "INFO:tensorflow:training step 7342 | tagging_loss_video: 5.859|tagging_loss_audio: 8.330|tagging_loss_text: 16.500|tagging_loss_image: 5.061|tagging_loss_fusion: 4.029|total_loss: 39.779 | 72.18 Examples/sec\n",
      "INFO:tensorflow:training step 7343 | tagging_loss_video: 4.867|tagging_loss_audio: 8.039|tagging_loss_text: 12.598|tagging_loss_image: 3.415|tagging_loss_fusion: 3.180|total_loss: 32.098 | 69.41 Examples/sec\n",
      "INFO:tensorflow:training step 7344 | tagging_loss_video: 5.118|tagging_loss_audio: 9.029|tagging_loss_text: 17.521|tagging_loss_image: 4.185|tagging_loss_fusion: 2.593|total_loss: 38.446 | 71.49 Examples/sec\n",
      "INFO:tensorflow:training step 7345 | tagging_loss_video: 5.181|tagging_loss_audio: 8.274|tagging_loss_text: 15.828|tagging_loss_image: 5.200|tagging_loss_fusion: 3.497|total_loss: 37.981 | 63.75 Examples/sec\n",
      "INFO:tensorflow:training step 7346 | tagging_loss_video: 5.532|tagging_loss_audio: 7.437|tagging_loss_text: 14.547|tagging_loss_image: 4.762|tagging_loss_fusion: 4.386|total_loss: 36.664 | 67.80 Examples/sec\n",
      "INFO:tensorflow:training step 7347 | tagging_loss_video: 5.717|tagging_loss_audio: 8.074|tagging_loss_text: 16.036|tagging_loss_image: 5.050|tagging_loss_fusion: 4.841|total_loss: 39.718 | 71.54 Examples/sec\n",
      "INFO:tensorflow:training step 7348 | tagging_loss_video: 5.822|tagging_loss_audio: 8.024|tagging_loss_text: 15.372|tagging_loss_image: 5.812|tagging_loss_fusion: 7.150|total_loss: 42.179 | 71.07 Examples/sec\n",
      "INFO:tensorflow:training step 7349 | tagging_loss_video: 5.147|tagging_loss_audio: 9.243|tagging_loss_text: 15.186|tagging_loss_image: 5.506|tagging_loss_fusion: 4.959|total_loss: 40.040 | 71.21 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7350 |tagging_loss_video: 5.722|tagging_loss_audio: 8.940|tagging_loss_text: 13.579|tagging_loss_image: 5.165|tagging_loss_fusion: 5.212|total_loss: 38.619 | Examples/sec: 62.12\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.82 | precision@0.5: 0.92 |recall@0.1: 0.97 | recall@0.5: 0.89\n",
      "INFO:tensorflow:training step 7351 | tagging_loss_video: 4.837|tagging_loss_audio: 7.350|tagging_loss_text: 13.325|tagging_loss_image: 4.588|tagging_loss_fusion: 3.606|total_loss: 33.706 | 66.63 Examples/sec\n",
      "INFO:tensorflow:training step 7352 | tagging_loss_video: 4.695|tagging_loss_audio: 8.022|tagging_loss_text: 14.574|tagging_loss_image: 5.038|tagging_loss_fusion: 3.350|total_loss: 35.678 | 69.98 Examples/sec\n",
      "INFO:tensorflow:training step 7353 | tagging_loss_video: 4.765|tagging_loss_audio: 8.147|tagging_loss_text: 11.493|tagging_loss_image: 4.602|tagging_loss_fusion: 3.585|total_loss: 32.594 | 70.80 Examples/sec\n",
      "INFO:tensorflow:training step 7354 | tagging_loss_video: 6.068|tagging_loss_audio: 8.511|tagging_loss_text: 18.822|tagging_loss_image: 4.514|tagging_loss_fusion: 3.925|total_loss: 41.839 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 7355 | tagging_loss_video: 6.901|tagging_loss_audio: 9.951|tagging_loss_text: 21.278|tagging_loss_image: 4.949|tagging_loss_fusion: 5.176|total_loss: 48.255 | 70.61 Examples/sec\n",
      "INFO:tensorflow:training step 7356 | tagging_loss_video: 5.929|tagging_loss_audio: 8.126|tagging_loss_text: 15.445|tagging_loss_image: 5.002|tagging_loss_fusion: 5.268|total_loss: 39.769 | 64.19 Examples/sec\n",
      "INFO:tensorflow:training step 7357 | tagging_loss_video: 5.657|tagging_loss_audio: 7.842|tagging_loss_text: 15.067|tagging_loss_image: 3.820|tagging_loss_fusion: 5.855|total_loss: 38.241 | 69.60 Examples/sec\n",
      "INFO:tensorflow:training step 7358 | tagging_loss_video: 6.096|tagging_loss_audio: 9.658|tagging_loss_text: 15.681|tagging_loss_image: 5.002|tagging_loss_fusion: 3.730|total_loss: 40.168 | 68.20 Examples/sec\n",
      "INFO:tensorflow:training step 7359 | tagging_loss_video: 5.610|tagging_loss_audio: 7.853|tagging_loss_text: 14.670|tagging_loss_image: 5.187|tagging_loss_fusion: 2.797|total_loss: 36.117 | 71.73 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7360 |tagging_loss_video: 6.574|tagging_loss_audio: 9.535|tagging_loss_text: 15.063|tagging_loss_image: 4.773|tagging_loss_fusion: 6.268|total_loss: 42.213 | Examples/sec: 68.82\n",
      "INFO:tensorflow:GAP: 0.92 | precision@0.1: 0.82 | precision@0.5: 0.94 |recall@0.1: 0.96 | recall@0.5: 0.87\n",
      "INFO:tensorflow:training step 7361 | tagging_loss_video: 5.752|tagging_loss_audio: 8.256|tagging_loss_text: 13.208|tagging_loss_image: 3.857|tagging_loss_fusion: 4.514|total_loss: 35.586 | 71.03 Examples/sec\n",
      "INFO:tensorflow:training step 7362 | tagging_loss_video: 5.396|tagging_loss_audio: 9.050|tagging_loss_text: 13.687|tagging_loss_image: 2.653|tagging_loss_fusion: 2.549|total_loss: 33.334 | 69.93 Examples/sec\n",
      "INFO:tensorflow:training step 7363 | tagging_loss_video: 5.554|tagging_loss_audio: 8.718|tagging_loss_text: 16.738|tagging_loss_image: 5.810|tagging_loss_fusion: 5.859|total_loss: 42.680 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 7364 | tagging_loss_video: 4.699|tagging_loss_audio: 7.094|tagging_loss_text: 15.333|tagging_loss_image: 4.967|tagging_loss_fusion: 2.607|total_loss: 34.699 | 59.11 Examples/sec\n",
      "INFO:tensorflow:training step 7365 | tagging_loss_video: 5.436|tagging_loss_audio: 8.409|tagging_loss_text: 13.512|tagging_loss_image: 5.241|tagging_loss_fusion: 4.728|total_loss: 37.325 | 68.63 Examples/sec\n",
      "INFO:tensorflow:training step 7366 | tagging_loss_video: 6.104|tagging_loss_audio: 9.297|tagging_loss_text: 19.335|tagging_loss_image: 5.062|tagging_loss_fusion: 5.481|total_loss: 45.279 | 70.85 Examples/sec\n",
      "INFO:tensorflow:training step 7367 | tagging_loss_video: 5.575|tagging_loss_audio: 8.244|tagging_loss_text: 16.200|tagging_loss_image: 3.541|tagging_loss_fusion: 3.200|total_loss: 36.760 | 62.94 Examples/sec\n",
      "INFO:tensorflow:training step 7368 | tagging_loss_video: 4.669|tagging_loss_audio: 8.644|tagging_loss_text: 16.272|tagging_loss_image: 5.085|tagging_loss_fusion: 3.027|total_loss: 37.697 | 69.05 Examples/sec\n",
      "INFO:tensorflow:training step 7369 | tagging_loss_video: 5.136|tagging_loss_audio: 8.215|tagging_loss_text: 16.012|tagging_loss_image: 3.579|tagging_loss_fusion: 3.086|total_loss: 36.027 | 69.29 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7370 |tagging_loss_video: 5.338|tagging_loss_audio: 8.293|tagging_loss_text: 14.980|tagging_loss_image: 4.882|tagging_loss_fusion: 2.875|total_loss: 36.368 | Examples/sec: 71.84\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7371 | tagging_loss_video: 6.092|tagging_loss_audio: 8.744|tagging_loss_text: 14.222|tagging_loss_image: 5.966|tagging_loss_fusion: 4.916|total_loss: 39.939 | 67.00 Examples/sec\n",
      "INFO:tensorflow:training step 7372 | tagging_loss_video: 5.779|tagging_loss_audio: 8.282|tagging_loss_text: 16.809|tagging_loss_image: 5.184|tagging_loss_fusion: 5.405|total_loss: 41.458 | 70.90 Examples/sec\n",
      "INFO:tensorflow:training step 7373 | tagging_loss_video: 4.502|tagging_loss_audio: 7.790|tagging_loss_text: 15.224|tagging_loss_image: 4.960|tagging_loss_fusion: 2.187|total_loss: 34.664 | 68.76 Examples/sec\n",
      "INFO:tensorflow:training step 7374 | tagging_loss_video: 5.196|tagging_loss_audio: 7.463|tagging_loss_text: 14.033|tagging_loss_image: 4.906|tagging_loss_fusion: 3.802|total_loss: 35.400 | 69.10 Examples/sec\n",
      "INFO:tensorflow:training step 7375 | tagging_loss_video: 6.116|tagging_loss_audio: 8.172|tagging_loss_text: 18.443|tagging_loss_image: 2.561|tagging_loss_fusion: 3.539|total_loss: 38.830 | 65.23 Examples/sec\n",
      "INFO:tensorflow:training step 7376 | tagging_loss_video: 5.776|tagging_loss_audio: 8.834|tagging_loss_text: 16.756|tagging_loss_image: 6.020|tagging_loss_fusion: 5.061|total_loss: 42.447 | 70.64 Examples/sec\n",
      "INFO:tensorflow:training step 7377 | tagging_loss_video: 4.259|tagging_loss_audio: 7.579|tagging_loss_text: 16.321|tagging_loss_image: 5.487|tagging_loss_fusion: 2.824|total_loss: 36.470 | 66.31 Examples/sec\n",
      "INFO:tensorflow:training step 7378 | tagging_loss_video: 5.102|tagging_loss_audio: 7.870|tagging_loss_text: 14.976|tagging_loss_image: 4.788|tagging_loss_fusion: 3.358|total_loss: 36.094 | 61.53 Examples/sec\n",
      "INFO:tensorflow:training step 7379 | tagging_loss_video: 4.252|tagging_loss_audio: 8.812|tagging_loss_text: 15.977|tagging_loss_image: 5.366|tagging_loss_fusion: 2.744|total_loss: 37.151 | 69.09 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7380 |tagging_loss_video: 4.823|tagging_loss_audio: 7.701|tagging_loss_text: 15.159|tagging_loss_image: 4.631|tagging_loss_fusion: 3.055|total_loss: 35.370 | Examples/sec: 67.56\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.87 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.93\n",
      "INFO:tensorflow:training step 7381 | tagging_loss_video: 5.989|tagging_loss_audio: 7.119|tagging_loss_text: 14.644|tagging_loss_image: 5.356|tagging_loss_fusion: 4.986|total_loss: 38.095 | 69.79 Examples/sec\n",
      "INFO:tensorflow:training step 7382 | tagging_loss_video: 5.906|tagging_loss_audio: 8.983|tagging_loss_text: 19.192|tagging_loss_image: 4.794|tagging_loss_fusion: 3.025|total_loss: 41.900 | 70.63 Examples/sec\n",
      "INFO:tensorflow:training step 7383 | tagging_loss_video: 4.336|tagging_loss_audio: 8.327|tagging_loss_text: 15.907|tagging_loss_image: 4.173|tagging_loss_fusion: 2.858|total_loss: 35.600 | 72.01 Examples/sec\n",
      "INFO:tensorflow:training step 7384 | tagging_loss_video: 6.177|tagging_loss_audio: 8.219|tagging_loss_text: 16.862|tagging_loss_image: 4.878|tagging_loss_fusion: 4.929|total_loss: 41.064 | 67.44 Examples/sec\n",
      "INFO:tensorflow:training step 7385 | tagging_loss_video: 6.000|tagging_loss_audio: 8.084|tagging_loss_text: 16.942|tagging_loss_image: 4.374|tagging_loss_fusion: 3.737|total_loss: 39.137 | 69.14 Examples/sec\n",
      "INFO:tensorflow:training step 7386 | tagging_loss_video: 5.180|tagging_loss_audio: 7.905|tagging_loss_text: 16.831|tagging_loss_image: 4.626|tagging_loss_fusion: 4.302|total_loss: 38.844 | 59.49 Examples/sec\n",
      "INFO:tensorflow:training step 7387 | tagging_loss_video: 5.563|tagging_loss_audio: 7.981|tagging_loss_text: 13.419|tagging_loss_image: 4.802|tagging_loss_fusion: 3.288|total_loss: 35.053 | 70.42 Examples/sec\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " Warning: file ../dataset/tagging/tagging_dataset_train_5k/audio_npy/Vggish/tagging/b0f487ea8a4fc44003c7e05e3afee3c9.npy not exits\n",
      "INFO:tensorflow:training step 7388 | tagging_loss_video: 5.935|tagging_loss_audio: 8.835|tagging_loss_text: 18.131|tagging_loss_image: 3.946|tagging_loss_fusion: 3.673|total_loss: 40.520 | 68.28 Examples/sec\n",
      "INFO:tensorflow:training step 7389 | tagging_loss_video: 6.289|tagging_loss_audio: 8.693|tagging_loss_text: 18.674|tagging_loss_image: 5.071|tagging_loss_fusion: 6.571|total_loss: 45.298 | 65.25 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7390 |tagging_loss_video: 4.719|tagging_loss_audio: 8.957|tagging_loss_text: 13.897|tagging_loss_image: 5.196|tagging_loss_fusion: 2.626|total_loss: 35.395 | Examples/sec: 69.95\n",
      "INFO:tensorflow:GAP: 0.96 | precision@0.1: 0.88 | precision@0.5: 0.97 |recall@0.1: 0.99 | recall@0.5: 0.95\n",
      "INFO:tensorflow:training step 7391 | tagging_loss_video: 5.230|tagging_loss_audio: 8.651|tagging_loss_text: 14.065|tagging_loss_image: 5.129|tagging_loss_fusion: 3.708|total_loss: 36.783 | 68.65 Examples/sec\n",
      "INFO:tensorflow:training step 7392 | tagging_loss_video: 4.731|tagging_loss_audio: 8.572|tagging_loss_text: 14.623|tagging_loss_image: 6.157|tagging_loss_fusion: 3.402|total_loss: 37.485 | 64.67 Examples/sec\n",
      "INFO:tensorflow:training step 7393 | tagging_loss_video: 5.303|tagging_loss_audio: 8.160|tagging_loss_text: 15.381|tagging_loss_image: 4.950|tagging_loss_fusion: 5.133|total_loss: 38.928 | 70.13 Examples/sec\n",
      "INFO:tensorflow:training step 7394 | tagging_loss_video: 5.614|tagging_loss_audio: 8.308|tagging_loss_text: 15.086|tagging_loss_image: 4.591|tagging_loss_fusion: 3.423|total_loss: 37.023 | 63.81 Examples/sec\n",
      "INFO:tensorflow:training step 7395 | tagging_loss_video: 5.378|tagging_loss_audio: 7.902|tagging_loss_text: 15.422|tagging_loss_image: 4.596|tagging_loss_fusion: 3.183|total_loss: 36.481 | 67.45 Examples/sec\n",
      "INFO:tensorflow:training step 7396 | tagging_loss_video: 6.136|tagging_loss_audio: 8.173|tagging_loss_text: 18.847|tagging_loss_image: 5.480|tagging_loss_fusion: 5.089|total_loss: 43.724 | 67.91 Examples/sec\n",
      "INFO:tensorflow:training step 7397 | tagging_loss_video: 4.468|tagging_loss_audio: 8.233|tagging_loss_text: 12.356|tagging_loss_image: 5.286|tagging_loss_fusion: 2.820|total_loss: 33.162 | 71.68 Examples/sec\n",
      "INFO:tensorflow:training step 7398 | tagging_loss_video: 4.405|tagging_loss_audio: 8.587|tagging_loss_text: 15.668|tagging_loss_image: 5.055|tagging_loss_fusion: 3.832|total_loss: 37.547 | 69.44 Examples/sec\n",
      "INFO:tensorflow:training step 7399 | tagging_loss_video: 6.485|tagging_loss_audio: 8.245|tagging_loss_text: 15.578|tagging_loss_image: 5.818|tagging_loss_fusion: 7.543|total_loss: 43.668 | 70.55 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7400 |tagging_loss_video: 6.380|tagging_loss_audio: 8.315|tagging_loss_text: 18.127|tagging_loss_image: 6.406|tagging_loss_fusion: 4.105|total_loss: 43.333 | Examples/sec: 61.39\n",
      "INFO:tensorflow:GAP: 0.97 | precision@0.1: 0.83 | precision@0.5: 0.94 |recall@0.1: 0.98 | recall@0.5: 0.90\n",
      "INFO:tensorflow:training step 7401 | tagging_loss_video: 4.490|tagging_loss_audio: 7.586|tagging_loss_text: 16.179|tagging_loss_image: 5.020|tagging_loss_fusion: 2.480|total_loss: 35.755 | 71.33 Examples/sec\n",
      "INFO:tensorflow:training step 7402 | tagging_loss_video: 6.690|tagging_loss_audio: 9.147|tagging_loss_text: 14.447|tagging_loss_image: 6.131|tagging_loss_fusion: 5.777|total_loss: 42.193 | 70.70 Examples/sec\n",
      "INFO:tensorflow:training step 7403 | tagging_loss_video: 5.670|tagging_loss_audio: 7.815|tagging_loss_text: 16.978|tagging_loss_image: 4.258|tagging_loss_fusion: 3.605|total_loss: 38.326 | 68.64 Examples/sec\n",
      "INFO:tensorflow:training step 7404 | tagging_loss_video: 4.257|tagging_loss_audio: 8.049|tagging_loss_text: 14.379|tagging_loss_image: 4.578|tagging_loss_fusion: 2.803|total_loss: 34.066 | 67.21 Examples/sec\n",
      "INFO:tensorflow:training step 7405 | tagging_loss_video: 5.951|tagging_loss_audio: 7.440|tagging_loss_text: 14.767|tagging_loss_image: 6.088|tagging_loss_fusion: 6.234|total_loss: 40.480 | 69.48 Examples/sec\n",
      "INFO:tensorflow:training step 7406 | tagging_loss_video: 3.726|tagging_loss_audio: 6.390|tagging_loss_text: 14.561|tagging_loss_image: 4.459|tagging_loss_fusion: 1.965|total_loss: 31.100 | 63.30 Examples/sec\n",
      "INFO:tensorflow:training step 7407 | tagging_loss_video: 5.681|tagging_loss_audio: 8.110|tagging_loss_text: 15.411|tagging_loss_image: 5.947|tagging_loss_fusion: 6.353|total_loss: 41.502 | 68.82 Examples/sec\n",
      "INFO:tensorflow:training step 7408 | tagging_loss_video: 4.991|tagging_loss_audio: 8.515|tagging_loss_text: 12.496|tagging_loss_image: 5.062|tagging_loss_fusion: 4.396|total_loss: 35.459 | 71.01 Examples/sec\n",
      "INFO:tensorflow:training step 7409 | tagging_loss_video: 3.739|tagging_loss_audio: 7.337|tagging_loss_text: 15.270|tagging_loss_image: 5.786|tagging_loss_fusion: 2.348|total_loss: 34.481 | 70.62 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7410 |tagging_loss_video: 4.463|tagging_loss_audio: 8.050|tagging_loss_text: 13.346|tagging_loss_image: 4.536|tagging_loss_fusion: 2.755|total_loss: 33.150 | Examples/sec: 65.90\n",
      "INFO:tensorflow:GAP: 0.98 | precision@0.1: 0.86 | precision@0.5: 0.96 |recall@0.1: 0.98 | recall@0.5: 0.94\n",
      "INFO:tensorflow:training step 7411 | tagging_loss_video: 4.731|tagging_loss_audio: 7.805|tagging_loss_text: 15.472|tagging_loss_image: 5.278|tagging_loss_fusion: 4.086|total_loss: 37.371 | 63.15 Examples/sec\n",
      "INFO:tensorflow:training step 7412 | tagging_loss_video: 5.130|tagging_loss_audio: 7.677|tagging_loss_text: 17.779|tagging_loss_image: 3.251|tagging_loss_fusion: 2.450|total_loss: 36.288 | 71.04 Examples/sec\n",
      "INFO:tensorflow:training step 7413 | tagging_loss_video: 5.255|tagging_loss_audio: 7.811|tagging_loss_text: 17.319|tagging_loss_image: 5.355|tagging_loss_fusion: 3.328|total_loss: 39.068 | 71.27 Examples/sec\n",
      "INFO:tensorflow:training step 7414 | tagging_loss_video: 5.585|tagging_loss_audio: 7.178|tagging_loss_text: 13.635|tagging_loss_image: 5.292|tagging_loss_fusion: 3.514|total_loss: 35.203 | 60.47 Examples/sec\n",
      "INFO:tensorflow:training step 7415 | tagging_loss_video: 5.746|tagging_loss_audio: 7.848|tagging_loss_text: 17.257|tagging_loss_image: 4.865|tagging_loss_fusion: 3.605|total_loss: 39.322 | 70.54 Examples/sec\n",
      "INFO:tensorflow:training step 7416 | tagging_loss_video: 5.940|tagging_loss_audio: 7.878|tagging_loss_text: 17.419|tagging_loss_image: 5.275|tagging_loss_fusion: 3.916|total_loss: 40.428 | 70.36 Examples/sec\n",
      "INFO:tensorflow:training step 7417 | tagging_loss_video: 4.029|tagging_loss_audio: 7.544|tagging_loss_text: 14.143|tagging_loss_image: 4.326|tagging_loss_fusion: 2.464|total_loss: 32.505 | 62.95 Examples/sec\n",
      "INFO:tensorflow:training step 7418 | tagging_loss_video: 6.375|tagging_loss_audio: 7.211|tagging_loss_text: 14.063|tagging_loss_image: 4.586|tagging_loss_fusion: 3.380|total_loss: 35.614 | 68.31 Examples/sec\n",
      "INFO:tensorflow:training step 7419 | tagging_loss_video: 3.099|tagging_loss_audio: 8.331|tagging_loss_text: 15.824|tagging_loss_image: 4.547|tagging_loss_fusion: 1.765|total_loss: 33.565 | 69.05 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7420 |tagging_loss_video: 5.379|tagging_loss_audio: 6.846|tagging_loss_text: 12.381|tagging_loss_image: 4.944|tagging_loss_fusion: 5.862|total_loss: 35.411 | Examples/sec: 63.87\n",
      "INFO:tensorflow:GAP: 0.94 | precision@0.1: 0.78 | precision@0.5: 0.91 |recall@0.1: 0.98 | recall@0.5: 0.86\n",
      "INFO:tensorflow:training step 7421 | tagging_loss_video: 4.702|tagging_loss_audio: 7.227|tagging_loss_text: 13.684|tagging_loss_image: 4.170|tagging_loss_fusion: 4.278|total_loss: 34.062 | 67.69 Examples/sec\n",
      "INFO:tensorflow:Recording summary at step 7422.\n",
      "INFO:tensorflow:training step 7422 | tagging_loss_video: 5.554|tagging_loss_audio: 8.634|tagging_loss_text: 16.489|tagging_loss_image: 5.347|tagging_loss_fusion: 3.562|total_loss: 39.585 | 52.21 Examples/sec\n",
      "INFO:tensorflow:training step 7423 | tagging_loss_video: 5.404|tagging_loss_audio: 8.024|tagging_loss_text: 14.725|tagging_loss_image: 4.865|tagging_loss_fusion: 3.889|total_loss: 36.907 | 69.09 Examples/sec\n",
      "INFO:tensorflow:training step 7424 | tagging_loss_video: 4.035|tagging_loss_audio: 8.550|tagging_loss_text: 17.371|tagging_loss_image: 4.447|tagging_loss_fusion: 2.517|total_loss: 36.920 | 64.18 Examples/sec\n",
      "INFO:tensorflow:training step 7425 | tagging_loss_video: 6.173|tagging_loss_audio: 8.396|tagging_loss_text: 17.570|tagging_loss_image: 6.035|tagging_loss_fusion: 5.990|total_loss: 44.163 | 62.93 Examples/sec\n",
      "INFO:tensorflow:training step 7426 | tagging_loss_video: 5.150|tagging_loss_audio: 8.003|tagging_loss_text: 14.159|tagging_loss_image: 5.065|tagging_loss_fusion: 3.109|total_loss: 35.486 | 67.83 Examples/sec\n",
      "INFO:tensorflow:training step 7427 | tagging_loss_video: 6.111|tagging_loss_audio: 8.263|tagging_loss_text: 14.987|tagging_loss_image: 2.731|tagging_loss_fusion: 3.334|total_loss: 35.426 | 72.32 Examples/sec\n",
      "INFO:tensorflow:training step 7428 | tagging_loss_video: 4.222|tagging_loss_audio: 6.961|tagging_loss_text: 15.847|tagging_loss_image: 4.213|tagging_loss_fusion: 2.870|total_loss: 34.113 | 60.64 Examples/sec\n",
      "INFO:tensorflow:training step 7429 | tagging_loss_video: 4.813|tagging_loss_audio: 6.913|tagging_loss_text: 11.286|tagging_loss_image: 2.534|tagging_loss_fusion: 3.092|total_loss: 28.638 | 69.37 Examples/sec\n",
      "(32, 82) (32, 82)\n",
      "INFO:tensorflow:training step 7430 |tagging_loss_video: 4.518|tagging_loss_audio: 7.374|tagging_loss_text: 14.670|tagging_loss_image: 6.266|tagging_loss_fusion: 3.821|total_loss: 36.649 | Examples/sec: 66.18\n",
      "INFO:tensorflow:GAP: 0.95 | precision@0.1: 0.84 | precision@0.5: 0.93 |recall@0.1: 0.99 | recall@0.5: 0.92\n",
      "INFO:tensorflow:training step 7431 | tagging_loss_video: 4.188|tagging_loss_audio: 8.001|tagging_loss_text: 12.879|tagging_loss_image: 3.117|tagging_loss_fusion: 1.603|total_loss: 29.788 | 72.65 Examples/sec\n",
      "INFO:tensorflow:training step 7432 | tagging_loss_video: 5.031|tagging_loss_audio: 8.031|tagging_loss_text: 15.864|tagging_loss_image: 5.646|tagging_loss_fusion: 3.319|total_loss: 37.891 | 69.15 Examples/sec\n",
      "INFO:tensorflow:training step 7433 | tagging_loss_video: 5.348|tagging_loss_audio: 8.885|tagging_loss_text: 12.509|tagging_loss_image: 5.087|tagging_loss_fusion: 4.225|total_loss: 36.054 | 65.84 Examples/sec\n",
      "INFO:tensorflow:training step 7434 | tagging_loss_video: 4.681|tagging_loss_audio: 7.752|tagging_loss_text: 16.023|tagging_loss_image: 4.517|tagging_loss_fusion: 2.125|total_loss: 35.098 | 68.06 Examples/sec\n",
      "INFO:tensorflow:training step 7435 | tagging_loss_video: 4.792|tagging_loss_audio: 8.378|tagging_loss_text: 15.254|tagging_loss_image: 5.296|tagging_loss_fusion: 3.555|total_loss: 37.274 | 69.06 Examples/sec\n",
      "INFO:tensorflow:training step 7436 | tagging_loss_video: 5.893|tagging_loss_audio: 8.615|tagging_loss_text: 12.891|tagging_loss_image: 5.594|tagging_loss_fusion: 6.449|total_loss: 39.443 | 67.23 Examples/sec\n",
      "INFO:tensorflow:training step 7437 | tagging_loss_video: 5.131|tagging_loss_audio: 7.650|tagging_loss_text: 15.375|tagging_loss_image: 5.074|tagging_loss_fusion: 3.549|total_loss: 36.779 | 70.04 Examples/sec\n",
      "INFO:tensorflow:training step 7438 | tagging_loss_video: 6.079|tagging_loss_audio: 8.035|tagging_loss_text: 17.002|tagging_loss_image: 5.918|tagging_loss_fusion: 6.619|total_loss: 43.653 | 70.05 Examples/sec\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod a+x ./VideoStructuring/run.sh && ./VideoStructuring/run.sh train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline的测试可以直接使用 ./run.sh test \\[CHECKPOINT_DIR\\] 进行，成功执行后会在VideoStructuring/MultiModal-Tagging/results/目录下生成tagging_5k_A.json结果文件。  \n",
    "提交这个文件就可以参与排名。  \n",
    "注意: ./run.sh test的特征提取时间较长（13小时左右），为了简化操作，baseline的测试集特征提取已预先完成，test时长约为半小时左右。用户可以自由地优化算法与相关流程。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10473128\n",
      "      4 checkpoint\n",
      "      8 config.yaml\n",
      "  23640 events.out.tfevents.1619959708.notebook-792qecvb4j-8566ddd7d7-9cl5c\n",
      "      4 export\n",
      "  23772 graph.pbtxt\n",
      "2073984 model.ckpt-3000.data-00000-of-00001\n",
      "     56 model.ckpt-3000.index\n",
      "  11100 model.ckpt-3000.meta\n",
      "2073984 model.ckpt-4000.data-00000-of-00001\n",
      "     56 model.ckpt-4000.index\n",
      "  11100 model.ckpt-4000.meta\n",
      "2073984 model.ckpt-5000.data-00000-of-00001\n",
      "     56 model.ckpt-5000.index\n",
      "  11100 model.ckpt-5000.meta\n",
      "2073984 model.ckpt-6000.data-00000-of-00001\n",
      "     56 model.ckpt-6000.index\n",
      "  11100 model.ckpt-6000.meta\n",
      "2073984 model.ckpt-7000.data-00000-of-00001\n",
      "     56 model.ckpt-7000.index\n",
      "  11100 model.ckpt-7000.meta\n",
      "total 28\n",
      "drwxr-xr-x 3 tione users 4096 May  2 20:57 step_1000_0.6873\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:05 step_2000_0.7064\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:13 step_3000_0.7228\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:22 step_4000_0.7425\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:30 step_5000_0.7461\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:38 step_6000_0.7465\n",
      "drwxr-xr-x 3 tione users 4096 May  2 21:47 step_7000_0.7468\n"
     ]
    }
   ],
   "source": [
    "!ls -s VideoStructuring/MultiModal-Tagging/checkpoints/tagging5k_temp\n",
    "!ls -l VideoStructuring/MultiModal-Tagging/checkpoints/tagging5k_temp/export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行前要创建results目录，要不会报错\n",
    "!mkdir VideoStructuring/MultiModal-Tagging/results\n",
    "# VideoStructuring/MultiModal-Tagging/videos/test_5k_A 保证这个路劲要存在，但是好像没用到videos数据， 直接使用特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要把数据保存到 VideoStructuring/MultiModal-Tagging/results/tagging_5k_A.json 里面，手动创建\n",
    "!touch VideoStructuring/MultiModal-Tagging/results/tagging_5k_A.json\n",
    "# !touch VideoStructuring/MultiModal-Tagging/results/outjson.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   注意！！！修改run.sh脚本, 不需要运行这里的代码\n",
    "#   ${OUTPUT_FILE}改为${OUTPUT_FILE_PATH)\n",
    "#   python scripts/inference_for_tagging.py --model_pb \"${CHECKPOINT_DIR}\" \\\n",
    "#                                           --tag_id_file \"${TAG_ID_FILE}\" \\\n",
    "#                                           --test_dir \"${TEST_VIDEOS_DIR}\" \\\n",
    "#                                           --output_json \"${OUTPUT_FILE_PATH}\" \\\n",
    "#                                           --load_feat 1 \\\n",
    "#                                           --feat_dir \"${TEST_VIDEOS_FEATS_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_CONFIG_ROOT_PREFIX= root_prefix: /opt/conda\n",
      "CONDA_ROOT= /opt/conda\n",
      "CONDA_NEW_ENV= taac2021-tagging\n",
      "JUPYTER_ROOT= /home/tione/notebook\n",
      "CODE_ROOT= /home/tione/notebook/VideoStructuring\n",
      "DATASET_ROOT= /home/tione/notebook/VideoStructuring/dataset\n",
      "OS_ID= ubuntu\n",
      "# conda environments:\n",
      "#\n",
      "                      *  /home/tione/notebook/envs/taac2021-tagging\n",
      "base                     /opt/conda\n",
      "JupyterSystemEnv         /opt/conda/envs/JupyterSystemEnv\n",
      "mxnet_py2                /opt/conda/envs/mxnet_py2\n",
      "mxnet_py3                /opt/conda/envs/mxnet_py3\n",
      "python2                  /opt/conda/envs/python2\n",
      "python3                  /opt/conda/envs/python3\n",
      "pytorch_py2              /opt/conda/envs/pytorch_py2\n",
      "pytorch_py3              /opt/conda/envs/pytorch_py3\n",
      "tensorflow2_py3          /opt/conda/envs/tensorflow2_py3\n",
      "tensorflow_py2           /opt/conda/envs/tensorflow_py2\n",
      "tensorflow_py3           /opt/conda/envs/tensorflow_py3\n",
      "\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2021-05-02 23:45:39.845189: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-05-02 23:45:39.861954: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-05-02 23:45:39.862495: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1cebdfcc60 executing computations on platform Host. Devices:\n",
      "2021-05-02 23:45:39.862529: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-05-02 23:45:39.863640: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-02 23:45:39.878343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.879354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 23:45:39.879577: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:39.880939: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 23:45:39.882240: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 23:45:39.882517: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 23:45:39.884131: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 23:45:39.885380: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 23:45:39.889340: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 23:45:39.889472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.890482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.891417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 23:45:39.891459: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:39.973128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 23:45:39.973165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 23:45:39.973175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 23:45:39.973338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.974327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.975308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:39.976248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 30556 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "2021-05-02 23:45:39.978101: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1cef8263b0 executing computations on platform CUDA. Devices:\n",
      "2021-05-02 23:45:39.978128: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4299599612933450326\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4543567774959038269\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32040825652\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7951764932146819647\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17106731617158984610\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "1.14.0\n",
      "1.20.2\n",
      "1.2.0\n",
      "[Info] TYPE is test\n",
      "/home/tione/notebook/VideoStructuring/MultiModal-Tagging\n",
      "[Warning] OUTPUT_FILE_PATH is not set, use default ./results/tagging_5k_A.json\n",
      "[Warning] TEST_VIDEOS_DIR is not set, use default /home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A\n",
      "[Warning] TEST_VIDEOS_FEATS_DIR is not set, use default /home/tione/notebook/VideoStructuring/dataset/tagging/tagging_dataset_test_5k\n",
      "[Info] test with parameters:\n",
      "  CHECKPOINT_DIR= checkpoints/tagging5k_temp/export/step_7000_0.7468\n",
      "  TAG_ID_FILE= /home/tione/notebook/VideoStructuring/dataset/label_id.txt\n",
      "  OUTPUT_FILE_PATH= ./results/tagging_5k_A.json\n",
      "  TEST_VIDEOS_DIR= /home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A\n",
      "  TEST_VIDEOS_FEATS_DIR= /home/tione/notebook/VideoStructuring/dataset/tagging/tagging_dataset_test_5k\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "pretrained/inception\n",
      "pretrained/finetuned_resnet101\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/utils/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "2021-05-02 23:45:44.374741: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-05-02 23:45:44.380432: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-05-02 23:45:44.380857: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd031acb820 executing computations on platform Host. Devices:\n",
      "2021-05-02 23:45:44.380901: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-05-02 23:45:44.382025: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-02 23:45:44.396952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:44.397938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 23:45:44.398018: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:44.399390: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 23:45:44.400716: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 23:45:44.401034: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 23:45:44.402713: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 23:45:44.404013: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 23:45:44.407900: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 23:45:44.408062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:44.409089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:44.410018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 23:45:44.410061: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:50.250200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 23:45:50.250242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 23:45:50.250254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 23:45:50.250483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:50.251521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:50.252485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:50.253416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29752 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "2021-05-02 23:45:50.255170: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd032809580 executing computations on platform CUDA. Devices:\n",
      "2021-05-02 23:45:50.255198: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "WARNING:tensorflow:From scripts/inference_for_tagging.py:32: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/imgfeat_extractor/youtube8M_extractor.py:172: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/imgfeat_extractor/youtube8M_extractor.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2021-05-02 23:45:54.991630: W tensorflow/core/framework/op_def_util.cc:357] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/imgfeat_extractor/youtube8M_extractor.py:179: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-05-02 23:45:55.152371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.153329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 23:45:55.153512: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:55.153541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 23:45:55.153555: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 23:45:55.153568: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 23:45:55.153581: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 23:45:55.153594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 23:45:55.153607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 23:45:55.153684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.154644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.155530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 23:45:55.155566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 23:45:55.155575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 23:45:55.155583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 23:45:55.155664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.156587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.157482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29752 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "2021-05-02 23:45:55.157646: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "2021-05-02 23:45:55.179958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.180905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:08.0\n",
      "2021-05-02 23:45:55.180989: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-05-02 23:45:55.181017: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 23:45:55.181031: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-05-02 23:45:55.181045: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-05-02 23:45:55.181058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-05-02 23:45:55.181071: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-05-02 23:45:55.181084: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-02 23:45:55.181157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.182077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.182967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-05-02 23:45:55.182999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-02 23:45:55.183008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-05-02 23:45:55.183015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-05-02 23:45:55.183100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.184020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-02 23:45:55.184912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29752 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0)\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "2021-05-02 23:45:55.185099: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:08.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/audio_extractor/vggish_slim.py:76: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/audio_extractor/vggish_slim.py:78: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tione/notebook/envs/taac2021-tagging/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/tione/notebook/VideoStructuring/MultiModal-Tagging/src/feats_extract/audio_extractor/vggish_slim.py:127: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "vggish/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193009: I tensorflow/core/common_runtime/placer.cc:54] vggish/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193036: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193050: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193061: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193073: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193084: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193094: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193107: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193118: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193129: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193142: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193154: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193167: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/pool1/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193179: I tensorflow/core/common_runtime/placer.cc:54] vggish/pool1/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193191: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193202: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193213: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193223: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193245: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193257: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193269: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193280: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193290: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193302: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193314: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193326: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/pool2/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193338: I tensorflow/core/common_runtime/placer.cc:54] vggish/pool2/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193350: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193360: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193371: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193381: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193391: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193401: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193415: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193426: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193438: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193450: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193462: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193474: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193485: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193496: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193506: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193516: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193525: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193535: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193546: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193557: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193568: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193579: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193591: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193603: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/pool3/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193623: I tensorflow/core/common_runtime/placer.cc:54] vggish/pool3/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193632: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193643: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193653: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193663: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193673: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193683: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193694: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193704: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193714: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193726: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.193738: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194691: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194704: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194720: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194731: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194741: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194751: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194762: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194773: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194784: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194795: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194807: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194819: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194831: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/pool4/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194850: I tensorflow/core/common_runtime/placer.cc:54] vggish/pool4/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194870: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194883: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/Reshape/shape: (Pack): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194896: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/Reshape/shape: (Pack)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194908: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194924: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194936: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194947: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194957: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194967: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194978: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194989: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases/Initializer/zeros: (Fill)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.194999: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195009: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195019: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195031: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195045: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195057: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195069: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195079: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195089: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195099: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195110: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195120: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195131: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases/Initializer/zeros: (Fill)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195142: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195151: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195161: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195172: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195184: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195196: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195207: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195223: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195233: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195243: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195253: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195263: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195274: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195284: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195294: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195306: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195317: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195329: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/embedding: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195341: I tensorflow/core/common_runtime/placer.cc:54] vggish/embedding: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195352: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/filename: (PlaceholderWithDefault)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195364: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Const: (PlaceholderWithDefault)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/save: (SaveSlices): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.195375: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/save: (SaveSlices)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195386: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.195397: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/RestoreV2: (RestoreV2)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195415: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195425: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_1: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195435: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_2: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195444: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_3: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195454: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_4: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195464: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_5: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195474: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_6: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195483: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_7: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195493: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_8: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195503: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_9: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.195512: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_10: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196545: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_11: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196556: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_12: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196567: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_13: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196578: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_14: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196588: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_15: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196598: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_16: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196608: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/Assign_17: (Assign)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196621: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/restore_all: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/input_features: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196633: I tensorflow/core/common_runtime/placer.cc:54] vggish/input_features: (Placeholder)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196646: I tensorflow/core/common_runtime/placer.cc:54] vggish/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196656: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196667: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196677: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196687: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv1/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196706: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv1/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196717: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196728: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196738: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196748: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv2/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196760: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv2/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196771: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196781: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196792: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196802: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_1/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196814: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_1/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196824: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196834: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196844: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196854: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv3/conv3_2/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196866: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv3/conv3_2/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196876: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196892: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196904: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196914: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_1/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196934: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_1/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196945: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196955: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196964: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196974: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/conv4/conv4_2/dilation_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196986: I tensorflow/core/common_runtime/placer.cc:54] vggish/conv4/conv4_2/dilation_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.196998: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197009: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197021: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/Flatten/flatten/Reshape/shape/1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197033: I tensorflow/core/common_runtime/placer.cc:54] vggish/Flatten/flatten/Reshape/shape/1: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197043: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197053: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197789: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197807: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases/Initializer/zeros/shape_as_tensor: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_1/biases/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197818: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_1/biases/Initializer/zeros/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197829: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197839: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197850: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197860: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases/Initializer/zeros/shape_as_tensor: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc1/fc1_2/biases/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197871: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc1/fc1_2/biases/Initializer/zeros/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197881: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197892: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197902: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish/fc2/biases/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2021-05-02 23:45:56.197913: I tensorflow/core/common_runtime/placer.cc:54] vggish/fc2/biases/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
      "vggish_load_pretrained/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.197935: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/filename/input: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/save/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.197955: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/save/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/save/shapes_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.197967: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/save/shapes_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.197979: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/RestoreV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "vggish_load_pretrained/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2021-05-02 23:45:56.197991: I tensorflow/core/common_runtime/placer.cc:54] vggish_load_pretrained/RestoreV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0003c0066fa7436317d8c507c596680e.mp4\n",
      "{\"video_ocr\": \"这黄瓜怎么卖的|老板茄子怎么卖|你这菜还卖不卖了|我这刷着视频领红包呢|不好意思|刷视频还能领红包|你这什么软件啊|快手极速版|这一分钱不花|天天刷视频|就能把红包给领了|而且现在邀请好友|还能够领红包|这白捡的便宜|咱能够不要吗|快看还真有红包|我也赚到了|老板这个从哪下载啊|点击视频下方链接|就能直接下载|详情见活动规则，奖励金额视任务完成情况而定|小o肉|动肉|坊\", \"video_asr\": \"这黄瓜怎么卖的。|老板茄子怎么办。|哎，你这差还卖不卖了啊哦，我这刷的视频领红包呢，不好意思，刷视频还能领红包，你这什么软件啊？快手极速版啊，你一分钱不花，天天刷视频就能把红包给领了，而且现在邀请好友啊，还能领红包，这才捡了便宜，咱们不要了啊！你赚到了老板三百万，点击视频下方链接就能直接下载。\"}\n",
      "2021-05-02 23:45:58.360246: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-05-02 23:45:58.614039: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "multi-modal tagging model forward cost time: 3.7119390964508057 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '填充', '多人情景剧', '平静', '亲子', '喜悦', '单人口播', '全景', '惊奇', '手机电脑录屏', '特写', '动态', '场景-其他', '室外', '配音', '路人', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.75', '0.74', '0.49', '0.36', '0.31', '0.17', '0.08', '0.07', '0.05', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/000411ade0c6ade2cdf12d2947ebd9ad.mp4\n",
      "{\"video_ocr\": \"中国四欢造富浪潮|抓住一玖你就会富起来|第一次经商造富|下海经商 成就7第一批富人|第二次房地产造富 2000年房地产改革|房价上涨造就了第二批富人|第三次互联网造富|2000年以后 互联网成了新风回|如果前三次你都错过了|那么你一定要抓住|第四玖理财造富|理财不是买基金不是买股票|而是用科学的方法 让我们的财富增值|不用耗体力和健康|但是却能通过 钱生钱的方式来赚钱|就拿七月初的股市上涨来说|懂得人一天赚到的钱|此普通人 一个月的工资还要多|所以说不是你赚不到钱|而是你不懂钱生钱的方法|推荐你报名小白理财特训营|一块钱五天的时间|学会如何获得非工资收入|还有班主任老师全程辅导|零基础也能轻松学会|点击视频下链接|马士跟我学|购课包邮 赠送实体理财读物|理财小白训练营|理财有风险，投资需谨慎.|比买教材划算多了! 0基础也能学 富人思维|爆款小白理财 训练营|无限回看不过期 在家学 被动收入超过工资收入|适合自己资产配置|老师手把手带着实操 省钱|23岁开始，每月存1000元，只需 8%的收益，50岁将拥有1000万|学理财 越早越好|163 P164 特惠1元购|美股上市公司|5天教你钱生钱|意节定位|SUNLANDS NYSE:STG|限售2020套|1元\", \"video_asr\": \"中国四次造富浪潮，抓住一次你就会富起来。第一次经商造富，下海经商成就了第一批富人。第二次房地产造富两千年，房地产改革，房价上涨造就了第二批富人。第三次互联网造富，两千以后，互联网成为新风口，造就了第三批富人。|果前三次你都错过了，那么你一定要抓住第四次，理财造富。理财不是买基金，不是买股票，而是用科学的方法让我们的财富增值，不用消耗体力和健康，但是却能通过钱生钱的方式来赚钱。就拿七月初的股市上涨来说，懂得人一千赚到的钱比普通人一个月的工资还要多。|所以说，不是你赚不到钱，而是你不懂钱生钱的方法，推荐你报名小白理财特训营，一块钱五天的时间学会如何获得非工资收入，还有班主任老师全程辅导，零基础也能轻松学会，点击视频下方链接，马上跟我学！\"}\n",
      "multi-modal tagging model forward cost time: 0.016612529754638672 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '推广页', '(马路边的)人行道', '动态', '填充', '平静', '公园', '静态', '室内', '室外', '过道', '喜悦', '拉远', '远景', '拉近', '大厅', '商场', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/000440c7537801fd2900042dc029ba11.mp4\n",
      "{\"video_ocr\": \"初中数学高分大题|也可以像1+1=2那样简单|你信吗|数与代数|宣角函数|圆|等等等等|听起来像是非常难以理解的样子|但是只要跟着我们高途课堂的老师|学会思维模型的方法|充分利用解题模型以及计算模型|就能把初中数学7大板块|一一进行解析|那么这一切就会变得非常简单|我是高途课堂童灏老师|用“秒杀技巧”的方式|做数学|还等什么？|点击【查看详情】|立即报名|可以这么简单|高途课堂 洺怦班|临忏班|圪师班|物师|格州班|童灏|名师特训班|高中数学资深主讲 中国科技大学学士 专注高中数学教学年|¥9|新用户专享 浙江卫视指定在线教育品牌|全国百佳教师带队教学 平均教龄11年|0.25- 140|浙江卫视|120-|又快又准|25|仅需\", \"video_asr\": \"初中数学高分大题也可以像一加一等于二那样简单，你信吗？竖你代数三角函数，二次函数圆。|等等等等，听起来像非常难以理解的样子，但是只要跟着我们高途课堂的老师学会思维模型的方法，充分利用解题模型以及计算模型。|你把初中数学七大板块一一进行解析，那么这一切就会变得非常简单。|我是高途课堂童浩老师，用秒杀技巧的方式做数学，又快又准，还等什么？点击查看详情，立即报名！\"}\n",
      "multi-modal tagging model forward cost time: 0.016800642013549805 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '教师(教授)', '单人口播', '静态', '平静', '室内', '填充', '影棚幕布', '场景-其他', '学校', '特写', '手写解题', '宫格', '朋友&同事(平级)', '多人情景剧', '手机电脑录屏', '混剪', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.86', '0.70', '0.41', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00347a887039e4fb764eff66b18e08d0.mp4\n",
      "{\"video_ocr\": \"包邮赠送给画大备|在家学习|立即报名|独材礼富为课程配套物品|适合3-8岁 1对1辅导|小解美术|美术宝出品|6 O|60|BEARAR 包邮|儿童绘画启蒙课|49元10节|小熊美术\", \"video_asr\": \"\"}\n",
      "multi-modal tagging model forward cost time: 0.016515731811523438 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '绘画展示', '配音', '静态', '动画', '才艺展示', '极端特写', '家', '教辅材料', '多人情景剧', '中景', '情景演绎', '图文快闪', '手机电脑录屏', '喜悦', '过渡页', '混剪', '商品展示'], 'scores': ['1.00', '1.00', '1.00', '0.36', '0.15', '0.07', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0038b33d89fdd38ad27d0fd8f12f1af2.mp4\n",
      "{\"video_ocr\": \"通过|的形式|40个动画人物|让16年级的孩子|对英语学习产生兴趣|30张思维导图|轻松学习|10种高效背单词的方法|快速记忆小学核心|课程由|” “超级拼读发明人|Sam老师亲自授课|课后还有|985毕业名师|1对1答疑解惑|4天时间|给孩子一个|蜕变的机会|9元报名|学完全额退款|家长们|赶紧点击视频下方|查看详情报名吧|领取超多学习资料 立即抢课|小学核心800词汇吗|出一本书的单词吗|跟谁学|小学英语单词速记营|“动漫+课程”|800词汇|102个动画故事|扫码入群|截图保存图片|你相信4天|孩子就能挑战|你相信5分钟|按以下步骤可领超多学习资料 提示报名成功|在线学习更高效\", \"video_asr\": \"你相信四天孩子就能挑战小学核心八百词汇吗？你相信五分钟孩子就能默写出一本书的单词吗？跟谁学？小学英语单词速记营，通过动漫加课程的形式，利用一百零二个动画故事，四十个动画人物形象，让一到六年级的孩子对英语学习产生兴趣。|再通过三十张思维导图，轻松学习十种高效背单词的方法，快速记忆小学核心八百词汇。|课程由超级拼读发明人散老师亲自授课，课后还有九八五毕业名师一对一答疑解惑，四天时间，给孩子一个蜕变的机会，九元报名，学完全额退款。|家长们，赶紧点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016152381896972656 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '单人口播', '推广页', '静态', '平静', '场景-其他', '教师(教授)', '配音', '影棚幕布', '教辅材料', '室内', '动态', '过渡页', '极端特写', '特写', '转场', '手机电脑录屏', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.87', '0.54', '0.09', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/003e3e284db6877b1e20e09397d48726.mp4\n",
      "{\"video_ocr\": \"未完待续... 点击下方 继续阅读全文|PgsTTiON|POslt!|PO|BEACTION|TION|0N\", \"video_asr\": \"想的是我。|在当时的许多事。|深圳真的。|想想我这一关。|我的嘴巴。|好像好了。|我离开她的生活才。\"}\n",
      "multi-modal tagging model forward cost time: 0.016151905059814453 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '填充', '室内', '平静', '特写', '场景-其他', '单人口播', '推广页', '动态', '极端特写', '才艺展示', '情景演绎', '多人情景剧', '手机电脑录屏', '喜悦', '混剪', '宫格', '古装/武侠'], 'scores': ['1.00', '1.00', '0.99', '0.98', '0.93', '0.91', '0.78', '0.59', '0.18', '0.17', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0060c5ddaab84ef662fd567cc5b34dec.mp4\n",
      "{\"video_ocr\": \"3-8岁是培养孩子|绘画天赋的黄金时期|拼的就是孩子的|创新思维和动手能力|小熊美术AI课|是由专业美院毕业名师授课|10节课仅需49元|就能给孩子更多可能性|现在报名还包邮赠送|绘画礼盒|包含每节课的|配套绘画工具|真的是太值了|熊美术|小能美术|小熊美术|美术宝 49元10节|包邮赠送绘画大礼包 在家学习|适合3-8岁 课后1对1辅导|随材礼盒为课程配套物品 不同级别的礼盒略有差异|儿童绘画启蒙课|立即报名|出品|包邮\", \"video_asr\": \"三到八岁是培养孩子绘画天赋的黄金时期，PE就是孩子的创新思维和动手能力。小熊美术AI课是由专业美院毕业名师授课，十节课仅需四十九元，就能给孩子更多可能性，现在报名还包邮赠送绘画礼盒，包含每节课的配套绘画工具，真的是太值了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016083717346191406 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '配音', '填充', '平静', '教辅材料', '场景-其他', '静态', '极端特写', '绘画展示', '室外', '手机电脑录屏', '动画', '室内', '特写', '动态', '中景', '课件展示', '商品展示', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.93', '0.74', '0.11', '0.06', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00898e8314d4c99adbd0af4120c930c5.mp4\n",
      "{\"video_ocr\": \"四年级|穴厅卧 专寸卫 较|培养思维|这些你都能给到孩子吗?|6-12岁是培养学科思维|黄金时期|千万不能再耽误了|我是高途课堂王勒老师|北京师范大学毕业|专注语文教学12年|4大习作类型|4士习作者科|5条素材积累原则|6大阅读考点|带孩子轻松搞定语文!|见证孩子的变化|还在等什么|点击视频下方【查看详情〗】|报名吧!|只有进行细更的观 才能写得准确。 较大的土块。它用强有力的后足踏|宅隐毫慎址良掘搜倾骤置抛|别的昆虫大多在临时的隐蔽所藏身。它们的隐蔽所得 名不光由于它的唱歌，还由于它的住宅|爬山虎的脚触着墙的时候，六七根细丝的头上就变成|须逊雪三分白、 上就变成|三法自日花却|全国百佳教师带队教学 平均教龄11年|义务教育教科书|充平台|爱护眼睛，保护视力|的门口，还可以看见蟋蟀从里面不断地抛出泥土来。|看见蟋蟀从里面不断地抛出泥土来。|[宋]卢钱 民了|[阁]同“橱”，放下。这|处处留心皆学问。|我发现“鸟寓” 也可以叫“鸟果”。|可以先表示认同，再继续补充。|—选自英国麦加文的(昆虫)，王琛柱译，有改动|本文作者是法国的法布尔，译者黄亚治，选作课文时有改动。|梅须逊雪三分白，雪却输梅一段香。|进行连续观察，学写观察日记。|不肢碎的功能|取保健保|宅临慎选择址 良|出纳原园|色8原讲它怎样在墙上爬 刚最乐6讲沱火纹自养墙”十能生在|酉渚1极春的健者、人扮彩 每征|句段运用 至一连，为下面的动物找到家。|让我们开展一次活动。先了解本班同学的视力情况；然后分 小组对班里同学的视力情况和影响视力的原因进行分析，并交流|课文把蟋蟀比作人，把蟋蟀的巢穴比作人的住宅，说说这样写 的好处。读下面的片段，想想在表达上与课文有什么不同。|里，用菜叶它请。玩在为 丁所孔蟋年，我|城 均柄蜗曲萎|J朗读课文，和|有感情地朗读课文。背诵课文。默写《题西林壁》。|地后腿止有两排锯，用它们将泥|土推到后面，倾地铺开。|来不费功夫，弃去毫不可惜 蟋蟀和它们不同，不肯随遇|小圆片，巴住墙。细丝原先是宜的，现在弯曲了，把爬山|，把爬山 却输梅一段|抬样芯-袋话. 雪却输梅一段香。|如何保护视力：最后，全班讨论怎样才能保护好视力，提出保护|蟋蚌体形微扁，头部圆形，触角长、呈线状。有翅时，翅 平叠于躯体上。多数体色呈褐色或黑色，深浅不一。雄虫利用|9在朝着阳光的堤岸上，青草丛中隐藏着一条倾斜的隧 道，即使有骤雨，这里也立刻就会干的。隧道顺着地势弯|道残阳铺水牛|想象“一道残阳铺水中，半江瑟瑟半江红”的景象，用自己的|浙红卫视|诺文园地|大做一点儿。这个洞可以随大气的变冷和它身体的增长而|高途课堂|加深加阔。即使在冬天，只要天气温和，太阳晒到它住宅|个可以随大气的变冷和它身体的增长而 吏在冬天，只要天气温和，太阳晒到它住宅|蟋蟀的住宅|U先讲不味虎啊领样她|雪争春未肯醉|再花触粤都训h细ぷ了|那微斜的门口，经过仔细耙扫，收 得很平坦。这就是蟠蚌的乎台，当四周很 “台上弹琴。|用自己的话个绍蟋蟀住宇的修过ペ，想相为什么蟋蟀的住宅|它们养在笼子 柔弱，所以人们对它的劳动成果感到惊奇|要瞧不起那些灰色的脚，那些脚巴在墙上相当牢固，要是 根茎。|当牢固，要是|扰其他小组。|位于前翅基部的脊产生求偶鸣声。多数雌性的产卵器很显著，|弯曲曲，最多九寸深，一指宽，这便是蟋蟀的住宅。出口|体会文章准确生动的表达，感受作者连续细|它用前足扒土，还用钳子搬掉|眼睛是心灵的窗口，它很脆弱，容易受到伤害。平时，你注意 保护自己的眼睛吗？|zhai yin hao shen zhi liang jue qing zhou zhi|居住在草地上的蟋蟀，差不多和蝉一样有名。它的出|人笔货评章|④ [评章〕评议。这里指评 不.只月特骚人等费评章|就萎了，后来连痕迹也没有了。触 青墙的，细丝和小圆片逐渐变成灰色。|来连痕迹也没有了。触 和小圆片逐渐变成灰色。|己一点儿一点儿挖掘的，从大厅一直到卧室。|画上蛟龙的爪子。崛虎脚的特临|暮瑟拳 yIa|mW|勺理解|默读课文，说说课文围绕蟋的住宅讲了哪两方面的内容，作 者是怎样观察的。|观察的。|(蟋蟀怎么会有建筑住宅的才能呢?它有特别好的工具 吗?没有。蟋蟀并不是挖掘技术的专家，它的工具是那样|爬山虎的脚要是没触着墙，不几天就萎了，后来连痕 迹也没有了。触着墙的，细丝和小圆片逐渐变成灰色。不|，后来连痕 变成灰色。不|ong W yuan|小间的顺序 爬山虎的脚要是没触着墙，不几|(们门的顺序|而安。它常常慎重地选择住址，一定要排水优良，并且有 温和的阳光。它不利用现成的洞穴。它的舒服的住宅是自|虎的嫩茎拉一把，使它紧贴在墙上。爬山虎就是这样一脚 一脚地往上爬。如果你仔细看那些细小的脚，你会想起图|这样一脚|暮瑟缘降骚逊输|很多家长觉得|小学语文很简单|自己就能教|可是|可以你真的会教吗|语文要培养的|不是摘抄背诵|视力的建议。|呈筒状或针状。|大或针状。|打下列诗句能理|话说一说。 说说你对下列诗句的理解。|cG|.G8|1a|上册|吟题健|yi t c 暮吟题侧峰庐缘|ィ士作米刑小|6大阅老占|华少|小组讨论时，注意说话的音量，避免干|新用户专享 立即体验 浙江卫视指定在线教育品牌|作者不但观察细 致，还连续观察了一|liang|你会想起图|更加全面和理|虎 操占嫩顺均叠 隙 茎柄萎瞧固|均叠|应山真面目，习|不识庐山真面目，只缘身在此山中。|致的观察。|pao|议梅与雪的高下。 5(进]不及，比不上。|正确尘资|创造力|培差思 创1迫刀|附看，还要用) 所，用心机|在儿|你的手指不|不重复别人说过的话。如果想法接近，|降阁费须逊输|5节名师直播课 专属辅导老师伴学|教育部审定|口谙父怀|zhdi|注释|色谁忽不服物|亿年教龄 ¥9|2019|0[)服输。|王勒|名师特训班|又搜索起它们|的米穴米|jun bing|写作高分大招|三年无限回放|强文|第三单元|仅E|仅需9元|里读ge。|IprG|sho|WO wei|45|37\", \"video_asr\": \"很多家长觉得小学语文很简单，自己就能教，可是你真的会教吗？语文要培养的。|是摘抄背诵，是思维，是创造，这些你都能给到孩子吗？六到十二岁是培养学科思维的黄金时期，千万不能再耽误了。|我是高途课堂王勒老师，北京师范大学毕业，专注语文教学十二年，总结出四大习作类型，五条素材积累原则。|六大阅读考点，带孩子轻松搞定语文！|只要九块钱见证孩子的变化，还在等什么，点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016372203826904297 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '填充', '教师(教授)', '静态', '平静', '配音', '影棚幕布', '场景-其他', '室内', '学校', '特写', '家', '手机电脑录屏', '教辅材料', '全景', '情景演绎', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.35', '0.33', '0.28', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0089b61693a0984e43630f3f16764a5c.mp4\n",
      "{\"video_ocr\": \"这是猿辅导小学|数学暑期系统班|他正按老师的要求解题呢|自从报了猿辅导啊|你的数学让人省心多了|我们班的同学也在上猿辅导\", \"video_asr\": \"你要吃饭？|这是猿辅导小学数学暑期系统班，他将让老师的要求解题呢，自从报了猿辅导的数学，然而事情多了，我们班的同学也在上猿辅导。\"}\n",
      "multi-modal tagging model forward cost time: 0.016713380813598633 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '特写', '喜悦', '悲伤', '朋友&同事(平级)', '惊奇', '手机电脑录屏', '平静', '极端特写', '夫妻&恋人&相亲', '动态', '路人', '拉近', '亲子', '单人口播', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.84', '0.79', '0.58', '0.45', '0.31', '0.21', '0.20', '0.12', '0.03', '0.02', '0.02', '0.02', '0.02', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/008b32ef7d9dfbd20debaee1f2c8cdc7.mp4\n",
      "{\"video_ocr\": \"瓜|桃子 桃子是粉红色的。|椰子肉是白色。|子皮是黄色的。|立即抢报|识字互动游戏书|京剧脸谱 北大中文系硕博精心打磨课程|瘰辅导在线额出|猿辅导在|猿辅导a9业|在行|斑马Al课|这是红苹果。|椰ヱ|苹果|大17|梨了|49元/10节课|原创动画教学 包邮赠送教辅大礼包|斑马A课语文体验课|蕉莓|桃瓜椰|非赠品|桃 蕉|OU4|急莓果|语文\", \"video_asr\": \"四。|为此辛苦。|哇塞，好帅。|习惯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016374826431274414 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '极端特写', '静态', '教辅材料', '中景', '配音', '手写解题', '幻灯片轮播', '动态', '拉近', '家', '课件展示', '喜悦', '宫格', '动画', '特写', '填充', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.02', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/008ddedf2a41f2dd59c5054ac6cbddd9.mp4\n",
      "{\"video_ocr\": \"这个月把|这些题也做了|下次月考不能 再考七八十分了|妈我跟你 说了多少遍了|我自己刷 再多的题也没用啊|你倒是去给我报|高途课堂的 全科名师班啊|9块钱的课 能有什么用嘛|妈|你以为我们班 那些考高分的|都是自己 刷题刷出来的吗|人家都在高途课堂|学了 解题思路和方法|一节课的干货知识|能抵上我自己 刷题刷20个钟头啊|上面都是 北大清华毕业的老师|和全国优秀教师|高中语数英物|四大科目必考的 重点难点易错点|课后还有辅导老师 一对一的答疑|根本不怕学不会|我哪能想到9块钱的 课那么有用嘛|妈你就给我报吧|我不想再做差生了|这次好不容易 高途课堂全科名师班|又加了报名名额|我也想冲140分啊|这次一定给你报上|赶紧点击 视频下方链接|跟我一起报名吧|新用户专享|视频为演绎情节|新用户专享 立即体验 浙江卫视指定在线教育品牌|立即体验|名师特训班|全国百佳教师带队教学 平均教龄11年|浙江卫视|仅需|Iph |Ishm|ph|华少|￥9|It\", \"video_asr\": \"这个月把这些题也做了，下次月考不能再考七八十分了吗？我跟你说了多少遍了，我自己刷再多的题也没用啊，你倒是给我报高途课堂的全科名师班啊，哎呀，九块钱的课能有什么用吗？把这些题目做了啊，你以为我们班那些考高分的都是自己刷题刷出来的吗？人家都在高途客。|全科名师班上学了的解题思路和方法，一节课的干货知识那顶上我自己刷题刷二十个钟头啊，上面都是北大清华毕业的老师和全国优秀教师，教的都是高中语数，英物四大科目必考的重点，难点，易错点，课后还有辅导老师一对一的答疑，根本不怕学不会。|那我哪能想到九块钱的课那么有用吗啊，如果我抱抱，我不想再做差生了，这次好不容易高途课堂全科名师班又加了报名名额，我也想冲刺一百四十分啊，这次一定给你报上，赶紧点击视频下方链接跟我一起报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016103267669677734 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '多人情景剧', '静态', '单人口播', '平静', '特写', '拉近', '家', '室内', '配音', '影棚幕布', '动态', '转场', '喜悦', '单人情景剧', '亲子', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.61', '0.19', '0.14', '0.12', '0.08', '0.05', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00999d06d3c5d4335bdfe0fced153c8c.mp4\n",
      "{\"video_ocr\": \"2021届高中生一定要注意!|不注重学考分离|再怎么努力都没有用|即将升入高的同学们|马上会面临一次摸底考试|你会发现考试内容|跟高一高二所学的内容|完全不一样|跟教材是严重脱节的|所以很多同学高一高二|还能考个120来分|结果一上高三|竟然不及格了|别慌!2步实现高三逆袭|第1,也是最重要的一步！|停止一切|英语周报、辅导报、模拟题|模拟考试题各种各样的练习|你只需要做|最近十年各个省份|高考英语的真题|第2步|背单词只背高频800词|然后跟我学习|完形填空12大秒杀技巧|阅读理解10大解题方法|等等等等|在我的直播课里面|三目录|点击视频下方详情报名吧|3元领取|辅导 董建成:到时候您可以试试|辅导 跟谁学京彩老师: 开|辅导 董建成:系统课是可以的|陌上花开:OK 辣椒酱:666|辣椒酱：:OK 王霞:|哈利·波特·薛:等下孩子回来听，请问可以连接迢|巴黎铁塔的微笑:可以跟的上|用户968539328:没问题|Ms仙人掌：老师这个直播课到几点|辅导董建成：试听课我还真不知道|开松宝贝:没听懂|出吗?|lxxxx:|棉花糖口:OK|鲁迅妹子:老师好! miracle：有回放吗|陌上花开：回放可以调语速么? 马特:好的|用户733431798:已经很慢了|9件|限准学|跟谁岁|民谁学|9日%H|辣椒酱: OK|miracle: 有回放吗|用户883979968 老师辛苦了!|用户755956698:|用户883979968:老师辛苦了！|?跟|在线学习更高效|跟谁学 在线学习更高效|美国纽交所上市公司|14|词性 一切相当于动词的东西|verbal 只有动词‘|—名师就在跟谁学|业诚信友善|徐磊英语语法|跟谁学金牌英语讲师 年一线教学经验|美国纽交所上市公司|高中全科培优特训营|形容词|问题:|v词性：|其他词性变n.|ャ徐磊|adj.|句子变n.|adv. 词性|名词|白国|V.\", \"video_asr\": \"二零二一届的高中生一定要注意，不注重学考分离，在怎么努力都没有用。即将进入高三的同学们马上就会面临一次摸底考试，你会发现考试内容跟高一高二所学的内容完全不一样，跟教材是严重脱节的，所以很多同学高一高二还能考个一百二十分，而结果一上高三竟然！|了，别慌，两步实现高三逆袭第一，也就是最重要的一步，停止一切英语周报，辅导，报模拟题，模拟考试题，各种各样的练习。|你只需要做最近十年各个省份高考英语的真题。第二步，背单词，只背高频八百词。|然后跟我学习完形填空，十二大秒杀技巧，阅读理解，十大信息方法等等等等等等，在我的直播课里面，我就会教大家前后呼应的方法和倪萍逻辑，让你实现整个英语的高分突破，点击视频下方详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016365528106689453 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '静态', '教师(教授)', '平静', '场景-其他', '特写', '室内', '影棚幕布', '配音', '手写解题', '宫格', '手机电脑录屏', '学校', '教辅材料', '拉近', '课件展示', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.71', '0.50', '0.09', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00a4b4c1a7676b96788be2f9f9b915d2.mp4\n",
      "{\"video_ocr\": \"你怎么不去|快财商学院|学习理财呢|算了我对这些 一窍不通|别担心|6天理财名师 爆款直播课|0基础小白也能学|18个财富增值 实用技巧|22种投资必备 避坑指南|手把手教你学理财|增强辨别 投资陷阱的能力|掌握投资技巧|以钱生钱|还有班主任 1对1答疑解惑呢|那我要去哪里报名啊|点击视频下方链接|就可以报名啦|华泰证券是如何赚大家钱的? 国制建说的 直播倾告：今天2000|只421291人已报名|直提报名|中国老龄化严重吗？|秒懂财经|粮食危机?夏粮小麦收购减.. 01:18|43297人已报名 02:49:10 头条听听|6天教你学会2020最火的财富秘籍|小白理财直播课|播放全部 00.50|视频为演绎情节|直播预|去领取|学理财 上快 财|大咖直播间|财商微课堂|17:10|快财|达 财商|更多|NG\", \"video_asr\": \"你怎么不去快财商学院学习理财呢？用这些窍别担心快财商学院的六天理财名师爆款直播课，零基础小白也能学十八个财富增值使用技巧，二十二种投资必备避坑指南，专业理财名师手把手教你学理财，增强辨别投资陷阱的呢！|一，掌握投资技巧，以钱生钱，还有班主任一对一答疑解惑呢，那我要去哪里报名啊？点击视频下方链接就可以报名啦！|但。\"}\n",
      "multi-modal tagging model forward cost time: 0.01623058319091797 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '喜悦', '手机电脑录屏', '极端特写', '多人情景剧', '朋友&同事(平级)', '惊奇', '动态', '餐厅', '静态', '平静', '特写', '单人口播', '悲伤', '室内', '配音', '愤怒', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.66', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00b7e4c83be269242686ea32fc6a141e.mp4\n",
      "{\"video_ocr\": \"木字进门正“闲”，|xian|日字进门坐中“间”，|chuang|才字进门紧“闭”眼， bi|聪明|斑马AI课 猿辅导在线教需出及|适合3-6岁宝宝的 趣味语文课|闲\", \"video_asr\": \"孟子进门正清闲，日字进门坐中间。|那次进门往里闯，才字进门紧闭眼。|好。\"}\n",
      "multi-modal tagging model forward cost time: 0.015995502471923828 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '配音', '填充', '课件展示', '平静', '动画', '中景', '静态', '室内', '宫格', '教辅材料', '知识讲解', '家', '幻灯片轮播', '极端特写', '办公室', '混剪', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.22', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00d6cd5f5284a57af7d5f3915931608c.mp4\n",
      "{\"video_ocr\": \"当你在家刷剧玩游戏|突然提示欠费要停机|拼多圣上充话费还能领”励|充话费还能领奖啦 拼多象上|无论移动联通电信都可以|首先点这里 打开拼|打开拼多多|充值中心ù 就有红包等着你|充值中心就有红包等着你|领完红包再充值哇噻好神奇|实惠靠谱到账快 你不要再犹豫|元2件|手机 男装 鞋包 食晶|装喜你获得话费好礼 每月一次，任选其一|尊融的客户，您的电话已欠 贾，为了保您的正通讯，|短信/彩信|￥3|关注“陕西电信”微信号、登录|多多|请尽快充值交费。话费查询可|中|中国电信>|151 7880|“电信营业厅”客户端(httpdll a.189.cn/JBdFD)。\", \"video_asr\": \"当你在家刷剧玩游戏，专题是欠费要停机拼多多上充话费还能领奖励，无论移动，联通，电信都可以。|拼多多首先比特。|充值中心就有红包等着你，领完红包再充值教程其实会靠谱，到账快，你不要再犹豫！\"}\n",
      "multi-modal tagging model forward cost time: 0.016088485717773438 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '手机电脑录屏', '特写', '填充', '单人口播', '喜悦', '室内', '混剪', '极端特写', '平静', '多人情景剧', '动态', '场景-其他', '家', '朋友&同事(平级)', '情景演绎', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.85', '0.82', '0.62', '0.38', '0.21', '0.16', '0.13', '0.13', '0.12', '0.09', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00f55fe23f2bdc7c953cc78b5c2ed8b9.mp4\n",
      "{\"video_ocr\": \"等|是不是你干的|不是我|不是你那你一直在附近转来转去干嘛|我在赚钱呢|赚钱·|走路怎么赚钱|走路当然能赚钱|在趣头条走路的步数能兑换金币|金币还能兑现|所以我每天都这么走|真的可以啊|还问不问了|说是不是你|91iI10100|高.115|i0i21200|CLOTHINO|:国三15|i0二210|即赚即提\", \"video_asr\": \"说是不是你干的。|不是我。|不是你那你一直在附近转来转去干嘛？我在赚钱呢，赚钱。|走路怎么赚钱？走路当然能赚钱，这趣头条，走路的步数能兑换金币，金币还费钱，所以我每天都怎么走，红包到账了。|哎，真的可以说是不是你。|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.01616501808166504 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '平静', '中景', '静态', '特写', '多人情景剧', '配音', '喜悦', '极端特写', '愤怒', '悲伤', '场景-其他', '办公室', '惊奇', '室内', '填充', '手机电脑录屏', '单人情景剧', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.64', '0.62', '0.32', '0.18', '0.09', '0.04', '0.04', '0.04', '0.04', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00fa6bfc42fc487bc9f33b638665ad84.mp4\n",
      "{\"video_ocr\": \"兄弟|你咋变化这么大|我女朋友在小红书上|TU 研究男生穿搭笔记|u1|给我搭配的|小红书?|打开小红书|搜索“男生穿搭”|众多达人分享穿搭笔记|拯救直男穿搭|像他今天这套衣服|就是照这篇笔记搭配的|我虽然交不起女朋友|但我看的起小红书|看时尚达人穿搭分享|10|S 想不帅都难喽|就看小红千|穿搭技巧|有女朋友前|少E三|马E10R己|日g天1|白三天1100|二E王100|二三0|9 天1080|不难喽|金纺哪个味道好闻 af和司区剃|a00Q件看面|635件商品|下午6507 秋冬穿招|n2|中国联通 nansheng chjan da|男生穿搭|Zane|nars服红I|双击图片有惊惠|色彩搭配|最简单男生穿搭技巧|每日穿搭1男生|立即下载小红书App>|金纺柔顺剂味道 金坊柔顺剂|nyX|nike NARS|更多穿搭攻略|历史记录 精华 面霜秋答|123 空格|选定|说点什么...|494|2725|最简单色彩|搜索|确认|取消|26万+剑笔记|1万+篇笔记|关注|nyx翻靴|1.7万 评论|收藏|追可设计|小红|2/9|y u|RED\", \"video_asr\": \"有女朋友钱，对面的女孩看过来，有女朋友后。|兄弟。|你咋变化这么大？好，我女朋友在小红书上研究男生穿搭笔记给我搭配的小红书，打开小红书，搜索男生穿搭，众多达人分享穿搭笔记，拯救直男穿搭向他，今天这套衣服就是照这篇笔记搭配的。我虽然江西女孩，但我看不起小红书。|求我。|上小红书看时尚达人穿搭分享，想不帅都难喽！|真。\"}\n",
      "multi-modal tagging model forward cost time: 0.016257762908935547 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '推广页', '中景', '静态', '场景-其他', '配音', '单人口播', '平静', '喜悦', '多人情景剧', '室外', '特写', '全景', '极端特写', '室内', '路人', '惊奇', '图文快闪', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.85', '0.64', '0.07', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/00fdd82702a607c0be3a70bc7dbf451b.mp4\n",
      "{\"video_ocr\": \"一个高二的学生|他为什么每次数学|都能考140分呢?|是他天生聪明吗?|还是靠多刷题?|是他掌握了解题思维!|其实|高中数学是一门|尤其讲究方法和技巧的学科|知识|知识+ 技巧+|知识＋+ 技巧+ 解题思维|才是稳定高分的关键|我是高途课堂陈国栋|十多年高考研究经验|熟知命题规律|自创果冻全解技巧|3秒一道小题|5分钟一道大题|不仅让你知道怎么学|更让你知道|怎么轻松拿高分|数学真的很简单!|跟着专业的人学习|还等什么?|点击视频下方|即可报名|们余年教学经验|累计学员超万|29元|园浙江卫视|AD年cD AC2.|平面向量及其应用|cD'+DB-Bc-|陈国栋|初高名师提分班|省高考状元带队授课 老师平均教龄11年+|资深一线名师直播授课 8节课突破重难点学科 课后辅导老师1对1答疑|果冻数 全解技巧|高途课堂|新学期专享 元|数学\", \"video_asr\": \"一个高二的学生，她为什么每次数学都能考一百四十分钟？是她天生聪明吗？还是考多少题？嗯。|他掌握了解题思维，其实高中数学是一门尤其讲究方法和技巧的学科儿，支持大技巧八解决思维的养成，才是稳定高分的关键。|我是高途课堂陈国栋，十多年高考研究经验，熟知命题规律，自创国栋数学全解技巧，三秒一道小题，五分钟一道大题，不仅让你知道怎么学。|更让你知道怎么轻松拿高分，数学真的很简单，跟着专业人学习，还等什么？点击视频下方即可报名！\"}\n",
      "multi-modal tagging model forward cost time: 0.016215801239013672 sec\n",
      "{'result': [{'labels': ['现代', '填充', '单人口播', '推广页', '中景', '教师(教授)', '静态', '室内', '平静', '极端特写', '配音', '教辅材料', '场景-其他', '情景演绎', '特写', '课件展示', '全景', '影棚幕布', '转场', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.85', '0.67', '0.08', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0102701e518ad76f488e878a14876f33.mp4\n",
      "{\"video_ocr\": \"我今天要跟大伙|分享一下|我儿子当学霸的秘密|就是它|这个就是|高途课堂全科名师班|我儿子以前做题又慢|错题又名|自从花9块钱报了它|简直神了|平均教龄11年|清华北大毕业名师|教孩子掌握|语数英物四科的|解题大招和秒题技巧|行了，赶快走吧|你还没说怎么报名呢|点击视频下方|就可报名|哎呀.快走啊|咱儿子当学霸就行了|别跟外人说那么多了|哎呀，我不走|我有一万个理由私藏它|我不说我难受啊|40RUEDESEVRES|请选择孩子9月升入年级|新用户专享 立即体验 浙江卫视指定在线教育品牌|高途课堂||园浙江卫视|ARSI|AR|初高中名师 特训班|9|全国百佳教师带队教学平均教龄11年|斗期|BALGAENCIA|名师特训班|夕r|加r唯|华少|仅需\", \"video_asr\": \"快走到学霸就行了，不要给我说那么多了，哎呀，我不走，我有一万个理由智商上，我不说我难受啊，我今天要跟大伙分享一下我儿子当学霸的秘密。|就是他这个呀，就是高途课堂全科名师班，我儿子啊，以前做题又慢，做题又多，自从花九块钱报了他啊，简直神了。|平均教龄十一年的清华北大毕业名师教孩子掌握语数，英物四科的解题大招和秒题技巧，赶快走吧！|怎么报名啊？点击视频下方就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01627373695373535 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '中景', '多人情景剧', '静态', '全景', '室外', '单人口播', '喜悦', '手机电脑录屏', '平静', '动态', '特写', '惊奇', '亲子', '极端特写', '朋友&同事(平级)', '愤怒', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.87', '0.54', '0.51', '0.19', '0.09', '0.07', '0.07', '0.03', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/010aa06244fc844fc3b88081e0015520.mp4\n",
      "{\"video_ocr\": \"网上那些个贷款广告也没一个靠谱的 具体额度以最终审核为准|要我说|只有卡牛瑞贷就跟他们不一样|最高额度|最快一放款|最长能分拿慢慢还|不用抵押不用面审|还能提现到银行卡|怎么申请|快下载卡牛瑞贷|凭信用卡就能申请|卡牛\", \"video_asr\": \"网上那些个贷款广告没个靠谱，要我说只有卡牛肉带给人不一样，最高二十万额度，最快三分钟放款，最长能分十二期，慢慢还不用抵押，不用面审，可能提现到银行卡怎么申请？我下载卡牛逼大连续用卡就能申请了。\"}\n",
      "multi-modal tagging model forward cost time: 0.015972375869750977 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '动态', '惊奇', '特写', '室外', '喜悦', '平静', '路人', '拉近', '单人口播', '愤怒', '手机电脑录屏', '朋友&同事(平级)', '亲戚(亲情)', '家庭伦理', '场景-其他'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.78', '0.58', '0.46', '0.42', '0.37', '0.25', '0.11', '0.07', '0.04', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01104ca9690834a5e0cbc10a7ee52611.mp4\n",
      "{\"video_ocr\": \"张驰老师|我们收集到|好多家长|我看看|孩子二年级了|写作思路不清晰?|这个问题很常见|三年级了|好词好句背了不少|写作文还是流水账|怎么办?|这个我要重点讲|孩子喜欢读书|但读了很多书|作文还是写不好?|这个我来解决|怎样引导孩子|从生活中|寻找素材?|en~这是个好问题|到底有多少条啊?|200多条|算了|来学而思网校|秋季语文特训班吧|我是学而思网校张驰老师|毕业于北京大学中文系|我会传授孩子|高分阅读写作方法|带孩子掌握|36大阅读写作|方法大招|24大语文核心知识点|10大知识板块|课后还有|专业辅导老师|一对一答疑解惑|归纳总结|让语文学习|成为一个|完整的闭环|学起来更轻松|现在报名课程|仅需9元|还包邮赠送|教辅大礼包|快点击视频下方|查看详情报名吧|390 好课+名师+教辅=轻松学 ￥|可爱了啊啊|太可爱了啊啊啊啊啊|啊啊啊啊|阿啊 收下我的膝盖|写作思路不清啊师啊啊|好可爱|17年北大毕业教研打磨|她老师好币 了很|多书|前力新能|4天班主任跟踪答疑|这个问题很常见|的膝盖|10课时名师互动直播|这是行|是什么神仙老师啊|好喜欢啊|卜老师韵|专攻重难点，阅读写作高效提分|毕业于北康火学|神仙|但读|爱了爱了|抱起老 |抱起老师就跑|子喜欢|张驰老师样棒哒|声音好好听师|发现宝藏老师|全国|声高女|声音好 顾|作还是写不妇|发现|句高能|好喜|包邮|哈哈哈哈说的|哈哈哈吟说的太对了|啊师可啊啊啊啊啊啊啊|就跑|阿啊啊啊啊啊啊|写作思路不|虾昨消思路不请啊啊啊阿啊啊|每天进步一点点一|小初高|教龄3年|(具体教辅礼盒以收到实物为准)|毕业于北京|张驰聊|毕教 业龄|于3|达吾力江|立即报名|特惠 立省\", \"video_asr\": \"张勋老师，我们收集到好多家长会，来的快不快了。|孩子二年级了，写作思路不清晰，这个问题很常见，三年级了好词好句背了不少，写作文还是流水账，怎么办？这个我要重点讲。|孩子喜欢读书，但读了很多书，作文还是写不好，这个我来解决，怎样引导孩子从生活中寻找素材，嗯，这是个好问题。|到底多少钱啊，两百多条。|算了，来学而思网校秋季语文特训班吧，不是学而思网校当时老师毕业于北京大学中文系，不会传授孩子高分阅读写作方法，带孩子掌握三十六大阅读写作方法，大招，二十四大语文核心知识点，十大知识板块，课后还有专业辅导老师一对一答疑解惑，归纳总结，让语文学习成为。|一个完整的闭环，学起来更轻松，现在报名课程仅需九元，还包邮赠送教辅大礼包，快点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01601386070251465 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '静态', '中景', '单人情景剧', '教师(教授)', '单人口播', '平静', '办公室', '场景-其他', '配音', '特写', '室内', '影棚幕布', '情景演绎', '学校', '极端特写', '动画', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.93', '0.83', '0.74', '0.23', '0.18', '0.08', '0.06', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0110663c4b207cc226e0c606444fd06c.mp4\n",
      "{\"video_ocr\": \"都是你的|我看你离开我|怎么生活|怎么养活孩子|全职妈妈的生活|让我从一个妻子|活成了一个保姆|却换来家人的|冷漠和白眼|这一刻我明白了|女人一旦没钱|婚姻就是坟墓|直到我遇见了|辣妈计划|通过手机就能开|属于自己的网店|在导师全程指导下|我学会如何用|部手机赚钱|产品经全是平台发货|不用花钱囤货|省去了我最大的烦恼|客户下单|利润自己就|到手机里了|现在我的收入|越来越多|也有底气去追求|自己想要生活|孩子我也可以|带的很好|点击视频下方链接|和我一起|改变你的人生|高利润¥28|待支付 待发货|全部|收货人联系方式: 收货人地址:|和滴N¥S|LIM'S|【1袋]马来西亚进口LIM 3罐装]徐州特产丁妈妈|+31.00|宝宝用品|掌柜推荐|享返现30%权益，有|亲，您的店铺有一份新的订单咯!|陈克明面条老妈厨房劲 [买一送一]挥货网红老|利润￥3|支出￥4115.94收入¥745.11 统计>|9.9特卖|优惠券 辣|辣辣农场|查看详情|备注:|牛蒡酱5种口味可选拌l...|¥49.9 ¥27.8|辣妈计划利润到账|利润￥8.1|送玻尿酸原液]EVM酵母肌活..|时间:|S零涩蓝山风味速溶咖...|7月24日09:00|未使用15元 我的|我的金币280|5月6日上午10:36|大额优惠券999-/0 限量1000张2月19日14点开 今日热销榜TOP|我的订单|94|酸奶吐司面包360g/箱... 道挂面 900g*2简/3.6斤.|订单号: 99200505|9920050523134700066|【2.24发货]顶...|[香港直邮]【新版]AGE20|05月05日 【香港直邮】新版]AGE20s爱...|售价:￥378 【香港直|利润:￥111.87|利滴¥3.|营养保健|球球和她的小伙伴 余额5197.47元|设置|5月5日 晚上23:14|账单 常见问题|广西紫香一号百.. ¥19.9￥24|v9.9￥99|海先生原味鱿鱼330 小养香辣海带头32q*15|¥29.9¥48|牌12g香酥牛肉...多婴儿湿巾谷... 发货]零食封口... ¥8.5￥9|不!要!钱!集市母亲节回馈加倍!|￥25|选择|黑金plus ，本期再下单5500元，下期|本期再下单5500元，下期才可成功保级|本期|价格托管 店铺名片|推荐商品|奶粉纸尿裤|逛街|球球和女 新订单通知|2020年7月v|送玻尿|爱敬|余额519+ 享退|1键加入立即赚钱|投资有风险|Care D|划利，|立即加入|5领线|NI|1元积|郸酣|店铺\", \"video_asr\": \"都是你的，我看你离开我怎么生活，怎么养活孩子。|全职妈妈的生活，让我从一个妻子，我成了一个保姆。|却换来了家人的冷漠和保养。|这一刻，我明白了，女人一旦没钱，婚姻就是坟墓。|两个人。|知道我遇到了辣妈，计划通过手机就能开属于自己的网店，在导师全程指导下，我学会如何用一部手机赚钱，产品全是平台发货，不用花钱囤货。|省去了我最大的烦恼，客户下单利润自己就到手机里了。现在我的收入越来越多，也有底气去追求自己想要的生活。|还不我也可以带的很好，点击视频下方链接，加入辣妈计划，和我一起改变你的人生。\"}\n",
      "multi-modal tagging model forward cost time: 0.016251564025878906 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '手机电脑录屏', '静态', '动态', '配音', '多人情景剧', '平静', '单人口播', '室外', '喜悦', '场景-其他', '室内', '特写', '极端特写', '愤怒', '拉近', '情景演绎', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.87', '0.73', '0.65', '0.44', '0.23', '0.21', '0.19', '0.16', '0.15', '0.10', '0.10', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/011c42ea011ec0262e3f85a09eaebd61.mp4\n",
      "{\"video_ocr\": \"从花季少女|到全职宝妈|我从来没有停下过|奋斗的脚步|我不聪明|但我懂得抓住空闲的时间—|去提升自己|很多朋友羡慕我现在的生活|不用上班|在家照顾孩子|没事还可以出去逛逛街|旅旅游|最重要的是|有一份不错的收入|而这|只不过是我选择了|辣妈计划的结果|在辣妈计划|传授销售秘籍 有导师一对一|不懂随时问|零基础也可以|万款好货都是进货价|商品一件代发|商品 件代发|在家就能掌握一项新技能|79 1224-年货节直播精选 查看更多|¥49.9|[我们的新年]中国原创3|双装|1224-爱的温度 12242100.12.312359|¥189 1224-明星同款半专场|每满200%15上不封顶|15|津贴券|249|12.24 20.00-122723.59|1224-加城雪地袜秒抢|件套|LAMAJI HUA|一条新订单|0|这富奔小取|甫甫|w甫甲|I968|开市啦|POF|PR\", \"video_asr\": \"从花季少女到全职宝妈，我从来没有停下过奋斗的脚步，我不聪明，但我懂得抓住空闲的时间去提升自己。很多朋友羡慕我现在的生活，不用上班在家照顾孩子，没事还可以出去逛逛街，旅旅游，最重要的是有一份不错的收入，而这只不过是我选择了辣妈计划的结果在了吗？计划有导师一对一传授消。|密集不懂随时问，零基础也可以，万款好货都是进货价，商品一件代发，在家就能掌握一项新技能。\"}\n",
      "multi-modal tagging model forward cost time: 0.01676321029663086 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '配音', '静态', '推广页', '场景-其他', '家', '单人口播', '平静', '多人情景剧', '喜悦', '极端特写', '情景演绎', '室内', '填充', '亲子', '特写', '动态', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.89', '0.87', '0.67', '0.34', '0.15', '0.10', '0.07', '0.04', '0.03', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/011dc468364f7daa375104fd5c85bd3b.mp4\n",
      "{\"video_ocr\": \"我要复读|你这分数上个二本可以了|我的目标可是双一流名校|欣欣考了668分|我喜欢的学校她可以 随便选|当初欣欣给你介绍|报高途课堂的时候|你嫌9块钱便宜不愿意学|现在人家逆袭了|你这个时候知道着急了|有什么用啊|我哪知道高途课堂|高中全科名师班|语数英物四科才9元|效果能那么好|平均教龄11年的|北大清华毕业教师|全国百佳教师带队教学|总结的高中数学|168个常考点|语文3大作文高分 黄金法则|英语阅读理解10字口诀|还有物理12通解模型|你说怎么可能没有效果呢|对啊 妈|所以我要复读嘛|这次我一定跟着|高途课堂的老师好好学|妈妈支持你|家长们|现在点击视频下方|查看详情|就可以报名啦|新学员9元专享 立即报名|高考月标 语之1 教学 英悟1班综2化，|名师特训班|省高考状元带队授课老师平均教龄11年+|2020年北京市普通高校创新高中考试成绩单 考生号:|总成绩 科目 105|姓名：张雪|身份证号:|英1班综2|高途课堂|综合 22|数学|英语\", \"video_asr\": \"我要复读，我要复读啊，你这分数上个二本可以了，我的目标可说。|刘明，名校请你姥姥六百六十八分我喜欢的学校，还可以随便选。那当初欣欣给你介绍报高途课堂的时候，你嫌九块钱便宜不愿意学，现在人家逆袭了。|你这个时候知道着急了，有什么用啊，我哪知道高途课堂高中全科特训班，语数英物四科才九元，效果能那么好，平均教龄啊，十一年的北大清华毕业教师，全国百佳教师带队教学，总结的高中数学一百六十八个常考点，语文三大作文高分黄金法则。|与阅读啊，理解十字口诀，还有物理十二图解模型，你说怎么可能没有效果呢？对了呢。|所以我要复读嘛，这次呀，我一定跟着高途课堂的老师好好学，妈妈支持你。|家长们，现在点击视频下方查看详情就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016632556915283203 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '多人情景剧', '亲子', '静态', '家庭伦理', '家', '拉近', '平静', '愤怒', '喜悦', '动态', '特写', '教辅材料', '室内', '极端特写', '夫妻&恋人&相亲', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.96', '0.29', '0.11', '0.06', '0.04', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0127933c5987efbf89ae861c8508a88d.mp4\n",
      "{\"video_ocr\": \"ADCE的中线。已知ABC的面积为2，求:CDF的面积。|正方形ABCD中，E为BC上的一点，F为CD上的一点， 四、旋转:|我是高途课堂李琪老师|初中数学高级主讲|熟知中考各种题型套路|我总结了初中数学|让你彻底告别过去|低效甚至无效的学习方式|有了好方法|数学真的很简单!|还在等什么|点击视频下方|查看详情]|报名吧!|高考失败可以复读|但中考有一次|乾坤未定|就算孩子初二数学|只考60、70分|跟对老师掌握了方法|依旧可以逆袭成为黑马|立即报名|建努氛猫|打好基弑 提分巩固|302イ考点|59|59 个州识溟块|只需9元|北大清华毕业师资 平均教龄11年以上|分析:过C点作AD垂线，得到全等即可。|分析:在此题中可在长线段BC上截取BF=AB，再证明CF=CD, 从而达到证明的目的。这里面用到了角平分线来构造全等三角|三、平移变换:|分析:联DF并延长，利用全等即得中位线。|李|293|302|AC平分BAD, CELAB，且ZB+D=180°，求证:|分析:在角上截取相同的线段得到全等。|高途课堂|分析:利用中线分等底和同高得面积关系。|分析:可由C向BAD的两边作垂线。近而证ADC与B之和 为平角。|分析:将△ADF旋转使AD与AB重合。全等得证。|个全等的三角形达到解题的目的。|分析:AB上取E使AC-AE，通过全等和组成三角形边边边的 关系可证。|一、截长补短法|如图，AB//CD，BE平分ABC，CE平分ZBCD，点在AD上|:ZA+ZC=180|AC的中点，求证:EF//AD|BE.求证:BD-2CE。|由中点想到的辅助线|形。另外一个全等自已证明。此题的证明也可以延长BE与CD 的延长线交于一点来证明。自己试一试。|(2)在梯形ABCD中，AD/BC，ZBAD-90°，E是DC 上的中点，连接AE和BE，求ZAEB-2CBE。|四、角平分线+平行线: 如图，ABAC，1=22，求证:AB-AC>BD-CD。|20|一、中线把三角形面积等分:|B=180|分析:将ACE平移使EC与BD重合。|由线段和差想到的辅助线|一、截取构全等:|如图，在四边形ABCD中，BC>BA,AD=CD，BD平分，求证|五、作中位线: (1)如图，在梯形ABCD中，AD//BC,E、F分别是BD|冲刺初中数学满分|初中全科名师班|名师特训班|4科仅需9元|新学员9元专享|个常考几何漠型|23个常考几同摸型|如图，AABC中，AD是中线，延长AD到E，使DE=AD，DF是|如图，已知AB>AD，ZBACEZFAC,CD-BC。求证:ZADC+|BE+DF=EF，求∠EAF的度数|名师有秘籍 领跑新学期|分析:延长此垂线与另外一边相交，得到等腰三角形，随后 全等。|如图，AB-AC,ZBAC-90，AD为ZABC的平分线，CE⊥|如图，AB一A/么BA己三0，AD为(ABC的平分线，CE1|分析:在梯形中出现一腰上的中点时，过这点构造出两|AE=AD+BE。\", \"video_asr\": \"高考失败可以复读，但中考只有一次，乾坤未定，就算孩子初二数学只考六七十分，根据老师掌握了方法，依旧可以逆袭成为黑马。|是高途课堂以及老师初中数学高级主讲，熟知中考各种题型套路，帮你打好基础，提分巩固。我总结了初中数学。|百零二个考点，五十九个知识模块，以及二十三个常考几何模型，让你彻底告别过去低效甚至无效的学习方式。有好方法，数学真的。|简单只要九元，还在等什么，点击视频下方查看详情报名吧！|糟了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01665329933166504 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '场景-其他', '单人口播', '中景', '平静', '静态', '教师(教授)', '配音', '影棚幕布', '幻灯片轮播', '特写', '课件展示', '过渡页', '手机电脑录屏', '宫格', '教辅材料', '手写解题', '学校'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.28', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0128662cdea7e0860aab1f8222fccf83.mp4\n",
      "{\"video_ocr\": \"妈妈|我真的只拿了一块钱|我没骗你|你相信我|~|乐乐妈|这怎么回事啊|这孩子|现在就是不学好|非得骗我说这两大盒东西|都是她报名猿辅导|数学名师特训班|和语文阅读写作课|免费包邮赠送的|这是真的|现在只需要29元啊|就能获得9课时的|而且再加1元|你看看这个大礼包啊|秋季热搜题Top10|五分钟口算题卡|数学原创彩绘讲义|常用公式定理挂图|通用体系挂图|还有这么多教辅用具|还有语文礼盒的|基础知识手册|拓展阅读|古诗文经典选读|29+1.00|30块钱|就能上18课时的直播课|这能靠谱吗|这你就有点偏见了|这个课程啊|是由清华北大毕业的老师|带队教学的|你就放心吧|课程啊|还支持无限次回放|孩子一次没学会呢|还可以多学几次|这么好的课|在哪儿能报名啊|点击视频下方|查看详情就可以报名啦|礼盒全国包邮|数学|ド数学常用公式定理挂|争小学数学51A+班通用体;|阅读|经典迭读|语文|向问题 圆环的面积|几何问题|第6讲 四边形里的卡字路|第5讲 一样高的三角形 第6讲 实践应用综合复习(二)|川州tp1|狼辅导|口算题|这才寺认训川到|加1元可换购9课时语文课|积：长宽 四柱的体积|为关系：a十h= a:h|长方形的面积：|除4U 摄形认知综合复习!|第4讲 图形认知综合复习(士|29元 见课时!|27件原创教辅|5分钟|手册|拓展|古诗文|韩甫身|韩辅甫导|第3讲 放大与缩小|语文特训班|实践应用|拿数学5R+班通用体系挂圈|第4讲 正与反|基于500万小学生搜账数据 02|年级|第U tt二 第1讲 神奇的如.! 第2叫 神奇的W第头(二)|第8讲 圆的综合应用 第7讲圆(二)|创新意识 图形S认知|第1讲 神奇的铅笔头(一)|秋季|秋季特训班讲义|猿辅导在线教肓|做攀|全国 包邮\", \"video_asr\": \"哎，真的是哪里快点，我没骗你相信我妈妈，嗯了了吗？是怎么回事啊？这孩子现在就是不学好，非得骗我说这两大盒东西都是他报名猿辅导数学名师特训班和语文阅读写作课免费包邮赠送的。这是真的，现在只需要二十九元啊，就能获得九课时的猿辅导数学名师特训班，而且再加一元就能获得九颗。|时的语文阅读写作课，你看这个大礼包啊，秋季热搜题TOP十五分钟口算题卡，数学原创彩绘讲义，常用公式定理挂图。|用稀挂图还有这么多教辅用具，还有语文礼盒的基础知识手册，拓展阅读，古诗文经典选读，原创彩绘讲义，二十九加一三十块钱就能上十八课时的直播课。还有这么多的教辅工具就能靠谱吗？这就有点偏见了，这个课程啊，是由清华北大毕业的老师带队教学，你就放心吧，课程啊还支持无限。|回放孩子一次没学会呢，还可以多学几次，这么好的课在哪能报名啊？点击视频下方即可报名。|没有。\"}\n",
      "multi-modal tagging model forward cost time: 0.016499757766723633 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '亲子', '动态', '平静', '喜悦', '愤怒', '朋友&同事(平级)', '特写', '教辅材料', '室外', '家庭伦理', '静态', '拉近', '手机电脑录屏', '惊奇', '悲伤', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.93', '0.90', '0.85', '0.82', '0.72', '0.45', '0.40', '0.32', '0.08', '0.06']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0130a5bf56dc29af99863950cdac76c5.mp4\n",
      "{\"video_ocr\": \"挑战高分吗|你知道初中孩子的成绩 想要快速提升|其实只需要三个步骤嘛|好多孩子都已经做到了|孩子成绩上不去|R Becau孩子成绩上不去|he was 一直缺少的都是方法|一直缺少的都是方法|我会先教孩子用一些秒杀 难题的解题技巧|快速做出一些平时不会做 不敢做的大题难题|为孩子建立自信和兴趣|第二步|我会引导孩子一步一步 的熟练掌握这些题|为什么可以这么做|以及解题的过程|知道其中的原理和方法|实现一题通|一类通|我会将几个科目的重点要点提 炼成课件发给孩子阅读学习|这样一来各个题型的思路技巧 都能熟练掌握|作业帮直播课推出了初中 多科名师训练营|包含语数英物化|由清北毕业名师带队教学|只需要9块钱就可以帮助孩子|掌握核心知识点和常考题|就算是初三的孩子也能来得及|快点击下方链接 给你的孩子报名吧|提升孩子成绩|填空题(本第紫题，每小题3分，共1$|万小5乙相鼎 米/时|xN丽与5乙相蹋|刘鑫|ers in his speeches.|20题图-1) 不同盐浓度|容化时的温度|A. 4|甲的来度|攻由改|Pe/*女|‘'(/来1)|面凹 回澎影事查甲要中近票┴|@初中生的家长 你敢用国庆7天的时间|g 1(千未/时)，乙的来度为≌| a10(千未/时)，乙的来度ゎ＆么 Y千未5经的时|千与络的对间|~小|9元|ATUPP|uY pEy 3q uOyM|i8ru Bq.I|三|NMsla|Kuu sa\\\"ON|S0N3d dll83770|山帮|初中多科|y= 10x.|10x.|小月5乙相湿|管てS hky专|暂以ワS|1￥o|019|w风CDE分别是△MBC的边u|课 作业帮直播课|s! AwCs.Joqaw za|IOq V ga|镇”￥Oi p|Iuf手OH|as7h 0q JPOSIoy A|q10H H wH|nuO3q|still faces many hardships in life, but he will mal|C. ZA=90°,AB=10|多科目新用户特惠|encourages others in his speeches. 46. Why were Jason's parents shocked whe A. Because he was born disabled.|Why were Jason's parents shocked when they sa|nat he has done.“Never be afraid of any diffic Iason's parents shocked when they saw him for th|如图，已知∠A=ZD,LB= LDEF,AB=D|P/*y)o|5年中|上课内容与收到礼盒请以实际为准|唯谁920770797|oaf-d7|puOOOs|io bim to|y.10M3uLou uoV|Atur sT \\\"ON Jatouq V|Jupoq v Atr SH\\\"ON|10MauOU|JosuyP VOg |YOMOtIOu|IOMatOy IOUIE V|D,Because he had big hands.|hands.|B.3.5|P/*1小录|6e ihat does the underlined word“ho B.贫夕 we learn the i|4a What does the underlined word “hardships’|R between his arns ths underlined word \\\"hardships mean|1(2019江苏南京金陵河西期中，18)如图，在|I u ApunS puoas aq|u 人eNu 人epunS puonsau|Appo ￥r时doqo o80r suwm sn Jo|Apon ￥red ao oao sugM sn y ouq uAeW u Aepuns puoosau|after high school. Now Jason still faces many hardships in|u can do what he has done.“Never be afraid|温度|第22题图|77つ|1，|opun aeqjooups aqp—“记 soqa V|测试卷|基级过关能力视侠|sso平p o1 qanoue pro Jpasn3y A|p@ uanou po sL DISIs AW°la|期中 期末卷|ends of his arms. He aso leared w et wit! in love with football. He never bnl s palr o! his feet. He only wore socks on the lootball f|of his ars He akso leared w eat witb burke o ve with football He never hads putr of hoe in h|hardwriting and coloring at schoot. holding the p He also learned to eat with forks at home. When he|可抢想了三种宾验方案|方案，如图-！所示.已知|him a respected(受尊重的)player on the schoo you can do what he has done.“Never be|na respected(受尊重的)player on the school team. Now Jason still faces many hardships in life, but|受尊重的)player on the school team. He went to a p. His strong arms and body, as well as some fant|下列选项中的条件能作出唯一的△ABC白 A AB=4,BC=5,AC=10|初一到初三|N6TEO80|JPsoy Ξ|游主子 OHS|s! ABCI s,Hoqow 88|SIAeCI s.-oqoI a|his hair was curly. iad lo write and color by holding the pen|点，则ZBOC=|pue awFV uE SPIN|atadhs ay|uads aH|daapsu|验一街|‘61 po1d aUR|qOT H \\\"ICpo1 ￥rsda a o80 suuM sn y sSap o qanouo pIO s 1aI8Is AH|psHsoPp o yanouo ppo s ia3sIs AW Iz|SSap o yznous pO S OISEA a|ssao平p o yq8nouo pOsI JosIs AW z|阅0|测道|部弧|his logs ie!|oqpioug V|wnre sodks while|nhr udly wure sods whble playing looth|免费 赠送|refused to give up. His strong arms and bodly,|feet. He only wore oeks on the lootball field. He ased to give up. His strong ars and body, as well|ball. He never had a pair of shoes in his life because y wore socks on the football field. He never won|(千来/时)，|求/明)，|C Because Jason leamned to write and cofo|he had big|isanox ys.p 叫 sopun geq.ooqps auvvっ Apo Ynedaqoogo sauuM sn Jo mu ysnt aaq uyPI叫H|iAnos ypop op pun 8eqoops a4 s一y ssn! aap H宇PI OH|0Z 3uOs sAnq q1M|B. AR/C|Lz-0|Y1!N |curly. and color by k B. betw|by holding th|Joj sauons spEOn|dectnig whee|.Fon the peassgt, we lear the following cdectnic wheelchair at|一州|谭梦云|刘颖妮|B3uJVuI SpIY Iood|Suooun sueaps|oI OAeu aM panun oq|soH yanu Ile noq|即叫|ZIuo uSAXIS|mpv|Lz|いolo l|VOl|3IA|uoaLnoy|话Ζ|8L|7业\", \"video_asr\": \"初中生的家长，你敢用国庆七天的时间提升孩子成绩，挑战高分吗？你知道，初中孩子的成绩想要快速提升，其实只需要三个步骤。|好多孩子都已经做到了孩子成绩上不去一直缺少的方法。|我们先教孩子一些秒杀难题的。|技巧，快速做出一些平时不会做不敢做的大题难题，为孩子建立自信和兴趣第二步。|会引导孩子一步一步的熟练掌握这些题。|为什么可以这么做，以及解题的过程，知道其中的原理和方法，实现一题通。|第三，不会相信的。|重点要点提炼成课件发给孩子们。|学习，这样一来，各个题型，思路，技巧都能熟练掌握作业帮直播课推出了初中多科名师训练营，包含语数。|就你话，由清北名师带队教学，只需要九块钱就去帮助孩子上。|核心知识点和常考题型。|就算下班的孩子也能来的及，快点击下方链接给你的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01681804656982422 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '手写解题', '平静', '极端特写', '家', '配音', '教辅材料', '情景演绎', '室内', '亲子', '手机电脑录屏', '喜悦', '多人情景剧', '学校', '混剪', '单人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.98', '0.64', '0.53', '0.51', '0.21', '0.11', '0.05', '0.03', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0136823e97322d9c916b26debe74edc9.mp4\n",
      "{\"video_ocr\": \"市场价899元|我在《网易有道精品课》|开设了|《7天英语实战蜕变营》|899的课程|我们在直播间不见不散哦~|现在仅需|人或动物的躲避处、避难所”|他们开设了一个收容所，为读市的无家可归者提供|anywhere toshelter.|有道精品课|杨亮英文|桶亮第美文|文|物兵游英文|播亮文|杨裹第亮文|需美文|杨亮|精亮饼痪文|n.躲避处；避难所|贝壳|shell|几明|躲避、遮蔽|我们遇上7暴风雨，没有避雨的地方。|现在只需9元就可以得到|点击屏幕下方报名|['jfelte]|shelt -er|sho|临时的住宿。|￥9.0|housing for thecity's homeless.|We were caught inastorm,without|They gpened asheliterto provide temporary\", \"video_asr\": \"SCHALLER，躲避处，避难所这个词你应该怎么记呢？SHOULDER前面的SHOUT从读音上和SHELL，贝壳相关。|后面啊是一个名词，后缀ER，所谓的SHELL，贝壳指的呀，是水里软体动物的保护层，而所谓的SHELTER指的是人或动物的躲避处，避难所。比如说THE OPEN TO SHOULDER TO PROVIDE TEMPORARY HOUSING FOR THIS IS HOMELESS，他们开设了一个收容所。|该市的无家可归者提供临时的住宿。生活当中，啊，SHELTER也经常用作动词，表示躲避，遮蔽。比如说，WE ARE CAUGHT IN A STORM WITH ONE WORD OR SHELTER，我们遇上了暴风雨，没有避雨的地方。我在网易有道精品课开设了七天英语实战的蜕变营，八百九十九的课程，现在只需要九。|元就可以得到，点击屏幕下方报名，我们在直播间不见不散哦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016569137573242188 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '静态', '推广页', '平静', '教师(教授)', '室内', '多人情景剧', '知识讲解', '单人口播', '特写', '影棚幕布', '喜悦', '朋友&同事(平级)', '工作职场', '上下级', '路人', '宫格', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.82', '0.45', '0.20', '0.07', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0139022fc40eda52f64802cf633f844d.mp4\n",
      "{\"video_ocr\": \"流水账|你家孩子是否写作文|字数少不生动|脑子里没词 不知道写什么|好不容易写出来 逻辑混乱毫无亮点|作业帮直播课|毕|清华北大毕业的师 带队教学的课|在课上教孩子掌握|10大写作方法|8大阅读技巧|让孩子快速掌握写作方法|学会自己写出高分作文|快速提高语文成绩|还包邮赠送 配套学习大礼盒|赶快点击视频下方给 你的孩子报名吧|4年级孩子|阅读写作能力|快来看看|猛提升|对他做了什么!|立即报名>|名师有大招 解题更态效|挥暨更高效|赠送12件套教辅礼盒|他的妈|语文|上课内容与收到礼盒请以实际为准|阅读写作提分班 29元( 20课时|中国女排|4年乡|00|的R\", \"video_asr\": \"四年级孩子阅读写作能力猛提升，快来看看她的妈妈对她做了什么，你家孩子是否写作文流水账，字数少不生动，脑子里没词，不知道写什么，好不容易写出来，逻辑混乱，毫无亮点。在帮直播课，清华北大毕业的名师带队教学的课，在课上教孩子掌握十大写作。|法八大阅读技巧，让孩子快速掌握写作方法，学会自己写出高分作文，快速提高语文成绩，还包邮赠送配套学习大礼盒，赶快点击视频下方给你的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016438722610473633 sec\n",
      "{'result': [{'labels': ['单人口播', '现代', '中景', '填充', '推广页', '场景-其他', '配音', '静态', '平静', '室内', '图文快闪', '喜悦', '拉近', '家', '手机电脑录屏', '教辅材料', '教师(教授)', '过渡页', '情景演绎', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/013a4f30adf409b37c92c9232d5c5700.mp4\n",
      "{\"video_ocr\": \"收租了啊|苏大强|把房租给我交了|包租婆|钱都给你转过去了|把手机拿来|再送你个礼物|输个手机号试试|100万|这是平安i动保|免费赠送你100万医疗保额|谢谢包租婆|小白|诶 阿姨|我马上把房租转给您啊|诶 给给给 阿姨|给|太谢谢你了阿姨|别客气了|这怎么领取啊|我让街坊邻居都领一份|现在点击视频下方链接|就能免费领取|这份100万的医疗保额|后续运动换保额|还可以领取|最高10万元的重疾保额|查看活动规别>|请输入脑证网|住院医疗保障 重大疾病保障|升级保障-|免费领最高110万元保障|下午3.30|810万元|免费领取成功 (限新用户)|免费享最高 110万元保障|投保需如实健康告知 保障内容以保险合同为准|2580 369|u a o|就上平安健康APP|下午3:47|获取枪证码|免费取|空格|平安 健康 买保险|返回\", \"video_asr\": \"做为苏丹想把房租给我，交了房租婆钱都给你转过去了，把手机拿来再送你的礼物。|输入手机号试试。|一百万医疗保额免费领取成功一保温，这个是平安I动保免费赠送你一百万医疗保额，谢谢王总，谢谢王总，冯小白，哎，阿姨，我们马上把房租转给您啊，把手机给我，也送你个礼物哎，给给阿姨。|GUESS一百万医疗保额免费领取，这是平安I动保免费赠送那个一百万医疗保额，太谢谢你了，一别克。|告诉我这怎么领取，我是监控领取都领一份，现在点击视频下方链接，就能免费领取一百万的医疗保额，后续运动换保额还可以领取最高十万元的重疾保额哦！|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016485214233398438 sec\n",
      "{'result': [{'labels': ['中景', '现代', '静态', '多人情景剧', '推广页', '特写', '动态', '喜悦', '夫妻&恋人&相亲', '惊奇', '愤怒', '拉近', '悲伤', '单人口播', '平静', '手机电脑录屏', '亲子', '极端特写', '路人', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.82', '0.74', '0.74', '0.67', '0.64', '0.38', '0.28', '0.17', '0.10', '0.03', '0.03', '0.02', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/013efb8c8cda8ea63711acee9b299239.mp4\n",
      "{\"video_ocr\": \"就做一件事|上小鹅通|小鹅通作为 互联网教育界的翘楚|让你轻松上线 自己的网校|全程一站式服务|我有个朋友啊 之前是做线下教育机构的|这光是宣传啊 整天就累的不见人|后来吧 我把小鹅通 推给了他|线下教育转线上|就宣传和招生 小鹅通全给做了|小鹅通学习方便|支持pc网页 微信h5|小程序等多种形式|同时呢 还打造了 自己的线上课程|8大主流知识课堂 形式更丰富|所以呀 别再奔波了|说不定你和成功之间 就差了一个小鹅通呢|现在点击视频下方链接|还能领取7天的|免费使用权利|赶快行动吧|流畅学习体验帮助优质内容全面营收|电子书 专栏|多种社群式助学工具|实时在线更近学员学习情况，帮助学员养成良好学习习惯|打造陪伴式学习效果|提供打通多生态的营销工具 力全丽本安转仙|助力全网获客转化|训练营 心会员|图文 音频|机 构想自己 的|作 为一个教育|网 椒上纽|网校上线|应眵怎么做|wWw.xiaoe-tech.com|直播|仅二疗 折加 高品|APP|打卡|理K|呼1\", \"video_asr\": \"作为一个教育机构，想让自己的网校上线，应该怎么做？就做一件事上小娥娥通小额通话，特别互联网教育界的翘楚，让你轻松。|上线自己的网校，全程一站式的服务，我有个朋友吧，之前是做线下交友机构的，这光是宣传啊，整天会累的不见人，后来把我把小屋风配给了他，现下教育转线上。|宣传和招生小偷全给做了，这小龙同学学习方便，这是PC网页，微信，H五小程序等多种形式，同时呢还打造了自己的线上课程，八大主流只是课堂形式，亚特风魄宗雅蝶再通过了，说不定就成功之间就差了个小额度呢，现在点击视频下方链接，还能领取七天的免费使用权理由。|快行动吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016142845153808594 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '静态', '平静', '推广页', '场景-其他', '办公室', '室内', '手机电脑录屏', '喜悦', '配音', '特写', '家', '多人情景剧', '惊奇', '教师(教授)', '悲伤', '愤怒', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.87', '0.61', '0.27', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/013f26c32626e5c75033891c3944341e.mp4\n",
      "{\"video_ocr\": \"9块9|帅哥|9块9是真的不|啥|我说|9块9秒杀100元话费|是真的不|真的|怎么弄啊|下载打开拼多多|首页搜索“立即领取”|查看更多|就可以找到9块9|秒杀100元话费红包啦|谢谢啊|年享2400G+1200分钟免费通话|Q周日23:00开抢 更多预告|903月29日 23:00即将开抢|EV350 话费快充|9.9秒杀1|5935哪新哦|95米际热|100元|移动、联通、电信通用|电动|鑫勒源纯电动 基唐:1333543344|13543344|8.8元|1349 18:00正在疯抢|购买同款|津AD C 5935|拼多多|0C5935|¥9.9\", \"video_asr\": \"九块九。|筷子九块九是真的不？|找我说九块九秒杀一百元话费是真的不真的怎么弄啊。|下载打开拼多多。|下载打开拼多多。|速度立即领取球员老婆就可以找到九块九秒杀一百元话费红包了。\"}\n",
      "multi-modal tagging model forward cost time: 0.0157773494720459 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '推广页', '中景', '多人情景剧', '喜悦', '填充', '静态', '配音', '单人口播', '惊奇', '场景-其他', '特写', '全景', '动态', '极端特写', '朋友&同事(平级)', '室外', '平静', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.64', '0.45', '0.41', '0.28', '0.21', '0.20', '0.17', '0.15', '0.09', '0.05', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0141e4466e2599b124e2eb4a023f0a40.mp4\n",
      "{\"video_ocr\": \"你儿子不是在上|8000多的辅导班吗|这个9块钱的|作业帮直播课|就让给我吧|9平一 我跟你说|我都了解清楚啦|紫尘国|这个课程啊|包含数学7大专项考点|物理 108个重难点秒杀技巧|我孩子物理 数学提分|全靠它了|我可不管|这上面化学4个常考解析|8个阶梯技巧|10个知识点速记|英语完形填空秒杀技巧|和阅读满分必备逻辑|要有了这些啊|我家孩子再也不用|熬夜刷题了|就是|我女儿在家|天天熬夜刷题|成绩也提高不了|其他同学都报了这个班|成绩噌噌的往上涨|咱们又没报上|怎么办啦|您好|您的作业帮礼盒到了|我在网上看|不是都没名额了吗|你们咋报的啊|什么|又有2000个新名额了|诶|你说的是那个9块钱|包含初高中多科|13节课的|作业帮直播课名额吗|就是这个9块钱的直播课|课程好|而且光是这个|报名就免费赠送的|教辅大礼包|就不止9块钱|现在名额刚放出来|你们也赶紧点击 屏幕下方专属链接|赠送超级大礼包|赠送超级|名师直播+6套资料包邮 12月3日开课O|上课内容与收到礼盒请以实际为准|名师直播+6套资料包邮+7天助教伴学|[限时1折] 高二重难点提分特训班(16|送得快|【模拟试卷】大数据模拟高二试卷|娑组[狂与婚|姿粗[狂与|当[狂与  PH|你挡|居模拟高二试卷|二试卷|名师特训]高二逆装提升课|班级剩余名额|退全将|已结束|平均一小时|送全城|名智星丫|￥职4铺属阶|活动已结束|初一到高三 多科目新用户特惠|(数语英物化)·21寒|21寒|[模拟试卷】 大数|验证码 2384|00:01 剩余支付时间|确认订角|[提分教辅】一本书拿下高二重难点 (语文)·21寒|身晔|V福|io oo wga|09:45|大角塑粉业节|手机号 18|96|名师训练营|送得安全|同安全|获取验证码|按全|视频为演绎情节。|刘颖妮 一〕梦云|免费包邮|一小时|小一小|h一小对|9:48|赠送|霸度国万|学)|(16节|\\\"44G|应付:|¥9|启付|免费\", \"video_asr\": \"哎，你儿子不是在上八千多的辅导班吗？这个九块钱的作业帮直播课就让给我吧，哎呀，我跟你说，我都了解清楚了，这个课程啊，包含了数学其他专项考点，物理一百零八个重难点，秒杀技巧，我孩子物理数学提分全靠它了，我可不管这上面化学四个常考解析，八个G技巧，十个知识点速记英语。|形填空秒杀技巧和阅读满分必备逻辑要有了确切，我家孩子再也不用熬夜刷题了。|我女儿在家天天熬夜刷题，成绩也提高不了，其他同学都报了这个班，成绩蹭蹭的往上涨，咱们又没报上，怎么办呢？您好，您的作业帮礼盒到了，我在网上看不是都没名额了吗？怎么打爆的什么？又有两千有新名额了，你说的是那个九块钱包含初高中多科十三节课的作业帮直播课名额吗？对。|就是这个九块钱的直播课课程好，而且光是这个报名就免费赠送的教辅大礼包都不止九块钱，现在名额刚放出来，你们也赶紧点击屏幕下方专属链接给孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016102313995361328 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '手机电脑录屏', '全景', '喜悦', '亲子', '平静', '惊奇', '极端特写', '单人口播', '愤怒', '路人', '家', '夫妻&恋人&相亲', '家庭伦理', '拉远', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.76', '0.42', '0.19', '0.10', '0.07', '0.06', '0.03', '0.03', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01493e5522df7c22a656e5ed8c43d325.mp4\n",
      "{\"video_ocr\": \"父老乡亲们注意啦|追剧！|打游戏!|上网课!|把你们的|小屏手机|都扔了吧!|哎|用大的|更方便!|凑合玩儿得了|一个iPad|两三千|我可买不起！|现在|拼多多官方补贴|9块9就能买到|Apple iPad 2019|新款平板|目前正在|大批放量|几万的用户|都已经拿到手了|你还在等什么!|打开拼多多|首页搜索|立即领取|津pm 进入秒杀界面|就能找到|9块9秒杀|苹果平板电脑啦!|正品保障 全部补贴品>|AirPods 2 有线充电版|万人团价|限量2件 03月29日 23:00开始|限量 商品售完时未能拼单者视为抢购失败，将发起退款 全国联保.全场包邮.假一赔十|Z x cvbn m|Z x  cvb|必联B-LINK无线路由器家用穿墙王光 纤高速中继智能wif信号放大器|￥9.9 ￥2301 提醒|10.2英寸 ￥9.|立即 里 里脊 李|红包 一了的微|大厂直供T恤万人 广西田东|Pay|Inm|ZxcV|的小编推荐 正品险 分期付款 ¥4599 已拼1万件|屏幕尺寸 上吊时间|拼小圈 O0u麦嘎 有新动态|三叶草男鞋 rnw面膜|查看更多>|能锅【4月15日发完】|苹果11手机|床垫|立即领奖新人手机|周日23:00开抢|距开始 152 26|距开始 152:01:26|来嘞喽啦楼累|破纪录|优惠券|戴尔灵越5000+ 10A9|iPad Air2019 A12仿生|限时秒杀|客服 收藏|14:58|进入>|正在疯抢 即将开抢|全球购|虾青素精华原液|¥9.9 ￥1090|￥2700 限量2件|鱼子酱化妆品|李佳琦推荐面膜|综合|¥9 ¥9.9|补贴价¥4949|Wei|国行原封正品|共22款商品正在疯抢 9秒前参|123 选定 确认|空格 搜索|已抢328件|4月15日发完]|2160X1620像素 分辨率|搜索发现 文胸聚拢|App专享|【品|充值中心 潮鞋馆 现金签到|[款|面膜内裤|1元抢 vivo手机，超级品牌日福利，速抢>|拍照搜同款|立即了|15分钟前参与拼单|0- 拼单|商品详情|口红铁锈红|李锦记蒸鱼豉油|秒杀万人团|能锅 限量|oppo手机壳女a11|【品牌款式丰富，随机发货】摩飞多Ⅱ|:00开始|百亿补贴|李锦记黄豆酱|￥3519|脑||多多暨大钱|【款式丰富，随机发货]iPad等平板电脑【4月15 】预售 App专享 全国联保|大牌珠宝抢免单 品质珠宝产地直播|未来官方旗舰店|4月|46%|【款式丰富，随机发货】笔记本电脑【|热门 手机|更多搜索方式|距开始|lijil|更多预告|vivo|立即领 华为|牌手|断码清仓|距结束09:01:30|【款式丰寞，随机发货] AirPods Prg 完】|百货|款式丰富，随机发货]iPhone11等品|多多果园|￥7.9起万人团价|3月29日23:00提醒|0开:|03月29|电器城|筛选|9.9元 秒杀|￥17.90|w e r t y  u|r t y  ui|iPad air3|团价|操作系统|￥0090|水果|美妆\", \"video_asr\": \"最近打游戏上网课，把你们的小编手机扔扔了吧。|用大的更方便，挨楱玩得了，一个IPAD两三千我可买不起。|现在拼多多官方补贴九块九就能买到IPHONE IPAD二零一九新款平板，目前正在大批放量，提问的用户都已经拿到手了，你还在等？打开拼多多首页搜索立即领取，进入秒杀界面就能找到九块九秒杀苹果平板电脑。\"}\n",
      "multi-modal tagging model forward cost time: 0.016267776489257812 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '手机电脑录屏', '静态', '单人口播', '室外', '平静', '动态', '喜悦', '配音', '愤怒', '惊奇', '场景-其他', '朋友&同事(平级)', '工作职场', '特写', '全景', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.91', '0.82', '0.53', '0.33', '0.30', '0.17', '0.08', '0.08', '0.07', '0.04', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01497593a1bfb9a54bba2c0814ab337d.mp4\n",
      "{\"video_ocr\": \"开学考后|大部分同学发现 物理成绩开始往下掉|甚至不及格|要知道啊|中考物理的 平均分只有六十多分|再不注重物理|初三很难跟上|最后变成 同学们的陪跑|物理多重要啊|这一门学科将 影响孩子|整个初中成绩的起伏|甚至在高考选科都起着|决定性的作用|你还不着急吗|我是高途课堂 刘怀宇老师|最考理综全省前|江湖人称“提分王”|我上万份试卷中 总结出了|296个知识点|提炼出|教你大题巧做|小题秒做|现在只需9元|就能体验清华学霸的 秒杀方法|名额有限 抓紧报名吧|焉罩|清华大学毕业|新用户专享 立即体验 浙江卫视指定在线教育品牌|浙江卫视|名师特训班|全国百佳教师带队教学 平均教龄11年|清华|大宇|太宝|刘怀宇|工TF|华少|儒|￥9\", \"video_asr\": \"开学考后，大部分同学发现物理成绩开始往下掉，甚至不及格，要知道啊，中考物理的平均分只有六十多分，再也不注重物理，初三很难跟上，最后变成同学们的牌，物理多重要啊，这一门学科将影响孩子整个初中成绩的起伏。|甚至在高考，学科都起着决定性的作用，你还不着急吗？我是高途课堂刘华宇老师，清华大学毕业，高考理综全省前三，江湖人称提分网。我从上万份试卷中总结出了二百九十六个知识点，提炼出大约清华方法。|你大题小作，小题秒做，现在只需要九元就能体验清华学霸的秒杀方法，名额有限，抓紧报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01603555679321289 sec\n",
      "{'result': [{'labels': ['单人口播', '现代', '中景', '推广页', '静态', '室内', '教师(教授)', '平静', '极端特写', '配音', '影棚幕布', '手写解题', '场景-其他', '教辅材料', '转场', '动态', '情景演绎', '填充', '学校', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.09', '0.07', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/015c8782c563a0ba37ea4f1b72198fc8.mp4\n",
      "{\"video_ocr\": \"哎，我辞职在家|哎，|当起了全职妈妈|可总让我操心的是|孩子的成绩和作业|但我文化水平又不够|每次改作业都头痛|不知道怎么帮孩子|孩子不笨，可机灵了|就是成绩老不好|有时候他自己 也抹眼泪|把我心疼坏了|幸亏有了大力爱辅导|一秒就能改完作业|语文数学什么作业 都能改|省了我不少心|孩子哪做错了|还能生成错题本|不会的题目反复练|他轻轻松松就把分 提上去了|看图编写一道数学问题，井解答。|题目|键拍照|第8课|细雨|细雨靠非 参考答案|度斗 正确|免费|1-6年级语文数学 免费批改|大 1-6年|大力爱辅导|11:23 学情|轻理|错题本|10|键拍照智能批改|答铺了|晃華|英语|数学\", \"video_asr\": \"哎，我辞职在家当起了全职妈妈，可总让我操心的是孩子的成绩和作业，但我的文化水平又不够，每次改作业都头疼，不知道怎么帮孩子，孩子不笨，可机灵了，就是成绩老不好，有时候他自己也抹眼泪，把我心疼坏了，幸亏啊！|有了大力爱辅导，一秒就能改完作业，语文，数学还什么作业都能改，省了我不少心，孩子哪里做错了，还能生成错题本不会的题目反复练，他轻轻松松的就把分提上去了，点击链接下载大力爱辅导APP！\"}\n",
      "multi-modal tagging model forward cost time: 0.016521453857421875 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '手机电脑录屏', '平静', '中景', '静态', '家', '极端特写', '情景演绎', '配音', '特写', '混剪', '场景-其他', '全景', '悲伤', '单人口播', '多人情景剧', '喜悦', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.92', '0.90', '0.84', '0.62', '0.21', '0.20', '0.11', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01647fc0c452646b6a9c6d948bd2d0d2.mp4\n",
      "{\"video_ocr\": \"french fries 薯条|Gg|coW一奶牛|banana bee|bag|猴子 熊|英语听说启蒙 亲子阅读专家|Bb|Cc|bear|宝宝单词记不住? 教你一招趣味学英语!|ant 蚂蚁 bear -小熊|Ff|airplane|monkey|elephant fox giraffe 一长甄|fox 狐狸|fox →—>狐狸|狐狸|frog一—青蛙|Jj|kangaroo|horse 一>马|hipp0一河马|giraffe—> 长颈鹿|hipp|koala 考拉|lion  狮子|foot|fan|fish|ellyfish|jellyfish一水母 ibis 鹪|do0!|egg eye|coke 可乐|Hh|dog 一狗|cat一猫|elephant 一大象|一大象|Dd Ee|Ee giraffe一 长颈鹿|dog|lizard—蜥蜴|伴鱼绘本 趣味学英语 好玩又好记|fried chicken 炸鸡|dolgbin|dolphin door|小斤 cOw|COW奶牛|cot|lon|考拉|giraffe 长颈鹿|ahl|ice cream 冰淇淋|袋鼠|pizza 披萨|cup|hamburger 汉堡|1101|100 koala-|Cat|ZCrO|appe|Aa|Kk|ant\", \"video_asr\": \"嗯。|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.01607370376586914 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '推广页', '课件展示', '教辅材料', '配音', '动画', '静态', '幻灯片轮播', '平静', '中景', '极端特写', '填充', '手机电脑录屏', '商品展示', '图文快闪', '宫格', '喜悦', '知识讲解', '混剪'], 'scores': ['1.00', '1.00', '1.00', '0.23', '0.09', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01698748d5eb5a400928d7c19ea81f05.mp4\n",
      "{\"video_ocr\": \"这款戴森吹风机有3档风速调节|4档精准温控|而且有多种风嘴打造多变发型|这么便宜怎么购买|现在下载打开拼多多|首页搜索立即领取|点击查看更多|就能找到这款|高浓度负离子的|戴森吹风机啦|“限时活动抢购价9.9元|9.9元秒杀|100|拼多多|1T\", \"video_asr\": \"这款再次被攻击，有三档风速调节，四档星九温控，而且有多种风格打造多变发型是怎么过啊？现在下载打开拼多多首页搜索立即领取，点击查看更多就能找到这款高浓度负离子的代替吹风机了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016248226165771484 sec\n",
      "{'result': [{'labels': ['推广页', '喜悦', '现代', '极端特写', '特写', '多人情景剧', '商品展示', '静态', '动态', '手机电脑录屏', '配音', '惊奇', '室内', '中景', '朋友&同事(平级)', '室外', '悲伤', '拉近', '场景-其他', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/016d2f81bf15fb5a3c2de7a25f691d2d.mp4\n",
      "{\"video_ocr\": \"8岁孩子快速算出|100以内的乘除法|你的孩子能做到吗|其实只要掌握了数学速算方法|你的孩子一样也可以这么优秀|建议屏幕前小学孩子的家长们|赶紧给孩子报名|高途课堂小学数学名师班|专为小学孩子打造|北大清华毕业的名师带队教学 请选择孩子9月升入年级|北大 仅需9元!领电新学期|平均教龄11年|帮孩子制定了学习计划|教授高效解题方法|课程生动有趣|随时随地都能学课程，购|只要9元就能报名了|你还在等什么呢|赶紧点击视频下方链接|报名吧|5节名师直播课+1对1答疑+3年无限回放|一年级|=课程指导价499-优惠券490 立省490元|清北毕业名师团队授课|90% 2%|中科院硕士|北京大学硕士|直播互动 5节秒杀课|科技赋能，使在线课堂更有趣 手机、平板、电脑随时学|1700|学习兴值 更加集中|你可能会有以下疑问|耐心等待 添加专属|课程面向全国中小学生，适合不同水平的孩子，建议根 据孩子所在年级报名|退换课说明|新用户专享 立即体验 浙江卫视指定在线教育品牌|4种高效解题大招|5次精讲互动直播课|到手价|10年 录取率不足2%|全程伴学，提升学习效果|课后 1对1答疑|注意力|华少|499|=|小学数学学科负责人 华杯、希望杯优秀教练|语音上麦 课堂体验|券后特惠|点高校及师范院校 主讲老师90%毕业于重|主讲老师教学经验丰富|上课提醒|清北毕业名师|名师|如何上课|错过直播怎么办|4000910188 (周一至周日9:00-22:00)|主讲老师每堂课|精心备课17天|3年回放|智能|交互课件 测评系统|遇到问题请随时咨询“在线客服”或直接拨打客服电话|手机、|为不同孩子|适合水平|7大数学思维专题训练|17天|15年教龄|沉漫式 超味|浙江卫视|名师有秘籍 领跑新学期|季9元！领跑新学期!|3轮严格筛选师资|欧新环|掌摄新知识|买后不支持调班、退课|高途|胡涛|预习资料 电子讲义|激发|全国百佳教师带队教学 平均教龄11年|支持3年内无限次观看课程回放，“进入我的课程-选择课|大数据为不同孩子 摄供定制评测练习|查看学习任务|辅导老师联系|惠课程，购|小学数学资深主讲|地址 北京市海淀区西北旺东路10号院东区7号楼博彦科技西楼 电话:400-091-0188|龚京|9元5节课|帮助孩子|支付|本课程报名成功即开通上课权限，课程为特惠课程，购|高途课堂中|辅老师|程-看回放”|遇到问题请|买后不支持|12X7|84|16X9-:3|48|56X2+4|¥9|小学数学名师班|高医课堂已‰红工视 名帅班|掌硕士|名师特训班|本课程报名|高途课堂特训营|高途课堂｜园浙江卫视|质士|际授课老师为准|立即报名|习报告|负贵人|仅需\", \"video_asr\": \"十二期二十四。|十六乘九除以三四十八，五十六乘二除以四二十八，八岁的孩子快速算出一百以内的乘除法，你的孩子能做到吗？其实，只要掌握了数学速算方法，你的孩子一样也可以这么优秀。|建议屏幕前小学孩子的家长们赶紧给孩子报名高途课堂小学数学名师班，专为小学孩子打造，北大清华毕业的名师带队教学，平均教龄十一年，帮孩子制定学习计划，教授高校解题方法，课程生动有趣，随时随地都能学，只要九元就能报！|了，你还在等什么呢？赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01654815673828125 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '单人口播', '静态', '场景-其他', '平静', '配音', '手机电脑录屏', '影棚幕布', '室内', '教辅材料', '教师(教授)', '喜悦', '知识讲解', '手写解题', '极端特写', '单人情景剧', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.85', '0.17', '0.07', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0175374721da6f762b77709c0e686922.mp4\n",
      "{\"video_ocr\": \"撞到了|快去追呀|他跑不了你看|你上次让我下载的这个拼多多|我在上面只花了9.9|就买到这个行车记录仪|一会再买一个|给咱爸妈车也装上|下载打开拼多多|搜索“立即领取|查看更多就可以找到同款啦|海尔无霜大容量 599|9.9|限时活动抢购价9.9元|刃|前后双录 防雨雾防炫光|9.66” 柔光全屏|到手价4899起|秒杀万人团|拼多多\", \"video_asr\": \"快去追呀，跑不了，你看你上次上网下载这个拼多多，我在上面只花了九块九就买了这个行车记录，一会再买一个给咱爸妈，车也装上下载，打开拼多多搜索，立即领取，查看更多就可以找到同行了。\"}\n",
      "multi-modal tagging model forward cost time: 0.022353410720825195 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '推广页', '静态', '中景', '配音', '平静', '填充', '特写', '单人口播', '喜悦', '场景-其他', '多人情景剧', '动态', '惊奇', '极端特写', '愤怒', '商品展示', '朋友&同事(平级)', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.95', '0.87', '0.83', '0.83', '0.82', '0.56', '0.11', '0.06', '0.04', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/017ab930459c2a38df4a836db19cdf1a.mp4\n",
      "{\"video_ocr\": \"孩子不用你管|我自己能行|张师傅|您这是|我今天送了一天礼盒|到你这儿了|我寻思着你快回来了|就等了你一会儿|太谢谢你了张师傅|我家孩子啊|就盼着这个呢|甭客气啊|诶对了|听说这作业帮直播课 日]<(到31|力度特别大|13课时的直播课|只要9块钱|是啊|这礼盒啊|就是报名免费赠送的|光这个礼盒里啊|就有语数英物化|En|五科的知识手册|单词本|INSTRUCTION 笔记本|和上课指南|那都能教些啥啊|里面的课程啊|包含数学7大专项考点|物理 108个重难点秒杀技巧|化学4个常考解析|8个阶梯技巧|10个知识点速记|英语完形填空秒杀技巧|和阅读满分必备逻辑|还有专业的课程规划老师|课堂内外制定学习规划|咱们啊可省心啦|那这次又是在哪儿报名啊|点击视频下方专属链接|就能报名啦|一刘鑫|#化学知识手|NOTEBOOK|-cROOK 并直兽|分的福|w21三|名师训练营 一〕梦云|2020|弓|上课内容与收到礼盒请以实际为准|初一到高三|视频为演绎情节。|DI|多科目新用户特惠|10|作业帮套播课|作y|DRYVENT|9元|初高中多科|斗3|00LBUS|刘颖妮|0L|上课 指南|免费 赠送\", \"video_asr\": \"孩子不用你管，我自己能行。|嗯。|啊，张师傅，您这是你哎，我今天送了一天礼，喝道你是了，我寻思着你快回来了，就等你了。|太谢谢你了，张师傅，我家孩子就盼着这个呢，我要去啊。|嗯，对的，听说这作业帮直播课力度特别大，十三课时的直播课只要九块钱，是啊，这里还真是报名免费赠送的，光这个礼盒里啊，就有语数英物化五科的知识手册，单词本，笔记本和上课指南。纳多都教些啥啊？里面的课程还包含了数学七大专项考点，物理一百零八个重难点，秒杀技巧，化学四个常考解析，八个解题技巧，十个知识点。|英语完形填空秒杀技巧和阅读满分必备逻辑，还有专业的课程规划老师给孩子啊，课堂内外制定学习规划，咱们俩可省心了，那这是不是在哪报名啊？点击视频下方专属链接就能报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01630711555480957 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '推广页', '多人情景剧', '喜悦', '单人口播', '平静', '极端特写', '特写', '手机电脑录屏', '惊奇', '教辅材料', '动态', '亲子', '室内', '全景', '路人', '场景-其他', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.96', '0.91', '0.90', '0.48', '0.27', '0.17', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01800f0be04b45549c9fc982d3ea0085.mp4\n",
      "{\"video_ocr\": \"人眼里，幸福是能驻|天使的眼里，幸福是蹉|D|对我来说|拥有妈妈的唠叨|才是最幸福的|这优优现在作文进步这么大|这要多亏我前段时间 给她报名了|高途课堂小学语文名师课|资深一线名师团队授课|她学了不少写作文的方法 和技巧呢|我现在看到了题目|确认了主题|脑海中就有大纲|下笔能就写|这学期的优秀作文评选大赛|我还得了二等奖呢|看给你厉害的|那这个课程肯定很贵吧|这么好的课怎么报名呀|点击视频下方查看详情 就可以报名啦|什么是幸福|在农民眼里|幸福是庄稼能有个好收成|在白衣天使的眼里|幸福是患者能获得健康|幸福是能驻守边境|为国效力|不贵|现在报名只要9块钱|点击这里 查看详情|仅需|1牛|新用户专享 立即体验|浙江卫视指定在线教育品牌|幸福|农民眼，幸福是庄稼|华少|高途课堂|浙红卫视|名师特训班|FINE2WRLA GOLOMIA|(OlOM3/A|全国百佳教师带队教学 平均教龄11年|Imiss yn|I miss you!|RIS|￥9|夕r\", \"video_asr\": \"什么是幸福？|在农民眼里，幸福是装甲能有个好收成。在白衣天使眼里，幸福是患者能获得健康。在军人眼里，幸福是能驻守边境，为国效力。对我来说。|拥有妈妈的唠叨才是最幸福，大姐这悠悠现在作文进步这么大啊，这要多亏了我前段时间给他报名了高途课堂，小学语文名师课，资深一线名师团队授课，他学了不少写作文的方法和技巧呢，我现在看到题目这主题，脑海中就有大纲下笔。|就能写这学期优秀作文评选大赛，我还得了二等奖呢，看变厉害的，那这个课程肯定很贵吧，不贵，现在报名只要九块钱。|不好的课怎么报名啊？点击视频下方查看详情就可以报名了。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016344070434570312 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '多人情景剧', '静态', '平静', '亲子', '家', '家庭伦理', '极端特写', '喜悦', '惊奇', '特写', '手写解题', '全景', '愤怒', '室外', '室内', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.59', '0.58', '0.44', '0.19', '0.11', '0.03', '0.02', '0.02', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01896d58bee40a8bc9ad4d605e43a7f2.mp4\n",
      "{\"video_ocr\": \"一共800元|扫我360借条额度吧|刷的这是什么呀|360借条你都不知道啊|最高可借20万|最快5分钟放款|点这里|就可以申请你的额度啦|￥ 360借条|360借茶|自60银|e借券|360 借条|36P惜备|贷款额度放款时闻等以实际审批为准 贷款有风险 借款需谨慎 请根据个人能力合理贷款|付款码|最高20万展展|SiN/TED KINGDOM|UNTED KINOPOM|￥\", \"video_asr\": \"这。|一共八百元销售三二零几条额度吧，放的是什么呀？三六零借条都不知道啊，这课教老师啊，这个五分钟放款点，这里就可以申请你的额度啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.01645517349243164 sec\n",
      "{'result': [{'labels': ['现代', '静态', '远景', '多人情景剧', '推广页', '特写', '惊奇', '朋友&同事(平级)', '喜悦', '中景', '全景', '手机电脑录屏', '室内', '单人口播', '夫妻&恋人&相亲', '平静', '悲伤', '愤怒', '配音', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/018dc5fddc2708383831410be604d43b.mp4\n",
      "{\"video_ocr\": \"师傅 师傅|给您掌掌眼|天然南红红玛瑙|而这一串体如凝脂|无胶无裂 玲珑剔透|您再看看这个|橄榄核八宝财神|浮雕手串|颜色均匀 肉厚皮薄|开脸生动|看得出是大师|设计的心血|多少钱买的|这两个加一起还|不到五百块钱|我上次也买了个|差不多品相南红|还花了一千多呢|你是青出于蓝了|过奖了|这是玩物得志APP|才有的捡漏价|玩物得志APP是什么啊|玩物得志是一款|文玩艺术品拍卖app|这上面的文玩|888|源头直采|覆盖了玉石翡翠|紫砂陶瓷 核桃珠串等|紫砂陶瓷 核桃珠串等|而且全场0元起拍|权威机构鉴定保真|假一赔百万|新人注册|还有888元红包|用完红包最低1元|就能包邮|赶紧点击视频下方链接|下载吧|鉴真宝.国家认证检测机构合作 查看详情 极速捡漏|7天无理由退贷|推荐分类 琥珀蜜虹|木雕盘玩|和田玉 翡翠|钱币邮票|宗教文化|紫砂 瓷器|黄龙玉 珍珠|玛瑙|其他|专业鉴定服务|珍品专场 参拍记录|国家权威检测|产地放漏|好货直播|南红|普洱茶|小叶紫檀 国画|贵重宝不|金银饰品|国家认证珠宝饰品质量检验中心合作机构|玩家优店|菩提珠8|绿松石|原石|每日上新买就送|沉香|玉翠珠宝|木质珠串|建盏|平台珠宝全品类保真承诺|主|分享 优惠券|书画篆刻|国风好1|关注 恭喜缘*刚刚购买1单144元|Q请输入商品名称或分类|推荐|鉴真宝·假一赔一百万|又玩核桃盘玩必看|玩物|送全套盘玩工具|点击查看|甄选佳品高品质好物|鉴定|茶酒滋补|珠宝饰界|文玩杂项|找国风好物 上玩物得志|玩物得志商城|去使用|工艺作|首页 卖家中心 我的\", \"video_asr\": \"师父，师父给你长长眼，天人能红红玛瑙这串体如凝脂，无胶无裂，玲珑剔透。|您在阿甘这个橄榄核八宝财神浮雕手串，颜色均匀，肉厚，皮剥开连生动，看得出来是大师设计的心血，多少钱买的？这两个加一起还不到五百块钱，不到五百。|我上一期人奶奶和这个差不多，平静的南湖还花了一千多呢，你是青出于蓝啦，过奖了，这也是我在玩物得志APP才能捡到甲烷。我的识APP是什么呀？文物得志是一款文玩艺术品拍卖。|APP这上面的文玩源头直采覆盖了玉石，翡翠，紫砂，陶瓷，核桃，珠串等，而且啊，全场零元起拍，权威机构鉴定保真！|假一赔百万件，注册还有八百八十八元红包，一个红包最低一元就能包邮，赶紧点击视频下方链接下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01616358757019043 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '手机电脑录屏', '多人情景剧', '填充', '静态', '平静', '场景-其他', '配音', '喜悦', '夫妻&恋人&相亲', '单人口播', '惊奇', '愤怒', '特写', '路人', '家庭伦理', '全景', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.80', '0.76', '0.48', '0.38', '0.16', '0.11', '0.07', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/018faa4c9a2fe2039bd9e551337f3baa.mp4\n",
      "{\"video_ocr\": \"我们疯读极速版|推出了最新的活动|不用抽手机|就能获得全新的 P40手机|停停停|你们这帮骗子|根本就抽不到手机|这怎么回事啊|她说我们送手机的活动 是假的|哦不好意思|这位女士|是我们疏忽了|不用抽 那怎么获得|现在只要 集齐10个手机碎片|就能兑换P40手机|登录签到|最高能获得 9个手机碎片|看15分钟小说 也能获得碎片|不|为了弥补 这位女士的损失|我们再加上新用户|看50章小说 就直接就送10个碎片|那在哪可以 下载参加新活动啊|点击屏幕下方链接 就可以下载了|免费下载|0/10 明日签到继续领碎片|签到成 已获得|签车 枚碎片|所以她找了个 之之下，|不是想招上门女婿吗？那就去给 要出钱，出足够的钱，把家里的|了?” 泽。|他跟太 人打|他，世界第 阅读领取|时间吧？|呢，不能让她有什么怀疑。|想你。“唐暖的情绪霸|拉着唐暖匆匆找到一个 “我去上个厕所，等|三急，她能理解。|她当上门女婿，但前提是她必须|求着不要把唐暖送班 有了唐暖这个新的家|手，应该能把陈虎给镇|双手托腮，想着小|困难解决掉..于这算不算坑了|两人一起|至身处黑色|HUAWE|谁我的奖品|他越发感觉身 病的时间间隔越来，|么能放心离开这个世界 而现在家里这般情|6天23时59分|白，自 出现的|她猜测|一直 “哥！“出了金辉大銃|自己有这个异常， 暖知道，要不然她要 唐暖倒是没怀|一劳永逸的解决!|，在路边遇到了还在 暖，当时爸爸妈妈找|唐牧带唐暖离开了 他相信，刚才自己|了，感觉有了依靠 只是，他左等 整个过|似也充满了故事，像急切要找个 老公似的，他答应了，应该算是|他到底还是错 分开就是九年时光!|久，所以他不想以后迁|肯定是发病的征兆一|必须要急速的解决现在|个意外，当时他也才 跟父母出去自驾游，|88|分类|但她又不能闯|“哥!“终于看到唐牧出现，唐|对不起！对不#|急忙忙跑过去。 坐是|了，就算上大号也|各取所需吧。 了把脸出去-—唐暖还在外面等着|雨季的那段成长... “哥，九年了!7|巨变，自己承受的|那么多了。|才分开!|他的身 很清梵些|不见唐牧出来，|有了这番打算，唐牧匆匆洗|年多啊|在有无数的话想对唐牧 着急|明日 看视频|哥的记忆，想着这 管怎么样，哥哥回|七岁当兵|一个拳头大 他们|钟的样|而且，他感觉林闵雨本身貌|非常高 只是他那时|边地狱|他摸出了林闵雨的名片，她|暖眼泪|的快速 ]时间真 病，他|竟然不 没当我|么长|暴去。|林闵雨，这个就再说吧，顾不了|他记得，收养店|第5章 心疼|般的侵|的父母，却都没找至|具体活动以实际情况为准|福利中心 规则 碎片明细>|新人福利|，说句|疯读 赢手机|题，也|OC|看了|？唐 解的\", \"video_asr\": \"梦不能极速版推出了最新的活动，不用抽手机就能获得全新的P四零手机，你们这帮骗子他们是抽不到手机，这怎么回事啊？他说我们送手机的活动是假的，我不好意思，这位女士是我们疏忽了，我们疯读极速版推出了最新活动，不用抽手机就能获得全新的P四零手机，不用抽，那怎么获得？现在只要积极。|个手机碎片就能兑换P四零手机登录签到最高能获得九个手机碎片，看十五分钟小说也能获得碎片。|为了弥补这位女士的损失，我们再加上新用户，看五章小说就直接送十个碎片，在哪可以下载参加新活动吗？点击屏幕下方链接就可以下载了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01619744300842285 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '路人', '工作职场', '上下级', '静态', '愤怒', '惊奇', '喜悦', '采访', '室外', '手机电脑录屏', '多人情景剧', '动态', '平静', '全景', '企业家', '悲伤', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0194d0e2cb00517f9767d4681342869c.mp4\n",
      "{\"video_ocr\": \"两种情况|第一种并列|这个最简单|比如说|I love Lucy|and I'ily|我爱 Iucy和 Iiy|这叫并列宾语|2、 第二种宾语是什么呢|就是，没有并列词|但是，也有两个|I give him a book|我给他一本书|主语是给|宾语呢他和书|为什么给这个事情|会有两个宾语呢|给这个事情|天然就需要两个对象|一个是内容，你给什么东西啊|第二个呢，对象 你给谁啊|10天 满满干货的研究|只需要 9元钱|点击屏幕下方|参加老钟的公开课程吧|什|什么叫做双宾语?|Ourd|love|(Owd|ルく|给ぐ内|给←ぐ内多|Lucy|L心特|大<内每|有道精品课·为你精选好课|人n しい|有道精品课|9元10节课|KE.YOUDAO.COM|ve ooK.|boo\", \"video_asr\": \"什么叫做双B女？|两种情况，第一种并列，这个最简单是吧，比如说I LOVE LUCY AND LILY，我爱LUCY和LILY，这叫并列。第二种比喻是什么呢？就是没有并列词，但是也有两个。|I GIVE HIM A BOOK我给她一本书终于输给冰雨呢，他和书。|为什么给这个事情会有两个宾语呢？给这个事情天然就需要两个对象，一个是内容，对吧？|给什么东西啊？第二个呢，对象你给谁呀？十天满满干货的研究，只需要九元钱点击。|吴下方参加了。|行。\"}\n",
      "multi-modal tagging model forward cost time: 0.016084909439086914 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '配音', '推广页', '平静', '课件展示', '静态', '极端特写', '知识讲解', '特写', '中景', '室内', '单人口播', '动画', '重点圈画', '手写解题', '填充', '手机电脑录屏', '家', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.91', '0.44', '0.12', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01a008bec3fa0e976a06a37b0d53119b.mp4\n",
      "{\"video_ocr\": \"我都跟不上它了.不一会儿，发|玩.到花园里它就直奔向草地|吃饱了我带着它到小区的花园里|我是高途课堂吴月光|北京师范大学博士生|教孩子从古诗经典中学阅读|从阅读中提炼写作方法|构建诵读写三位一体写作方法|语文真的很简单|那么到底有没有这样神奇的课程呢|欢迎点击视频下方|报名体验吧|你相信吗?|通过科学的方法|在技的道过科学的分法|在找通过科学的方法有一|在找一个三年级的孩子|一|我分钟就能写出一篇 300字作文|并且语言生动 别具一格|你不相信|是因为你没有 见识过真正的方法|作文高级表达诗文合一|浙红卫视|XAUN日兔|了号小白兔|寸X号又小白兔|O IAN旧日 兔|W音区X人小白兔|我找|号欢小白 兔|可普|又人小白兔|白兔|可X古1X人小白兔|找的时候，却发现草丛里有一|却发现草丛里有二 在找的时候，|现它不见了我急得团团转。我正|我急得团团转。我正 现它不见了し|专属辅导老师伴学 し三年无限回放|鼻子一扇一扇的非常可爱!|古诗图片i|古诗图片记忆法|古讳风记忆法|5节名师直播课 写作高分大招|小白兔不仅贪吃还贪玩呢！|华少|点声响就会立刻竖起来倾听，|全国百佳教师带队教学 平均教龄11年|奶奶家养了一只可爱的A、白|高途课堂|古诗|作文三情法|的绒毛，那是它美丽的外衣|白相间的绒毛L，那是它美丽的外衣.|珑的小面孔上镶嵌羞两颗像红|从的大眼睛.还有那樱桃般的|长得十分伶附，它有一身黑|免！它长得十分伶俐它有一身黑|.7年教龄|竖着一对长长的耳朵，只要|它头上竖着一 对长长的耳朵只要|新用户专享 立即体验 浙江卫视指定在线教育品牌|吴月光北京师范大学博士|名师特训班|7年投於|74教怜|吴月光|写一写|¥9\", \"video_asr\": \"你相信吗？通过科学的方法，一个三年级的孩子十分钟就能写出一篇三百字的作文，并且语言生动，别具一格。|不相信，是因为你没有见识过真正的方法。我是高途课堂吴月光，北京师范大学博士生，自创作文三秦法，古诗图片记忆法。|教孩子从古诗经典中学阅读，从阅读中提炼写作方法，构建送读写三位一体学习法语文真的很简单，那么到底有没有这样神奇的课程呢？欢迎点击视频下方报名体验吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016153812408447266 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '平静', '教师(教授)', '填充', '室内', '影棚幕布', '场景-其他', '手写解题', '极端特写', '拉近', '特写', '转场', '配音', '教辅材料', '幻灯片轮播', '图文快闪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.89', '0.83', '0.50', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01b228f99619b5ba45aaeb0d8df6ae54.mp4\n",
      "{\"video_ocr\": \"你们都给我注意点自己的形象|这是什么0红啊这么滋润|这个听|是猪猪天使口红|你看它质地柔滑|上嘴水润保湿一点都不粘腻|还富含蜂胶和维E|能修护唇纹呢|这个质感也太棒了|居然完全不掉色|它能持久防水|不掉色也不沾杯|这么好用一定很贵|在拼多多上买只要一块钱|在拼多多 上买只要一块钱|下载并打开拼多多|找到个人中心|打开新人一元购|就能找到这款口红|真的只要 块钱就缺买到啦|小主人， 快带走我吧!|*限时抢购价1元|新人专享 平台补贴|新人1元限时抢购|超值好货推荐|拼单狂欢节|购买同款\", \"video_asr\": \"你们都给我注意点自己的形象。|为什么同样的软件看上去柔滑，上嘴水润保湿一点的会议，而且他还有公交回去了呢？太棒了。|房子也不加好友，能上哪？只要一块钱来下载吧。|找到东西，打开心元一元购就能找到这款口红，真的只要一块钱就能买到了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016203880310058594 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '特写', '填充', '家', '手机电脑录屏', '喜悦', '平静', '极端特写', '配音', '夫妻&恋人&相亲', '悲伤', '朋友&同事(平级)', '惊奇', '场景-其他', '动态', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.97', '0.95', '0.72', '0.67', '0.66', '0.32', '0.04', '0.03', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01ba9504a8d13f7e569822c02de17ca5.mp4\n",
      "{\"video_ocr\": \"报报报还不行吗|块钱吗 不就|M3|V|我们同学|作业帮直播课 语文阅读写作提分班|的老师学习写作文|这次考试|紫晴写的作文|又被刊登在校报上了|我那么辛苦背的|30篇范文|却一点成效都没有|那谁能想到|就能上|清北毕业名师|带队授课的课程呀|还是7ZO课时|他们写作文|根本不用打草稿|30分钟|就能写出高分作文|紫晴说了？|这课上有3|必备阅读写作卡招|你问问你们班的紫晴|在哪报的名|爸爸也给你报|点击视频下方链接|就可以报名了|现在报名|还包邮赠送教辅礼盒呢|语文阅读 写作提分班 29元20节直播课抢|班主任老师1对1答疑 清北毕业名师带队授课|作业帮累计用户超8亿|展示礼包为小学礼包 【上课内容以实际为准】|M62|IVI’|等帖本|T4个|Z9|Z9块钱|免费|赠送\", \"video_asr\": \"报报报还不行吗？不就二十九块钱吗？|我们同学都跟着作业帮直播课语文阅读写作提分班的老师学习写作文，这次考试自行写的作文就被看到，那叫报上了我那么辛苦背的三十篇范文。|是一点都不像都没有，那谁能想到二十九块钱就能上清北毕业名师带队授课的课程？|还是二十个课时，他们写作文根本不用打草稿，三十分钟就能写出高分作文。|听说了，这课上有七十四个必备阅读写作大招呢，好孩子，你问问你们班的紫琴在哪，报的爸爸给你报，点击视频下方链接就可以报名啦，现在报名还包邮赠送教辅大礼盒呢！|是什么。\"}\n",
      "multi-modal tagging model forward cost time: 0.016088485717773438 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '亲子', '多人情景剧', '静态', '室外', '全景', '家庭伦理', '愤怒', '平静', '配音', '喜悦', '特写', '公园', '极端特写', '路人', '宫格', '远景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01c0fe65cadd5144ef3b3fb440100d31.mp4\n",
      "{\"video_ocr\": \"您好|是尾号2802的乘客吗|对去一中|好嘞|不好意思|没事 师傅您是 家里有什么事吗|孩子呀 今年上初三|这不数学辅导班 又该报名了吗|现在的辅导班呀|太贵了 但是别的孩子都报|你不学肯定落后呀|你可以给你孩子 报一个|高途课堂初高中 全科名师班|9块钱就有4科 名师直播课呢|专门教孩子呀 语数英物四科的|解题大招和秒题技巧|特别好|那这么便宜|能教得好吗|这就是你的偏见了|人家都是|清华北大毕业 老师带队授课|而且平均教龄 11年以上呢|那太好了|我这就给我媳妇 打电话|那在哪报名呀|现在点击视频下方 就能报名啦|老婆|老婆(*津)|全国百佳教师带队教学 平均教龄11年|微信支付|试 为可透支额度不足)，请核实后再|.il|K/s|23) 16:56|15:56|练导班学费修改|新用户专享 立即体验 浙江卫视指定在线教育品牌|老公，孩子的数学辅导班又该 交学费了，一共6000，赶快打 给我啊!|高途课堂 名师特训班|银行卡可用余额不足(如信用卡则|浙江卫视|￥6000|添加新卡支付|转账金额|取消|华少|¥9\", \"video_asr\": \"ZZZZ。|哎，你好是尾号二八零二的乘客吗？去一中哎好嘞。|哎，不好意思啊。|没事，师傅您是家里有什么事吗？哎，孩子呀，今年上初三，这部数学辅导班又该报名了吗？现在的辅导班太贵了，但是别的孩子都抱你不学，肯定落后啊。|你可以给孩子报一个高途课堂，初高中全科名师班呀，九块钱就有四科名师直播课呢，中文教孩子语数，英物，四科的解题大招和秒题技巧。|特别好，还这么便宜，异能骄傲的好，这就是你的偏见了啊，人家都是清华北大毕业老师带队授课，而且平均教龄十一年以上的。|太好了，我这就给我媳妇打电话，哎，我在哪报名啊？现在点击视频下方就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016314983367919922 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '汽车内', '中景', '多人情景剧', '喜悦', '手机电脑录屏', '惊奇', '悲伤', '静态', '路人', '极端特写', '特写', '家庭伦理', '室外', '拉近', '全景', '亲子', '动态', '平静'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.95', '0.93', '0.87', '0.35', '0.28', '0.13', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01c6fefae1f1b5d3cfc4f9eccb38d0b0.mp4\n",
      "{\"video_ocr\": \"今天教大家女朋友生日|没钱买礼物怎么办|方法一|首先我们先把衣服脱掉|SPICY|然后扎个蝴蝶结 ZAS|TA|Surprise! (惊喜)|这样你就得到了一个礼物|非常简单粗暴|就是有点费脸|方法二保有葫用户|点击视频下方链接|下载有钱花|哇~|有钱花|额度最高可达20万|日利率最低0.02%|借干块钱|每天的利息|最低也就是两毛钱|这么划算的借款平台|还不赶紧|输入手机号|测测你的额度吧|最高能借20万 万元日息低至2元起|P|OUALITY|PRENIUM|最快10秒|智能加密审核 24小时随时提现|同意 有钱花用户服务协议、 隐私政策 和 小满分服务协议|官方客服中心|贷款额度、放款时间、还款方式、借款期限以实际审批结果为准|贷款有风险，借款需谨慎 请根据个人能力合理贷款，理性消费，避免逾期|华合生 t林由洼|官方审核 获得额度|工作人员不会使用个人手机号、微信、QQ等非官方渠道联系用户|资金来源:重庆度小满小额贷款有限公司|如遇可疑情况，请拨打有钱花官方客服电话95055得帮时|Guoy Slou|Bw GPuicouy 、Sto1s|的EW Guaeay Suo|3步拿钱 轻松申请|查看我的额度|隐私政策 及隐私保护声明；同时注册百度账户并同意 百度用户协议|身份认证|阅读并接受度小满(有钱花)隐私政策、有 服务协议和小满分服务协议及隐私保|请输入验证码 获取验证码|贷款口度，放款|贷款额度、放款时间、还款方式、借|有钱花在发放贷款前不会收取任何费用|工作人员不会使用个人手机号、微信、|资金来源:重庆度小满小 北京百度网讯科技有限公司|IOTGHI|需身份证/银行卡号|¥200000|可选3、6、12、24月*|当日借款、次日可还、按日计息|实际额度以银行审批为准|STABLISH|'ED19?A|请根据个人能力合理贷款，理实际额度以银行审批为准|阅读并接受 度小 服务协议科|阅读并接受 服务协|同意 有钱花用户服务协议 及隐私保护声|当日借款、次日可还、按日计息*|声明|3步享说扛TAT|旮能加申|有钱花｜度小满金融旗下信贷服务|信用分|10000元借1天利息最低仅2元)|最低年化利率7.2%(10000元借1天利息最低仅2元)|详情|产品详情|最高借款|分期期限|还款规则|钱花用户|用户|隐私政策 和小满分服务协议 百度用户协议|sh.baidu.com|<STABLIF实际额度以银行审批为准|TASTE A”|“A9|最高借款额度(元):200000.00|教嬷度(元:200000.00|10:18|18|王化利亵72%. |PROKUy|PRPIT|PREW JY|ISHED192A|OCHINI|BLISHEO|712|有有钱花712|“A4”|进观则|私政笔|OWe|CY|UM\", \"video_asr\": \"今天教大家女朋友生日没钱买礼物怎么办？方法一，首先我们先把衣服脱掉，然后扎个蝴蝶结SUPER。|那你就得到了一个领悟，非常简单粗暴。|就能免费的方法啊，点击视频下方链接下载有钱花。|有钱花额度最高可达二十万，日利率最低百分之零点零二，借一千块钱每天的利息最低也就是两毛钱，这么划算的借款平台，还不赶紧点击视频下方链接，输入手机号测测你的额度吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016517162322998047 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '推广页', '场景-其他', '配音', '静态', '多人情景剧', '单人口播', '喜悦', '平静', '全景', '惊奇', '特写', '悲伤', '极端特写', '室外', '路人', '朋友&同事(平级)', '(马路边的)人行道'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.92', '0.89', '0.78', '0.20', '0.12', '0.09', '0.08', '0.06', '0.06', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01c939d71dd6bff6ad3b0dfe1bceac1c.mp4\n",
      "{\"video_ocr\": \"就是你|天天欺负丽丽|怎么|还装傻是吧|ES|搞错了搞错了 Shine|我男朋友在那边|这怎么赔吧|大哥你消消气|要不你下载个全民铃声|现在有活动|可以抽华为P30|华为P30|就是这个全民铃声|在里面选择你喜欢的铃声|完成一次铃声设置|就可以获得一次|抽奖集碎片的机会|试听铃声也可以|获得抽奖机会哦|十个手机碎片|轻轻松松就集齐了|你看我这个华为P30|就是在全民铃声抽的|这么好啊|我可以换新手机了|快告诉我怎么下载|点击屏幕下方链接|就可以下载了|hi 3|UEENAGEMUTANTN|SAGE|ENAGEMUTAM|NINA|JTANT|URes|TSURIEs|NURzEs\", \"video_asr\": \"就是你天天欺负丽丽。|怎么还装傻，是吧？你搞错了，搞错了，我男朋友在那边呢，这怎么赔吧？大哥你消消气，要不你下载个全民铃声，现在有活动可以抽华为P三零呢？P三零就是这个全民铃声。|在里面选择你喜欢的铃声，完成一次铃声设置就可以获得一次抽奖集碎片的机会，试听铃声也可以获得抽奖机会哦，十个手机碎片轻轻松松就集齐了，你看我这个华为P三零就是全民铃声，抽的这么好啊，我可以换新手机了，快告诉我怎么下载，点击屏幕下方链接就可以下载了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016120195388793945 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '平静', '路人', '喜悦', '室外', '极端特写', '动态', '特写', '手机电脑录屏', '惊奇', '朋友&同事(平级)', '家庭伦理', '配音', '场景-其他', '悲伤', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.90', '0.75', '0.74', '0.52', '0.52', '0.38', '0.30', '0.24', '0.08', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01e36bc480a5ce677091cbc377a2ad4a.mp4\n",
      "{\"video_ocr\": \"从今天起你就是我老公了|你说可笑不可笑|因为你|三年里我几次差点死了|现在我却嫁给了你这个植物人|明明是妹妹撞的，|为什么让我替她去坐牢|如果你要不替你妹妹顶罪的话|你母亲的病|我会停止治疗|罪我已经替妹妹顶了|咱们两清了|在我身边|还敢走神|晋渊 你总算醒了|她 就是三年前|开车撞你那个女人|温宁|陆晋渊|温启墨|陆老爷子\", \"video_asr\": \"从今天起。|你就是我老公了。|你说可笑不可笑。|三年里我几次差点死了。|现在我却嫁给了你这个植物人。|明明是妹妹撞的人，为什么要我替她去坐牢。|如果你要不替你妹妹顶罪的话，你母亲的病。|我会停止治疗。|所以我已经提了一点。|咱们两清了。|在我身边。|还敢走神。|竟然你总算醒了，他就是三年前开车撞你那个女人。|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.01607680320739746 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '多人情景剧', '填充', '平静', '中景', '特写', '静态', '亲子', '极端特写', '家庭伦理', '愤怒', '悲伤', '室内', '路人', '动态', '室外', '喜悦', '手机电脑录屏', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.47', '0.35', '0.22', '0.17', '0.10', '0.08', '0.06', '0.06', '0.04', '0.03', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01e43e3485616d88ece7846800dd13d9.mp4\n",
      "{\"video_ocr\": \"新人礼包 立享多重优惠|强光射程 心相印茶语抽纸|居室内地板棉拖鞋防滑棉拖|404人在拼单，可直接参与 查看全部|干什么你|你不爱我了|哪有不爱你|你看|在拼多多上|新用户 元钱秒杀价|就能买到防滑 又保暖的棉拖鞋|下载拼多多|1元抢购|一见你就笑 还差1人拼成|强光LED远射程手电|新人专享 卡帝乐2019秋冬季珊瑚绒居家情侣拖鞋家|新人特权｜退货包运费·100%成团·优先发货|新人专享 棉拖鞋女冬季情侣防 24|胸牌卡套厂牌证件套 变形金刚玩具车模型|新人价￥1 发起拼单|大能亮|已拼10万+件|查看更多>|卡帝乐珊瑚绒家居服|1/9|新人价 ¥1|指甲剪套装|单返现|去拼单 開|丝飘足量300张8 棉拖鞋女冬季情侣|剩余08:08:54.9|品牌特卖 今日上新1146 洁柔”|404|王雪爸爸159 商品评价(29102)|新人专享|券 一元必得 包邮到家|1元 季情侣|围巾女冬季韩版百搭|剩余 04:42:53.8，还差1人|收藏|2.7折|新人专享，平台补贴 ￥1￥30.9|9.9 9块9特卖 多多果园|充值中心 百亿补贴 现金签到|小身材|棉拖鞋女|新人一元超值抢|鞋包|热门|DACKINDLX 距结束 00:13 :22|电器|限时秒杀|立减4元|金猪赚大钱|￥29.9 单独购买|全球购|热门 男装 手机|男装|首页 推荐 搜索|客服|店铺|断码清仓|足量|聊天|HUAWEI|拼多多|CAR|食品|百货\", \"video_asr\": \"二十年。|有关你看看多少钱能买下来就能买套别墅，赶紧在拼多多上给妈妈买一双！\"}\n",
      "multi-modal tagging model forward cost time: 0.016171693801879883 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '配音', '场景-其他', '推广页', '喜悦', '静态', '平静', '多人情景剧', '动态', '惊奇', '家', '特写', '红包', '拉近', '室内', '朋友&同事(平级)', '单人口播', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.57', '0.51', '0.47', '0.27', '0.07', '0.05', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01e9f00a05dd9e8abe17fb57986bc729.mp4\n",
      "{\"video_ocr\": \"你们是想气死我吗|以前你们说 抽手机太难了|那我们成语大侠 就改成不用集碎片|上线就可以直接 抽iPhone11|你们又说 奖品太少了|我们又增加了 P40手机|iPad等奖品|现在你们竟然说 我们送福利太多|不相信我们的 活动是真的|那我再最后重申一次|我们成语大侠一上线|并且玩游戏闯过5关|还有机会再获得|还有iPad 小米手环等奖品|等你来兑换|下载链接 就在视频下方|我求你们试一试|点击下方链接|手机碎片*0.1|看视频抽1次|第15关|12天 连续签到碎片更多|x2|6天|元宝*20|闯关得碎片，最多获得28碎片 碎片需看视频领取|16|做任务获得更多抽奖机会|结束仅剩|规则|10/21题|我的奖品 查看详情>|情|立即抽奖|第9关|0/10|72秒|福利中心|中心|闯关得 3:59:24 碎片需看|领奖|新人限时任务|具体活动以实际规则为准|小米充电宝|13大|手机碎片*2|看视频领碎片0/3 看视频 看3个视频获得1个碎片|B国定|玩成语 抽手机|闯关领碎片|小米手环4|签到|04月08日星鬼三|0%|午5:39 -all|aill|幸运转转转|08:08|下载成语大侠|武林大会|碎片\", \"video_asr\": \"你们是想气死我吗？以前你们说抽手机太难了，那我们成语大侠就改成不用集碎片上线就可以直接抽IPHONE十一，你们又说。|奖品太少了，我们又增加的P四零手机IPAD等奖品，现在你们居然说我们送福利太多，不相信我们的活动是真的，那我再最后重申一。|我们成语大侠一上线就可以直接抽IPHONE十一，并且玩游戏闯过五关，还有机会再获得一部P四零手机，还有IPAD小米手环等奖品等你来！|下载链接就在视频下方，我求求你们试一试。\"}\n",
      "multi-modal tagging model forward cost time: 0.016071796417236328 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '手机电脑录屏', '静态', '办公室', '配音', '单人口播', '场景-其他', '喜悦', '愤怒', '多人情景剧', '平静', '拉近', '动态', '工作职场', '室内', '惊奇', '特写', '单人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.91', '0.80', '0.80', '0.61', '0.36', '0.08', '0.05', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01f2969ceb9f31c16d5f36c692a2df0d.mp4\n",
      "{\"video_ocr\": \"表哥|你这是怎么了|珊珊|你怎么来了|你这是发生什么事了吗|你嫂子过两天要手术|手术费要30多W|我实在是熬不住了|出事了你怎么不说一声啊|我这不是怕给你们添麻烦|我之前叫你和嫂子领取的|平安i动保|你们领取了吗|我看它是...|免费领取的|也保不了几个钱|怕麻烦|我们就都没领|你们糊涂啊|是可以免费领取|最高100W住院医疗险|而且责任范围内|请不限疾病|不限社保用药|都可以申请报销..|还有最高10w重疾险|也是免费领的|哎呀我真是|你赶紧 点击视频下方链接|给自己也领一份吧|你要是倒下了|那可怎么办啊|嗯~|这卡里面有30W|你先拿着用|剩下的我来想办法|平安健康保险|卓十書康保险|平安(了课险|平安优个二余险|平安健康 呆险立|健康特权体验官|平安健房 半女I动保|平安健康保险早最尚|保险 春|平安健7康保呆限|平安健康保隘级保障|平安E兄险|平安建和早险|平金 康 呆|亚安是康保呆险|请输入手机号 请输入验证码|查看活动规则>|给全家人稳稳的保障 给爱人的健康保障|平安e生保2020 住院医疗报销|最高400万|200.00元/年起|给爱人保障|给父 i康保·老年医疗|(升级版) 65周岁、住院医疗报销1周岁、三筒、|重大疾病保障|100万元|升级保障|i康保·重疾|给爱人的健康保产品由平安健康险公司承案|住院医疗报销 投保需如实健康|100种重疾+30种轻症|投保需如实健康告知，保障内容以保险合同为 |授保婧实康告芄，保障内容以保险合筒为潍|获取验证码|糖尿病可保|65周岁、住院医疗报销|保障内容以保险合同为准|险公司承保 保险合同为准|本产品由平安健康险公司承保|岁、住院医疗#|HelloRun健康特权体验官|HelloRun|尿付饮14酒官 专业让生活更简单|平安 健康|就上平安健康APP|70周岁、三高、糖尿病可保|投保需如实健康告知，保障内容以保险合同为准|查看活动规则>本产品由平安健康险公司承保|平安健康险公司承保|给全家人稳稳的保产品由平安健康险公司承保。|(限新用户)|(走路换保额)|免费享最高|中国平安 PINGAN|110万元保障|つ智能客服|PING AN|智能客|￡码|医疗\", \"video_asr\": \"呵呵。|看他。|小哥，你这是怎么了？珊珊你怎么来了？你这是发生什么事了吗？你嫂子过两天要手术了，手术费要三十多万。|我实在是熬不住舒适的，你怎么不说一声啊，我这不是怕给你的添麻烦，我之前叫你和嫂子领取的平安动保你们领取了吗？我看哪是免费领取的，也跑不了几个钱。|怕毛啊，我们就都没领，你能糊涂啊，这个平安I动保可以免费领取最高一百万住院医疗险呢，而且责任范围内不限疾病，不限社保用药都可以申请报销，还有最高十万重疾险也是免费领啊！哎呀，我就是爱你，赶紧点击视频下方链接，给自己也领一份吧，你要是倒下来那可怎么办啊？|小哥哥，这卡里面有三十万，你先拿着用剩下的我来想办法。\"}\n",
      "multi-modal tagging model forward cost time: 0.01647496223449707 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '平静', '单人口播', '喜悦', '全景', '惊奇', '场景-其他', '朋友&同事(平级)', '配音', '极端特写', '手机电脑录屏', '室内', '路人', '悲伤', '动态', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.87', '0.77', '0.41', '0.36', '0.31', '0.24', '0.21', '0.18', '0.12', '0.09', '0.09', '0.08', '0.05', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/01fe46d8fbd4d50e26eb2beb3e6bf1d0.mp4\n",
      "{\"video_ocr\": \"裸辞不可怕|在该拼搏的年纪|没有赚到足够的财富|几年前的我|唯一的收入来源|就是工资|可不管我怎么努力|都存不下来钱|直到我遇见了|尚德小白|理财实操训练营|才知道单靠工资|是很难过上|我想要的生活|现在的我|跟着十年理财导师|学习如何使用|基金股票债券等|理财工具|用钱赚钱|增加额外收入|点击视频|起开启|财富之路吧|学会理财投资技巧 5节直播课|静职信|¥331.67|手把手教学|查看理财课程详情>>|带你挑战财富翻倍\", \"video_asr\": \"不可可怕的是，在该拼搏的年纪没有赚到足够的财富，几年前的我唯一的收入来源就是工资，可不管我怎么努力都存不下来钱。直到我遇到了上的小白理财实操训练营，才知道单靠工资是很难过上我想要的生活，现在的我跟着十年理财导师。|学习如何使用基金，股票，债券等理财工具，学会了用钱赚钱，增加额外收入，点击视频，一起开启财富之路吧！|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.01674032211303711 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '静态', '喜悦', '手机电脑录屏', '平静', '多人情景剧', '动态', '单人口播', '全景', '愤怒', '情景演绎', '夫妻&恋人&相亲', '室外', '特写', '配音', '路人', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.96', '0.83', '0.73', '0.18', '0.17', '0.11', '0.07', '0.05', '0.05', '0.04', '0.04', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0204c853cfac6792165a9dd690eb87b8.mp4\n",
      "{\"video_ocr\": \"下个月我们就要办婚礼了|你有什么好的想法呢|简简单单|两家亲戚吃个饭就完事了|我不嘛|我觉得|应该在我出场的时候|天空上飘洒着许多的玫瑰花瓣|你还真以为自己是公主啊|结个婚那么多事|人这一辈子就结一次婚|我就想办的好点|有错吗|你也考虑下实际情况吧|咱们现在的情况|哪够办这样的婚礼|可以用360借条啊|360借条|是啊|360借条最高可以借20万|而且随借随用|不提现不收费|最快5分钟放款|借1万用1年日息最低2.7元|还可以最长分12期还款呢|这么好|在哪里可以申请|我也给咱申请点钱|把咱的婚礼办好点|点击屏幕下方链接|就可以申请你的|360借条额度啦|最高可借20万 借4万最长免息2~|￥ヨ6口借条|￥彐cO借条|￥彐60借条|￥二 =口借条|￥|请根据个人能力合理贷款 避免逾期|息费减免优惠以具体活动规则为准|贷款有风险借款需谨慎|贷款额度 放款时间以实际审批结果为准\", \"video_asr\": \"下个月我们就要办婚礼了，你有什么好的想法呢？讲讲嫩嫩，两家就请吃了饭就完事了，我不。|我觉得啊，应该在我出场的时候，天空上飘洒着细雨中的玫瑰花瓣，你还真以为自己是公主呀，结果婚这么多事，忍着一辈子就结。|一次婚我就想办个好点，有错吗？你也考虑一下实际情况吧，咱们现在的情况哪够换这样的婚礼，用三六零借条呀。|六零线条是啊，三六零借条最高可借二十万，而且随借随用，不提现不收费，最快五分钟放款。|借一万用一年，日息最低两块七，最长分十二期还款呢，这么好，在哪里可以喝酒，我也给咱申请点钱，把这个婚礼办好点。|点击屏幕下方链接，就可以申请你的三六零借条额度了。|急用钱就用三六零借条。\"}\n",
      "multi-modal tagging model forward cost time: 0.017958402633666992 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '喜悦', '家', '单人口播', '惊奇', '平静', '夫妻&恋人&相亲', '特写', '愤怒', '室内', '动态', '悲伤', '朋友&同事(平级)', '极端特写', '配音', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.95', '0.89', '0.41', '0.33', '0.14', '0.13', '0.13', '0.10', '0.07', '0.06', '0.05', '0.05', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/020648c6b6b1a309ac20742146b3c285.mp4\n",
      "{\"video_ocr\": \"明明可以拂袖而去 为何却要委屈求全|只因三年前的一个约定 且看上门女婿的复仇之路|2|点击下方 继镶阅|点击下沉喝知读全|惊击下方继续阅读全文|尽在番茄|18.0c|番茄小说|狼婿\", \"video_asr\": \"一。\"}\n",
      "multi-modal tagging model forward cost time: 0.016178131103515625 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '静态', '混剪', '极端特写', '特写', '教辅材料', '图文快闪', '填充', '城市景观', '中景', '教师(教授)', '动态', '手机电脑录屏', '配音', '家', '动画', '室内', '平静'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/020ca122b3c3ea3a305871249f10ffc6.mp4\n",
      "{\"video_ocr\": \"玖富万卡最高有|20万借款额度|最快3分钟放款|最长可分24期慢慢还|这么好啊|怎么申请啊|点击屏幕下的链接|最高20万额度|试试你能借多少吧|具体额度安等根据 息费减免优惠以具体活|为准|具体额魔费率|息费减免优惠政真|实际额度以审批结果为准，请珍视信用，按时还款，勿过度举债|最高可借20万元|测测你能借多少钱|ONECARD|玖富万卡\", \"video_asr\": \"办卡提高了二十块，三分钟放款，最长可分二十四期，慢慢还这么好啊，怎么申请啊？点击屏幕下的链接，最高二十万额度，就是你能借多少吧，二十万多少额度吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.022063732147216797 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '静态', '中景', '多人情景剧', '喜悦', '手机电脑录屏', '场景-其他', '配音', '惊奇', '平静', '特写', '悲伤', '单人口播', '动态', '路人', '朋友&同事(平级)', '拉近', '愤怒', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.80', '0.71', '0.68', '0.60', '0.55', '0.42', '0.35', '0.07', '0.06', '0.05', '0.03', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0210888724ef9ba8e0097a39e3750b4a.mp4\n",
      "{\"video_ocr\": \"就完成了这个季度的数据报表|搞定了|啊?|我也搞定了|你们到底搞定了什么啊???|我们只用了5分钟|这么快|你们是怎么做到的|用Python!|高效办公|Python是什么?!|Python是新时代的职场技能|智能分析数据|高效整理文档表格|减少重复性工作|现在点击视频报名|只需要6块9!|￥6.9立即体验|Python基础课 数据类型|数据分析，让数据说话|教育顾问|不用下载任何软件，对新手超友好|从安装到放|>- <少>|<扇贝编程|涨薪季|学什么|专业教研团队|教学特色|用轻松高效的方式学编程，理论+实战，不止教你理解编|经·典.组·合|项目负责|了解基础语法|￥6.9/4天|领|掌握编程思维|Python基础课+认知课|王子期 课程总编|理论|实战|互动式课程，学习一段知识，就进|认识5个常见函数 fx 完成10+编程作业|天带你从0入门|学会3大数据类型|视频为演绎情节|6.9 立即购|扃见编程|扇贝|学完后将收获\", \"video_asr\": \"搞定了搞定了，我也搞定了。|什么？我们只用了五分钟就完成了这个季度数据报表，好吃么？你们是怎么做到的？拍摄又开始高速办公室拍摄视频，拍的是新时代的职场，既能只能分析数据，调整整理文档表格，减少重复性工作，现在点击视频报名只需要六块九。\"}\n",
      "multi-modal tagging model forward cost time: 0.015895843505859375 sec\n",
      "{'result': [{'labels': ['静态', '推广页', '现代', '中景', '多人情景剧', '特写', '喜悦', '朋友&同事(平级)', '工作职场', '平静', '惊奇', '动态', '手机电脑录屏', '夫妻&恋人&相亲', '办公室', '路人', '极端特写', '愤怒', '悲伤', '企业家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.93', '0.11', '0.08', '0.05', '0.04', '0.03', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/023bdcfd22dac9113b1c2607e1c845ec.mp4\n",
      "{\"video_ocr\": \"厨房太乱了|我忙不过来|王|没关系的|我在拼多多|秒杀了个调料盒 THE|只要九块九|哇偶|我们一起做饭呗|搜索立即领取|就能找到限时秒杀调料盒|THE ROAD|EROAD|拼单狂欢节|低至1元起|居家百贷|HI\", \"video_asr\": \"那太棒了。|娱乐性的我在拼多多上秒杀来的调料盒只要九块九。|哇。|打开拼多多搜索，立即领取，就能找到健身的沙甜了和。\"}\n",
      "multi-modal tagging model forward cost time: 0.016241073608398438 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '静态', '中景', '手机电脑录屏', '填充', '喜悦', '多人情景剧', '配音', '平静', '场景-其他', '夫妻&恋人&相亲', '极端特写', '家', '单人口播', '特写', '单人情景剧', '动态', '悲伤', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.76', '0.49', '0.24', '0.17', '0.04', '0.03', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0246088a66e3c98da957877b8f1acfa6.mp4\n",
      "{\"video_ocr\": \"昨晚 好不容易正要签单|结果 正要签单的时候|客户看上了别家的产品了|这保险销售太难做了|你这个销售模式也太传统了|现在保险代理人都用它了|里面有超过80种保险产品|任你选|这是什么?|云保|是帮助保险代理人展业的pP|它可推广超过8家的产品|推广收入高|无业绩考核|续保还能拿奖励|哎我能不能也加入啊|点击下方链接就能报名了\", \"video_asr\": \"昨晚好不容易正要签单，结果正要签单的时候，客户看上别家的产品了，这保险销售太难做了，你这个销售模式也太传统了。|现在保险代理人都用它了，里面有超过八十种保险产品任你选，哎，这是什么啊？他云宝是帮助保险代理人展业的APP。|他可推广超过八十家的产品，推广收入高，无业绩考核，续保还能拿奖励，唉，我能不能也加入啊。|点击屏幕下方链接就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016407251358032227 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '推广页', '平静', '多人情景剧', '办公室', '特写', '单人口播', '工作职场', '家', '朋友&同事(平级)', '极端特写', '喜悦', '手机电脑录屏', '动态', '上下级', '愤怒', '企业家', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.95', '0.94', '0.91', '0.29', '0.14', '0.07', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02581a3cbe41350fae5a04914ff0ac24.mp4\n",
      "{\"video_ocr\": \"启禀皇上|民间传闻看小说可以赢手机|用扎堆小说APP|全站海量小说随意看|种类齐全个性推荐|不断便更不乱码|快告诉朕如何下载|点击下方链接|下载扎堆小说|保强战|绝之中|贴高惠手|2收月|景强战兵|新书畅读|男生眼热|贴手|婚天久地|8|火热推荐|特爸\", \"video_asr\": \"清明皇上，民间传闻，看小说可以用手机用家里小说APP，全站海量热门小说随意看，种类齐全，自行建造，更无乱码。|点击下方链接下载扎堆小说。|对对对对对。\"}\n",
      "multi-modal tagging model forward cost time: 0.015900373458862305 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '推广页', '多人情景剧', '静态', '喜悦', '特写', '动态', '单人口播', '惊奇', '夫妻&恋人&相亲', '平静', '愤怒', '极端特写', '路人', '朋友&同事(平级)', '室外', '场景-其他', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.66', '0.59', '0.38', '0.37', '0.02', '0.02', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/025b023bfb3be996261f49a948acdb46.mp4\n",
      "{\"video_ocr\": \"白送手机都不要|全砸了|我全烧了|这一山手机|牛总|不能烧啊|白送手机|他们都不要|还不如全烧了|再给他们一次机会吧|好|我最后一次强调|我们疯读极速版|这次是真的|给用户送手机|用户登录签到7天|就能获得9个碎片|看小说也能|直接获得碎片|只要你集齐|30碎片 10个P|10个P30碎片|我们就把P30手机|给您包啷邮送到家|一个星期送不完|只有通过|本视频下方链接|下载正版疯读极速版|就能免费换手机了|你也赶快去试试吧|明日签到继续领碎片 签到提醒|你已签到3天， 新人福利|极品桃运医圣 每认真阅读五章得1权，最高可得20+碎片|夫人又征婚了|他刚伸手，美女就突兀地坐|读享快乐|认真阅读文章|确认提交|收货地址:|你已签到3天，别中断哟|每日阅读轻松赚碎片|阅读60分钟|1天|正在马不停踏地准备奖品邮寄，预计七|你哦~|我的奖品|0/10|神医弃妃-帝凰之|兑奖进度|距结束仅剩|已领取 桃运区圣|阅读领取|已兑换|请确认收货信息，|身份证号:|23:59|继续阅读 了起来，双手抱住赵途杰...|提交后不可修改哟|登录 福利中心|3碗片|填写中奖信息|恭喜您!|剩余200/共200份|08:08|您的姓名:|ipad air2碎片|6天 23:59|P30手机|查看详情>|别中断哟|去阅读|填写收货地址|日内可送达！有任何间题，请邮箱联系扎堆君|制+1碑片|23:59:59|3/3|12:30|获得|读享快乐每日阅读轻松赚碎片|具体奖励以活动规则为准|NALl|分类|书城\", \"video_asr\": \"送手机都不要锤砸了。|白送手机都不要，我太丑了，手机都不能少呀，白送手机他们怎么样？还不如全杀了，还没一次机会吧。好，我最后再强调，我们疯读极速版这次是真的给用户送手机，用户登陆签到七天。|能获得九个碎片，看小说也能直接获得碎片，只要你集齐十个P三零碎片，我们就把P三零手机给你包邮送到家界首，近一个世纪从还。|拳头了，只有通过本视频下方链接下载正版疯读极速版就能免费换手机了，你也赶快去试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.02227473258972168 sec\n",
      "{'result': [{'labels': ['推广页', '手机电脑录屏', '现代', '静态', '中景', '单人口播', '配音', '场景-其他', '多人情景剧', '喜悦', '平静', '动态', '室内', '拉近', '惊奇', '极端特写', '全景', '路人', '愤怒', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.88', '0.61', '0.55', '0.22', '0.05', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02606b903828ed10293e6191df8c404a.mp4\n",
      "{\"video_ocr\": \"哈喽|我亲爱的学弟学妹|你好呀|我是徐芮老师|从本科到硕士|都来自传说中的北京大学|孩子们都习惯地叫我芮哥|无论以前|你对语文课的印象怎么样|来到芮哥的课堂|你都会突然发现|手里的语文书变香了|还能够感受到弹指一挥|让难题灰飞烟灭的|高级快感|特别是|每一次笑傲考场|都能够成为|你最炫酷的亮相|针对大家最头痛的阅读题|特别是其中的标题作用题|芮哥还帮你|总结了五个字的口诀|叫做|想知道怎么样|送命题秒变送分题|解锁更多的大招吧|跟着芮哥走|提分全都|跟着芮哥走 提分全都有|现在报名作业帮直播课|就能够包邮赠送|教辅大礼包|快点击|视频下方的链接报名吧|业帮直播课|课 作业帮直播课|王索|内对王索趣|29元|名师 有|名师有大 招 解题更|解题更廴高 效|北京大学本硕毕业 作业帮直播课明星教师|北京|北京大学本硕毕业|徐芮\", \"video_asr\": \"HELLO我亲爱的学弟学妹，你好呀，我是学院老师，从本科到硕士都来自传说中的北京大学，孩子们都习惯的叫我睿哥，无论以前你对语文课的印象怎么样，难道锐哥的课堂啊，你都会突然发现，哎，手里的语文书变香了。|能够感受到弹指一挥让难题灰飞烟灭的高级快感，特别是每一次笑傲考场都能够成为你最炫酷的亮相。针对大家最头痛的阅读题，特别是其中的标题作用题，瑞哥还帮你总结了五个字的口诀，叫做那对术所去想知道怎样让阅读送命题。|变成送分题吗？快来倒瑞哥的课堂，解锁更多的大招吧！跟着瑞哥走，提分全都有！现在报名作业帮直播课只需二十九元，就能够包邮赠送教辅大礼包，快点击视频下方的链接报名吧，名师有大招，解题更高效！\"}\n",
      "multi-modal tagging model forward cost time: 0.016044139862060547 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '单人口播', '平静', '静态', '室内', '教师(教授)', '动态', '配音', '特写', '场景-其他', '情景演绎', '办公室', '多人情景剧', '家', '学校', '喜悦', '极端特写', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.26', '0.25', '0.15', '0.02', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/026e309ee7ae5fa7b5d897d4c98a20b5.mp4\n",
      "{\"video_ocr\": \"你就是爷爷非让我娶的丫头|看来爷爷真是老糊涂了|走开|对不起|我给你收拾|出去|顽皮|丫头|怎么了|我发现我喜欢上你了|霍先生|你一直都是我的|小说 阅读吧|点击搜索《霍先生，你是我的言不由衷》|J888|俗话说女追男隔层纱|如何俘获冷酷先生|看敢爱敢恨的女主|介心ㄏ\", \"video_asr\": \"你就是爷爷非让我娶的丫头。|看来爷爷真是老糊涂了。|走开。|对不起，我跟你说什么。|我找不到。|好的原因去阻挡这一切的情意。|这感觉太奇异。|我感觉能说明。|顽皮。|爱情的。|姐姐会发生也不一定。|热的清洁是好性感哦。|我发现我喜欢上你了。|可是我刚刚。|霍先生，你一直都是我的。|这是个不知道。|不像我。\"}\n",
      "multi-modal tagging model forward cost time: 0.015981435775756836 sec\n",
      "{'result': [{'labels': ['中景', '推广页', '填充', '现代', '静态', '多人情景剧', '特写', '动态', '愤怒', '室内', '夫妻&恋人&相亲', '家', '惊奇', '极端特写', '拉近', '悲伤', '室外', '全景', '平静', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.83', '0.76', '0.59', '0.55', '0.46', '0.44', '0.30', '0.25', '0.23', '0.12', '0.06', '0.04', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/026f5c479e8f657804715e5e14dfe2ac.mp4\n",
      "{\"video_ocr\": \"老虎 tiger|英语听说启蒙 亲子阅读专家|伴鱼绘本 宝宝单词记不住?教你一招轻松学英语!|兔子 rabbit|monkey|猪 pig\", \"video_asr\": \"猪猪PIG PIG PIG LAB LAB TIGER TIGER TIGER TOOTH TOOTH RABBIT RABBIT RABBIT HOPE HOPE FOR MONKEY MONKEY MONKEY。|ZZZZ。|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.015824317932128906 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '配音', '平静', '课件展示', '教辅材料', '静态', '极端特写', '动画', '宫格', '手机电脑录屏', '填充', '中景', '知识讲解', '才艺展示', '喜悦', '室内', '绘画展示', '重点圈画'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.49', '0.39', '0.09', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0273742ae3803b4433e41dbc8651c386.mp4\n",
      "{\"video_ocr\": \"现在我在看快手极速版|我在刷视频|我在领红包|提现门槛低|领到21|今天超开心|恭喜领取现金红包|福|新用户现金红包 +0.36元|预计10分钟内到账微信钱包|HEME JADLAN|21.00|你还在等什么呢|视频下方链接|下载吧|2.金币预计会延迟1分钟到账，部分流水会合并展示 金币明细|3元 10元|提取|上快手极速版|刷视频就能领红包|娱乐赚钱两不误|现在|我的收益|微信 支付宝|微信支付:商家付款入账通知|2.金币预计会延迟1分钱|选择提现金额|48.8|赶紧点击|看视频 赚现金|好友赠送新用户红包 2020.4.2 +1元|提现成功!|每天刷3分钟视频，哪花钱就有了，你还不知道？ 广告 了解详情|收益说明|审核中|汇率:10000金币=1元 1.若连续30天未登录，未提现的收益将过期清空|1.若连续30天未登录，未提现的收益将过期清空|首次专享|去提现|提现到微信:|展示|9465金币|1.3|坚待听三十天 56元|详情见活动规则 奖励金额视任务完成情况而定|好友|2020\", \"video_asr\": \"太忙了，看快手极速版。|刷视频大红包可能看的小二B超开心。|上快手极速版，刷视频就能领红包，娱乐赚钱两不误，你还在等什么呢？赶紧点击视频下方链接下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022400617599487305 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '平静', '手机电脑录屏', '场景-其他', '配音', '单人口播', '喜悦', '多人情景剧', '室内', '特写', '填充', '室外', '影棚幕布', '拉近', '动态', '宫格', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.38', '0.17', '0.12', '0.07', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/028721344422943c99ad92db44a49f2d.mp4\n",
      "{\"video_ocr\": \"诶|小美|你为什么嫁给他呀|马上就要到收玉米的季节了|我爸年纪大了|干不了重活|我可以啊|他家有收割机啊|不就收割机嘛|我也正准备买一套呢|得了吧|你也就会骗骗小姑娘|就你那点家底儿|我还能不清楚|看好了|我在京东金融士上面可是有8万额度呢|哇这么多钱|狗剩哥你可真厉害|这算啥|在京东金融上面最高可借20万额度|而且最快1分钟到账|借钱容易|你有没有考虑自己能不能还得上|放心吧|在京东金融上面借万元日息低至2.5元钱|还能分12期慢慢还呢|我可不可以申请啊|点击视频下方链接就可以申请啦|最高可借20万最快1分钟到账|京东金融APP|京东争MP|京炼金AR|宗掀金融PP|京你金融|宁你融AP|贷款额度放款时间等以实际审批为准 贷款有风险，借款需谨慎请根据个人能力合理贷款|贷款有风险 借款需谨慎请根据个人能力合理贷款|附近银行 金婷宿户经理 更多|5项保障中 最高可借20万 白条|3.31% 新一贷 ￥50万|0.3%-3.25%|段情动态|平安银行(深圳东门支行)|电动有限公司|产品收益区间|手机充值|o0|最高可贷\", \"video_asr\": \"哎，小美你为什么嫁给他呀，马上就要到收玉米的季节了。|我把年纪大了，干不了重活，我可以啊，她才有收割机呀，不就收割机吗？我也正准备买一套呢，得了吧，你也就会骗骗小姑娘，就你那点家人。|都不清楚看好了，我在京东金融上面可是有八万额度的哇。|那么多钱我赚够，你可真厉害，这算啥？在京东金融上面最高可欣二十二度，而且最快一分钟到账，借钱容易，我没有考虑过自己能不能还的上。哎，放心吧，在京东金融上面借万日下低至两点五元钱还能分十二期慢慢还呢，客户可不可以申请啊？点击视频下面链接就可以申请了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016520977020263672 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '惊奇', '喜悦', '室外', '路人', '动态', '全景', '亲子', '特写', '平静', '夫妻&恋人&相亲', '单人口播', '极端特写', '家庭伦理', '(马路边的)人行道', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.78', '0.75', '0.72', '0.41', '0.29', '0.29', '0.24', '0.08', '0.08', '0.06', '0.03', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/028da13b79f24ba5623ef792464f669f.mp4\n",
      "{\"video_ocr\": \"妈妈|2+11等于几呀|你等会儿啊|来客人了|2+11-9?|哎呀你这算的什么呀|一会教你啊|小朋友/几岁啦|我5岁|5岁了还没有掌握|100以内的加减法|3-6岁是孩子|数学思维启蒙的黄金期|可不能错过了|哎我最近听说|好多人都报了那个|斑马AI课思维体验课|而且人家是 动画形式教学|就是孩子看动画|做游戏就把数学给学了|而且价格还不贵|说是那个|10节课就要49块钱|我当时也想报|我抢没抢上|明白|马上再增加 5000个名额|老板|咱们这课程这么火|要不价格也|价格不变 礼盒照常送|好的马上安排|哎我现在给孩子报名啊|谢谢 谢谢|点击视频下方链接|就可以报名了|斑马A课|8罗上斑|技辅|2日|面马学国维学|狼辅号+|斑|斑马A厅i甲|狼铺导g|l+1二12 M+2= 1?|2-8岁上斑马学思维学英语|猿辅导在线教育 出品|2 8岁士返国摄g|2B岁上起8联销|2-8岁|痕辅导|德铺导|顺辅号|13+4m l3+5~|2+8|11书\", \"video_asr\": \"妈妈，二加十一等于几呀，你等会来客人了啊，你好明天还有点什么。|你这算个什么呀，一会教你啊，小朋友几岁了。|五岁了，还没有掌握一百以内的加减法，三到六岁是孩子数学思维启蒙的黄金期，可不能错过了哎，我最近听说好多人都报那个斑马AI课思维体验课，而且人家是动画形式教学，就是孩子看动画做游戏就把数学给学了，而且价格不贵，说是那个十节课就要四十九块钱。|我当时也想报，我抢没抢上，明白吗？站在增加五千个名额，老板，咱们的课程这么火，要不价格因价格不变，礼盒照常送。|马上安排，哎，我我我现在给孩子报名啊，谢谢，谢谢谢谢，点击视频下方链接就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016272306442260742 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '亲子', '填充', '路人', '静态', '平静', '喜悦', '全景', '特写', '动态', '极端特写', '家庭伦理', '悲伤', '餐厅', '手机电脑录屏', '愤怒', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.71', '0.48', '0.36', '0.29', '0.18', '0.18', '0.06', '0.04', '0.04', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02a6f227965af1f84f8719da3449a6a6.mp4\n",
      "{\"video_ocr\": \"大姐呀 你这房子|也可以借给我抵押贷款吧|上次的你还没还呢|我这不没办法嘛|我结婚 买房 装修 买车…|我结不糖得痢钱嘛车|这不都得用钱嘛|我考虑考虑|房子被我抵押炒股了|啊...|你这个败家子|炒股可要赔惨了|哎别来找我啊|我没钱没房|妈|妈 其实我是炒比特币|赚了五百万|昨天把我姐赶出去|今天还敢来我们公司撒野|你在这上班|赶紧给我滚出去|你干什么|这是我们投资人|您这边请|这可是要投一个亿资金的大金主|你给我长点脑子|他不是赚了五百万吗|从哪来的一个亿|点击屏幕下方了解更多精彩内容|更多精彩内容 点击屏幕下方了解详情|投资天才赶走无赖小姨|去公司投资竟赚了一亿|常东小姨|天才投资人|筷手老板|《神级投资》|常东|宿化|小说|全本免费\", \"video_asr\": \"大姐啊，你这房子也可以借给我抵押贷款吧，上次也没还。|我这不没办法了，我结婚买房，装修，买车，这不都得花钱吗？|我看一看。|房子没法抵押炒股啊，你这个败家子炒股可要赔惨了。|别来找我啊。|没房。|其实我是抄比特币赚了五百万了。|昨天帮我姐赶出去，今天还敢来我们公司。|在这赶紧给我滚出去。|干什么？这是我们投资人。|这只是一涂一遍自己的房间。|长点脑子。|从哪来了一个亿。\"}\n",
      "multi-modal tagging model forward cost time: 0.01634812355041504 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '多人情景剧', '愤怒', '现代', '静态', '特写', '填充', '动态', '夫妻&恋人&相亲', '惊奇', '全景', '喜悦', '厌恶', '室内', '家', '平静', '拉近', '室外', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.97', '0.76', '0.68', '0.68', '0.48', '0.37', '0.16', '0.09', '0.08', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02c52b46424909eaea6d99b9a50db94d.mp4\n",
      "{\"video_ocr\": \"萝莉声，御姐音|哦|大家好|我是蜡笔小新|我是不是可可爱爱|听众朋友们|大家晚上好|星光璀璨的今夜|此时此刻的你|正错过了谁|娱乐早班车|新闻早知道|你敢相信|这是同一个人的声音吗|点击下方，关注公众号|领取配音速成课|我在课堂上|萝莉萌妹音|可爱正太音|濡暖电台音|新闻播音腔|拥有百变嗓音 是一种什么样的体验\", \"video_asr\": \"拥有百变嗓音是一种什么样的体验？萝莉萌妹音，萝莉音，御姐音。|可爱正太音。|大家好，我是蜡笔小新，我是不是可以买买听着朋友们大家晚上好。|叮，恭喜他的精液。|此时此刻的你，正错过的神仙，蓝光一下娱乐早班车，新闻早知道啊！|你敢相信这是同一个人的声音吗？点击下方链接领取开因素，乘客会在课堂上等你。\"}\n",
      "multi-modal tagging model forward cost time: 0.017020702362060547 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '静态', '平静', '室内', '特写', '推广页', '才艺展示', '场景-其他', '单人情景剧', '情景演绎', '全景', '动态', '宫格', '喜悦', '单人口播', '拉近', '配音', '混剪'], 'scores': ['1.00', '1.00', '0.98', '0.76', '0.49', '0.29', '0.12', '0.06', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02ceadcd46314313bf8ba5cfb6f5f52e.mp4\n",
      "{\"video_ocr\": \"正长身体的时候|怎么能吃这个|我家里给的饭钱|把手机给我|你看|下载打看 多多|束微打开现金|点击天天领现金|就有现金奖励了|这样午饭钱不就有了么|那这钱|不是白给的吗|对呀|拼多多春节不打烊|现金领到你手软|低至4.28元|咳嗽贴|炫峰一下|50元天无门槛券|100.00|100元暖金将在24小时内|多多\", \"video_asr\": \"掌声的时候怎么能吃这个，我家里给我生命给你看，下载打开拼多多，点击天天领现金就有现金奖励。|丫，发钱不就有这钱不是白给的吗？对呀，拼多多，春节不打烊，现金领到你手软。\"}\n",
      "multi-modal tagging model forward cost time: 0.016139984130859375 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '手机电脑录屏', '推广页', '静态', '朋友&同事(平级)', '平静', '特写', '动态', '单人口播', '极端特写', '喜悦', '工作职场', '填充', '愤怒', '惊奇', '配音', '场景-其他', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.82', '0.76', '0.25', '0.14', '0.09', '0.08', '0.07', '0.05', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02d4aa9a6024b8c8c650e302b42dd69f.mp4\n",
      "{\"video_ocr\": \"有的话|可千万别滑走城|给大家看看|9块Q都能给孩子 买到什么|数学|503个 必考知识啊点|176个 常考易错点|模 42个几|42个几何模型|以及 9大代数考法|让孩子|语文公式法|3步 完成阅读大题|36个技巧应对|作文5大模块|万能模版 轻松搞定作文|英语|5种单词记忆法|阅读完形大招|4小时 高效学法|挑战|点击屏幕下方|查看详情|输入手机号|新用户特惠|只需要9元|即可报名|无限次回放 课程3年内|课后还有辅导老师 1对1答疑|帮助孩子巩固复习|直到|名师特训营 省高考状元带队授课 老师平均教龄11年+|高途课堂|专项突破|不失分|学会为止|孩子的家长|家里有初高中|手里有9块Q吗?|名师出高徒网课选高途\", \"video_asr\": \"家里有初高中孩子的家长手里有九块钱吗？有的话可千万别划走给大家。|九块钱都能给孩子买到什么？数学五百零三个必考知识点，一百七十六个，常考易错点四十二个几何模型，以及九大代数考法，让孩子专项突破语文公式法，三步完成阅读大题。|十六个技巧应对作文五大模块，万能模版轻松搞定作文英语五种单词记忆法加阅读完型大招，四小时高效学习法，挑战不失分！|点击屏幕下方查看详情，输入手机号，新用户特惠，只需要九元即可报名，课程三年内无限次回放，课后还有辅导老师一对一答疑。|帮助孩子巩固复习，直到学会为止。\"}\n",
      "multi-modal tagging model forward cost time: 0.01633143424987793 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '静态', '单人口播', '平静', '多人情景剧', '亲子', '室内', '家庭伦理', '喜悦', '室外', '家', '动态', '极端特写', '亲戚(亲情)', '惊奇', '学校', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.88', '0.79', '0.66', '0.08', '0.08', '0.07', '0.04', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02d96747304c73a51c1e1e3070dff3e6.mp4\n",
      "{\"video_ocr\": \"发工资|今天答应大家的钱|一分钱都不会少|我在小米贷款|有最高30万的额度|最快一分钟放款|今天大家所有人的工资|都能发|点击视频下方链接|输入手机号|就可以测试你的额度啦|实际额度以银行审批为准|息费减免优惠以具体活动规则为准|实际|小米金融消费信贷服务|300.000|MI\", \"video_asr\": \"今天答应大家的钱，一分钱都不会少，我在小米贷款有最高三十万的额度，最快一分钟放款今天大家所有人的工资。|都能吧，点击视频下方链接，输入手机号，就可以测试你的额度啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.015790224075317383 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '静态', '推广页', '手机电脑录屏', '惊奇', '平静', '全景', '朋友&同事(平级)', '室外', '喜悦', '单人口播', '动态', '配音', '填充', '场景-其他', '宫格', '愤怒', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.81', '0.40', '0.15', '0.11', '0.10', '0.06', '0.05', '0.03', '0.03', '0.03', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02dbfd77673997116cb40f4035a80cb3.mp4\n",
      "{\"video_ocr\": \"我们家那个孩子啊|今年5岁|幼儿园上大班|之前为他的英语启蒙|没少费心|走了不少弯路|因为开始想法简单了|觉得简单的英语单词|自己教一下不就好了吗|结果孩子不感兴趣|什么也记不住|今天教 他会了|明天你再问他又不会了|所以说|孩子的教育这事|还是不能马虎|要选择正规大品牌|腾讯开心鼠英语|又一次让我看到了希望|腾讯荣誉出品|专为3-8岁孩子英语启蒙打造的|课程通过动画、儿歌、游戏的形式|来提高孩子学习英语的兴趣|还有8000多个互动学习内容|带孩子打开英语启蒙的大门|时代在进步嘛|不能再用传统的教育方式|让孩子背单词了|现在报名15节课只要49块9|点读笔和教辅礼盒全部都免费送|点击视频下方|为孩子报名|适合3-8岁孩子|让孩子主动学英语|教材大礼包全国包邮|49.9元15节课|ABCmouse|Am|NotBig 成长纪念册|价值398元|超绝心建牛|WeGet aPet Ican H|包\", \"video_asr\": \"我们家那个孩子啊，今年五岁，幼儿园上大班，之前为他的英语启蒙没少费心，走了不少弯路。|又开始想法简单了，觉得简单的英语单词自己教一下不就好了吗？结果孩子不感兴趣，什么也记不住，今天教到会了，明天你再问他又不会了。所以说啊，孩子的教育这事还是不能马虎。|要选择正规大品牌腾讯开心鼠，英语说让我看到了希望，腾讯荣誉出品专门为三到八岁孩子英语启蒙打造的课程，通过动画，儿歌，游戏的形式来提高孩子学习英语的兴趣，还有八千多个互动学习的内容，带孩子打开英语启蒙大门。|现在在进步吗？不能在用传统的教育方式让孩子背单词，现在报名十五节课只要四十九块，九五点读笔和教辅礼盒全部都免费送。|即视频下方为孩子帮。\"}\n",
      "multi-modal tagging model forward cost time: 0.016059160232543945 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '静态', '推广页', '特写', '平静', '室内', '喜悦', '场景-其他', '配音', '教师(教授)', '办公室', '家', '教辅材料', '极端特写', '动态', '企业家', '单人情景剧', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.53', '0.51', '0.46', '0.27', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02edc0769bb8bc609cffc8bbe6a3d7c7.mp4\n",
      "{\"video_ocr\": \"送一个大礼包|王老师怎么了|作业帮直播课|推出这么好的课程|居然还有家长|没给孩子报名的|是不是定价太高了|语数英物化|五科才9块钱|一顿早餐的钱都还不到|而且还送出|这么大一套教辅礼盒|贵吗|报名还免费送|这么大礼盒|那这里面都有些什么啊|来来来我给你看看啊|语|五科的知识手册|还有高考小百科|以及一整套的|大数据优选模批试卷|是不是怕遇不到好老师啊|那就更不用担心了|是由清华北大毕业名师|带队教学|倾注平均教龄|10年的宝贵教学经验|在线传授高效学习方法|和各类解题大招|课后还有老师|一对一的辅导答疑|所有课程|支持3年内无限次回放|那是不是大家|不知道怎么报名啊|点击视频下方|专属链接就能报名|名额有限|售完不架|你也赶紧给孩子报名吧|百凶三三兰三|三凶看三|百/三三|弥当至号三|舌D兰三三三|百四当/1号|舌”台二/三号三|百四台三/1三m三三|舌四当三三号三|百国兰三三三号三|舌凶台三//”号三|百色三/2三|险台至三|防三|舌弥兰1|N.|三山当兰三三兰三|舌而三百星三|舌凶二三号三|1004|三凶三百吴三|08|名师有大 招|@高中生|宝书审都下高二重难点|BN 高二模拟试卷|2021-寒假|大拟试老|30g|06Rg|￥0g0w|70G|初一到高三 多科目新客特惠|初高中多科|名师训练营|寒假特惠立省490元|上课内容与收到礼盒请以实际为准|-中国女排为作业帮直播课代言-|9元|视频为演绎情节|9元13节课|新用户特惠|94164|多科目|谭梦云|C8之|作业|THPy|TMP|16|免费 赠送\", \"video_asr\": \"AT高中生作业帮直播课寒假特惠，立省四百九十元，九元十三节课送一个大礼包，售完下架。|王老师怎么了？作业帮直播课推出这么好的课程，居然还有家长没给孩子报名的，是不是定价太高了，语数英物化五科才九块钱一顿早餐的钱都还不到，而且还送出这么大一套教辅礼盒，贵吗？报名还免费送这么大礼盒，那这里面都有些什么呀？来来来，我给你看看啊，语数英物化物。|科的知识手册，还有高考小百科，以及一整套的大数据优选模拟试卷，是不是怕遇不到好老师啊，那就更不用担心了，出来帮直播课啊，是由清华北大毕业名师带队教学，倾注平均教龄十年的宝贵教学经验，在线传授高效学习方法和各类解题大招，课后还有老师一对一的辅导答疑，所有课程支持死。|年内无限次回放，那是不是大家不知道怎么报名啊？点击视频下方专属链接就能报名，名额有限，售完下架，你也赶紧给孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016132354736328125 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '中景', '静态', '平静', '单人口播', '配音', '室内', '喜悦', '教辅材料', '场景-其他', '极端特写', '教师(教授)', '单人情景剧', '动态', '特写', '拉近', '全景', '过渡页'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.96', '0.87', '0.54', '0.54', '0.52', '0.38', '0.07', '0.05', '0.05', '0.04', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02ee8411645d5d5117c9906c91af5acc.mp4\n",
      "{\"video_ocr\": \"你见过吗?|的小牛娃|双翼在线课程吧!|在线小班|外教时光|互动课堂|注意力|爱表达|双证中外 优选师资|懂孩子|剑桥、北大|无缝衔接|语言框架|5岁流利朗诵 莎士比亚|快来英孚 少儿英语|专注的|1对4|牢牢抓住|自信听说|10|100%|爱孩子更|搿|英孚 携手哈佛、|启蒙阶段|全球权威|哈佛 研究生院|语言实验室 剑桥|北大 研究课题组|To be,or not to be|that is the question|Whether 'tis nobler|in the mind to suffer|9.9元抢100节在线课|立抢价值2188元 外教100节课大礼包仅9.9元|EF|CERTIFICATE|英孚课程级别|3-6岁|国际证书|PET FCE 托福/雅思|15-18岁|探索课程|全项突破|青少年|KET|课程\", \"video_asr\": \"五岁流利朗诵莎士比亚的小牛娃，你见过吗？|WHEN IM快来英孚少儿英语翻译在线课程吧，专注的在线小班，专属的外教时光，一对四互动课堂牢牢抓住注意力，一对一外教课堂，自信听说，爱表达，百分百双胸外优选师资，爱孩子，更懂孩子，携手哈佛，剑桥北大，从启蒙阶段无缝衔接全球权威语言框架。\"}\n",
      "multi-modal tagging model forward cost time: 0.01581120491027832 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '配音', '推广页', '中景', '平静', '静态', '动画', '填充', '手机电脑录屏', '课件展示', '知识讲解', '室内', '特写', '单人口播', '宫格', '极端特写', '喜悦', '教辅材料', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.73', '0.60', '0.53', '0.29', '0.11', '0.03', '0.02', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02f18b566c1ae6754f46143f0762d4fd.mp4\n",
      "{\"video_ocr\": \"三盒抢购价只要 块六|我好饿啊|我要吃夜宵|盒抢购价要三热六|心购价只要三块六|金要三块|你们怎么都在吃小火锅好香啊|你可以上萌推上买 三渝 购价要三块六|新人专享活动|一盒只要一块二|来来来我教你|下载打开萌推APP|首页搜索真爱火锅|品|要块士|赶紧点开下方链接抢赠吧|包邮到家 下载领取新人福利业|3.6|地道的川粉|食用方法|七天无理由退换|5.00|限量1折抢|配料表|69减25|TUV WXYZ|真爱真差 真棒 真诚|火锅 婚婚 活该 霍窑 国货|折起|又劲道|萌捷推 精选好货|滑口又劲道|PQRS|英小山\", \"video_asr\": \"我号一天下来。|这次。|快点快点。|打打打打打打打打打打打打打。|快去打卡。|快来快来。|老大。|怎么之前火锅好香啊，你可以在萌推上买新人专享活动，三盒只要三块六，一盒只要一块二OOO下载打开萌推APP首页，搜索真爱火锅。|新人专享活动，三盒只要三块六，一盒只要一块二，赶紧点击下方链接抢购吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016144275665283203 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '手机电脑录屏', '推广页', '多人情景剧', '朋友&同事(平级)', '填充', '喜悦', '特写', '平静', '惊奇', '配音', '极端特写', '悲伤', '单人口播', '场景-其他', '家', '动态', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.97', '0.87', '0.84', '0.74', '0.22', '0.07', '0.06', '0.05', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02f3fd1a6001b094e291848204b6512b.mp4\n",
      "{\"video_ocr\": \"老板好|别人家能借多少钱|10万|360借条呢|别人家多久放款|5到6个工作日|最快5分钟放款|那我们现在就|放开申请|凭借本人身份证|和手机号即可申请|不够|最长要分12期还款|这会不会太亏了|从现在开始|将利息调整为万元|日息费最低0.027%|同时借4万|最长可免息30天|借4万最长免息30天|￥ヨ6口借条|￥ 60借务|￥ヨ60 杀|￥|￥到ち0借条|￥B日O借条|￥借条|￥彐目Ξ借条|￥三E◎借条|￥≡后C借条|请根据个人能力合理贷款避免逾期|贷款额度 放款时间以实际审批结果为准|360金融集团旗消费信贷品牌 最高下|最高可借20万|息费减免优惠以具体活动规则为准|贷款有风险借款需谨慎|借4万旦.|T1\", \"video_asr\": \"老板。|给人家能借多少钱？十万三六六零零借条呢？二十万人家多久放款到十五日，我们三六零借条呢？|那我们现在就放开申请玲姐本人身份证和手机号即可申请。|不够至少要八十二十分完，这会不会太亏了呀，老板，老板从现在开始将利息调整为万元，日息费最低百分之零点零二七，同时借四万，最长可免息三十天。|急用钱就用三六零借条。\"}\n",
      "multi-modal tagging model forward cost time: 0.01648426055908203 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '工作职场', '动态', '愤怒', '全景', '上下级', '喜悦', '惊奇', '远景', '特写', '企业家', '朋友&同事(平级)', '平静', '单人口播', '极端特写', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.75', '0.55', '0.44', '0.20', '0.06', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/02f8ba71c531ebb04d98a7c02e50495b.mp4\n",
      "{\"video_ocr\": \"肌友们领取免费P40的时候|一定要填写详细地址|不然就白白损失了|一部价值六千多的手机|现在来教你们|正确的领手机方式|疯读极速版现在|免费赠送手机了|点击视频下方链接|下载疯读极速版|登录看50章小说|就送10个碎片|10个P40碎片 就能免费兑换啦|最重要的是|和具实的手机号码|然后就可以|包邮给您送到家啦|现在的活动十分火爆|我们保证将每一个手机|都送到用户的手上|不信的朋友也可以试试看|也许就这么随便一试|你就真的获得了新手机|下载吧|FP40|轻松赢全新P40|疯读|一脸三宝，总拉影地腿结力|08:08|恭喜您 获得P40 Pro 5G手机 填写收货地址|￥0.00|5043-|赢手机|具体奖励以APP内活动规则为准|疯读极速版 轻松赢全新P40|已发送|双正地已尊失去一|地已经预约好了流|可口敢一这1|红美了美。|疯读极速版(...|收货人姓名|上滑解锁|洪山区|让你一开 就与众|就与众不同|04月08日星期三|申请售后 加购物车|今日已读/分钟|产品自营旗舰店|网试|手机号 详细地址|订单号: 兑换时间:|bongp|11月21日星期六 债子年十月切七|健源亮的女人|完善信息后，客服人责将在24小时内与您确认!|16146|4C721A19XPCH239|刺转术有银会司 208072|P40碎片+1|用疯|用疯读极速版 轻松赢全新P40|详|疯读成功（备货中)|实付金额:|多动联通电….. 数量:1颜色:红色 版本:128GB|华为技术有操公罚 20|福利|进网试用|ELS-ANO0|运输中 已签收|00-5043-208072|除国来了。|提交|9XPCN239|11:20|D、|Q合|HUANE|2020-07-03 15:30:23 -07-13 12:00:00|12:00:00|09/|LEICA|A口|ae|lade|ch\", \"video_asr\": \"朋友们，领取免费P四零的时候一定要填写详细地址，不然就白白损失了一部价值六千多的手机。现在来教你们正确的领手机方式。疯读极速版现在免费赠送手机啦，点击视频下方链接下载疯读极速版，登录看五十章小说就送十个碎片，十个P三零碎片就能免费兑换。|最重要的是，一定要填写详细的地址和真实的手机号码，然后就可以包邮给您送到家了。|现在的活动十分火爆，请保证将每一个手机都送到用户的手上，无限的朋友也可以试试看，也许就这么随便，一是真的获得了新手机，现在就点击视频下方链接下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022128582000732422 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '配音', '场景-其他', '推广页', '平静', '商品展示', '静态', '极端特写', '室内', '中景', '特写', '动态', '情景演绎', '办公室', '单人口播', '喜悦', '宫格', '填充', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.93', '0.59', '0.49', '0.24', '0.06', '0.05', '0.05', '0.04', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/030e2b2f65c26101e51bce9d2baa72b4.mp4\n",
      "{\"video_ocr\": \"馆FC|不好意思我以为 这些衣服你都不要了|没事没事|我就是回来拿包的|不过|你怎么穿这些衣服呀|孩子爸生了重病|光医药费 就要三十多万|我每个月工资 只有三干|负担孩子爸爸的 医药费都很难了|只能从日常生活上 省点钱了|你们家庭收入 只有工资来源吗|那怎么能行呢|你可以学习学习 理财呀|增加自己的被动收入|碰上个小病小灾的|也有钱 可以帮助你们渡过呀|学习理财还得报班吧|就那么点工资|哪有钱用来学习呀|你可以报名 蓝鲸理财课呀|市场价599元的课程|现在仅需0元|6天的课程|教你掌握基金 股票等投资工具|知名讲师一对一辅导|就算你是理财小白 也可以轻松地学会|我也能报名吗|在哪能报名啊|点击视频下方链接 就能报名了|轻松掌握基金、股票投资工具 实操教程 知名讲师全程1对1辅导|已经有486732正在学习|*视频内容为演绎情节|天小白理财 训练营|立即报名 听课方式|手机号 请填写手机号|市场价 599元|限量前300名|C|¥0|超值\", \"video_asr\": \"哎。|五。|不好意思。|没事没事，我就是回来拿包的衣服穿这些衣服呀，上次发生过重病。|光医药费就要三十万，我每个月工资只有三千，但是爸爸的医药费都负担你要投入条生活上。|你们家庭收入就只有工资来源吗？那怎么行呢？你可以学习学习理财呀，增加自己的被动收入，冲上个小冰箱，家呀，也有钱可以帮助你们度过呀，去去理财还得报班吧，我每个月就那么点工资。|哪有什么来学习。|你可以报名蓝鲸理财课呀，市场价五百九十九元的课程啊，现在只需要零元，六天的课程，教你掌握基金，股票等投资工具，知名讲师一对一辅导，就算你是理财小白。|你轻松的学会我也能报名吗？在哪能报名啊？点击视频下方链接就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016209840774536133 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '室外', '多人情景剧', '喜悦', '惊奇', '单人口播', '平静', '全景', '夫妻&恋人&相亲', '特写', '亲子', '悲伤', '(马路边的)人行道', '动态', '朋友&同事(平级)', '愤怒', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.85', '0.83', '0.15', '0.07', '0.05', '0.04', '0.02', '0.02', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03161610f8f021638de75a1d8213ad0b.mp4\n",
      "{\"video_ocr\": \"去公司面试|面试官永远会问你一个问题|这不废话嘛?|因为我没钱啊|没钱不知道努力啊|考个学历老赚钱了!|爸！我录视频呢！|隔壁小丽就去提升学历 =成人本科|几个月后就升职加薪了|你看看你|我没有基础不行的|咋不行啊?|报个|高中，大专毕业也能学啊|毕业拿国家认可学|考证书，评职称都可以|还可以这样?|不说了啊|快点击下方查看详情|就能测试你是否符合报名条件|拜拜!|低学历也能升本科|向上的|乐|学|派|向|上|的|乐帮1版|南上的乐学1派|面1上的乐|学 派|面「上「的 乐|学1派|面1上￥的余学|派|你为什么 要来我们 公司|我先去报名了|每天30分钟|升学教肓 SXMAPS|MNL|升教育|救育|R|1/125|我穷啊!|方薪|秒经$|向｜上｜的｜乐｜学｜派|OPEN|3dB\", \"video_asr\": \"去公司面试，面试官永远问你个问题，你为什么要来我们公司？|这不是废话吗？因为我没钱啊，我穷啊，没钱不知道努力啊，考个学历老赚钱了吧，我录视频呢。|隔壁小丽就去提升学历，几个月后就升职加薪了，你看看你，我没有基础不行那不行啊，报考成人本科，高中，大专毕业也能学啊，毕业拿国家认可学历，升职加薪，考证评职称都可以，还可以挣，不说了，快点击下方查看详情就能测试你是否符合报名条件啦，我先去报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016276836395263672 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '特写', '平静', '夫妻&恋人&相亲', '愤怒', '动态', '悲伤', '家庭伦理', '惊奇', '家', '极端特写', '喜悦', '朋友&同事(平级)', '单人口播', '办公室', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.91', '0.78', '0.35', '0.28', '0.18', '0.06', '0.04', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03164c76dad8c6b6d9292b99510423b2.mp4\n",
      "{\"video_ocr\": \"cut|哈喽 小可爱们好|我是作业帮直播课的袁慧老师|英文名Doris|对应的中文是桃乐丝三个字|所以大家喜欢叫我桃子老师|桃子老师本硕都是英语专业|深研英语学习7年|毕业于全国8所外语院校之一的天津外国语大学|获国内英语最高认证|专业八级证书|在6年的一线英语教学中|累计帮助50多万学员突破英语困境|在桃子老师这里|你接触的是不一样的英语世界|Now, let's do the right thing in the right way. 让我们用正确的方式做正确的事情|Let's be the master of English learning. 成为英语学习的主人|我是桃子老师|and I'll stand by you. 我会一直挺你|drew|6年教龄|高中英语教师|获得英语专业|累计帮助学员50W+|ask 问，询问|中国国家女子排球队官方教育品牌|sbick小木棍，小木解|more 更多的(量)，较多的(量)|findout 发现，弄清|paint （用顾科)绘画，箱色|at al一点都 American 美国的:美国人的:美国人in在(将来一段时间）之后|piece 张片，块|bulding 建筑物|taxi 出租车，计程车|soon不久，很快 women (woman的复数形式)|moon 月亮，月球|wore (wear的过去式)穿 bring带来，拿来|put （put的过去式)放、安放|evening傍晚，晚上 late近日春的:近深夜的:时间|history 历史|American 美国的:美国人的:美国|读|Module7|another另一个 in在(将来一段时间)之后|quarter一国钟 get 到达|(woman的复数形式) women|中国女排为作业帮直播课代言.|worry 焦虑，招心|actor 演员|cot (cultirnrr云式)购，切.割|*上课内容与收到礼盒请以实际为准 作业帮直播课 中国女排|(wear的过去式)穿|oariy 早的|lefter 信，书信|\\\"string 线，横子|英语单词语法名师课 9元 13节课|，你|worker 工人|question 问题|stay停|told|I'm Doris|16|forget忘，忘记|July七t月 remembor 记E得|专慧 #语教师|Modu|w8s 东，东|job|立即报名|best 最好的 north 北t,|包y|have a rest 休息一下|自试谁递话|rest 休息|laugh英|June 六月|theatre剧院|rode (nide的过去式)骑|不早的|naper|单词|去式\", \"video_asr\": \"单词单词单词HELLO小凯们好，我是作业帮直播课的猿搜老师，英文名DORIS对应的中文是桃乐丝三个字。|所以大家喜欢叫我陶造老师，陶老师本硕都是英语专业申研，英语学习青年，毕业于全国八所外语院校之一的天津外国语大学。|和国内英语最高认证专业八级证书，在六年的一线英语教学中，累计帮助五十多万学员突破英语困境。|在淘淘是这里，你接触的是不一样的英语世界NOW LETS DO RIGHT THING IN THE RIGHT WAY LETS BE THE MASTER ENGLISH LEARNING AND EYES AND ALL STAND BY YOU。\"}\n",
      "multi-modal tagging model forward cost time: 0.016135692596435547 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '平静', '静态', '推广页', '场景-其他', '室内', '特写', '教师(教授)', '影棚幕布', '喜悦', '配音', '手机电脑录屏', '拉近', '课件展示', '情景演绎', '动态', '教辅材料', '演播室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.84', '0.52', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/031d1d944904d038d3bc0556f9e5ad4a.mp4\n",
      "{\"video_ocr\": \"这个月省下了500块生活费 又可以给家里多打钱了|大哥 你没事吧|没事...|你工作这么辛苦|就吃泡面呀|你一定是营养不良才晕倒的|我都习惯了|让我歇会就好|大哥 你把手机给我|怎么了|我帮你报名一个 快财商学院吧|这是啥|一个教你赚钱的课程|0元加入一对一辅导|18个低风险赚钱技巧|22种赚钱避坑指南|带你系统的从攒钱思维|到赚钱思维的改变|手把手教会你钱生钱|非工资收入|比工资收入还高呢|你以后就不用这么辛苦了|投资有风险，选择需谨候风险责仁由购买者自行承担|险贵王由购买者自行承担 投资有风险，选择需谨慎，|投资有风险，选 责任由购者目行承担|投资有风险，选择道，壳由购买者自行承担|麽谨，风险责任由购买者自行承担|理财上快财|UODIXe|快财 直播\", \"video_asr\": \"AS。|三。|大哥你没事吧。|哦。|啊，没事。|你工作这么辛苦就吃泡面呀，你一定是营养不良才晕倒的，我都习惯了，你让我歇会就好哎，大哥，你把手机给我。|我怎么啦，我帮你报名一个快财商学院吧，是啊，一个教你赚钱的课程，零元加入一对一辅导，十八个低风险赚钱技巧，二十二种赚钱的坑指南，带你系统地从攒钱思维到赚钱思维的改变，手把手的教会你，钱生钱呗，工资收入比工资收入还高呢，你以后呀，就不用这么辛苦了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01613593101501465 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '平静', '路人', '夫妻&恋人&相亲', '手机电脑录屏', '惊奇', '全景', '喜悦', '单人口播', '动态', '悲伤', '朋友&同事(平级)', '极端特写', '愤怒', '特写', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.77', '0.62', '0.51', '0.36', '0.31', '0.07', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/032642630b09c1e6581f57635296d481.mp4\n",
      "{\"video_ocr\": \"老公|给我几个橙子|老公给我几个火龙果|飘了呀|好多老客户|在我的|辣妈计划网店里|下单了呦|啥是辣妈计划|让每一个辣妈|都能拥有自己的网店|数百个导师|手把手教你|开店卖货|数万款精选好货|不用囤货进货|商品卖出后|平台还会直接帮你发货|再也不用起早贪黑|出去打工了|也有更多的时间|去陪宝宝了|你也赶快点击屏幕|了解详情吧|对呀，我都用了半年了，666|嗯嗯，今天试了一下，质量真不错|啊，哪买的?给我推荐下|嗯嗯，直接网店下单就O啦!|下啦!|恭喜发财，大吉大利|恭喜发财，|位置:店铺首页 备注： 点击查看详情以获得更多访问信息。|¥98|独立小包装营养更均衡|下午401|佳佳妈妈，你家那个推车真的只要|佳佳妈妈，那款红豆薏米粥味道真不 错!再给我来十盒!|下午3:59|发财，|大吉大利|[2.20发货]咪克玛卡乳|辣妈计划网店，我发给你|6666|我这边来了一款补气养血茶，一会给 你送一盒先尝尝!|2月22日下午15:03|99?|身份:来自[球球和她的小伙伴]的客户 时间: 2020年02月22日12时29分|¥36 ¥59.9|39.66|亲，过去1小时内，总共有1名好友访问了你的店铺，|辣妈集市|鸭蛋4枚|可以|快来看看吧。|香港众星STARS什锦果仁 142g|[东北大米]裕插利东北特 产高品质新米鸭田稻米一|经典排装牛奶巧克力10..|随机发货]拉面说新品一|下午5:17|查看详情|扇贝3斤|DEF|日15时03分|LA MA JI HUA|没事|发送|人员来访提醒|日同粥|快递|我好 对呀|我给|我这刚 去吃饭了|我这 我|[2.17发货]喜普花房双|MNO|获得更多访问信息|ABC|对呀|一会|WXYZ|贝贝妈|子豪妈|夏妈在家|TUV|起I|秦起拼|销售部|JKL|GHI|PQRS|分词|换行\", \"video_asr\": \"老公我几个橙子。|老公给我几个火龙果。|香的呀，好多老客户都在我的辣妈计划网店里下单了哟，儿子计划了吗？计划让每一个妈妈都能拥有自己的网店，都摆个老师手把手教你开店卖货着万块。|你选好货不用存货，结果商品卖出后会直接帮你发。|但也不用起早贪黑出去打工了，也有更多的时间去陪宝宝了，你也赶快点击屏幕了解详情吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01638054847717285 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '手机电脑录屏', '多人情景剧', '单人口播', '喜悦', '惊奇', '特写', '办公室', '室内', '家', '平静', '场景-其他', '动态', '工作职场', '配音', '朋友&同事(平级)', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.92', '0.39', '0.15', '0.13', '0.12', '0.08', '0.07', '0.05', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/033c287c017b3455a788558ea77c9492.mp4\n",
      "{\"video_ocr\": \"否则 O-o|我就把你那晚的事说出去 OO|你是说 0.O|那晚我和她 Q、O|一夜定情的事是吗|那晚那么黑|你怎么可能记得她|是你|00|《今夜星辰似你》|免费看书100年 七猫免费小说|七猫免费小说APP|(本故事纯属虚构)|O、0|阅读中含有广告内容\", \"video_asr\": \"否则我就把你那晚的事说出去，你是说那晚我和她。|你也听什么说什么，那我那么黑，你怎么可能还记得她是你。|免费看书，一百年七猫免费小说。\"}\n",
      "multi-modal tagging model forward cost time: 0.01592278480529785 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '多人情景剧', '特写', '静态', '平静', '愤怒', '极端特写', '家', '动态', '惊奇', '喜悦', '夫妻&恋人&相亲', '家庭伦理', '悲伤', '手机电脑录屏', '办公室', '室内', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.93', '0.81', '0.80', '0.50', '0.21', '0.12', '0.11', '0.09', '0.09', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/033d80f07c29a8f32c51d20a9f093670.mp4\n",
      "{\"video_ocr\": \"我是高途课堂周帅老师 新用户专享|省级高考状元|高考数学满分|在15年教学生涯中|我总结了|不只数学|英语、语文、物理|都有清北毕业名师在线授课|4科16节课只要36元|无论你基础如何|我们都能帮到你!|[查看详情]就能报名!|Cbehind the scene|Cbehind the|C.up|D.above tl|above the law|C.meet|浙江卫视指定在线教育品牌|D.casually D.well|.quarel|D.casually|新用户专享 立即体验|高途课堂名师特训班|高途课堂 名师特训班|D.blame|B.expect|C.handle 59.A.stress|以前的我|偷瞄一眼|现在|周帅|高中数学资深主讲老师|B.secretly|B.shop|B.trick|55.A.work|80个易错点|57个难点失分点|华少|54.A.lesson|58.A.out of sight|52.A.by chance Con purpose|53.A.nude|B.such|B.st|439个知识点|167个考点|Ccareless|Cpatient with|D.apologized|50.A.relaxed|Csurprised Cprayed|Cr|Cskill|浙江卫视|D.on duty|n duty|D.lonely|D.qu|C.other|D.embarrassed|B.polite|B.displeased with|B.|B.decide|44.A.lorget|45.A foods|A.agrce|B pretending|60.A.ruins|B.mal|B.makes|两读下面短文，从短文后各赐所始的A.B.C和D四个透项中，选山可以填人空白处的最值选项， Aas a busineswoman. l care doeply about ty cusomen Bunt like anyone ior wham you feel affection. dl cmn aiso crive you mad. Theyllcone runhinx in. 42 thcir handbag becn stolen.|网该下面短文，从想文后各题所恰的A.B.C和D四个选项中，远出可以填人空白经的最佳选项， Asa businesswoman. l care deeply about ty cukomer. But like anyoe fo whom youfeel|同滨下面短文。从短文眉各题所给的A.B.c和D四个选项中，想出可以填人空自处的量佳选项， Asa businesswomuan,fcare derpby about ty cuseen.But like unyone tie whom yoa tio!|阿:下面照文，从短文后各题所抢的A.B、C和D限个透项中，远出可以填人空自处的最佳选项，|阿读下面题文，从短文后各题所给的A.B.C和D四个店项中，选出可以哦人空白姓的最佳透项。 Aa abusinesswomuan., lcare dorply abou ty cuamen. Bua lke asyone tir whom you leel|As a businesswtman.,l cure doeply about my cuamek Biut like aryope tor whom you fioel|Asa buinesswtman. lcare doeply about my cusaeerx. But hke nryone lor wthom youfoel|Asa busihesnwoman.lcare doeply abou my cusomons. Bea like aryooe lir wbom yoafieel|A.shopkeepers 42A.saying B.custorners|A|B.displeased with|B.delighted|D receptionists|day|ionists|51.A.searched|D.natural|D.goods D.changing|C.match|C.personally|C.pers|sually|B.diferent|B.smiling|law|D.assume C belongings|46.A.particular 47.A.fighting|D.dificult|56.A.kindly|57.A.ready|C discover|Cimagine|sta|C hanc|C star|starts|C matching|B.argued B.by herself|￥9|C guessing D.replying|D.swear|found it too much|the berinnipg. She had pl|C.waiting|B.away|B.in the way|B.in the way|48.A.generous 49.A.curious about|was right and l was rather 50 that shc became a `regular’. Aner a while. shae 5ltor the way she bchaved at the beginning. She had split up with her husband the week before. was living in afiat52 mnd since she'd|warunich and lwas rather 50 that she became a \\\"regui\\\". Aner a while, she 5llior the way she bchavod at|asriohu and l|wasnidh and lwas rather|wasuridh andlwas rnather 50 that she became a `\\\"tegukar’ Ahera while. she5lfor the ay she behaved at theheoinning.She had split up with ber husband the week before, was living in anat 52 nd since she'd|wasuriohe and lwas rather 50 that she became a \\\"regulr Ahera whie. she 5l for the way sbe behuaved at the besinnling.She had split up with her husband the week belorte, was living in anat 52 and sinoe she d|wasuricht and l was rather 50 that she became a \\\"regular’. Aher a while. she 5lfor the way she behavod at the bevinning.She had splt up with her husbund the week before. was living in afat 52and since she' d|全国百佳教师带队教学 平均教龄11年|D.sves|D uncertain about|立即体验|B.catalogues|B.difl|That taught me a valu customer is rude or o|not 5Z’Always waterl|not 57\\\"Always|customer is rude or diflicult. jus not5Z.\\\" Always water it down|C salespersons|the whole thing develops into an unplcasant scene and that 60 everyonc's day|68,lflyou do, you wont be able to 59i and|not 5Z’Aways water it down and dont et your cgo(自我)pu 58，If you do, you wont be able to 59it and|the whole thing develops into an unpleasant scene and that 60 everyone’s day,|found it too much to cspe|That taught me a valuable 5f andl pass it on to the people who 55 in the market. Don't take it 56 not 57Z’Aways water it down and dont et your ego(自我)get58.If you do, you wont be able to customner is rude or difficult. just think “Maybe she'shad a row with her husband. Maybe her chil|\\\"Maybe shes had a row with her husband. Maybe her child's|That taught me a valuable 54 andlpass iton to he people who 55 in the marke. Donttake it b6 la  dthink \\\"Maybe she'shad a row with her husband. Maybe her child’s|with her husband. Maybe her childis|nd Maybe her childs u won`t be able to59itand|customer is rude or difficult. just think “\\\"Maybe she's had Uyou do, you won’t be ableto59itand|customer is rude or dificult. just think “Miaybe sheshad a mwujth her husband. Maybe her child's|not 57|the way she behavod at|Theyl 43 that theylet it in the charging room, create havocE品)and thcn tit hnd been|affection. 4l.can also drive you mad Theyll come rushing in. 经2their handbgs bemn solen|Theyu 43 that they letit in the changing rom, crute havoco,园 and thmn 4lhad boma in their ar all|Theyll 43 that theylet it in the changping room. crtae hvoet能乱 andthe j4thad been in their car all|aflecion, 4l can also drive yoe mad They\\\" ome ruashing in. 42their handbags been stolen Theyll43that theyletit in the changing roum, crote havoct服乱) mndthe 44lhad been in their car all|the whole thing develops into an|found i too much to cope wih (应对h.she'd tnken it ou on 53 people|lound it too much to cope with对.shed taken it out on 53 people.|found it too much to cope iii对h. she d taken it out on 53 peopie That taught me a valuable 54 andlpass it on to the poople wtho 55 in the market. Dont tnke i 56. 1a|They1i 43 that they left it in the chaniging toom. create hawocd混 乱) and then 44.l had been in their car al the time Dheyll have out half the s5 in the shop,and want the only wtyle you don't haove lenl ina.s6 colour|Theyll 43 that they leht it in the chanding toom. create hwvocdl 乱) and then s4.ll had boen in their cxr all thatimeaDheyl have out half the s5 in the shop, ahd want theo only ayie you dont huve lio ina s6 oolour|the time pheyl have out half the 45i in the shop, and want the cnly style you dont have lch tna 56 colour.|the time.apaeyll have out hall the 45 in the shop,and want the only tyle yu dont have lehina s6.colour.]|thetime Dheyl aveout half the 45 in the shop, and wan the only styie you dont have lenina 46 colour.|do know how upset the shop stalft can gt. butltry to peruade them to koep 4Z lremember the finst really s8 customer we had at Covent Garden. She was 49 absolutely everthing. was right and l was nather 50 that she became a \\\"regular’.Aner a while. she 5lior the way she bct|do know how upset the shop staf can gut. butltryto perude theu lokees (7 I remerber the finst really 48.customser we had at Covent Gi rden She was 49. absolutely everything. nothing|autely evenything. nothing|do know how upset the shop safl cn sp Iremember the fint really|do know how upoe the shop stafl can get. butltry bopersuade then to keep 47 Iremember the finst really 48 customer we had at Covent Garden. Shewas 49 abnolutely eerything. nothing|1夕|高途|仅需\", \"video_asr\": \"回首掏。|幽鬼刀一开看不见走位，走位手里干难受。|嗯。|嘿嘿。|我是高途课堂周帅老师，省级高考状元，高考数学满分，在十五年教学生涯中，我总结了四百三十九个知识点，一百六十七个考点，八十个易错点，以及五十七个难点，失分点不止数学，英语，语文，物理都有清北毕业名师在线授课。|四科十六节课，只要三十六元，无论你基础如何，我们都能帮到你，查看详情就能报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.01635599136352539 sec\n",
      "{'result': [{'labels': ['中景', '现代', '推广页', '单人口播', '室内', '填充', '教师(教授)', '静态', '平静', '多人情景剧', '学校', '手写解题', '家', '极端特写', '动态', '朋友&同事(平级)', '全景', '混剪', '场景-其他', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.72', '0.09', '0.07', '0.06', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0345e20c72728ffedafe8eb17aa748ca.mp4\n",
      "{\"video_ocr\": \"少年心气散尽|中年修为没来|看事还没看透|花钱还没花够|上有老下有小|简单点说|就是不上不下的阶段|在职场上|年轻人犯个错|叫血气方刚|我们犯个错|危机四伏|你是否想过|工作是体力的透支|回报是有限的|其实呢|如果80后进行|资产配置 合理的资产配置|就能大大增强|生活抗风险能力|通过组合基金|证券保险等理财工具|就可以从压力|里解脱出来|如果你不擅长这些技巧|推荐你报名|小白理财训练营|每天巧5分钟|7年投资经验|上市公司理财顾问|手把手教你|如何配置资产|投资理财|5天课程|仅需1元|点击下方查看详情 报名吧|80后如何 让赚钱轻松点\", \"video_asr\": \"少年心气散尽，中年修为没来，看事还没看透，花钱还没花够，尚余老下雨，简单来说就是不上不下的阶段。在职场上，年轻人犯了错，教学系方罡，我们犯了错，危机四伏。你是否想过，工作是体力的透支，回报是有限的，其实呢？|八零后进行合理资产配置，就能大大增强生活抗风险能力，通过组合基金，证券，保险等理财工具。|就可以从压力中说出来，如果你不擅长这些技巧，推荐你报名小白理财训练营，每天十五分钟，七年投资经验，上市公司理财顾问手把手教你。|如何配置资产？投资理财五天课程限时只需一元，点击下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016172409057617188 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '单人口播', '平静', '多人情景剧', '喜悦', '办公室', '场景-其他', '惊奇', '室内', '动态', '夫妻&恋人&相亲', '配音', '特写', '家', '工作职场', '拉远', '拉近'], 'scores': ['1.00', '1.00', '1.00', '0.98', '0.79', '0.70', '0.27', '0.26', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/035b30335b8cd7d0b9e2c1e98b35f06c.mp4\n",
      "{\"video_ocr\": \"为什么我们不得不学好Excel?|小李小李|咱去吃饭吧|我这报表还没做完呢|下班咱去健身吧|不了不了|我的数据表还没分析完|我们分手吧|Excel 工作小白看过来了|4天3课|每天20分钟|30个免费模板|0基础Excel|- 实战训练营开课啦|智能AI交互授课|一对一导 师指导|一对一导师指导|三天培训课程|教你做出老板喜欢的|可视化数据报表|提升工作效率|现在报名只需8.9元|点击视频 解详情吧|5月6日 572615|点击产品类别对应的透视表 211192|T60179|请先点击切片器的“深圳”按钮，根据生成的柱|魔品 6000|业情总额|总计|王大刀|138402|广11 16390|销售城市|广州|设置数据系列格式|制作日期:|北京|TRUE|55894 816|液锁赵小平 表姐|私小平|2点击[确定)|状图，哪位销售员在深圳的业绩最差?|请先点击切片器 状图，哪位销售|风龙 5100n|宝贝|用同样的方法，我们为产品类别对应的透视表建一个饼国:点击第一个透视表，点|@第二个框输入”成立的结果:1000 分公司|F2|点击[分析]选项卡|表姐牌口罩 “年度业绩汇报|表姐牌口罩 年度业绩汇报|凌祯|纯棉口家|15028|第三个框输入“不成立的结果”:300，然后点击 D3-C3|Logicnl test|样式|单元格|业霸目标 实际业城|IF(D3>C3)|B3>C3，1000)|医用口票|A张盛茗|武汉|香则，奖金300|图合并戒题中 %，|查找和选择|奖金300 妈际业绩>业绩鼎标，奖金1000来否则，|实际业绩>业绩目标，奖金1000;否则，奖金300。|奖金1000:否则，奖金300。 实际业绩>业绩目标，|Ctrl|24%|明量口|95口|Value if false|1000|谓先点击切片题的保驯“按蛆，棍握生成的柱|D.凌祯|Value jf true 是 logical test为 TRUE时的返回值，如果忽略，则返回TRUE.IF|产品类别 业绩总额|上海 深圳|对齐方式|剪贴板|不同的配色方室|帮助 福听PDF|-IF(D3>C3，100|数据 审 视厨 开发工具|福MPDF 帮助|每天20分钟 下B0个免费模板|67、|张兴华|4T03|审阅 视固 开发工属|审阅|数据|开发工具|12:00AM|5:30PM|Excel实战训练营|茶营|C赵小平|B.王大刀|秒可督学一|文件|插入|开始|提作说明搜素|李大伟|业绩奖金|贝画布局|确定。|3课|15|共享|绘图|南昌|赶紧\", \"video_asr\": \"小李小李，咱去吃饭吧不了，我这报表还没做完呢。|咱去健身吧，不了不了，我的数据表还没分析完，不了不了。|工作小白看过来了，零基础到实战营开课了，智能AI交互授课，一对一导师指导，三天培训课程，教你做出老板喜欢的可视化数据报表，提升工作效率。现在报名仅十八点九元，赶紧点击视频了解详情！\"}\n",
      "multi-modal tagging model forward cost time: 0.02245926856994629 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '中景', '静态', '多人情景剧', '场景-其他', '工作职场', '办公室', '悲伤', '特写', '配音', '朋友&同事(平级)', '惊奇', '极端特写', '平静', '推广页', '喜悦', '愤怒', '企业家', '上下级'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0362216e043df96fbb844c42aa8b4c93.mp4\n",
      "{\"video_ocr\": \"想知道我是如何 搞定老板的|评估下|新的薪酬制度的影响|分析下最近各部门|工作上的问题|找一下近日|离职率高的原因|好的老板|very good|秘诀其实是|圈外同学数据分析课|通过课程|我学会了用数据分析问题|并找到最合适的方法|解决问题|提升了个人职场技能|完成了老板安排的各项工作|我下个月又要涨薪了哦|挑战年薪30万!|圈外同学|外同字|圈タ」了|匿外同|数据分析课 提高个人职场竞争力|点击视频关注公众号后报名|8.9元学4天|SHORT VIDEO\", \"video_asr\": \"评估下新的薪酬制度的影响。|分析一下最近都可能工作上的问题。|找一下近日离职率高的原因，好的老板。|同时不得。|想知道我是如何搞定老板的秘诀，其实是从外同学数据分析，可通过课程我学会了用数据分析问题，并找到最合适的方法解决问题，提升了个人职场技能，完成了老板安排的各项工作，我下个月又要涨薪了。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016289234161376953 sec\n",
      "{'result': [{'labels': ['现代', '工作职场', '静态', '中景', '推广页', '特写', '平静', '上下级', '办公室', '多人情景剧', '企业家', '室外', '动态', '单人口播', '愤怒', '情景演绎', '路人', '全景', '朋友&同事(平级)', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0372554e1799fcedd31383169540698c.mp4\n",
      "{\"video_ocr\": \"大少|咱们天耀的防御系统被入侵了|银狐|你果然还是来搅局了|妈妈你在看什么呀|这是木木和妈妈之间的秘密哦|嘘|难道是她|爸爸|老公 你回来了|天枢十二殿会议今晚十二点|银狐会来|银狐你到底想要什么|你猜呀|娶了一个村姑回家|真是丢了我们慕家的脸|少棠|坏人|落落|天耀传媒学权人|天枢十二殿银狐|慕程北 慕氏集团二少爷|《爹地，大快妈咪掉马了》|看这对夫妻如何互揭神秘身份|她是天枢十二殿的超级黑客|他是天耀传媒的掌权人|本故事纯属虚构|番茄小说|这薇的礼|10\", \"video_asr\": \"大少，咱们天耀的防御系统被入侵了。|你果然还是来搅局。|三。|ZZZZ。|嗯。|爷爷。|妈妈你在看什么呀。|我和妈妈之间的秘密。|难道是他爸爸老公你回来了？高手天数十二就会一定的，十二点并不会很好。|别哭，你到底行不行。|你猜。|娶了一个村姑，真是丢了我们穆家。|快。|对不起。|洛洛。\"}\n",
      "multi-modal tagging model forward cost time: 0.01622462272644043 sec\n",
      "{'result': [{'labels': ['中景', '推广页', '现代', '静态', '填充', '多人情景剧', '特写', '愤怒', '极端特写', '平静', '动态', '夫妻&恋人&相亲', '喜悦', '拉近', '惊奇', '室内', '悲伤', '拉远', '手机电脑录屏', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.86', '0.84', '0.83', '0.60', '0.47', '0.37', '0.24', '0.23', '0.08', '0.05', '0.04', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0390641555f61597e691a7959cdefba4.mp4\n",
      "{\"video_ocr\": \"仅仅一科|就和其他同学|拉开了非常大的差距|其实解决英语偏科很容易|只需做好一个步骤|跟我玩转高中英语提分尼体系|先从简单题和同类型题出发|逐点突破 循序渐进|好的方法|远比盲目努力更重要|谁能在最短的时间内|找到最高效的学习方法|谁就能在高考中实现翻盘|我是张冰瑶老师|香港中文大学硕士|有近十年的一线教学经验|我是徐磊老师|14年高考一线教学经验|加入我的|高中英语抢分课程|不论你什么基础|只要你认真学习|一定会有收获|赶紧点击视频下方|查看详情|抢它!|一数学｜英语|中|国重点高中提的招生考试全多(三十|清北名师授课|高中英语强化特训营|山0|24+9|数英专项 提分训练营|hDE B.0 日R|点D.E B.D Ga无R|既以xD旺b.0=b|清北毕业|秒杀难题秒出答案|指导价1688|囚重点高中提前招生考试全真试卷(二十|全国重点高中提前招生考试|名师带队教学 平均教龄 11年以上|D.8D的长|一个正是|先到先得|新同学9元报名|老师教的好孩子才能考得好|<K<左滑视频即可报名|跟谁学 在线学习更高效|在kAME中，W伽-w4年专|2019年自支超生)设实数不，满是小2y5.y2*5.用，|课程仅需:9元|D.-24|基224|22-2|至)\", \"video_asr\": \"一个学生数学月考一百四十分，但英语只能考四十九分，仅仅一科就和其他同学拉开了非常大的差距。其实解决英语偏科很容易，只需做好一个步骤，跟我玩转高中英语提分你体系，先从简单题和同类型题出发。|逐点突破，循序渐进，好的方法远比盲目努力更重要，谁能在最短的时间内找到最高效的学习方法？|谁就能在高考中实现翻盘？我是张彪老师，香港中文大学硕士，有近十年的一线教学经验，我是徐磊老师，十四年高考一线教学经验，加入我的高中英语抢分课程，不论你什么基础，只要你认真学习，一定会有收获，赶紧点击视频下方。|查看详情抢他。\"}\n",
      "multi-modal tagging model forward cost time: 0.0163266658782959 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '静态', '教师(教授)', '填充', '场景-其他', '配音', '平静', '特写', '室内', '影棚幕布', '教辅材料', '拉近', '手写解题', '情景演绎', '宫格', '手机电脑录屏', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.91', '0.88', '0.76', '0.71', '0.33', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03941c88bcde0700eb440a9925e6368e.mp4\n",
      "{\"video_ocr\": \"我的成绩一直平平无奇|家里对我 能不能考上大学|也不抱有太大希望|更别说双一流名校了|对我来说|更是可望而不可及的|直到我用了高途课堂|听了上面 北大清华毕业的名师|分析讲解后|我才知道|高考并不是一件 很难的事情|上面的解题技巧|确实让我在学习中|如同乘风破浪|学校更注重 培养孩子的基础得分|老师花3年的时间|反复讲解一些|常见的题型|很少对重难点题型|进行详细分析|而高途课堂|则是教学生 考试中的考点|和重难点|从更专业的角度|如何快速提分|看着我自己成绩|一点点的进步|我终于意识到|我找到了属于我的 一套学习计划了|我成功了|祝你在学习中|赶紧点击 视频下方的链接|来体验一下|9元16节名师课的|考点精讲｜真题训练｜名师指导|手精通高等学校招生全国袜一考试(全国卷)|四行写结果|敌列相关问题|1|全国百佳教师带队教学平均教龄11年|2带翻标疑|如果超一次必须要一致|我们上课讲的对位相减|直援四行写信里|全部秒杀|华少|2场R|新用户专享立即体验 浙江卫视指定在线教育品牌|浙江卫视|S*|N*、|“”$|P等|名师特训班|仅需|￥9\", \"video_asr\": \"我的成绩一直平平无奇，家里对我能不能考上大学也不抱有太大希望，更别说双一流名校了，对我来说。|更是可望而不可即的，直到我用了高途课堂，听了上面北大清华毕业的名师分析讲解后，我才知道，高考并不是一件很难的事情。|上面的解题技巧确实让我在学习中如同乘风破浪，学校更注重培养孩子的基础得分，老师花三年的时间反复讲解一些。|见的题型很少对着难点题型进行详细分析，而高途课堂则是教学生考试中的考点和重难点，从更专业的角度去教学生如何快速提分。看着我自己的成绩一点点的进步，我终于意识到，我找到了属于我的一套学习计划，我成功了。|我做到高途课堂，助你在学习中乘风破浪，赶紧点击视频下方链接，来体验一下九元十六节名师课的高途课堂吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016152381896972656 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '多人情景剧', '静态', '特写', '平静', '动态', '极端特写', '室外', '悲伤', '亲子', '全景', '家', '单人口播', '愤怒', '家庭伦理', '单人情景剧', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.90', '0.70', '0.65', '0.26', '0.15', '0.13', '0.09', '0.06', '0.04', '0.04', '0.04', '0.02', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03948d62146128e1537ef064d581cd32.mp4\n",
      "{\"video_ocr\": \"高中要想不掉队|数学至少拿下130分|高中数学503个 必考知识点|176个常考易错点|42个几何模型|和9大代数考法|你家孩子都掌握了吗?|现在有一个机会|只需要9块钱 就能改变孩子的未来|没错|高途课堂高中 全科名师班|语数英物四科高质量 直播课仅需9元|主讲名师来自于 每年能输送|上百名清华北大 学子的一线名校|有全国百佳教师|中高考研究专家 和省高考状元|平均教龄11年|总结了语文36技|数学7大必考模块|英语4小时高效学习 和物理108技|专攻孩子答题思维 和解题技巧|课程3年内支持 无限次回放|您放心买 让孩子安心学|查看详情 抓紧报名吧|U1T|XOLOMSIA|COLOMHMIN|VENEHELA|tMf7EA|全国百佳教师带队教学 平均教龄11年|高途课堂｜园浙江卫视|浙江卫视|2|名师出高徒·网课选高途|： 学生抢分秘诀，孩子高分不|新用户专享 立即体验 浙江卫视指定在线教育品牌|【快结束了】高途课堂直播课|名师特训班|用愁!|华少|pr ￥9|仅需\", \"video_asr\": \"高中要想不掉队，数学至少拿下一百三十分，高中数学五百零三个必考知识点，一百七十六个，常考易错点，四十二个，几何模型和九大代数考法，你家孩子都掌握了吗？现在有一个机会，只需要九块钱就能改变孩子的未来，没错，高途课堂高中全科名师班，语数，英物四科高。|直播课仅需九元，主讲名师来自于每年能输送上百名清华北大学子的一线名校，有全国百佳教师，中高考研究专家。|为省高考状元平均教龄十一年，总结了语文三十六计，数学七大必考模块，英语四小时高效学习和物理一百零八技。|专攻孩子答题思维和解题技巧课程，三年内支持无限次回放，你放心买，让孩子安心学，查看详情抓紧报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016887664794921875 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '单人口播', '静态', '平静', '室内', '配音', '家', '动态', '喜悦', '教辅材料', '场景-其他', '办公室', '情景演绎', '混剪', '单人情景剧', '拉近', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.84', '0.24', '0.23', '0.14', '0.14', '0.08', '0.07', '0.05', '0.03', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/039b2767573e1f81c5da0b7e535d9e77.mp4\n",
      "{\"video_ocr\": \"我的天 莹莹|哇哦|你这数据整理的|怎么这么快|开玩笑|这可是我花了|重金学的Python|做表格|只需简单输入几行代码|上百个表格|5分钟就能整理完了|以后再也不用加班了|啊 我也想学|咱两拼个团吧|你还要拼团|能不能有点格局|怎么可能才3.9|我上的辅导班|都要好几千呢|你看现在扇贝编程|4天的课程只需3.9元|那我去哪报名|扇贝编程Python课呀|点击视频就可以报名啦|￥3.9立即体验|学什么 经典·组·合 Python基础课+认知课|条件分支|专业教研团队|教学特色|Fx|王子期|交互式课堂+作业实操+每日一练|三大数据类型 10+编程作业|Python基础课|优惠名额1001 限时优惠|0048:22|理论|购课即送|课程顾问|Python自动化办公|学完后将收获|¥3.9|涨薪季|掌握编程思维 认识常见函数|完成10+编程练习|初识Python 了解基础语法|该视频为情景演绎|扇贝编程|3\", \"video_asr\": \"这天夜里这个数据整理的怎么这么会开玩笑，这可是我花了重金取得太子做表格，只需要简单的输入几行代码，上百个表格啊，分钟就能整理完了以后啊，再也不用加班了。我也想学咱俩拼个团吧，三块九拍刺客，你还要拼团，能不能有点格局啊，怎么可能才三块九，我上个辅导班。|好几天了，你看现在下载编程推出的PYTHON课，四天的课程只十三点九元，那我去哪里报名？三倍变成开城门。|点击视频就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016529321670532227 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '特写', '手机电脑录屏', '喜悦', '朋友&同事(平级)', '工作职场', '动态', '悲伤', '平静', '极端特写', '办公室', '惊奇', '室内', '愤怒', '夫妻&恋人&相亲', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.94', '0.86', '0.73', '0.68', '0.26', '0.18', '0.08', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/039b522c898ce9d47ca8f3751dfaa90f.mp4\n",
      "{\"video_ocr\": \"过节想去哪|隔壁小溪上了猿辅导网课|他的老师还是北大毕业的|我也想上|猿辅导在哪上|直播网课在家就能上|给你报|49元14节直播课 小学数学暑假系统班|现在报名加送5铅笔+ 5贴纸+1错题本+1草稿本|立即报名|猿辅导\", \"video_asr\": \"嗯。|过节想去哪？隔壁小区上了猿辅导的老师还是北大毕业的，我也想上猿辅导在哪上直播网课，在家就能上给你报。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016323328018188477 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '特写', '家', '亲子', '喜悦', '家庭伦理', '惊奇', '夫妻&恋人&相亲', '平静', '朋友&同事(平级)', '动态', '悲伤', '愤怒', '拉近', '极端特写', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.92', '0.67', '0.40', '0.40', '0.19', '0.16', '0.06', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/039e0390ffe107d1459841153d86838e.mp4\n",
      "{\"video_ocr\": \"都怪你|总让我用笨方法学习|乐乐他们早都用|解题大招做题了|我也没想到|高途课堂初高全科名师班|效果居然这么好|平均教龄11年的|北大清华毕业名师授课|还能教你们|各种解题大招和方法|之前和你说了你还不信|乐乐他们才上了几节课|成绩都已经进步很多了|你别着急|妈马上去给你抢课|你们说的是啥课啊|还是北大清华|毕业名师授课|教孩子掌握必会知识点|还有考点 易错点|难点和失分点|孩子掌握了|成绩自然就提上去了|而且语文数学英语|物理都有|课后还有辅导老师|1对1答疑呢|这课这么好|一定很贵吧|不贵|现在9元就能上|4科16课时|不仅在家就能学|而且课程三年内|还可以无限回放呢|那怎么报名啊|我给我儿子也报一个|点击视频就能报名了|极尽热战|极挑战|高途课堂|极限会松饭|PETiM|名师出高徒.网课选高途|同 P|FIGHTING!|EASYLOGe|EASY'TIOG|EAST nGO|LOGO|王先意老师ィ3年教学经历 周帅老师省级高考状元 胡涛老师北京大学硕士|EAYT0G|BAST10C|BASTOG|EASY TLOuO’|《极限挑战》第六季官方推荐 中小学生在线教育平台|BAY'LOG|EAY\\\"LO|EASTTO|即刻出发 逆袭学霸|OGo|Toco|名师在线 挑战极限|2020全科名师班|BALO0\", \"video_asr\": \"哎呀，都怪你总让我用笨方法学习了了，他们早都用解题大招做题了，我也没想到啊，高途课堂初高全科名师班效果居然这么好，教龄十一年的北大清华毕业名师授课，还能教你们各种解题大招和方法，之前和你说了，你还不信了了，他们才上了几节课，成绩都已经进步很多了。|急嘛，马上去给你抢课啊哎，你们说的是啥？可还是北大清华毕业名师授课，高途课堂，初高全科名师班，北大清华毕业名师授课，教孩子掌握必会知识点，还有考点，易错点，难点和失分点，孩子掌握了解题，大招和方法，成绩自然就提上去了。而且语文，数学，英语，物理都有课后啊，还有辅导老师一对一答疑呢。|这么好一定很贵吧，不贵，现在九元就能上四科十六课时，不仅在家就能学，而且课程三年内还可以无限回放。|那怎么报名啊，我给我的那一抱，点击视频就能报名啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016421079635620117 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '室外', '静态', '喜悦', '全景', '单人口播', '动态', '路人', '平静', '极端特写', '惊奇', '亲子', '特写', '家庭伦理', '愤怒', '配音', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.84', '0.79', '0.79', '0.76', '0.16', '0.14', '0.11', '0.10', '0.02', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/039e4dba4622d827caf532a2347f963e.mp4\n",
      "{\"video_ocr\": \"9元9节 在线少儿美术课|为4-12岁孩子 量身定制 1对1教学·美院师资·家长可旁听|美术宝1对1|立即报名\", \"video_asr\": \"我们是一群小小的羊，小小的羊儿都很善良，善良。|只会在草原上懒懒的美美的晒太阳，虽然邻居住着灰太狼，虽然有时候没有太阳。\"}\n",
      "multi-modal tagging model forward cost time: 0.01668238639831543 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '配音', '极端特写', '绘画展示', '静态', '才艺展示', '填充', '平静', '重点圈画', '动画', '手机电脑录屏', '手写解题', '课件展示', '教辅材料', '宫格', '转场', '幻灯片轮播', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.96', '0.93', '0.26', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03a09d55fe535c01a2fb26bd4b383f30.mp4\n",
      "{\"video_ocr\": \"这不是你来的地方|走走走|他是谁啊|他说他是齐鸿的弟弟|别让他进来|凡爷齐鸿的弟弟找来了|嫂嫂这是着急千嘛去啊|齐鸿死后|唯一能继承遗产的就是你|没想到|你自己送上门来|莫非嫂嫂以为|拿的下我|光凭我是有点危险|但加上凡爷呢|凡爷|是谁呀|也是你能叫的吗|战神齐昆仑|齐帅我要知道齐鸿是您哥哥|你给我十个胆子我也不敢啊|凡爷你怎么|搜索《都市超级战神》 查看更多精彩内容|没人知道死去的齐大少|有这样一个弟弟|华国唯一的|上月麻子良|五星战神|点众 阅读|在美\", \"video_asr\": \"哎，这不是你来我这坐坐坐坐它呀，它让它是提供地铁，别让他去。|ZZZZ。|樊爷齐宏的弟弟找来了。|嫂嫂这是着急干嘛去。|齐鸿死后，唯一能继承遗产的就是你没想到。|自己送上门来。|莫非嫂嫂以为。|拿下光凭我是有点危险，但加上樊爷呢。|樊爷。|是谁呀樊爷。|也是你叫。|试试战神齐昆仑，其实我要知道齐宏是您哥哥，你给我十个胆子我也不敢呀。\"}\n",
      "multi-modal tagging model forward cost time: 0.01650238037109375 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '现代', '静态', '多人情景剧', '填充', '办公室', '平静', '工作职场', '全景', '特写', '门口', '动态', '愤怒', '上下级', '惊奇', '单人口播', '拉近', '路人', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.23', '0.06', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03a416e2a5200fdf578ea193919f95b8.mp4\n",
      "{\"video_ocr\": \"唉今天晚上又要加班了|诶你学这个真的有用吗|当然7|你看我这个月有加过班吗|自从学7Python|做表查资料这些轻松多了|学这个难吗|我是0基础担心学不会|对比其他编程语言|Python是很简单的|来风变编程|里面情景对话式教学|我现在都会用自己2写的代码|自动化办公|每天都能准点干班|哇快带带我|这个在哪报名啊|别急|点击之长关报名|点击视频报名|8.9元轻松入门|学习Python前|报表进度 30%|100%|g\", \"video_asr\": \"哎，今天晚上又要加班了哎，你学这个真的有用吗？当然了，你看我这个月有加过斑马线人学的PYTHON做表查资料继续轻松多了。|那学这个难吗？我是零基础还是学不会？对比其他编程语言，开始是很简单的来风变编程里面情景对话式教学，我现在都会用自己写的代码自动化办公，每天都能准点下班。|在我这在哪报名啊？别急，点击视频报名吧，点九元轻松入门！\"}\n",
      "multi-modal tagging model forward cost time: 0.01659989356994629 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '单人口播', '室内', '平静', '特写', '多人情景剧', '手机电脑录屏', '愤怒', '喜悦', '办公室', '家', '单人情景剧', '朋友&同事(平级)', '教师(教授)', '动态', '悲伤', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.95', '0.94', '0.85', '0.19', '0.12', '0.08', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03ae0c81c1cba0b716dc047f3632f395.mp4\n",
      "{\"video_ocr\": \"全方位提升我的工作效率|你连Python都不会|让你做个财务报表|你都做多久啦|还想不想干啦|干什么呐|我让你学的Python|学的怎么样了|学了老板|你看这些报表|都是我今天做的|速度比以前快多了|有了它|办公自动化|数据可视化|还可以大批量处理表格|嗯不错|好好学Python|从此不用再加班|屏幕前想要提高工作效率的朋友们|现在报名只需要3.9元|赶紧点击视频|给自己一个提升的机会吧|￥3.9立即体验|数 1854581235|数据分析 1596858556|今日数据|学会你将收获 了解基础语法 学会三大数据类型|认识常见函数 完成10+编程练习|掌握编程思维|系统数据可视化界面|42152|视频为演绎情节|9858， 4587+|扇贝编程|Π编程|招财进宝|56%|三8n|01\", \"video_asr\": \"他怎么都不会让你做什么，你都做多久了，还怎么点半赶到，怎么办呢？那你现在的学校怎么样了起来，把他这个牌子真的不错，你看这不都是我今天做，速度比以前快多了，有了它办公自动化，数据可视化，还可以大批量处理表格，全方位提升工作效率。|不错，好好加班菜，从此不用再加班。|屏幕前想要提高工作效率的朋友们，现在报名只需要三点九元，赶紧点击视频给自己一个提升的机会！\"}\n",
      "multi-modal tagging model forward cost time: 0.016194581985473633 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '单人口播', '喜悦', '特写', '配音', '场景-其他', '平静', '手机电脑录屏', '家', '动态', '朋友&同事(平级)', '办公室', '多人情景剧', '惊奇', '室内', '家庭伦理', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.95', '0.87', '0.82', '0.48', '0.38', '0.35', '0.35', '0.17', '0.05', '0.03', '0.02', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03b1d64b22305f04656364e123334365.mp4\n",
      "{\"video_ocr\": \"还呗APP|那人家呢~|你最高额度己口万|偶咧买咧噶!|我得告诉我的姐妹们~|点击视频下方链接|凭身份证和信用卡|就可以在线申请|你的可用额度啦|贷款有风险，借款需通值 请根据个人能力含理贷款，理性消费，避免逾期|贷款额度。放款时间以实际审批结果为准|APP|还\", \"video_asr\": \"那人家哪有最高额度二十万？告诉姐妹们，点击视频下方链接，凭身份证到现场就可以在线申请每天可用活动了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016023635864257812 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '全景', '室外', '喜悦', '动态', '平静', '特写', '手机电脑录屏', '路人', '惊奇', '单人口播', '情景演绎', '极端特写', '配音', '愤怒', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '0.97', '0.92', '0.64', '0.54', '0.28', '0.08', '0.05', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03b48a490527d87864c1b00e1ed15de2.mp4\n",
      "{\"video_ocr\": \"朱败|又输了|小美 你的皮肤太丑了|怪不得每把都输|你送我啊 哪有钱买|我还真有一个免费送皮肤的妙招|就是这个 QQ浏览器|敲黑板 划重点|ワ口浏览器每天都能|领取免费的超大游戏礼包|更有王者荣耀。Q飞车|更有王者荣耀 。Q口飞车|和平精英等|海量神秘包惊喜升级中|你也快来领一份吧|一剑斩仙|小喇叭×1 胜经验卡×2|梦奇*1天1胜经验卡×1钻石×5|王者荣耀注册礼包|改装科技箱(小)×1|紫薰之恋服饰(女)(15天)|和平精英独家礼包|和平精英每日登陆礼包|金币*58 领取|金币*108 紫色ROCK长袖T恤(3..|金币×188喷涂-顶级神操作(永...|湾湾网友热议FPX夺冠:SKT就输给G2这种 烂队?LCK没救了!|期待榜 新游榜|88点券×1高级能量体×1 QQ飞车每周礼包|第1天 和平精英第1天签到礼包|金币×188 喷涂-到此一游(红）×3|新游热游抢先玩，超多礼包领不停|追梦计划|第1天 王者荣耀签到1天|签到礼包第1 天|QQ浏览器|高爆版仙侠，送VIP、时装 下载 造梦西游0L|紫薰之恋服饰(女)（(15天)×1...|×1 ...|周年|女娲*1天胜经验卡×1钻石×5...|更多礼包6|闪观|阿现 伤害|很意外!|11月11日上线 时之扉|欢乐斗地主(腾讯)|斗破苍穹:异火重燃|剑网3:指尖江湖|搜索或输入网t 主推荐视|视|游戏中心 工具箱|50000矿石×5 50000石油×5 城. 红警OL建筑工厂8级|50000石油×850000矿石×8城.|10000金币×5 求助指令×2 言灵.|10000金币×3圣白勇士秘钥×1|11|CHAMPIONS|I801|当前共5个礼包 打开|明日可领 王者荣耀签到2天|明日可领 QQ飞车手游签到2天|WEA|游戏中心即将搬家至游戏频道，不要迷路啦 搜索游戏、礼包、攻略、直播|奇趣派对|龙族幻想40级礼包|剑网3|一级五行石x30 金月糕×3金币...|1000金币×1小哈(15天) QQ飞车新用户注册好礼|55|psD|QQ飞车手游|小说|小游戏|22:58|自选牌|曾荒古城|剑网3-30级礼包|贝塔考比寒1汉|第1天QQ飞车手游签到1天|互动阅读 一零零 礼包|我的兴趣世 界|社车|刘日X|控制|以复|二8\", \"video_asr\": \"正在被攻击。|YOU说小美，你的皮肤太丑了，怪不得没把你送我呀，那我还真有一个免费送皮肤的妙招就是这个。QQ浏览器教黑板划重点QQ浏览器每天都能领取免费的超大游戏礼包，更有王者荣耀，QQ飞车，和平精英等海量神秘礼包惊喜升级中，你也快来领一份吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.02078557014465332 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '静态', '推广页', '场景-其他', '平静', '喜悦', '单人口播', '特写', '多人情景剧', '动态', '配音', '拉近', '惊奇', '室外', '路人', '室内', '混剪', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.69', '0.68', '0.23', '0.13', '0.02', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03b83c31d788c6faf871ae936ddf194b.mp4\n",
      "{\"video_ocr\": \"玖富万卡最高有|20万借款额度|最快3分钟放款|最长可分24期慢慢还|那我也去申请一个|点击屏幕下的链接|试试你能借多少吧|测|测测你能借多少钱|具体额度费率等根据实际审批为准|为准|高可借入0万元|实际额度以审批结果为准，请珍视信用，按时还款，勿过度举债|玖富万卡|ONECARD\", \"video_asr\": \"有房卡最高有二十万借款额度，最快三分钟，放款最长可分二十四期，慢慢还。谢谢一个，点击屏幕上的链接，最高二十万借款额度，试试你能借多少吧，最高二十万，来论坛你有多少额度吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.02127981185913086 sec\n",
      "{'result': [{'labels': ['推广页', '静态', '现代', '中景', '平静', '多人情景剧', '特写', '单人口播', '手机电脑录屏', '喜悦', '动态', '场景-其他', '朋友&同事(平级)', '室内', '惊奇', '愤怒', '工作职场', '办公室', '极端特写', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.87', '0.84', '0.76', '0.62', '0.55', '0.50', '0.41', '0.07', '0.05', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03d6f3f2ffd62eb02f67f0cf63495b1d.mp4\n",
      "{\"video_ocr\": \"高中数学成绩差的 孩子一定要看|你有没有想过|你的孩子一个月之后|可以挑战年级第一|对|不管基础怎么样|不管之前的成绩如何|报名高途课堂|全科强化特训营|成绩可以飞速提升|我不会教孩子|常规的解题方法|而我会先用一些|讨巧的解题技巧|能够让他很快的|做出一些|难题和压轴题|但是注意这个时候|孩子的基础|并不扎实|不过呢|他已经有了|数学解决问题的兴趣|而不是像以前|看到数学|就本能的抵触|那再之后|我再引导孩子|一步一步的掌握|为什么我们可以|这么做|以及解题的过程|应该怎么样来书写|先来听听|我的方法吧|9元16课时|让你的孩子可以完成|高分逆袭|新学员9元专享 立即报名|名师出高徒.网课选高途|省高考状元带队授课 老师平均教龄11年+|名师特训班\", \"video_asr\": \"高中数学成绩差的孩子，一定要看你有没有想过你的孩子一个月之后可以挑战年级第一。|对，不管基础怎么样，不管之前的成绩如何报名高途课堂全科强化特训营。|成绩可以飞速提升，我不会教孩子常规的解题方法，而我会先用一些讨巧的解题技巧，能够让他很快的做出一些难题和压轴。但是注意，这个时候孩子的基础并不扎实不过呢。|他已经有了数学解决问题的兴趣，而不是像以前看到数学就本能的抵触，那再之后我再引导孩子一步一步的掌握。|为什么我们不可以这么做，以及解题的过程应该怎么样来书写？先来听听我的方法吧。|九元十六课时，让你的孩子可以完成高分。\"}\n",
      "multi-modal tagging model forward cost time: 0.01618218421936035 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '静态', '中景', '平静', '室内', '配音', '场景-其他', '过渡页', '家', '情景演绎', '极端特写', '喜悦', '办公室', '教师(教授)', '手写解题', '特写', '转场', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03d901b8fa1b948e4e2e57bf71d033a3.mp4\n",
      "{\"video_ocr\": \"你们这个课程|怎么回事|怎么买都买不到|就是怎么回事啊|对啊|女士对不起|我们9元的课程|已经售完了|小张啊|老板|我们9元16节的|2020初高全科名师班|售卖的太快了|导致很多客户|都没有抢到|都跑来投诉呢|各位放心|我们决定啊|再加5000个名额|让家里有初高中的|孩子的家长啊|都能够购买到|我们会亏本的|为了让学生|考一个理想的分数|能够进入满意的学校|我们这点牺牲|算什么呢|另外啊|我在此宣布|高途9元课|包含语数英物|4科的同时|还将配备每一科|一名主讲老师|和一名辅导老师|让咱们家长啊|花9元|就能买到8名老师的|教学和辅导机会|那你们的师资|还是平均教龄11年的|北大清华毕业的 名师吗|必须是|太好了|点击视频下方|查看详情|就可以继续抢购|课程名额了|名额有限|快点击下方|报名吧|1 尽一热战|热低|高途课堂|艺热R|加尽热低|一热饭|加挑低|加K|会热怀|《极限挑战》第六季 官方推荐 中小学生在线教育平台|即刻出发 逆袭学霸|名师在线 挑战极限\", \"video_asr\": \"你们这个课程怎么回事？怎么买都买不倒又是怎么回事啊？怎么不是对不起，我们九元的课程已经售完了这张了。|怎么回事啊，老板，我们九元十六节的二零二零初中名师班售卖的太快了，导致很多客户都没有抢到，都跑来投诉呢。各位放心，我们决定啊，再加五千个名额，让家里有初高中的孩子，家长啊都能够购买到，我们会亏本呢。规律让学生考一个理想的分数，能够进入满意的学校。|这点牺牲不算什么。另外啊，我在此宣布，高途九元课包含语数，英物四科的，同时还将配备每一科一名主讲老师和辅导老师。|让咱们家长啊，花九元就能买到八名老师的教学和辅导机会，那你们的师资还是平均教龄十一年的北大清华毕业的名师吗？必须是。|点击视频下方查看详情就能继续抢购课程名额了，名额有限，赶快点击下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016283750534057617 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '填充', '静态', '平静', '室内', '单人口播', '多人情景剧', '极端特写', '室外', '动态', '路人', '喜悦', '全景', '特写', '亲子', '家庭伦理', '教师(教授)', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.73', '0.49', '0.35', '0.30', '0.26', '0.15', '0.04', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03e0fe0c8c0c0d325af8203a623e4fd4.mp4\n",
      "{\"video_ocr\": \"喂 老张|吃饭啊|我这正陪客户吃着呢|今轮到你请了昂|好好好|刚谈了个客户|你看把人忙的|都是自家兄弟|有什么事|别藏着掖着|我藏啥掖啥了|公司办黄了不丢人|靠劳动吃饭也不丢人|但是你得学会理财啊|就我现在这点财|还用理吗|即使少量的资金|也需要有科学的 paniteIOO?|理财方式|这样才能提供|资金积累|减轻劳动收入负担|微淼小白理财训练营 Dantel00?|理财名师授课|从记账到消费习惯|再到 基金 保险 股票 pantelOo7|全方位打造理财体系|学习复利思维|实现 睡后收入|这个课程贵吗|12块钱12节课|也就一顿饭钱|这个|怎么报名呢|点击视频下方|微淼商学院 学理财上微淼|微淼 商学院|廊学 院|微孤 言学|淼？院|矜犭商院|说淼商|徵态 商字！|敏|学理|理财上|DanieOOr|视频为演绎情节|微淼|商|学|院\", \"video_asr\": \"为保障吃饭，我这正没空，好好好好好好。|刚谈个客户，哎呀，你看把人忙的都是自家兄弟。|什么是你要藏藏啥掖着？|公司猫不就是靠劳动吃饭也不丢人，但是你得学学啊，就我现在这点财还用理吗？其实少量的资金也需要有科学的。|才方式，这样才能提供资金积累，减轻劳动收入负担。|报个微淼小白理财训练营吧！|理财名师授课，从记账到消费习惯，再到基金，保险，股票，全方位打造理财体系。|学习部例行实现税后收入，这个课程贵吗？|十二块钱，十二块钱也就一顿饭钱，这个微淼小白理财训练营怎么报名啊？点击视频下方即可了解详情。\"}\n",
      "multi-modal tagging model forward cost time: 0.016041278839111328 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '惊奇', '平静', '喜悦', '悲伤', '特写', '愤怒', '动态', '路人', '全景', '夫妻&恋人&相亲', '拉近', '朋友&同事(平级)', '家', '单人口播', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.91', '0.75', '0.59', '0.14', '0.10', '0.09', '0.09', '0.07', '0.05', '0.04', '0.03', '0.02', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03eef31ddca99ddad541fe379359ab4b.mp4\n",
      "{\"video_ocr\": \"息，还剩 万|种投资必备避坑指南|种投资必备避坑指南|种投|为自己做|/资心|种投备避垃爆本|掌握“富人思维”开启“睡后收入”|来算，那么|来算，|按照|按照 来算，那么年|按照彡那么|按照%来算，那么 年后，|个财富增值实用|5|个财富增值实用技巧|18个财富增值实用技巧|月薪3000也能理财! 直播教学，揭露理财的秘窑!|这就是理财和不理财的区别|这就是理财和不理财的区别|赶紧点击屏幕下方链接 为自己做一笔免费的投资吧|赶紧点击屏幕 为自己做一笔免费|空吧|为自己做(兔费的拎冷吧|走べ击屏草方链接|赶紧屏幕了链接|另外—万本金做|年，|另外 万本金做|万本|另外＿か|另外＿万本:|天时间，一元免费学习|天时间， 元免|垌，＿元免费学习|天时间 元免费学习|费学习|国内多家知名私募基金LP|你除了有一套子，还完银行利|你除了有一套子，太|你除了有一套 子，还完银行利|1行利|支巧|指南|我们来算个账|万全款买|理财不是买基金买股票|天理财名师爆款直播课|贷款 万，贷款期限年，|贷款 万，贷款|贷款期限|贷款7万 贷款期限|贷款70万，贷款期限30年，|快财商学院小白理财课|快财商学院小E|快财商学院小白理财课|飞理财课|假如你有 万全款买|假如你有/00|O0|假如你有/00万全教买|而是通过科学的方法配置资产|年后，你只有|套子，|/套|掌握 生 的方法|掌握·生|掌握生 的|掌握 的方法|万做首付|如果你拿出|年度金牌分析师 特许注册金融分析师CFA二级|如果你拿出万做首|如果|推荐你报名|推荐你报名|牧之老师|的话|款买|22|按照 来|理财名师教你掌握|理财名师教你掌握|名师教|里财|里财道，|另外了0万*理财趟，|另外]0 万本金做理财道，|理财有风险入市需谨慎|请输入验证码 获取验证码|IC亏|请输入手机号|来算，那么年后，|王后，|东方证券消费行业年度金牌分析师|摆脱月光 财富增值|5南妥|+86\", \"video_asr\": \"我们来算个帐，假如你有一百万全款买房的话，三十年后你只有一套房子，如果你拿出三十万做首付贷款，七十万贷款期限三十年，另外七十万本金做理财，按照百分之九来算，那么三十年后你除了有一套房子，还完银行利息。|省一百二十四万，这就是理财和不理财的区别，理财不是买基金，买股票，而是通过科学的方法配置资产，掌握钱生钱的方法。推荐你报名快财商学院小白理财课，六天时间零元免费学习，理财名师教你掌握十八个。|速增值实用技巧，二十二种投资必备避坑指南，赶紧点击屏幕下方链接，为自己做一笔免费的投资吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015877723693847656 sec\n",
      "{'result': [{'labels': ['现代', '填充', '场景-其他', '推广页', '配音', '平静', '静态', '极端特写', '手写解题', '手机电脑录屏', '重点圈画', '单人口播', '室内', '惊奇', '幻灯片轮播', '知识讲解', '特写', '中景', '绘画展示', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.89', '0.72', '0.66', '0.15', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/03f8220f8a7a76a8d3cdad021fbf521c.mp4\n",
      "{\"video_ocr\": \"找个原画师有这么难吗|老板|我们开这么高薪水 找这么久都没找到|要不还是算了吧|继续找|我们公司现在朝游戏 和影视行业偏移|这原画师对我们 太重要了呀|找到了找到了|你找到合适的原画师了|我找到潭州教育原画师 的免费直播课程啦|免费原画师直播课 潭州教育有个|在线就能学习，特别方便|里面的老师讲的很清晰|新手小白也能学会|下了班空余时间 在家就能学|高薪聘请不如 我们自己学呀|那怎么报名啊|点击视频下方链接 就可以免费领取|潭州教育原画师的 试听课名额啦|把你的兴趣变成职业技能|—让你从小白变成CG行业大神—|选择课题内容 快来根据你的喜好/选择你的专属课题|<<<<<<＜·游戏建模 .>>>>>>>|—|原画场景 游戏原画|游戏场景设计|每天花费 1小时|g <<<<<<·游戏原画·>>>>>>>|想知道你能成为一个游戏原画师/建模师吗？|0基础免费带你入门|游戏原画·>>>>>>>|<<<<<|<<<<<<<.|原画|C游戏行业|>>>>\", \"video_asr\": \"找个原画师有这么难吗？老板，我们开这么高薪水，找这么久都没找到，要不还是算了吧。|继续找我们公司，现在抄游戏和影视行业偏移，这原话是对我们太重要了呀，老婆。|找到了，找到了，你找到合适的原画师了，我找到潭州教育原画师的免费直播课程了，免费直播课程，谈州教育有个免费原画师直播课，在线就能学习，特别方便，里面的老师讲的很清晰，新手小白也能学会，下了班空余时间在家就能学学高薪聘请，不如我们自己学呀，那怎么报名啊？|点击视频下方链接，就可以免费领取弹州教育原画师的试听课名额啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016063213348388672 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '手机电脑录屏', '平静', '场景-其他', '喜悦', '单人口播', '悲伤', '配音', '室内', '朋友&同事(平级)', '家', '惊奇', '夫妻&恋人&相亲', '全景', '动态', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.92', '0.91', '0.23', '0.14', '0.10', '0.08', '0.05', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04149af61279fd11b9a2e9b93d8163a9.mp4\n",
      "{\"video_ocr\": \"我玩游戏可是高手让我试试|哎呀一边去|我玩游戏可是为了免费买东西|啥玩游戏还能免费买东西|你打开拼多多|点击多多爱消除|找到这个挂烫机|点击免费拿|玩玩游戏就能免费领了|下载拼多多|更多商品等你拿|热门 女装 百货 鞋包 食品母要 内衣|玩游戏领奖品|已送出4247件|华心手持挂机|今日上新1137|养生|多岛爱消除|Bonus time|拼|斩升级|拼多多\", \"video_asr\": \"我玩游戏是高手，让我试试一边去玩游戏可是为了免费买东西，啥？玩游戏还能免费买东西啊？你打开拼多多，点击多多爱消除，找到这个吧，相机免费的，玩玩游戏就能免费领了，下载拼多多，多多商品等你来！\"}\n",
      "multi-modal tagging model forward cost time: 0.015765666961669922 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '手机电脑录屏', '推广页', '多人情景剧', '家', '特写', '喜悦', '配音', '平静', '单人口播', '愤怒', '夫妻&恋人&相亲', '惊奇', '朋友&同事(平级)', '亲子', '场景-其他', '室内', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.88', '0.71', '0.54', '0.33', '0.26', '0.06', '0.05', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/041746ed3adf4eaa872c6883b6722df5-1557114072774.mp4\n",
      "{\"video_ocr\": \"我学日语三年了|我是用传统的学习方法|背单词 做阅读听力训练|感觉过程特别枯燥|是一种折磨|灰|手机和电脑端都能用|日语零基础|有动漫日语|日语配音|日语歌曲教学等|种类非常多|零基础也可以轻松入门|日话课程|我学习的效果很差|背下来的单词 句子|很难在生活中应用|过段时间|又全部忘光了|每天上线上和真人直播互动|一边看一边跟着读|过程轻松有趣|甚至停不下来|感兴趣的朋友赶快填写信息|一起学习吧|潭州课堂|2019抖音短视频运营实操技术|潭州日语A模块 周末班 潭州日语A模块|7422人报名 视频|免费|综合日语2(山东师范大.. 新经典日本语|838人报名 日语基础精品视频|日语口译基础(浙江外国|语法精品视频 ￥ 499.00|日语五十音入门视频 ￥39.90|4 人报名 北原带你玩转五十音|あaい う。え：。お。 キ、クku|Yshi|片假名|阿标|工作党 高中生|前端开发|日语零基础一节课学会...|趣味ACG日语随到随学…..|语学院)|日语初级语法精品视频|日本外教Kanari先生真人 口型演示日语发音课|日语小白直通大神之路|Uhふfu|您的称呼|日语 淘宝运营 游戏模型|学会五十音|趣味ACG学日文|¥99|潭州|riるruれre|电话番号|您的身份（匹配合适课堂)|19|第二册|119人报名 潭州日语精品录播小课|しshすsuせ シshスsu| tsu|周末班|从零开始|日语基础五十音|君の名は|提交|领取课程|上网学习·就来潭州|￥ 2180.00|¥500.00 302人报名|213 人报名|大学生|深州日本播专9二-NA|日本语の粉级文法飞高名匕手才|自闭少女|感尬 进入房间|sh|C++|平面设计|小直播中|基础教程|757人报名|れre|Python|真人口型演示|みむu|bingな少年|ACG日本情老学ふ二と力好专飞す|万一起级价|Vdleo,.201 第用饮件安|Vdeo|州堂|全部|北百世你雄工┴立|4.1w人报名|先领取一节直播试听课，开启日语学习之路|アa|イ:ウu|ウu|イI|精品录播小课|7APANESE 达人之路|虞生 riるru|W |搜索你要查找的课程|萌新额的日语课，快速学日语|点击领取|教喜你获得10舌跃值|直播公开课|49.9|屏幕大了|巴力于ュウ进入房间|ka きkiくkuけke|TANZHOUEDU.COM|(Ce8 1-8n)|(8 1+80)|在kiクku|クku|ウ今スツヌ 工クセラテ|ko|oO 00|を。|抖音吸粉|(shス，|にnI|假一借，名→字|10:33|魅力榜|高山仰止，景行行|33.69厘米|chiつ|VIP课|图片工具|幻灯片放缺|安全|主持模式|重设大小|なはま|サタ|更改圈|财富榜|高度:|云服务|/\\\\|八he|イtKチ|tSu|mi|mIu|切换 动画 审阅|周榜|ne|hl\", \"video_asr\": \"我学日语三年了，我学日语半年了，我是用传统的学习方法背单词做阅读听力训练，感觉过程特别枯燥。|是一种折磨，我是用谭州课堂APP学习，手机和电脑端都能用，有动漫，日语，日语配音，日语歌曲教学等。|种类非常多，零基础也可以轻松入门，我学习的效果很差，背下来的单词句子很难在生活中应用，过段时间就全部忘光了。我在兰州课堂APP上，每天在线上和真人直播互动，一边看一边跟着读，过程轻松有趣，甚至停不下来，感兴趣的朋友赶快填写信息，一起学习吧！|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016391754150390625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '平静', '家', '喜悦', '办公室', '室内', '填充', '配音', '多人情景剧', '手机电脑录屏', '场景-其他', '混剪', '动态', '宫格', '朋友&同事(平级)', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.81', '0.24', '0.14', '0.13', '0.06', '0.05', '0.02', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/042333f147f1767f137be406bad246da.mp4\n",
      "{\"video_ocr\": \"忽然有一群大雁向南飞|有2 个小朋友在扫落|一人雁向南飞|小学作文300字|你家孩子1个小时都写不完!|还有救吗?|其实不是孩子太笨|也不是孩子不努力|方法错了!|语文就是生活|教会孩子有意识地观察生活|写作真的很简单!|我是高途课堂王勒老师|北京师范大学毕业|专注语文教学12年|5条素材积累原则|18种文章结构技巧|还在等什么|点击视频下方[查看详情〗]|报名吧!|在跳舞，]又像一只只 舞的黄蝴蝶。拟人|蝴蝶 舞|只需9|去，秋天到了，秋天真|伙天到了 秋天真|带孩子轻松搞定作文|新用户专享立即体验 浙江卫视指定在线教育品牌|2.谁在哪儿?|全国百佳教师带队教学 平均教龄11年|在一棵大树底下|黄下在 变落孩|亿年教龄|来了，像一个|女孩|下来了，像一个小女孩|4大习作类型|像一个小女孩|1.什么时候? 3.干什么?|下来了|，|舞的黄O蝴燃3拟本|黄了，风一吹，树|风一吹，树|风一吹|了I，T|风一，树叶落|秋天到了，越叶变|语言生动|别具一格|仅需|秋天到了，上|越叶变|她叫变|在跳舞，又像|一只只飞|手N|黄了，|高途课堂|名师特训班|浙江卫视|下来了【，像一|时间|王时|的到了!|有忽去|¥9|夕r|只只|华少\", \"video_asr\": \"帮我背上书包出发。|第一次离开家。|妈妈的小小学作文三百字，你家孩子一个小时都写不完，还有救吗？其实不是孩子太笨，也不是孩子不努力，而是方法错了。|文就是生活，教会孩子有意识地观察生活，写作真的很简单，我是高途课堂王勒老师，北京师范大学毕业，专注语文教学十二年。|总结出四大写作类型，五条素材积累原则，十八种文章结构，带着孩子轻松搞定作文，只要九块钱，还在等什么？|点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016238689422607422 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '教师(教授)', '中景', '单人口播', '场景-其他', '静态', '室内', '平静', '手机电脑录屏', '影棚幕布', '配音', '幻灯片轮播', '教辅材料', '重点圈画', '知识讲解', '特写', '混剪', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.33', '0.13', '0.09', '0.06', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/042b9569889d7b6a7bfb854e76790e78.mp4\n",
      "{\"video_ocr\": \"小姨|你是不是给我报 作业帮直播课|小初高数学名师 提分班呀|是啊怎么了|你成绩都那样了|还不得给你报个班啊|不是吧 小姨|你也太抠门了吧|30块钱的课能有啥效果|你这死孩子 瞎说什么呢|我可不是看价格 才给你报名的|人家可是清北毕业 名师带队教学|师资雄厚|能教会你们小学|22个计算题解题大招|25个应用题经典解法|和12个代数几何 专项突破|快速攻克数学 重难点和易错点|轻松应对各类考题|不管你基础怎么样|都能快速挑战高分|你咋就不知道我的 用心良苦呢|小姨 真的有这么好啊|当然啦|除了30块钱18节课以外|还包邮赠送配套的 学习礼盒呢|小姨小姨|这么好的课一定很难抢吧|你是怎么抢到的|点击视频下方就 可以报名啦|EMAN|THE60|VETMANME|DE|MEEFU|MAh|M|上课内容与收到礼盒请以实际为准|N9EO%4|MAso|*DE港|MB27U|的，作业帮直播课小初高数学名师提|分班。请下载，并登录作业帮直播|名师提分班 30元18节课|您好，您已成功购买，30元18节课|短信/彩信|今天1118|*赠送12件套教辅礼盒 立即报名＞|短信信|IARSUN|1Ans0N|课APP，查看具体开课时间。|騒情|M2A|MAISON|Lets|THEGOI|MEMNME|小学数学公式 30 DAYS|THE|MEEFU|GADE|草稿本 成长笔记|ITENTION|.il4G|致直播课|11:18|ER|IC|EB Il|TEP aI|TEBIC|ON|IDN\", \"video_asr\": \"小姨，你是不是给我报作业，帮直播课小初高数学名师提分班呀，是怎么了？|你成绩都那样了，还不得给你报个班啊，我是个小鱼，也太抠门了吧。|十块钱的课哪有啥效果？你这死孩子瞎说什么呢？我可不是看价格才给你报名的人一下可是清北毕业名师带队教学，师资雄厚，能教会您的小学二十二个计算题解题大招，二十五个英用题经典解法和十二个代数几何专项突破。|快速攻克数学重难点和易错点，轻松应对各类考题，不管你基础怎么样，都能快速挑战高分，你咋就不知道我的用心良苦呢？小姨真的有这么好啊，当然了，除了三十块钱十八节课疑问，还赠送配套的学习礼盒呢，找一找一个！|好的课一定很难抢吧，你是怎么抢到的？点击视频下方就可以报名哦。\"}\n",
      "multi-modal tagging model forward cost time: 0.0160367488861084 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '静态', '全景', '中景', '平静', '亲戚(亲情)', '喜悦', '家庭伦理', '特写', '手机电脑录屏', '悲伤', '路人', '惊奇', '动态', '愤怒', '亲子', '(马路边的)人行道', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.94', '0.82', '0.67', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/042d5f5f60a82791b0ba3f415abbc430.mp4\n",
      "{\"video_ocr\": \"姐姐|你就帮帮我吧|啊|都高一了|还有那么多英语单词记都不住|我怎么帮你啊?|你就告诉我|你是用什么方法|记单词的吧|救命啊|我的方法就是|找杨文哲老师|哲哥英语|教你 漫画形象记忆法|生动形象的带你|高效记忆单词|系统掌握词汇串|达到记一通十|关注公众号 0元领取试听课程\", \"video_asr\": \"姐姐你就帮帮我吧，就帮帮我啊，都高一了，还有那么多英语单词都记不住，我怎么帮你啊？你就告诉我你是用什么方法记单词的吧，救命啊，我的方法就是找杨文哲老师。|哲哥英语教你漫画形象记忆法，生动形象地带你高效记忆单词，系统掌握词汇串，达到记忆充实，那就点击下方关注公众号即可领取免费课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016171693801879883 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '多人情景剧', '中景', '家', '静态', '喜悦', '惊奇', '亲子', '特写', '单人口播', '家庭伦理', '悲伤', '朋友&同事(平级)', '平静', '动态', '夫妻&恋人&相亲', '室内', '手机电脑录屏', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.41', '0.36', '0.29', '0.29', '0.12', '0.07', '0.04', '0.04', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/043f606d79d2be951b1c2de4ce291238.mp4\n",
      "{\"video_ocr\": \"我不懂|为什么还有这么多的家长|依旧让自己的孩子用是为了|依旧让自己的孩子|在一遍遍的读文章找答案|醒醒吧|这样做不仅效率低|而且还会间接导致孩子|丧失学习语文的信心|要知道|不管是小说|散文|还是诗歌文言文|都有其对应的|技巧和方法|只要掌握了这些方法|就有机会挑战|一分不丢|在这里我推荐|高途课堂小学金牌 双师无忧特惠班|是由北大清华毕业的名师|是由北大清华毕 业的名师|带队教学|定制化小灶带队教学|紧跟考纲 覆姜9%知识点 教孩子难点|四类高效作文写作模板|八大阅读写作提分大招|6大阅读模板|课后还有专业辅导老师|对答疑解惑|现在报名仅需18元|加一元还能再得一科英语|而且课程三年内|免费无限次回放|别再犹豫了|赶紧点击视频下方链接|为孩子报名吧|行说明。（3分|两个方面来进行说明。(3分 4 分)|地面三四尺，不妨 碍上面种庄稼。地道里每隔一段距离就有一个大洞，洞顶用木料|道里怎么能了解地 面上的情况呢”，|只要我们按下手电筒的开关，立刻会出现一束光柱。光的速度是惊人|加1元再得10课时英语名师课|小学六年级|习可手左此大 学习没方法|熬时间拼体力|不怕错过直播|3大写作妙招|专注语文教学12年|\\\"十分精致的粱|止。它们隐匿在高大的阔叶树上，用麻 雌鸟在巢里产2一4枚粉红色有玫瑰璃|止。它们隐匿在高大的阔叶树上，用麻、草茎、棉絮编 雌鸟在巢里产2~4枚粉红色有玫瑰斑点的卵，在摇|嘛、草茎、棉絮蝙织成十分精致的粱|广分精致的菜|的卵，在|1942到1944那几年，日本侵略军在冀中平原上“大扫荡”，还 修筑了封锁沟和封锁墙，十里一碉，八里一堡，想搞垮我们的人民|斗争方式，敌人毒辣透顶的“扫荡”被粉碎|有了地道战这个斗争方式，敌人毒辣透顶的“扫荡”被粉碎 了。冀中平原上的人民不但坚持了生产，还有力地打击了敌人，在|带着问题读也可以加快阅读的速度。遇到不懂的 词语，在不影响理解课文内容的情况下可以先不管它，|3大阅读技巧提高理解能力|全方位知识输入 成绩进步快|结尾法&人物概括|生动传神写结尾|平均教龄 重难点覆盖率|的读书体验|:语句通顺、流畅，内容具体，字里行间能为大家|家分享你的读书体验|精彩的。”在你看过的书里|高二四科|科学方法|合理想象编故事|满分作文万能开头|4类阅读技巧 古诗助力作文添彩|由于课程持续研发上课内容以系统实际安排为准|教研团队|升级服务|上文末表递进关系的归结句。（| 土文末表递进关系的归结句。(4分)|撑住，很牢靠。大洞四壁又挖了许多小洞，有的住人，有的拴牲 口，有的搁东西，有的作厕所。一个大洞容得下一百来人，最大|样的？在地道里怎么打|大题不会做 小题老出错|巧开头&写中间|高分叙事想象过程|作文基础思维训练|荷兰莱顿大学文学硕士|征着生机勃勃。黄鹏是上海地区的夏候|千鸡馆”，杭州有全”精派闲露”，人们都装|中I|5天10时状元名师带队|请选择孩子当前所在年级|初三双科|成绩不见提升 与同学差距大|1对1学情沟通|随时随地学|鼓励孩子成长|阅读绘本写诗句|高途课堂|阅读的生命是精彩的。”在你看过的书里，肯定有- 么喜欢这本书呢？向你的好朋友推荐推荐吧!|么喜欢这本书呢？向你的好朋友推荐推荐吧|和|的能容二百多人。洞里经常准备着开水、干粮、被子、灯火，在 里面住上个三五天不成问题。洞里有通到地面的气孔，从气孔里|课后不敢问|《知识体系补给包》|五感法看图秒杀|二年级 看图写话妙结尾|秒解修辞手法 文章概括&叙事作文|硕博毕业名师教学经验丰富|据统计每|各种昆虫，其中百分之九十都是农业害虫。在哺|在哺雏的18天中，据统计每|；，据统计么|航上双亲自己吃|创造了新的斗争方式，这就是地道战。 人来了，我们就钻到地道里去，让他们扑个空；敌人走了，我们就|读得快还要想得快，要做到一边读一边想，抓住关|键词句，才能及时捕捉有用的信息。|摸底测试+小灶课专攻薄弱项|出飞，这期 自吊篮里孵化|在远处警戒并担任觅食工作。 雌鸟经过18天孵化，雏鸟出壳，再经过|告的卵，在摇啊摇的吊篮里孵化 的哺育，小鸟长成出飞，这期|雌鸟经过18天孵化，雏乌出壳，再经过18天的哺育，|十都是农 壳，再经过18天的，|为了粉碎敌人的“扫荡”，冀中人民在中国共产党的领导下，|留下了惊人的奇迹。|我国抗日战争史上留下了惊人的奇迹。|三情法五感法搞定高分作文|鹤色黄鹏五种，美中黑放责诚分在最广，深枕黄城有红，自嘴基起懒过哦|秋声美，而且总是私秦连在一起。我国产有金章丽、黑头煎以、和色美一 精色责鹏五种，美中黑枕黄秘分有最广，黑就黄丽嘴红，自嘴基起懒过郡|秋声美，而且总是和套选在一起，我国产有金章丽、夏头亲诚都色亲听|秋声秉，而且总是和秦选在一起。我国产有金黄画、黑太戴秘、和色教略|秋声美，而且总是私秦连在一起，我国产有金章画黑太亲通色新晒|科色责鹏五种，美中黑枕蚕丽分布放广。黑枕前秘嘴红，自需基起横过眼|活。黄鹏很少下地，飞速极 年鸡馆”，杭浏有全”秘没闻需”，人们都|高理解能力|《家庭教育私房课》|都有两追黑后，教在金责色的羽毛中，超和尾领漆黑、考丽、往选|有时似雁语声声 雄鸟小叫时似行云流|每一个学科|阅读题&作文结构|黄鹂不仁|育喂食 70次，再加上双亲自己吃的，消灭了大量的|大量的害虫。可见黄鹂不仁|其数的地道，横的，竖的，直的，弯的，家家相连，村村相通。敌 说起地道战，简直是个奇迹。在广阔平原的地底下，挖了不计|略 垒任丘搁陷拐岔|俗话说:“熟能生巧。”让我们不断练习，努力做到|呢?向你的好朋友推荐推 书吧!|学习不主动|20+课时精讲|送一套知识点汇总|￥18+1元|分)|3分）|生了疑问：“地道是什么 看到课文题目，我产|你读这篇课文用了几分|手个三到|越冬，春未初夏在上海地区营菜管莲，还有计海继豉向北，去我国 在活黄一很少下地，飞选极快，睡同消不见其影，特别|主活。黄鹏很少下地，飞速极快，瞬间消道，往往只闲其声，不见其|感冬，春末初夏在上等注，还有许多经过上海继续向北，去我国|月，雄鸟小叫时似行云流水，悠扬颤持;大叫时请脆悦耳，音韵多变，有时|重点归纳|课程重磅大升级 阅读写作双提升|北京有个“听鹏馆”，杭测有个~种派属鹭”人们新爱需歌燕舞，源也一心 征看生机物劫，黄鸥是上海地区的夏银身，也是旅身。黄国冬在南穷直至|北京有个~听鹏馆”，杭浏有个招淡闲营”，人们都金需歌燕费，源出一.|征看生机勃物，黄鹏是上海地区的夏候鸟，也是旅鸟。黄画冬在南商直主|北京有个“听酷馆”，杭州有个~和没闲需”，人们都爱需歌燕费。源出一心 征着生机劫勃，黄鹏是地区的夏候身，也是赫鸟。黄城冬在南商直柔|它们隐|冀中的地道战|人分布在各处，发现了敌情就吆喝起来，一个接一个，一直传到指|语文园地|报名即送价值￥199|为每一个孩子|教研名师精心打磨 快速掌握重难点|有始有终效果好|传神描写有神威|深外闲溪16 分|深分闲浮16分)|深分闲读(6 分》|在金责色的领毛中，超私毛领源黑|状元名师带队|做一个提分计划| 不了多久，黄鹂的|时似|真妙不可言！但过不了多久，黄鹂的|的多变，有时心|棉絮|好了的。为了打击敌人，什么办法都想出来 穷无尽的。|表示什么意思是早就约好了的。为了打击敌人，什么办法都想出来|地读，不要一个字一个字地读，更不要回读。|全国教材|14天全程伴学|立即报名|用，雄鸟小叫时似行云流水，悠扬颤抖;大叫心再，音的 ，有时似雁语声声，有时似猫耳轻呼，真|支，有时似雁语声声，有时似猫耳轻呼，真妙不可言!但|神间消逝，往拉只闲其声，不见美影，特别 身好太叫时清脆悦耳，音韵多变，有时仙|经过|了，人民的智慧是无穷无尽的。|修辞手法巧赏析 妙笔生花写开头|介绍;句中有|的特|货过15 字)(2分) 的特征来介绍;句中有|地道的式样有一百多种。就拿任丘的来说吧，村里的地道挖 这种坚强的堡垒，冀中平原上的人民坚持了敌后游击战争。|冬，春末初夏在上海地区营梁繁|一级|一四个|定制化小灶班|≠鹂的  上」文末表递 1(30分)|文末表|仗呢？”带着这些问题读， 我对课文内容理解得更快|自创作文“三情法”|筑堡党丘妨蔽陷拐|侵略筑|堡党丘妨蔽陷 拐|跟踪辅导|有益|苏洁的语言概打扌|言概括第一自然段的P|《小学语文知识体系补给包》|不少于450字。|:语句通顺、流畅，|金牌主讲|18元10课时语文课|你的孩子是否正面临?|18+1元 孩子能收获|11年 100%|在远处警戒并担任觅食工作。|70次，再|武装。|年终特惠|线的句子使用的说|上课听不懂|教研名师精心打|吴月光 张莹菁|长书是你特别喜|肯定有一本书是你特别喜|了，阅读的速度也快了。|地道战取得成功的关键是什么?结合课文内容说一说。|仅做部分展示 实际授课老师以系统安排为准|无忧伴学|面。地道有四尺多|交流自己的阅读体会。|内容?和同学交流自己的阅读体会。 读到“人在地|爬到我头上 如不服气。|大作城|师范大学|高途课堂金牌名师带队|一 小学语文 －|新洁的语言概括第自然段的内容。( 自然段中画线的句子，抓住黄鹂的|打洁的语言概括第一自然段的内容。(不超过15宁|打洁的语言概括第一自然段的|从地道里出来，照常种地过日子，有时候还要打击敌人。靠着地道 在街道下面，|眼睛看得快，脑子想得快。 运用|词句段运用|狠抓知识点|多经过止;|务轻过上|【解地面上的情况呢？民兵指挥部派出一些 攵情就吆喝起来，一个接一个，一直传到指|人在地道里怎么能了解地面上的情况呢？民兵指挥部派出一些|高，个儿高的|的，大约是30万千米每秒，比流星体的速度要快几千倍!|吧！|本文作者周而复，选作课文时有改动。 25|地道战取得成功的关键 给课文内容说|提高阅读的速度。|得很快。|及时概括语句的|×008m|平凡之险|1兔|白M美4|三E美|三E鼎a|小字语文金牌双师|金牌双师无忧班|在广阔平原的地底下，挖了不计其数的地道，横的，竖的，直的， 弯的，家家相连，村村相通|用古诗文打开写作大门|交流平台|年终服务升级|越冬，春末初夏在上海地区营采管殖，还有诗多经过上海排继向北，去我园|这冬，春末初夏在上海地区营莱餐殖，还有许多经过上海继技向|主活，黄融很少下地，飞速极快，瞬间消逛，往在只闲美声，不见其影，特别|手续向北，去我国|这冬，春未初夏在上海地区营果餐建，还有许多经过上海继续向北，去我国|挥部里。老百姓管这种吆喝叫“无线电”。地道里面可就用“有线 电”了，一根铁丝牵住一个小铜铃，这儿一拉，那儿就响，拉几下|不见其影，特别|注只闻其 脆悦耳|吆喝叫“无线电”。地道里面可就用“有线|么喜欢这本书呢？向 :你也读读这本书吧!|:你也读读这本书吧!|也不怕|民不但坚持了生产，还有力地打击了敌人，在|带着问题，用较快的速度默读课文，记下所用的时间。|上一个小铜铃，这儿一拉，那儿就响，拉几下|为了提高阅读的速度，我们要集中注意力，尽量连|我就知道这段话要 讲什么，所以我读|自创作文阅读36计|金组不同的|企组不同的懂丽鹏|同的黄丽腹 画线的句子，抓住|eL 08|个1Ht建且播|田课保在持续研反上保谷以系犹头际安排为准|止。它们隐团 雌鸟在巢里，|虫，其”|天射|主活。黄鹏很少下 月，雄鸟小叫时伽|雌鸟经过18天|往下读。|说明的是|每隔一段 人拿一机|阅读的生 么喜欢这|各种昆虫，其中 育喂食 70次，#|进来了， 那儿|王那等|见或建议|洞口准 改人放毒 去子更|:语句通顺|意思，能够帮助我们|王勒|小子|，大|客服|电子资料|图)罗工|难发现 搬进来|F”。 阿，水攻|人洞口选|读下而|读下面的语句|指导价499ナ科|白兔 |aN|gO|L口|CLO|独月|典小月|会。|点，再起|，再选|北师大硕士|画风和有|三国三直|体方法|ren qiu|qinlue diao|师？\", \"video_asr\": \"我不懂，为什么还有这么多的家长，一一就让自己的孩子在一遍遍的读文章，找答案，醒醒吧，这样做不仅效率低，而且还会间接导致孩子丧失学习语文的信心。要知道，他管事小说，散文，还是诗歌，文言文。|都有其对应的技巧和方法，只要掌握了这些方法，就有机会挑战一分不丢。|在这里我推荐高途课堂小学金牌双师无忧可谓班是由北大清华毕业的名师带队教学，教孩子四类高校作文写作模板，八大阅读写作分大招。|六大阅读模板，课后还有专业辅导老师一对一答疑解惑，现在报名仅需十八元加一元还能再得一颗英语，而且课程三年内免费无限的回报，别再犹豫了，赶紧点击视频下方链接为孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015906333923339844 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '推广页', '静态', '平静', '配音', '室内', '场景-其他', '手写解题', '极端特写', '手机电脑录屏', '教辅材料', '过渡页', '家', '动态', '教师(教授)', '转场', '特写', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.96', '0.44', '0.37', '0.19', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0440e7fef8c9cb43980bd174e72b7498.mp4\n",
      "{\"video_ocr\": \"非赠品 拼音为系统课s3课程|拼音为系统课s3课程|课s3课程|太|月亮|星星|白 云|下下|雪 花|大 山|白然·天气|yang|yue|shan|阳月|雨雪|山水\", \"video_asr\": \"自然天气阳太阳说。|月亮，星星，星云，白云与下雨雪，雪花山，大山。|水水花。\"}\n",
      "multi-modal tagging model forward cost time: 0.016249656677246094 sec\n",
      "{'result': [{'labels': ['场景-其他', '推广页', '现代', '静态', '极端特写', '教辅材料', '配音', '宫格', '动画', '手写解题', '喜悦', '课件展示', '中景', '转场', '商品展示', '室内', '填充', '绘画展示', '才艺展示', '重点圈画'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.88', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04430ae1231f6bbaf9f6d6f71285daf9.mp4\n",
      "{\"video_ocr\": \"妈妈你别生气了|我真的只花了9块钱!|9块钱?|你就骗我吧!|那可是北大清华毕业名师|平均11年的教龄|名师上课会只要9块钱?|妈妈我没骗你|妈妈|呦|莹莹妈这是怎么了?|这个孩子语文学不好也就算了|现在还学会撒谎了!|她跟我说，报了一个什么|高途课堂小学语文名师班|就花了9块钱!|怎么可能才9块钱?|现在你刘阿姨在这|我看你怎么狡辩?|现在年中大促|真的只要9元!|啊?|课后还有专属辅导老师|进行详细的分析|一对一答疑|学语文啊除了要从小积累之外|还要掌握科学的方法|语文思维打好基础|孩子才能受用一生!|屏幕前的家长们|如果你家孩子|语文作文不会写|阅读理解总出错|别再犹豫了|赶紧点击视频 下方链接报名吧!|新学员9元专享 立即报名|资深一线名师团队授课 4类高效作文写作模板|7天掌握阅读核心技巧|平均教龄10年以上 主讲老师教学经验丰富|时会链|点高校及师范院校 主讲老师90%毕业于重|资深一线名师 直播互动|5节精讲直播互动课|科快诚端，使在线课堂更有仙|高途课堂|仅需9元!领跑新学期!|全程伴学，提升学习效果|请选择疯子9月升入….|:l@|C:lO|精心备课17天|主讲老师每堂课|学习报告 3年回放|北京师范大学|8年教龄|名师有秘籍 领跑新学期 9元5节课|90%|2%|3轮严格筛选师资|5节秒杀课 课中|录取率不足2%|幼升小 升小学2年级|赵亮 吴月光|小学语文资深主讲|期味|智能|17天|北大清华毕业师资 平均教龄11年以上|指导价:499 5课时+1对1答疑+3年课程回放|别让孩子玩手机了，这堂语文|提分课推荐给你，仅需9元领|小学语文名师课|名师 沉漫式|王勒|电子讲义 预习资料|上课提醒|名师特训班|跑新学期|查看详情|10年|高途\", \"video_asr\": \"妈，你别生气了，我真的只花了九块钱，九块钱，你就骗我吧，那不是北大清华毕业名师平均十一年的教龄，名师上课我只要九块钱，妈妈，我们明天你妈妈用名熊猫就是怎么了？这个孩子语文学不好就算了，现在还学会撒谎了。|他跟我说报了个什么高途课堂，小学语文名师班，就花了九块钱，哎，那可是清华北大毕业名师啊，怎么可能才九块钱，现在你刘阿姨在这。|我看你怎么狡辩，现在年终大促真的只要九元啊。|这个高途课堂，小学语文名师课，课后呀，还有专属的辅导老师根据每一个孩子的学习情况进行详细的分析，一对一答疑，直到学通学透，学会为止。学语文呀，除了从小积累之外，还要掌握科学的方法，语文的思维，打好基础。|可能受用一生，屏幕前的家长们，如果你们家孩子语文作文不会写，阅读理解啊，总出错，别再犹豫了，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01569533348083496 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '多人情景剧', '室外', '手机电脑录屏', '喜悦', '惊奇', '静态', '平静', '全景', '动态', '路人', '亲子', '单人口播', '场景-其他', '家庭伦理', '极端特写', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.91', '0.87', '0.82', '0.78', '0.73', '0.72', '0.68', '0.47', '0.47', '0.30', '0.12', '0.05', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0453f286f4da7a888aa62b60717e2d96.mp4\n",
      "{\"video_ocr\": \"老板|2根黄瓜|一个萝卜|四个茄子|这藕|付账啊 NT W|你在这跑什么呢|我看金币提现|有多少|跑步还能得金币|直接就能提现|你糊弄谁呢|用趣头条啊|每天走路|就能得金币|直接兑换现金|随时都能提现|看新闻|看视频|看资讯|哎|你帮我看着摊啊|我也下载了趣头条|跑一圈试试去|哎哎哎|2根黄瓜作为报酬|行不行啊|可以可以|真的有哎|你们也赶快去|下载趣头条吧|BABY|REAM B|BY|M BABYL|被骗演“白骨精”，如今75岁满头银 1元|活动中心|走路的步数即可兑换金币|步数兑换金币哦|连续兑换7天领取500金币|幸运大抽奖 直播领金币|AM BABY DRE|REAM BABY|影视扒哥 3评论 2分钟前 首次观看就得0.48元|挖矿达人|快来领取金币哦 马上参与|专享红包已到账 立即领取|言至今仍恨西游记导演|历史浏览 我的收藏|无论输赢必送现金红包|未兑换|领9元现金 借钱|JJ|免流量|全民斗地主|500金币|玩游戏赚钱|种菜赚金币|双11“提钱”买|今日步数|小视频|新相亲|日息低至0.5元/天｜可分3/6/12期|1元福利已到账|今日累计赚取(金币)|输入邀请码|北京|趣头条借钱|玩游戏 拿海量金币|提现兑换|1.03|即赚即提|猜成语赚钱 金币小农场|点击兑换75金币|30元|具体金额根据任务而定|斗地主赢红包|油总近|同私油总|天天走路赚金币|750|3350|3768|免费提现|￥6.88|专属福利 面对面红包|开心消糖果|邀请好友 唤醒老友|额外奖励|推荐|[微信红包]恭喜发财|趣头条|广告|抢购|现在 大吉大利|14770|领取 北京 抢购 好货 热点|好货 热点十|走路就能赚钱|1000-50，000|消息\", \"video_asr\": \"老板两个黄瓜替头条到账，呃，一个萝卜。|据头条到账，四个茄子投降到账哦，这都负重啊，这开什么呢？我看GB期限有多少？跑步还能得金币，直接就能提现这个谁呢？有趣头条，每天走路就能得金币，直接兑换成现金，随时都能提现，看新闻，看视频，看资讯都能赚金币！|哎，你帮我看着他，我也下载趣头条泡脚梳头，哎，两个乖乖抱着瓶子啊，可以为你们也赶快下载趣头条吧。打开趣头条，点我的，点这里的活动中心，找到这个，走路就能赚金币，就能开始走路赚钱啦！|也来试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016078472137451172 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '填充', '多人情景剧', '手机电脑录屏', '单人口播', '喜悦', '场景-其他', '愤怒', '夫妻&恋人&相亲', '配音', '惊奇', '平静', '室内', '路人', '特写', '亲子', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.89', '0.78', '0.38', '0.32', '0.14', '0.14', '0.10', '0.07', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/045606fa3cc78e65788f7160b579b1a4.mp4\n",
      "{\"video_ocr\": \"68红包券到账|老板娘|给我炒两个菜|今天怎么吃得|这么好啊|我这是|玩骏游斗地主|红包到账了|玩游戏还有红包啊|就是这个|《斗州|新人红包 新人登陆游戏啊|即送最高68红包券|游戏过程中|无论输赢|都可获得红包奖励|每打满5局|打满5局奖励 还有堆积红包|那怎么下载啊|我也要玩|点击视频下方链接|就能下载了|￥|37分钟俞开山的卫 告筘取200 剩0元|随机翻倍:1-10倍|免费红们 288人正在赚红包|27 26分钟|赚红包|¥0.00|2 KQJJJ 1099655|待提现红包|抽取|438053人正存额红包中 12分|恭喜获得奖励 68元|三连胜|开心转转转|3.68元|元可领哦|剩10元可领哦|丁获得|得约 天得2|我的零钱|2 2 10 9|08.01|送VIP 领记牌器每日礼包|顽记牌器|斗地主 领红包|91.70元|剩余次数:10|具体金额及奖励以实际情况为准|2109元|游存616553|21分的前|A A|到微信|永久免费!|4.00元|每日礼包|碎片兑|片兑奖|幸运转|骏淅主|泰安|04\", \"video_asr\": \"我。|六十八红包券到账。|老娘，我炒两个菜，今天怎么吃这么好啊，我这是我们就有斗地主，红包到账了，玩游戏还有红包啊，就是这个圈有豆地主，新人登录游戏即送最高六十八红包。|不能说无论输赢都可获得红包奖励，每打满五局啊，还有堆积红包，他怎么下载？点击视频下方链接就可以下载了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016066789627075195 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '手机电脑录屏', '喜悦', '单人口播', '愤怒', '平静', '红包', '极端特写', '特写', '惊奇', '拉近', '动态', '全景', '配音', '场景-其他', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.74', '0.57', '0.57', '0.12', '0.07', '0.05', '0.05', '0.05', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/045f250937204537b7edb5a4d08e42ef.mp4\n",
      "{\"video_ocr\": \"一定|一定要看|老师|我想报名 作业帮直播课|语文阅读写作提分班|我们班同学 都在用这个学语文|我这就把报名费转给你|现在不用这么多|只要这么多就够了|啊|29块钱能学什么呀?|你可别小看这29块钱|来，给你看|74个必备 阅读写作大招|12种语文思维能力|5种语文学习方法|覆盖语文 必考7大板块|哇~好超值啊!|还有呢|清北毕业名师带队教学|课后辅导老师 1对1答疑解惑|还有这个|超值12件套 教辅大礼包|这些也都是 免费包邮赠送的|这里面的东西|都是孩子们用得到的东西|那我怎么报名啊?|点击视频下方查看详情|就能立即报名啦|价值499元的课程|现在只要29元|你还在等什么|赶紧点击视频下方 马上报名吧|语文|语文成绩上不去|想在|想在开学逆袭的孩子|¥499|¥29|4 必备阅读写作大招|客师有文格|名师有大招 00|解精更高效|7 大板块|中国女排\", \"video_asr\": \"语文成绩上不去，想在开学逆袭的孩子一定要看导师，我想报名作业，帮直播课语文阅读写作提分班。|班同学都再用这个学语文老师我正在报名费转给你。|现在不用这么多，只要这么多就够了啊，二十九块钱，二十九块钱能说什么呀？你可别小看这二十九块钱啊！来给你看七十四个必备阅读写作大招，十二种语文思维能力，五种语文学习方法。|语文必考七大板块哇，好超值啊！还有呢，清北便名师带队教学，课后辅导老师一对一答疑解惑，还有这个！|超值十二件套教辅大礼包，这些压抑也都是免费包邮赠送的，这里面的东西啊，都是孩子们用的到的东西，我怎么报名啊？点击视频下方查看详情就能立即报名了！|二十四百九十九元的课程，现在只要二十九元，你还在等什么？赶紧点击视频下方马上报名吧！|名师有大招，解题更高效！\"}\n",
      "multi-modal tagging model forward cost time: 0.021909475326538086 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '单人口播', '平静', '配音', '场景-其他', '室内', '静态', '手机电脑录屏', '特写', '办公室', '喜悦', '极端特写', '动态', '混剪', '单人情景剧', '拉近', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.99', '0.29', '0.13', '0.04', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04638cd410ec5e45707c72d88446cc4b.mp4\n",
      "{\"video_ocr\": \"作文其实|是分为几个方面|课|观察|描写|构篇|及综合运用|就是从不同的视角|能够去写出不同的文章|一定要去注意|千篇一律的文章|在考场当中|是拿不到高分的|可能|你写了40分钟的作文|但是在阅卷老师的眼里|批改时间|大概只有1分钟左右|那你的作文|一定要写的与众不同|这就是观察的作用|什么叫做描写|就是我们所说的|把作文写长|写具体|其实这一点|是我们很多小朋友|到目前没有做到的|还有另外一个|非常重要的|就是构篇|叫做作文的|谋篇布局能力|高级的构篇技法|也就是如何能够|把作文这些材料|所去搭建成一个|高楼大厦|这是我们现在每一位|想要对自己有追求的小朋友|需要去做到的|我是|作业帮直播课语文老师|王赫为|在我的课上为同学们总结了|十大高分写作技法|20节课只要29元|赶快点击视频下方链接|报名吧|新学期语文高分特训营 29元=11节语文直播课+9节多科提升课|作业帮直播课小学语文教学负责人|七年一线教学经验|士咖7|上课内容与礼盒详情以实际为准|作业帮直播课 中国女排|双师伴学 辅导老师随时答疑，课程3年内无限次回放|2.5亿题库，更懂学生 一线名师直播授课，随时随地放心学|阅读写作双提升\", \"video_asr\": \"作文其实是分为几个方面，观察，描写，构篇，以及综合运用，观察就是从不同的视角能够去写出不同的文章。一定要去注意千篇一律的文章在考场当中是拿不到高分的，可能你写了四十分钟作文，但是在阅卷老师的眼里，批改时间大概只有一分钟左右。|你的作文一定要写的与众不同，那这就是观察的作用。什么叫做描写呢？就是我们所说的把作文写长写具体哪？其实这一点是我们很多小朋友到目前没有做到的。还有另外一个非常重要的就是购片。什么叫做购片？叫做作文的谋篇，布局能力，高级的购片技法，也就是如何能够把作文这些。|了所去搭建成一个高楼大厦，那这是我们现在每一位想要对自己有追求的小朋友需要去做的。我是作业帮直播课语文老师王特围。|在我的课上为同学们总结了十大高分写作技法，二十节课只要二十九元，赶快点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01631641387939453 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '静态', '推广页', '平静', '单人口播', '教师(教授)', '室内', '场景-其他', '极端特写', '手写解题', '多人情景剧', '配音', '手机电脑录屏', '教辅材料', '特写', '学校', '单人情景剧', '影棚幕布'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.91', '0.05', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/046b4aabc6a36f6a2dda2c05dd658da6.mp4\n",
      "{\"video_ocr\": \"3~8岁是培养孩子|绘画天赋的黄金时期|拼的就是孩子的|创新思维和动手能力|小熊美术A课|是由专业美院毕业|名师授课|10节课仅需49元|就能给孩子更多可能性|现在报名还包邮赠送|绘画礼盒|包含每节课的绘画工具|岔|真的是太值了|kiaoxiongmeishu.com|小熊美米|美术宝出品|MARKERI|包邮赠送绘画大礼包|在家学习 适合3-8岁 1对1辅导|随材礼盒为课程配套物品 不同级别的礼盒略有差异|记号署|S2|儿童绘画启蒙课|49元10节|立即报名|deui 力|oO|包邮 BEARART|远山\", \"video_asr\": \"三到八岁是培养孩子绘画天赋的黄金时期的，就是孩子的创新思维和动手能力。小熊美术AI课是由专业美院毕业名师授课，十节课仅需四十九元，就能给孩子更多可能性，现在报名还包邮赠送绘画礼盒。|包含每节课的配套绘画工具，真的是太值了。|都。\"}\n",
      "multi-modal tagging model forward cost time: 0.01628899574279785 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '极端特写', '配音', '场景-其他', '绘画展示', '静态', '平静', '室内', '教辅材料', '全景', '才艺展示', '中景', '商品展示', '喜悦', '室外', '多人情景剧', '混剪', '情景演绎', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.93', '0.82', '0.16', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/046b753a074b2e98a0129568cafc4032.mp4\n",
      "{\"video_ocr\": \"喂，老婆|孩子读书太贵了|家里都指着你呢|见|现美|办婚礼和买房还羞这么多钱|可怎么办啊|喂?|阿姨?|对了|咱们这个项目还差多少钱|龙总，还差10万|试试玖富万卡|这靠谱吗|当然|最高可申请20万借款额度|最快3分钟放款|最长还能分24期慢慢还|太好了龙总|我这就去办|咱们也能申请么|当然可以|点击屏幕下方|输入手机号就能申请了|美好|业莲人日养计划 八选t他见美好|选播 见美好|螽人培养计划|螽人让划|一个选择 个生|余生才|用余生#|经理|最高可借20万元|贷款有风险，借款需谨慎 请根据个人能力合理贷款，理性消费，避免逾期 贷款额度、放款时间以实际审批结果为准|审核额度、放款时间 最终以持牌金融机构或具备 放款资质的审核标准为准|计划|ONECARD|测测你能借多少钱|最宫品供7nTt|行悔|，见美好\", \"video_asr\": \"喂，老婆孩子读书太贵了，家里都指着你呢。|魏阿姨，我礼盒买房还差这么多钱，可怎么办？喂，阿姨，对了，咱们那个项目你还差多少钱？|还差十万四十九万卡，这靠谱吗？当然，通过九放款，最高可申请二十万的借款额度，最快三分钟放款最长还能分二十四期，慢慢还太好了，能走我这就去办，咱们也能申请吗？当然可以点击屏幕下方输入手机号就能申请了，最高二十万来，请你不问他这次你有多少额度吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016190528869628906 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '惊奇', '平静', '愤怒', '喜悦', '特写', '动态', '朋友&同事(平级)', '路人', '全景', '填充', '夫妻&恋人&相亲', '单人口播', '悲伤', '室外', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.67', '0.57', '0.41', '0.34', '0.07', '0.07', '0.07', '0.04', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/046dea7ac25aec01719969533d930384.mp4\n",
      "{\"video_ocr\": \"识字互动游戏节|京剧脸谱 北大中文系硕博精心打磨课程|斑马AI课 辅导钱教|F|头目一|洱胆袒|耳口手|原创动画教学 包邮赠送教辅大礼包|人鼻|49元/10节课|斑马Al课语文体验课|猿辅导，|出品|头月|口手千|头口ョ|立即抢报|非赠品|眉牙|目口|1m|语文\", \"video_asr\": \"人为二位。|保安。\"}\n",
      "multi-modal tagging model forward cost time: 0.016018152236938477 sec\n",
      "{'result': [{'labels': ['现代', '极端特写', '静态', '场景-其他', '推广页', '教辅材料', '中景', '配音', '手写解题', '宫格', '课件展示', '平静', '幻灯片轮播', '特写', '商品展示', '家', '室内', '动态', '拉近', '才艺展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/048c1f61c1c98498ab99688cb6787b89.mp4\n",
      "{\"video_ocr\": \"小喝|你们怎么都不喝?|现在风变编程上|8.9元就能优惠体验|四天Python实操课和 人工智能认知课|少喝一杯奶茶就能给自己 多一个机会|人工智能认知课+PYTHON实操课 立即体验|点绑茶避!|*CS烟ST VE T0R|A5RSTUVETO|*A5SAST0 VETOR|*TAIS 超R T VE TR|98AST0 VE T城|sTASIS AST0 VET硕|38STO VETOR|Ang S们0VET0R|新学员8.9元体验4天|Dickies|AOECOUSE|oTSB ABTO YETOA|OE CL|mt10¥2&|u10:2&|风变编程|畅|Cep|Mo\", \"video_asr\": \"点奶茶了，不喝不喝不喝你怎么都不喝，现在风变编程上八点九元就能优惠体验四天P三十到课和人工智能认知和，少喝一杯奶茶，就能给自己多一个机会。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016366958618164062 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '特写', '朋友&同事(平级)', '惊奇', '喜悦', '动态', '手机电脑录屏', '悲伤', '平静', '工作职场', '家庭伦理', '愤怒', '极端特写', '夫妻&恋人&相亲', '办公室', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.94', '0.75', '0.73', '0.70', '0.53', '0.49', '0.18', '0.09', '0.03', '0.03', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/048f9863353fbc58f3c785779edd4cc0.mp4\n",
      "{\"video_ocr\": \"发现妻儿被人欺负，|北被人欺负|龙卫军全部出动|战神杨风|4|个电话|小兑|书说|15\", \"video_asr\": \"\"}\n",
      "multi-modal tagging model forward cost time: 0.016408205032348633 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '混剪', '中景', '平静', '动态', '静态', '特写', '场景-其他', '城市景观', '喜悦', '配音', '室外', '办公室', '单人口播', '手机电脑录屏', '远景', '愤怒', '情景演绎', '极端特写'], 'scores': ['1.00', '1.00', '0.94', '0.93', '0.88', '0.78', '0.58', '0.05', '0.05', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/049d66e48c5409248cec372f06ae71d3.mp4\n",
      "{\"video_ocr\": \"兄弟们|跟我一起|填成语领红包|一鼓作光+ 3|自从找到了这个|能提现的成语闯江湖|答对一道题|最高就能拿两元红包|再答题再拿|而且都能提现|给你们看看我提现的|真的是实打实的|现金红包|存到微信里|就能直接花|就是这个成语闯江湖|每次答题最高拿两元|￥ 一天最高拿几十|成语闯江湖|不做虚假宣传|答题就能拿红包|没有那么多套路|而且题目都非常简单|拿到红包啊很容易|点击下方链接|玩游戏填成语 拿大额红包|正在解压缩，本过程不消耗流量|提现到账|查看订单详情|12 : 15|13: 10|医疗健康|获得神宠|提现金|申请时间|提现到账成功 已存入余额|生活缴费|手机充值|00:06 色彩斑|大笑|哄堂 大笺|抽手机|成语闯江湖-到零钱 16.00|湖-到零钱|￥26元|钱包|已充入当前余额|到账| 37|00:45内完成关卡|成语|您获得一个现金红包|恭喜!|答对全部成语|Tldi 咖mMaster|¥15元|色彩 斑 斓|涌媛|当前状态 发起提现|具体活动及金额以实际规则为准|具体活欲际规则为准|武林大会|体活动及金额直服则为准|成语 成语闯江湖|恭喜发财，大吉大利!|10: 55|12: 15|18:30|新春换肤|2020-07-14 16:35:05|城市服务|免费|理财通|忧扰|纷纷|纷扰扰|两脚|三拳 两脚|还差 5 关可以升级头衔 开始|扰缤斑|则为准|预计2020-07-1418:35:05前到账|连击+|第1关|首页 下一关|一鼓作气+|换肤提示 规则|提示|100%|慧眼识珠|R力h6TS6th|宠物|话快|三国故|通发|换肤|选择关卡|拳大|佳击+2|4Gml Gnil 0.2K/s|微信处理中|11 : 46|可语工湖|初出茅庐|新加|设置|常见问题|腾讯公益|全线奖励|收付款|返回|已满|50|恭喜发财|用卡还款|99%|服务\", \"video_asr\": \"兄弟们，跟我一起填成语，领红包，红包，红包娃娃自从找到了这个能提现的成语，闯江湖，答对一道题。|拿两个红包在答题在哪，而且都能提现给你，看看我一见的真的是实打实的现金红包，请到微信里去直接划就是这个成语闯江湖，每次答题都拿两元，一天最多拿几十，同一款家和一六七来疯传，答题就能拿红包，没有那么多套路，而且题目都非常简单，到红包呀！点击下方链接，现在下载成语闯江湖！|赶紧去拿红包吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.01606607437133789 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '手机电脑录屏', '推广页', '配音', '静态', '中景', '红包', '平静', '游戏画面', '单人口播', '办公室', '商品展示', '室内', '填充', '极端特写', '喜悦', '动态', '全景', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.88', '0.80', '0.75', '0.08', '0.06', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04aa5c215be636bdfb7c6689bc2cd408.mp4\n",
      "{\"video_ocr\": \"你看啊|这是一款日本海淘APP|我平常买的大牌都在这儿买|价格比专柜便宜不少呢|你看这个神仙水|专柜卖1635|在这里|只要1099|还有这个紫苏水乳|正好可以改善你的皮肤|专柜要整整766呢|豌豆公主上|只要598|而且保证正品|假一赔十呢|诶|那我要在哪下载呢|点击视频下方链接|现在还可以领取|1088元的“新人优惠券”呢|晚上吃饭走|晚上去看电影吧|40|礼遇红向|￥10|限量领取|新人专享1088元超值优惠券|狂片一天&|狂们片一 天|圣赶抓礼城|RMB|限日式美食|限个护美妆|11:50|日本优品一站购|日本优品一站购|优品一站|全场可用|新人大礼包 128|.00|立即下载|圣遁钜\", \"video_asr\": \"你看啊，这是一款日本海淘APP，我平常用的大牌都在这儿买，价格比专柜便宜不少呢，你看这个神仙水专柜买一千六百三十五，在这里只要一千零九十九，还有这个紫苏水乳正好可以改善你的皮肤，专柜要整整七百六十六码，关注公主上只要五百九十八，而且保证正品，假一赔十呢。|那我要到哪下载呢？点击视频下方链接，现在还可以领取一千零八十八元的新人优惠券呢，吃饭总，那我晚上去看电影吧。|ZSY。\"}\n",
      "multi-modal tagging model forward cost time: 0.016143083572387695 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '静态', '手机电脑录屏', '推广页', '多人情景剧', '配音', '单人口播', '特写', '夫妻&恋人&相亲', '场景-其他', '惊奇', '喜悦', '室内', '平静', '动态', '极端特写', '工作职场', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.73', '0.52', '0.47', '0.41', '0.26', '0.18', '0.17', '0.09', '0.08', '0.08', '0.05', '0.04', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04aacd31524f1e42b94b200973e37deb.mp4\n",
      "{\"video_ocr\": \"mom，|Good! 真棒|That's right. 你是对的|额..因.因照诶|因照诶沃..|我呀给孩子报名了这个|把孩子交给这个有着。|教孩子正确记忆单词的方法|iGA 而不是让孩子一味的死记硬背|UOA|proof /pru:f/证据 从发音到语法再到单词|赶紧点击下方报名吧|为何选择‘超级拱读’？|扫码进群，领取资料 安排上课|妈妈|this is my mouth, 这是我的嘴|and thise is my eyes.|诶呀 你歇会吧|不要钱的 小学英语单词速记训练营|十几年一线教学经验的 Sam老师吧|轻松学习冲刺满分阵营|专业的扫码进群还能高效 领取免费的学习资料|让孩子成为 别人眼中的佼佼者吧|在线学习更高效|[跟谁学]教育平台全国名师|超级拼读]发明人|所有课程已申请专利|CCTV专访名师|0948:19|209:48，|0948|写强弱 加not|Wepiyping:p0p|个 一-一 森链的|ear|小学英语0基 英文睡前|英文|跟谁学]小学英语第一人|全部课程都是故事|TESOL国际教师资格证持有者|英语拼读教学11年|改为否定句|听音写单词的结构|Leor梨|LenY|语法大全 短语总结|上eaY熊|ear 熊|手机号注册，截图保存|找动词|形音意|耳采|工作实政优秀证明|TESOL国际英语教师教学资格证|更系统|找者大|用户7563.:|故事绘本 故事朗诵版|跟谁学|在线学习更高效|报名后扫码进群|+一eaY宝怕|eaY 平朵|平朵|上eaY 紫怕|ZeoY桀|UKONG|DELL|十年磨一剑，Sam老师集十余年的教学经验 于一体，研发超级拼读法|We dor iglhy ping.pong|倾听|亲复的|未复伯|[免费领资料】|英语提分训练营|础发音大全|必考知识点|更高效 更专业|一周前\", \"video_asr\": \"THIS IS MY MOUTH THIS IS MY NOSE AND THIS IS MY EYES COULD THAT RIGHT一周前。|音音音教，因为呀，你歇会吧我呀，给孩子报名为这个不要钱的小学英语单词速记训练营，把孩子交给这个有着几十年一线教学经验的SAM老师，把教孩子正确记忆单词的方法，而不是让孩子一味的死记硬背，从发音到语法再到单词，通过一个原因联系记忆。|几十个单词轻松学习，冲刺满分阵营，赶紧点击下方链接报名吧！扫码进群还能领取免费的学习资料哦！让孩子成为别人眼中的佼佼者吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016705036163330078 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '平静', '场景-其他', '配音', '静态', '手机电脑录屏', '单人口播', '特写', '多人情景剧', '亲子', '喜悦', '室内', '家', '家庭伦理', '动态', '夫妻&恋人&相亲', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.51', '0.50', '0.47', '0.46', '0.29', '0.26', '0.13', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04b756f21094c97b2ee236c3f4e2e2be.mp4\n",
      "{\"video_ocr\": \"虚啊|有点虚|你怎么说话呢|上来就说我虚|我身体好着呢|我没说你身体虚|五行|缺钱|大师啊|你有什么方法吗|门道倒是有|就看你这|身上有没有|哎呀|这种人的话|你都信|你看你衣服|都破了个洞|谁都看得出来|你没钱|你怎么回事啊|我急用钱|人家在帮我|急用钱你可以|找玖富万卡呀|通过玖富万卡|最高可以申请|20万的借款额度|最快3分钟放款|最长还能|分24期慢慢还|我能不能也在|玖富万卡上借钱|点击视频|下方链接|输入手机号|你的额度啦|ONECARD|测测你能借多少|玖高万卡|人能t合理锁款|放款资质的审核标准为准 最终以持牌金融机构或具备|20w|中国电信 零钱明细|最高可借20万元|￥200000.52|纳斯达克上市企业旗下品牌|审核额度、放款时间|贷款额度放款时间等以实际审批为准 贷款有风险借款需谨慎请根据个人能力合理贷款|贷款有风险借款需谨慎 请根据个人能力合理贷款|恭喜您在玖富万卡获得额度200000|三步极速借款|测测你|品|充值|符号\", \"video_asr\": \"嘘嘘，有点虚，你怎么说话了，上来就说我虚，我身体好着呢，没说你身体虚，我说你到了一曲五星。|缺钱大师啊，你有什么方法吗？没得到倒是有，就看你真是哎呀，这种人的话你都信，你看你衣服都破了个洞，谁都看。|出来你没钱你怎么回事啊？我急用钱，人家在帮我急用钱，你可以找玖富万卡呀，通过玖富万卡最高可以申请二十万的借款额度，最快三分钟放款，最长还能分二十四期慢慢还我，我就不借二十万，我能不能也在九万卡上借钱啊，点击视频下方链接，输入手机号就可以申请你的额度啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016391992568969727 sec\n",
      "{'result': [{'labels': ['中景', '现代', '多人情景剧', '推广页', '静态', '喜悦', '愤怒', '惊奇', '单人口播', '夫妻&恋人&相亲', '手机电脑录屏', '悲伤', '动态', '特写', '全景', '路人', '平静', '室外', '极端特写', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.93', '0.84', '0.59', '0.51', '0.34', '0.18', '0.15', '0.13', '0.09', '0.04', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04c0509f2bb3041990854ecb74e9a2d4.mp4\n",
      "{\"video_ocr\": \"熬夜刷题145|英语没有 天天背单词147|语文作文不套模板132|物理不用背公式98|怎么做到的吗 想知道我是|高中想要考出好成绩|不单靠努力|更重要的是|掌握学习方法|提高学习效率|来高途课堂|高中全科名师班|清北毕业名师带队教学|根据课标和考纲|总结数学必备|439个知识点|物理考试|108个解题大招|英语单词7大记忆法|语文作文高分36技|课后老师仅对1辅导答疑|让孩子|做一道题会一类题|9元4科16节课|语数英物全面提升|轻松学习|高效提分|不要再犹豫了|赶紧点击视频下方|查看详情报名吧|新学员9元专享 立即报名|D. 1+i|*求数据控制者永久删除有关数据主体的个人数据， 合法的理由。在大数据时代，数字化、廉价的存储器、|的理山。太大数掂时心，数字化、A价的 有关数据主体的，|在电磁感应现象中的具体体现(|省高考状元带队授课 老师平均教龄11年+|C.0.7|道忘叔”更是一项主动性的权利，其权利主体可自主决定是 公开的有关个人信息进行删除，是数据主体对自己的个人信|上就是能量转化的过程故选D 女阳的公韩约可视为习速阅周运动，它们的向心加|则该学校阅|数据时代数宇化记忆伦理的意义。|之化记化理的意又。|电流的磁场阻碍引起感M|地防御自己的隐私不受侵犯，而是主体能动地控制个人信 步说，是主体争取主动建构个人数字化记忆与遗忘的权利。|“自己的不变笑租，而是主体他|电流的磁场阻碍引起感应电流的原磁场的磁通量 动将其他形式的能转变为感应电流的电能，所以楞|十，在每小题给出的西个选项市。其销|在每小题给出的西个选暖中，贝箱|h，在何小题给出的严个选暖中，月籍|时间:120 分钟；命题人：xxx 六|理科)(新课标Ⅲ)|卷副标题|招生全国统一考试|楼梦》是中国古典文学瑰宝，并成为中国古典小|发展的四大驱动力，改变了记忆的经济学，使得海量|这展的四大雅动力，改交了记忆的经齐学 典的成本文低记忆和谴|f焰教通感行的速率分别为|们通牧道福行的速率分剧为|f的我道福行的速车分别为|视频为演绎情节|和人类记忆与遗|名师出高徒·网课选高途|名师特训班|总分|D.(0.1.2)|话选择（共5题)|题（共5题)|大数据狂出永久 |C.1-i|to do?|高中数学没有|式卷（浙江卷)|若东|名著的情况|阅读过《红|的磁通量|业女局，|七 八|招生|47|11|Ⅱ卷|分)\", \"video_asr\": \"高中数学没有熬夜刷题一百四十五，英语没有天天背单词一百四十七，语文作文不套模版一百三十二，物理不用背公式九十八，想知道我是怎么做到的吗？高中想要考出好成绩，不单靠努力，更重要的是掌握学习方法，提高学习效率，来高途课堂高中全科名师班。|清北毕业名师带队教学，根据课标和考纲总结数学必备四百三十九个知识点，物理考试一百零八个解题大招。|与单词七大记忆法，语文作文高分三十六计，课后老师一对一辅导答疑，让孩子做一道题，会一类题九元四科十六节课，语数英物全面提升，轻松学习，高效提分，不要再犹豫了，赶紧点击视频下方查看详情报名吧！|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.01665496826171875 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '静态', '推广页', '平静', '室内', '场景-其他', '配音', '教师(教授)', '极端特写', '喜悦', '动态', '情景演绎', '课件展示', '教辅材料', '拉近', '填充', '手写解题', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.74', '0.56', '0.19', '0.13', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04c4841a37e83876b6262d685399085d.mp4\n",
      "{\"video_ocr\": \"我看别的孩子学习|每天做那么多题|我也给你买了。|这几套卷子我都 做过了|那你怎么每天还有 这么多时间|玩手机呀|哎呀|别再打扰孩子 听课了|现在还想着 刷题提分|只是浪费时间|我给他报名了|高途课堂 全科名师班|北大清华毕业名师 直播授课|掌握答题技巧|这学习根本不用 那么费劲|就是你上次 给他花9块钱|报的那个直播课|对啊|老师教给了我们很多 解题大招|用对的方法 吃透一类题|胜过刷 这1000道题|我现在做题 又快又准|正在突破 重难题型呢|没想到 高途课堂的老师们|这么有方法|别的孩子还在|顶着那么大 压力刷题|你这呢|在这么短的时间内|可以挑战高分了|镜头前的同学们|不要再盲目学习了|课程还有报名机会|只需要 9元|赶快报名吧|儒|新用户专享 立即体验 浙江卫视指定在线教育品牌|浙江卫视|视频为演绎情节|名师特训班|全国百佳教师带队教学平均教龄11年|￥9\", \"video_asr\": \"我看别的孩子学习每天做那么多题，我也给你买了这几套卷子，我都做过，你怎么每天还有这么多时间玩手机呀，哎呀。|别再倒腾孩子听课，现在还想着刷题提分，这是浪费时间。我给他报名了高途课堂全科名师班，北大清华毕业的名师直播授课，掌握答题技巧。|学习根本不用那么费劲，就是你上次给他花九块钱报那个直播课会啊，老师交给了我们很多解题，用对的方法吃透一类题。|胜过刷这一千道题，我现在做题又快又准，正在突破重难题型呢，没想到高途课堂老师这么有方法，别的孩子还在顶着那么大的压力刷题。|你这呢，在这么短的时间内可以挑战高分了，镜头前的同学们不要再盲目学习了，课程还有机会报名，只需要九元，赶快不明白。\"}\n",
      "multi-modal tagging model forward cost time: 0.01652669906616211 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '多人情景剧', '静态', '平静', '亲子', '家', '家庭伦理', '喜悦', '极端特写', '全景', '朋友&同事(平级)', '动态', '愤怒', '夫妻&恋人&相亲', '悲伤', '惊奇', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.91', '0.85', '0.22', '0.15', '0.12', '0.09', '0.05', '0.04', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04c68c156d434c6912dd9dad9e6cafb9.mp4\n",
      "{\"video_ocr\": \"这里放个1|家里有3-6岁的孩子|提醒你一下|咱就说孩子|应该什么时候学数学|这件事|作为家长|你是不是|还没认识到重要性呢|看下面这道题 7+9-?|逻辑思维强的孩子|可以直接算出答案|而你的孩子 7+9=16|是不是要列坚式|才能得出答案|所以啊|斑马Al课 思维体验课|把直观感受融入生活场景 49元/10节课|边玩边学|引导孩子主动记忆|让孩子爱上数学|现在10节课仅需49元|49元|今天报名|还赠送全套教具礼盒|斑马AI课思维体验课|从小培养孩子数学思维|节|猿辅导在线我m 出摄|猿辅导在线教育出选|狼辅导在线教算 出|猿辅导在模权育 出|从小培养孩子数学思维学数学最好是从小抓起|从小培养孩子数学思维专为3-6岁孩子量身打造|从小培养孩子数学思维从小培养孩子数学思维|斑马用维|现马思组|思维|减号出现减少啦 减少啦|加减等号是一家 是一家|2-8岁上斑马 学思维 学英语|小桥等号直直哒 啦啦啦 啦啦啦|点击视频下方查看详情|适合3-6岁宝宝|S3\", \"video_asr\": \"这里放个一，这里放个三，家里有三到六岁的孩子提醒你一下，咱就说孩子应该什么时候学数学这件事作为家长，你是不是还没有认识到重要性呢？看看下面这道题，逻辑思维强的孩子可以直接算出答案，而你的孩子是不是要列竖式才能得出答案？所以呀，学数学最好是从小抓起斑马。|AI课思维体验课专为三到六岁孩子量身打造，把直观感受融入到生活场景，从小培养孩子的数学思维。|玩边学，引导孩子主动记忆，让孩子爱上数学，现在十节课仅需四十九元，今天报名还赠送全套教具礼盒。\"}\n",
      "multi-modal tagging model forward cost time: 0.01623702049255371 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '场景-其他', '中景', '配音', '静态', '单人口播', '室内', '平静', '教辅材料', '课件展示', '手机电脑录屏', '喜悦', '特写', '极端特写', '动画', '知识讲解', '拉近', '商品展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.64', '0.47', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04d67e79920565a099146d760c99338e.mp4\n",
      "{\"video_ocr\": \"9块钱|只要9块钱|就有机会|挑战双一流名校|这是什么啊|就是高途课堂的 初高全科名师班啊|我这开学就升高二|该分科了|可我高一就跟不上|现在更吃力了|我还有机会吗|当然有|这个课课后|有辅导老师1对1答疑|课程3年内|免费无限次回放|听不懂随时问|想听随时听|直到你学会为止|那这个课都有|什么老师上课啊|平均教龄11年的|北大清华毕业名师|带队教学|这些老师|还是全国百佳教师|全国优秀班主任|功勋教师|双师辅导|还有各科学霸|和高考状元给分享|考试经验和技巧呢|那这个能学会|各种秒杀技巧吗|这是必须的呀|教你语文作文36技|应对作文5大模块|数学考试|7大必考模块|专项突破|英语4小时高效学习|搞定阅读写作|物理108技巧|秒杀重难点|那这要怎么报名啊|高途课堂|浙江卫视\", \"video_asr\": \"九块钱九块钱，只要九块钱就有机会挑战双一流名校！|这是什么啊？就是高途课堂的初高全科名师班呀，我开学就是高二，该分科了，可我高一就跟不上，现在更吃力了，我还有机会吗？当然有这个课呀，课后有辅导老师一对一答疑，课程，三年内免费无限次回放，听不懂随时问，想听随时听，不知道你学会为止啊，那这课都有什么老师上课啊？|平均教龄十一年的北大清华毕业名师带队教学，这些老师啊，还是全国百佳教师，全国优秀班主任，功勋教师，双师辅导，还有各科学霸和高考状元给分享考试经验和技巧呢，那这个能学各种秒杀技巧吗？这是必须的呀，教语文作文三十六计，应对作文五大模块，数学考试七大必考。|专项突破英语四小时高效学习，搞定阅读写作，物理一百零八技巧，秒杀重难点。\"}\n",
      "multi-modal tagging model forward cost time: 0.022110700607299805 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '填充', '中景', '静态', '平静', '推广页', '室外', '特写', '全景', '朋友&同事(平级)', '路人', '家庭伦理', '亲子', '喜悦', '惊奇', '动态', '悲伤', '宫格', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.87', '0.79', '0.31', '0.23', '0.22', '0.06', '0.05', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04d79a8a258bfe3807e86a61b92bdaf5.mp4\n",
      "{\"video_ocr\": \"经常上课走神|做事没条理|学习不自主|你家孩子是不是|也遇到这种问题|12岁之前 不矫正|孩子成绩很难提升!|网易有道少儿编程课|专为8-12岁孩子设计|由中科院博导研发|4节课|激活孩子逻辑大脑|教会孩子|6种递推思维模式|10类思维潜力|提升专注力|培养孩子学霸思维!|12岁学习不自主|网易有道 0基础少儿编程课|My|注意力集中|1|中科院博导+网易有道联合开发|为什么一定要学编程?|0基础入门 培养逻辑思维能力|9.9元抢10课时|有道精品课|提升专注力，培养学霸思维|9.9元抢10课时 仅限100套|别不舍得9.9元，而耽误孩子一辈子!|学习积极|i9岁做事没条理!|版罗学不再重|8-12岁孩子|有道小图灵|黄体右游\", \"video_asr\": \"经常上课走神，做事没条理，学习不自主，你家孩子是不是也遇到过这种问题，十二岁之前不矫正孩子的成绩真的很难提升。网易有道少儿编程课中为八级十二岁孩子设计由中科院博导研发四节课，比如孩子逻辑大脑，教会孩子六种必备思维模式。|激发孩子独立思维潜力，提升专注力，培养孩子的学霸思维。八到十二岁孩子为什么一定要学编程？八岁上课走神，四节课后注意力集中。|九岁做事没条理，四节课后做事有条理，十二岁学习不自主，四节课后学习积极，网易有道零基础少儿编程课，提升专注力，培养学霸思维，九点九元抢十课时仅限一百套，也不舍得九点九元而耽误孩子一辈子。\"}\n",
      "multi-modal tagging model forward cost time: 0.02184581756591797 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '静态', '中景', '室内', '场景-其他', '平静', '幻灯片轮播', '推广页', '配音', '手机电脑录屏', '填充', '教辅材料', '室外', '教师(教授)', '特写', '动态', '重点圈画', '课件展示', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04dd4e8327f875c533fa1d6f4d3fde6d.mp4\n",
      "{\"video_ocr\": \"oVer怎么理解它呢|Ve的本意就表示|over的本意就表示翻越过去|比如说这一条河流啊|Cross the river.|平行穿越|Jumpovertheriver. 越过去的|什么叫结束啊翻过这篇了|Weareover. 我们分手吧|还有我们打游戏的时候|什么叫跨越啊上课的时候 老师首先会讲到|GO over the article.|把这个文章再浏览一遍|over mydied body. 这什么意思呢宁死不从|over无论是表示结束也好|翻过去越过去也好|泰界的是种越这个过|都表示的是跨越这个过程|时间上的这个跨越的过程|overthe past20years.|thepast.20yE|在过去的20多年当中|10节课满满干货|还有各种服务|各位今天只需要9元钱|9元钱就能够轻松参加了|500个名额名额有限|赶紧哦点击屏幕下方报名吧!|有道精品课|钟平|创立中英文对切公主|20年英语执教经验|心|钟平 20年英语执教经验|创立#|创立中本|老师就是好!|NYSE\", \"video_asr\": \"OVER这么理解他呢，OVER的本意就比较是翻越过去，比如说这一条河流啊CROSS THE RIVER OF COURSE平行穿越。|OVERDRIVE说过去的什么叫结束啊？翻过这篇了OR OVER我们分手吧，还有我们打游戏的时候GAME OVER什么叫跨越啊？上课的时候老师首先讲到GO OVER GO TO GO把这个文章啊，在浏览一遍，OVER MY BODY这什么意思呢？IS OVER。|无论世事结束也好，翻过去越过去也好，都表示的是跨越这个过程，时间上的这个跨越的过程OVER THE PAST TWENTY YES在过去的二十多年当中，十节课满满干货，还有各种服务，各位今天只需要九元钱。|九元钱就能够轻松参加了五百个名额，名额有限，赶紧哦，点击屏幕下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.0160980224609375 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '静态', '平静', '教师(教授)', '单人口播', '室内', '知识讲解', '配音', '场景-其他', '过渡页', '单人情景剧', '办公室', '多人情景剧', '特写', '喜悦', '手机电脑录屏', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.92', '0.32', '0.14', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04de3635c47a7c673e4213b1d982b6d2.mp4\n",
      "{\"video_ocr\": \"6.9元试学 收藏备用 适|适用人群|经常处理事务繁忙的加班族 办公软件高频使用者|职业发展受限老 管理者/创意工作者|学完后你能收获什么|高效办公的 认知升级|完成课程3个|教学特色|交互式课堂，闯关式学习|社群助教鼠踪答疑，高效学习|办公知识点|编程作业实操|简单实用的，追程课|办公自动化 首现仅需6.9元零基础入门|单实用的 编程课|单实用的小白编程课|的小白编程课|掌握2个自动化|重复工作效率低|加班秃头伤身体|那是因为你还没有学Python|来闪光编程学Python|6.9元就可以体验3节自动化办公课|简单易学，让你工作更高效|想准时下班吗?|那就关注领课吧|新学员6.9元体验3节 高效办公思维课+Python自动化实操课|59 64|FDo|初识 Python|掌握10个Python|936|掌握 维程思维|在线代码实操，边学边练|596|物识用|闪光编程|立即体验\", \"video_asr\": \"重复工作效率低，加班秃头上身体，那是因为你还没有学PYTHON来闪光编程学潘森，六点九元就可以体验三节自动化办公课。|单易学让你工作更高效，想准时下班吗？那就关注领课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016571044921875 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '平静', '手机电脑录屏', '路人', '极端特写', '喜悦', '朋友&同事(平级)', '特写', '室外', '单人口播', '全景', '动态', '悲伤', '办公室', '惊奇', '填充'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.78', '0.27', '0.21', '0.17', '0.15', '0.07', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04e213192623a1e9d59103d51d874884.mp4\n",
      "{\"video_ocr\": \"你是怎么买菜的|上面的叶子全焉了|你让人怎么吃啊|我一下班|就赶去菜市场了|可是...|那你怎么不去|叮咚买菜|上面的食材|全是新鲜食材|蔬菜绿色无添加|而且品种又多|最快29分钟|就可以送达哦|还等什么|赶紧下单吧|哇|新用户注册|还有108元的|买菜红包耶|真的好快|而且|比我在菜市场买的|新鲜好多耶|叮咚|some|no nlace|3买菜|最快29分钟仓鲜到家。|Jfone|nopace|即可领取|108元|最快只需|极速送达|买莱\", \"video_asr\": \"你是怎么买菜的？丈夫的叶子全掉了，你下班就感觉菜市场啊。|那你怎么不去的都买菜啊，上面的食材选择食材，蔬菜绿色，无添加，而且种类多，就怕二十九分钟P可以送达，等什么赶紧下单吧！|新用户注册还有一百零八元两包送来。|真的好快，而且我的车好多新鲜好多，最快二十九分钟，抢先大家。\"}\n",
      "multi-modal tagging model forward cost time: 0.016180753707885742 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '中景', '静态', '平静', '单人口播', '配音', '场景-其他', '动态', '室内', '拉近', '多人情景剧', '特写', '宫格', '喜悦', '家', '学校', '课件展示', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.93', '0.85', '0.79', '0.71', '0.57', '0.11', '0.09', '0.05', '0.02', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04e5ce94c7fdfe28d5368e54b6f554d8.mp4\n",
      "{\"video_ocr\": \"为什么上了初中以后|孩子成绩突然就跟不上?|天天抱怨|我就是不喜欢学数学!|其实他不是不喜欢学习|只是不喜欢|怎么都学不会的挫败感!|那大叔怎么做呢?|首先啊|大叔会先用一些解题大招|让孩子们在特定题型上|也能秒杀学霸!|虽然这个时候啊|孩子的基础并不扎实|但是他已经有了|学习的兴趣|A 然后呢|我会一步步告诉孩子|为什么这么做|最后你会发现|提分真的很简单|我是高途课堂 陈冠男老师|毕业于同济大学|被同学们 亲切地称为|在大叔多年的|一线教学生涯中|总结出了一套|“初中数学组合拳法”|包括|2个考点|还在等什么？|点击视频下方的 [查看详情]|报名吧|12.如l， 的轨迹为( )|的软迹力 ）|的软速为)|为正方形，倒面P|的轨透为|I2. 如图，在四校销|ARGD)为正方用|如国图。在阳校银P-HGD中，倒面PD 为正三角形.抗面RC0力正方形，侧面PA|12 伽，在四技维产-AD中，C面P0D 为正三角形，底面4CD为正方服，剩面户PA0|12 伽国，在四技维 产-ABCD中，餐面P0 为建三角形，权面4RGD为正方形，妈面P0|如图。在奥校值户-4GD中.剩面P4D力正三角形，面 8D为正方形，银面PM|在国核信P-BCD中.剑低PA4D力重宝角形，起面 48GD力正方用，剑面户M|华少|BB与EF成60-角|CAR与CD成60角|DM与EF成6角|全国百佳教师带队教学 平均教龄11年|浙江卫视|高途课堂|园浙江卫视|(a-b|高途课堂｜，|“陈大叔”|自块|59个知识模块|仅需|陈冠男|名师特训班|新用户专享立即体验 浙江卫视指定在线教育品牌|深耕数学牚一线教学 数学资深主讲|a+b)|的值是|D，|常考几何模型|23个常考几何模型|A-5|D.5|¥9\", \"video_asr\": \"为什么上了初中以后，孩子的成绩突然就跟不上，天天抱怨我就是不喜欢学数学，其实他不是不喜欢学习。|只是不喜欢怎么学都学不会的那种挫败感，那大叔怎么做呢？首先啊，大叔会用一些解题大招，让孩子们在特定题型上也能秒杀学霸，虽然这个时候孩子的基础并不扎实。|是他已经有了学习的兴趣，然后呢，我会一步步告诉孩子为什么这么做，最后你会发现提分真的很简单，我是高途课堂陈冠男老师。|毕业于同济大学，被同学们亲切地称为陈大叔，在大叔多年的一线教学生涯中，总结出了一套初中数学。|组合拳法包括三百零二个考点，五十九个知识模块，以及二十三个常考的几何模型，还在等什么，点击视频下方的查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.017343521118164062 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '配音', '平静', '教师(教授)', '静态', '单人口播', '手写解题', '场景-其他', '室内', '极端特写', '手机电脑录屏', '特写', '情景演绎', '混剪', '教辅材料', '朋友&同事(平级)', '学校'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.97', '0.95', '0.80', '0.55', '0.06', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/04f86195025a3653e6c23ce9cebc22d5.mp4\n",
      "{\"video_ocr\": \"我们应该怎么样去理解定冠词逻辑呢|各位同学 学英语的最大的|一个最好的方法就是|想清楚道理而不是死记硬背|定冠词 “a 、 the”的添加|如果想不明白的话是一团浆糊|首先呢 泛指|就是不是特别注重是哪一个|比如说I need a friend. 我需要一个朋友|那我说我要把特定化|什么叫特指呀|在众多当中|你特殊的指到某一个就是特指|I need the one who has a car. 我需要那个有车的那个人做我的朋友|所以这就叫特指和泛指的区别|这些规则很简单|但是不是很难学呢|逻辑英语20年原创心血研究|用公式看懂所有句词|用逻辑想明白语法背后的道理|用特别有趣的方法去理解单词暴涨的奥秘|听说读写能力全面提升|尽在逻辑英语的研究|各位 今天只需1元|点击屏幕下方 赶紧报名吧|500个名额 手慢就没有了|易NETEASE|铜易NcvEAsE|用公式|钢易NrTEAs|钢易NeE|绸易NerEA|有道精品课|钟平老师|语法 逻辑+道理|重新定义中国未来英语教学 英语老司机拥有18年教学经验 著有《逻辑英语语法》《英文观止》等畅销书|看值|看懂句词|dohnannl laners|ddhuand lanmnh |Aihvmnd leamnerh|ddannd learnrh|ddranad armens|Adhanel letrnrs|dhaal keamrs|Aahannl beamrh|Aheanal bamers|Ahaond Learrsa|Ahuanl Leamnrk|Maandbearnrs|Oxford\", \"video_asr\": \"我们应该怎么样去理解定冠词逻辑呢？各位同学，学英语的最大的一个最好的方法就是想清楚道理啊，不是死记硬背你这个定冠词啊OFFER的添加如果想不明白的话，是一团浆糊。首先呢，范词是不是特别注重是哪？|比如说I MEET SURPRISE我需要一个朋友，那我说我要把特定化那个特质啊，在众多当中，你特殊的指导的某一个就是特制的IMS的YUI SCAR，有需要那个有车的那个人做我的朋友。|特指和泛指的区别，这些规则很简单，但是我很难学呢，和英语二十年原创新学研究，用公式看懂所有句子，用逻辑想明白语法背后道理，用特别有趣的方法去理解。|词暴涨的奥秘，听说读写能力全面提升，尽在逻辑语的研究！点击屏幕下方，赶紧报名吧，五百个名额，手慢就没有咯！|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.01624894142150879 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '单人口播', '室内', '平静', '家', '配音', '极端特写', '教师(教授)', '场景-其他', '喜悦', '混剪', '情景演绎', '多人情景剧', '室外', '特写', '远景', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.76', '0.61', '0.05', '0.02', '0.02', '0.02', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05064eba0f9b3662bb1be55fae155ef9.mp4\n",
      "{\"video_ocr\": \"孩子爱不释手的美术课 前100名6.6体验！速抢!|o71|啦啦|0\", \"video_asr\": \"\"}\n",
      "multi-modal tagging model forward cost time: 0.017136335372924805 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '绘画展示', '才艺展示', '极端特写', '静态', '推广页', '商品展示', '配音', '教辅材料', '手机电脑录屏', '动画', '幻灯片轮播', '填充', '转场', '宫格', '拉近', '重点圈画', '手写解题', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/050eba4089b28d0fb921a300332de2b7.mp4\n",
      "{\"video_ocr\": \"蒋力|好啊|我们抽不到iPhone 11|你自己却是用上了!|你们机工坊|玩游戏集碎片|抽免费iPhone11|全都是骗人的!|确实是骗人的|你!|正版飞机工坊|玩游戏通关就有机会|直接获得1部iPhone/11|根本不用集碎片!|你下载的是盗版 飞机工坊!|内容弘 对不起啊|是我搞错了|那最正版的去哪里下载?|想要免费领取 iPhone11的朋友|赶快点击视频下方链接|下载正版飞机工坊吧！|具体奖励以实际为准|new balond|newbalar|newlboibnce|10|N3\", \"video_asr\": \"哦，好啊，我们抽不到IPHONE十一你自己全身有上啊，你们飞机工坊玩游戏集碎片免费抽IPHONE十一圈都是骗人的，确实是骗人，你正版飞机工坊玩游戏通关就有机会直接获得一部IPHONE根本不用集碎片。|你下载的是盗版飞机工坊，对不起啊，是我弄错，那最正版的去哪里下载？想要免费领取IPHONE十一的朋友，赶快点击视频下方链接下载正版飞机工坊吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01610732078552246 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '静态', '推广页', '惊奇', '愤怒', '全景', '喜悦', '室外', '朋友&同事(平级)', '夫妻&恋人&相亲', '单人口播', '路人', '手机电脑录屏', '平静', '动态', '悲伤', '极端特写', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.86', '0.61', '0.49', '0.24', '0.10', '0.07', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05163f6851f21c21bac3674c3cde440d.mp4\n",
      "{\"video_ocr\": \"9.2后-么2 一点|192赤一赢”-点|92点一赢”以一点 4市<南=2前动）|3<A+含<智 女<sm<A+多|由此有CosA SinC取值苑国(受号|三点在同-条直线上，从而有器头 动 解sn-站|多 解得三an-|结论8a.夏Ap刷梁3 结论心0‘偶-特=nd，|结论8知a.是Ap，刚得头3也冕邻，公差兰 结论1S保-分奇nd 舞a|求形如(a好+63，求炒”的数它的展开顶教是|报了那么多辅导班|成绩却还是不见提升|那是你们家的孩子|没跟对老师|没掌握正确的学习方法|他知道高中英语892个|核心词汇都有哪些吗|他知道语文阅读中|如何3步完成|阅读答题吗|他知道高考数学必备的|42大几何模型吗|他知道掌握物理口诀法|能秒杀10大难点|失分点吗|高途课堂推出的|暑期高中全科名师班|16节北大清华|毕业的老师|直播课|让你家高中生轻松掌握|这些答题技巧|1分钟1道小题|暑假回来的第一个考试|惊艳所有人|点击视频下方链接|仅需9块钱就能上|名额有限快来抢课吧|超强圆锥曲线 结论/过圆y+=2上任意点P作圆=d的两条|论1s过圆字+-2d上任意点P作圆犷=a的两条|w止户A台|设锐兑角三角形ABC的内角A.B.C的对边分别为0、|当你发现.数列中只有-个等式时，设每-项为|选 填技巧|过城ぺ巧5|浙江卫视|高途课堂|园浙江卫视|1. e”万x+1|切线.则两条切线重宜.|设锐兑|结论1.等差数列与-次函数的关系|直接解出方程就可以|结论1.@n=a,+(n-1D)d =am+n-md|有-条侧棱垒直于底面(QH法则)外接球 步马:O正弦定理直接算|结|高途课堂|园浙江卫视|8.(1+3)》，片n3 岁>-|()当点m在双又曲线左支上， 则焦半径M下zC-Q)|有CosA+sihC取值苑围|从而爹<A+告<年 所从 一sim(A+2<号|别-|双曲线中e= |fih(8+1》) 布圈与双曲线共焦点时.有-个非常好的=级|结论9 SmtSn+mnd=- Sm+n|日套用上面的r公式解出r 日代入式子中求出系教即可|金道oWr买式创货|In57e 10 帝4lnc1+%)y4e'-1|10市y1n(I+%)ey4e”-1 9In3万-e|In万-西 7. 1n(み1》万赤|的两条切线，则两条切线重直 结论牛点m0%)在物线y=2p8上，过点m作抛|(22求CosA+ nc的取值范日 “7in)由a=2bsinA，根据正石得A=:25Bssin.|\\\"in)由a=2bsinA，根据正孩得A =:.2iBcSinn.|线的科率，由此可用三点共线解解决等差数列. 结论2.等差教列前n项和公式变形Sn= 是n+(a-数n|焦半径的倒教之和为定值|结论3. Qm*Qn+0k三ap+qq+|结论3. 0m+an +0K=ap+qq+Qr p+q+r=m+n+k|底面是下列三角形.分别形成以下结论: 0等边三角形的三棱锥:R=小本片+言a|小N特时事美陪H战|I5赢m2J l6.26(1六户43 CnEer] 18(1+3川力什n当 为>-|15赢2 三|I5赢w|下显双曲线长 卡的右焦点， M是双曲线上任意-点|由AABC为锐自形如: -A多-B要-子=多|求这个数列的前3n顶的和 Sn。 例 己知在等差教列知m中.Sn-3，Szm二44，|在布图中e三 Sio+Shp|On 2m-!.S2nl|Ca'rb 0将括号内为或者的全部调整|结论5:|则过A.B的切7线的交点M必在相应准住线上|六 SinB= 士|麻花公式4z+zy，二|例:0、0q.@ …分|例:0、0q.a7…… 公差为3d|日等腰三角形的三梭锥：R=云行+Q|12ペ2(对 13.2(-原)清<2G厅-厅)|12 ←24式-* 13、20-)42u厅-n|13.20丽-)赤26-R) 12心24)|AB为物线的焦点弦，|[SAgAt=ccocosA- sincu-香-A) 中由aBc角急角三角形イB|5“sA+二cCoscosA- sincu-售-A) 浏ACsA -wsA+ simA|CoSA+SiC =CosA 十sin(兀-飞-A)|neN，所有的点(n.制)都在同-条直线上，从而对m， naN有量盘即数列得￥]一个笑差数列!|nc有量需是即数列得-个等差数列!|抛物线的结论，焦点孩AB影“通径最久)|例：S.S-S. S6-|例冽：S.S-S. S-S4”w么差4d|日 狂形的四棱锥:R-J平厅+本(a+5) 田直角三角形的三棱锥：R=h+c|ezex ez% 5.In371す|3. 6、 In%き|2.e'zex|结论2:过圆什y=a+B上任意点p作有圈茶=|结2:过圈少y=aB上任意点P作枰有圈茶+！|对于公差为dld+o)的等差教列和n其通项公式|2eP 2a6|例:a7=Q,+6d- Qa+4d|4.1n为为-1 5、n岁7上方|结论3:过圆许y-a-B上任意点P作双曲线等告|(1)求B的大小|b.C, a=2bsinA|为Gn=dn+(Q-d)，则点(n.@n)CneN*共线，又dニ|焦点弦百长度IAB= Fecose  Q-ccos'9|结论2.Qm+an=a,+0g m+n-p+q 例:@+Qg=as+ag= @;+ay|@再和体的高直接形成|1市In1+3)e3 e1|物线的切线方程为yy=PC8+3)|CosACosAs，sin(+A)|由 公Bc角之角三角形气B”|解:(1)由a=2bsimA，根据正弦得SinA=2SinBSinA， 由AABC为钱角三角形得B=る|等差数列前n项和公式的变形Sn=号ゃ+(a-多n 两边同除以n得音=号n+(a-生.该式说明对任意|弦长公式|mM= 2abA+8N心A+6B-己 aA2+6B|倾 a+Qg+07=Qz+a 结论4.Q.Qrm. @v+2m …|例:a+Q;+a7=Qz+04 +日5 结论4.Q.Qrm. @v+2m ……公差为md的A|②等聘直角三角形的三棱锥:R=本府+士a 日平方和相加求出外接球半径|由题意妇，(n，普）(2n，篑) (3n，新|结论7ny 2n-1 T2n┤|社风.N|成幂函娄形成|名师出高徒.网课选高途|In%￥|an-lm cn士m.所从d为过(m.Qm入CnQn)两点的直|则AB的切线交点m必在准线上|万((A+至)|一a((月+季，|=O0SA+士osA+暨sinA 二(A+含|结论61m仙h了为AP=>|结论6m3 仙b了为AP＝kantm了 也是AP|结论6AB为双曲线的焦点弦，|SAASSA -生的A+ SimA|SACosAsrSin（+A)|=CosA+Sin(香+A)|⑥.圆锥曲线通径 囵和双曲线通径长为会|结论5. Sm. S.m-Sm. Sm S|结论5.Sm、 Sam-Sm. SmSam……公差Kd Skan主mbn]|导数放缩|你肯定会错镨|巧用三点共线|+数列|有步 起列|执边物线的通径长为2|则焦半径M下3 C+a|sina-5inB|再不买就没课了【售完即止】|高-高-荒废了|高三这样做|照样98|只需9元|查看详情|双击保存|SiB=|bm\", \"video_asr\": \"高一高二荒废了，高三这样做照样九八五。|ZZZZ。|说了那么多，辅导班成绩却还是不见提升，那是你们家的孩子没跟对老师没掌握正确的学习方法。他知道高中英语五八百九十二个核心词汇都有哪些吗？他知道语文阅读中如何三步完成阅读答题吗？他知道高考是。|学必备的四十二大几何模型吗？他知道掌握物理口诀法能秒杀十大难点失分点吗？高途课堂推出的暑期高中全科名师班十六节北大清华毕业的老师直播课，让你家高中生轻松掌握这些答题技巧，一分钟一道小题。|三分钟一道大题，暑假回来的第一个考试惊艳所有人，点击视频下方链接，仅需九块钱就能上，名额有限，快来抢课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.0227658748626709 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '单人口播', '中景', '平静', '室内', '配音', '场景-其他', '静态', '教辅材料', '极端特写', '过渡页', '拉远', '拉近', '绘画展示', '办公室', '转场', '重点圈画', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.26', '0.24', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/051832840e47c216f08a8991319b54bb.mp4\n",
      "{\"video_ocr\": \"妈妈|大象怎么读啊|等会啊|来客人了|里边进|dog狗|比丽屋 大象|等一会妈下班了帮你问问老师啊|小朋友|大象应该读elephant|elephant|这下读对了|老板|这孩子愿意学啊咱得培养|这道理我都明白|可这好英语班这一个月就得上万块钱|你可以给孩子报名作业帮直播课|英语单词语法名师课啊|那课我知道|这国内外名校毕业老师带队教学|从听说读写各个角度|培养孩子英语能力|这课后还有老师一对一辅导答疑|但是|我买了好几次都没买到|不对|现在又有名额了|现在报名9元13课时|还包邮赠送教辅大礼盒呢|我这就给孩子报个名去|不用那么麻烦|现在点击视频下方链接就能报名啦|快谢谢叔叔|三克油|16 立即报名|作业箫查插课 中国女排|作业帮直播课|名师有大招 解题更高效|中国女排为作业帮直播课代言，|*上课内容与收到礼盒请以实际为准|中国国家女子排球队官方教育品牌|9元 13节课|00|课\", \"video_asr\": \"妈妈大象怎么读啊。|你等会啊，来客人了里边进大哥。|你例子大象，妈妈，大象到底怎么读啊？等一会吗？下班了帮你问问老师啊，小朋友。|大家应该独立这下对，老板，这孩子愿意学啊，咱得培养，这道理我都明白，可是好英语吧，这一个月就得上万块钱，你可以给孩子报名作业，帮直播课英语单词语法名师课呀，那可我知道这国内外名校毕业老师带队教学，从听说读写各个角度培养孩子英语能力，这课后还有老师一对一辅导答疑。|但是我买了好几次都没买到，不对，现在又有名额了，现在报名九元十三课时还包邮赠送教辅大礼盒，那我这就给孩子报了名，而且不用那么麻烦，现在点击视频下方点击就能报名了，快谢谢叔叔三，可以。|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.01837754249572754 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '多人情景剧', '中景', '静态', '平静', '亲子', '特写', '动态', '餐厅', '单人口播', '喜悦', '悲伤', '愤怒', '惊奇', '家庭伦理', '手机电脑录屏', '全景', '路人', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.96', '0.86', '0.78', '0.72', '0.67', '0.39', '0.22', '0.15', '0.12', '0.08', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0524f104114344a782ad5ee7b887e30f.mp4\n",
      "{\"video_ocr\": \"Tk|预秋|王哥我捏的还可以吗|一会能让我先玩吗|王哥我这力度还行吧|我先|你们这是干啥呢|这么巴结小王|因为他手机里有|星星消除计划|它可是一款|非常给力的|休闲消除游戏|要通过自己的策略|完成通关|80 获得满屏星星消除的|视觉效果|是特别特别解压的|奖励2000|那还抢啥|现在微信小游戏里面|就有啊|我这不正玩着呢|赶快点击微信小游戏|一起来玩星星消除计划吧|量消除计场|a.ne|星消除许剑|ne|专星消除件奶|关卡:5 目标:8000 离开游戏|9390|9455|95.20|9613|9722|97.49|5连消 125分|连消20分|20分 连消|4连消 80分|15|planned|anred|D|剩余20个星星|小小游戏推广|高开戏|STAR|24|x1|丽众|锤子 笔刷 流星|广告|开始\", \"video_asr\": \"王哥能力的孩子什么衣服让我千万吗？网课我的力度还行吧，一会。|当有些晚了，我先我先问，你这人这么多。|新合同计划，这个是一款非常出色的游戏，要不合理的特点，完全通关获得奶瓶星星消除的视觉效果是特别特别解压，那个小子现在微信小游戏里面就有啊，我这不正玩得，赶快点击这个小游戏一起来玩，谢谢！|地方的。\"}\n",
      "multi-modal tagging model forward cost time: 0.01659083366394043 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '平静', '单人口播', '惊奇', '喜悦', '特写', '愤怒', '夫妻&恋人&相亲', '路人', '极端特写', '家', '全景', '朋友&同事(平级)', '室内', '手机电脑录屏', '拉远'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.45', '0.33', '0.18', '0.12', '0.05', '0.05', '0.03', '0.01', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/052511b37e127e0b49e6932351153a89.mp4\n",
      "{\"video_ocr\": \"还以为租房像开盲盒|租到的都是雷|因为你没有上安居客|不然你会看到这些|安居客上个人房源|不管是整租还是合租|都可以|便宜的好房不仅带装修|房东还能包水电哦|快下载安居客|马上找到你心仪的房子|点击下方下载|550/月|1000/月|3|WX|挑好房 上安居客|安居客|anjuke.com|WHEELS|Anjuke\", \"video_asr\": \"还以为租房像开门合租到的都是累，因为你没有上安居客，不然你没看到这些。|安居客上个人房源，不管是整租还是合租都可以，便宜的好房，不仅在装修，房东还包水电哦，快下载安居客，马上找到心仪的房子！\"}\n",
      "multi-modal tagging model forward cost time: 0.016836166381835938 sec\n",
      "{'result': [{'labels': ['现代', '中景', '家', '静态', '推广页', '单人口播', '平静', '配音', '手机电脑录屏', '场景-其他', '拉近', '情景演绎', '喜悦', '动态', '填充', '多人情景剧', '办公室', '混剪', '亲子', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.83', '0.45', '0.34', '0.07', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05363aeec3701512611f65ae78c869b4.mp4\n",
      "{\"video_ocr\": \"语法的背后 都是非常严谨的逻辑|你看比如我们 从小学可数不可数|其实你会发现|没有人讲清楚|我们对比来看可数名词|比如说车|一辆车acar|两辆车two cars|看见没有|我不需要加a什么of a car|为什么呢|因为车的道具是量啊|可出名词相对单位比较固定|就这个单位|所以不用说你也知道|所以把它给省略掉了|还有一些词 是即可数又不可数的|为什么呢多义词嘛|比如说time这个词|有时间还有次数看见没有|时间可以数吗不可以数|但是次数可以数吧|次是一个恒定单位|当你想通了这一点之后|就会发现 这个知识点会非常简单|用逻辑的方式讲透英文|10天满满干货|今天只需要9元钱 就可以参加|点击屏幕下方|500个名额手慢就没了|北京.江苏.安徽等卫视专访名师|D|2l|有道精品课， 老师就是好!|有道连续两年最佳课程获得者|钟平|NYSE\", \"video_asr\": \"语法的背后是非常严谨的逻辑，你还比我们从小学科数不可数。其实你们发现没有人讲清楚什么叫可数不可数。|我们来对比一看可数名词，比如说车，一辆车而两辆车TUK看见没有，我们不需要家伙有什么OFFER卡，为什么呢？|有车的高就是亮啊，可数名词单位相对比较固定，就是这个单位，所以不用说你也知道，所以把它给省略掉了。还有些词是既可数又不可数的，为什么呢？|多一词吗？比如说她按你这个词有时间还有次数，看见没有时间可以数吗？不可以数，但是次数可以数吧，看见没有哎，次是一个肯定单位。到底想通了这一点之后，就会发现这个知识点会非常简单，用逻辑的方式讲透英文十天满满干货。|今天只需要九元钱就可以参加，点击屏幕下方一百个名额，手慢就没了。|是。\"}\n",
      "multi-modal tagging model forward cost time: 0.01669788360595703 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '中景', '静态', '单人口播', '教师(教授)', '平静', '室内', '影棚幕布', '情景演绎', '知识讲解', '多人情景剧', '配音', '单人情景剧', '家', '特写', '场景-其他', '学校', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.90', '0.14', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/053c67bd7b89747fb6afffa4ae850e31.mp4\n",
      "{\"video_ocr\": \"你看|这是我给你|亲手做的礼物|我真的受够你了|咱们分手吧|为什么呀|我受够|你这些廉价的礼物了|你照照镜子好吗|你这么穷|爱我|你配吗|以后别再联系我了|老板|我来接您了|诶|谢谢您|维护了我的尊严|没事小伙子|谁没穷过呢|你把手机给我|我给你报名了一个|快财商学院的|小白理财训练营|人赚钱难|钱赚钱容易|学会理财|每天打开手机|就有收益进账|比日薪高多了|再也不用赚辛苦钱了|这靠谱吗|大哥还会骗你吗|我圈子里的老板们|就没有不会理财的|那这个课很贵吧|不花钱|0基础学理财|市场价299元的课程|现在0元抢|赶紧点击视频下方|报名吧|限前200名|视频为演绎情节|直播|快财|学理财上快财|ROA\", \"video_asr\": \"你看，这是我给你亲手做的礼物。|我真的受够你，咱们分手吧，为什么呀？我受够你这些廉价的礼物呢？你照照镜子好吗？你这么穷，爱我，你配吗？以后别再联系我了。|老板我来接你了。|ZZZZ。|谢谢您，我也哭了，我的建议，没事，小伙子谁没穷过的，你把手机给我。|ZZZZ。|我给你报名了一个快财商学院的小白理财训练，人赚钱难钱，赚钱容易，学会理财，每天打开手机就有收益进账，比日薪高多了，再也不用赚辛苦钱了，这靠谱吗？大哥还会骗你吗？我圈子里的老板们就没有不会理财的，那这课很贵，贵吧？不花钱，零基础学理财，市场价二百九十九元的课程，现在零元抢，限前两百名，赶紧点击视频下方报名吧！|拜拜。\"}\n",
      "multi-modal tagging model forward cost time: 0.023930072784423828 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '填充', '室外', '愤怒', '汽车内', '动态', '平静', '夫妻&恋人&相亲', '特写', '悲伤', '全景', '亲子', '路人', '喜悦', '手机电脑录屏', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.90', '0.81', '0.80', '0.79', '0.50', '0.39', '0.39', '0.29', '0.23', '0.06', '0.05', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/054aaa52c7211d98519ced289b888a9b.mp4\n",
      "{\"video_ocr\": \"谁说理财就只是 炒股 债券|这些高大上的东西|假如你是月光族 不知该如何存款|那么养成记账的 习惯 定期复盘|逐步减少不必要花费 把每月收入合理支配|40%强制储蓄 30%用于基本生活|20%用于投资理财 剩下10%其他支出|慢慢地你会发现 自己开始有了存款|这 就是理财|学习理财是培养 健康的财富观念|合理规划自己的资金 让你的钱持续增值|5天时间加入 小白理财训练营|教你从身边 开始理财|点击下方 立即领取吧|GUG 10.14|GNG ho w4 z6 2s|Ac 5a se 1.87|GSG B.20|CyG|42s -8.20|S +425 -8.20|ES S.24|dA 425 -6a|yA 425|113s B.66|11.s 8.se|Hp42s -a 2w|SA CkZ s.ses 8.aB|G.66|SXO 11.3G G 66|SXO 1136 B ss|SX0 11.ss B66|SX口|一2. 62|FO se 6S|63.GS 一 6ヤ|-e8.83 Ck2 8E9s 8s 6e 6s-e 8v|-a.aア|s-e.8|一s 146e|AGs S3.3e 18?7|Aa 14.66 S3.36 1.87|一8.71 B6 1.687|2白.7e -a 71 1.0マBS S24|lBS B24|1|最适合月光族的|60000|171.3S|-262|BS 524 -8.3s|-8.71 Ao 14.se -ae3 CKz sga|PDK 44.8|CiZ 8e.99 AG Ss.se 167|一e 8s CkZ 8299 eias AG SB.z6 1.87|g 8e CKz ses see|4e -2.83 CKZ 829s B.s|-8 sa Ck2 8aso a.o|aB3 CkZ a29つ|Ao 146a -8 ss Ckz 8e.99 1.87|0 146G -8 sS CkZ|-B.33 Ao 140 -as3 Ck|Ao 146e -e s d a3s|一92 202|62|93|22 696|一重1e|一睡7|理财方式|三斗|SIH|S55|13SZ\", \"video_asr\": \"谁说理财就只是嘲讽战士？铁血高大上的天气？假如你是月光族，不知道该如何新团，那么养成记账的习惯，并且不满足仅仅花费。|八美元的收入合理的机制，每百分之四十强制储蓄百分之三十七的。|百分之二十用于投资。|剩下的知识进行其他人，慢慢的就发现自己打还是正在学习理财，是没有健康的财富观念，所以规划自己的自己，让我的显示，选手，市县的时间加入小白理财训练营，教你从身边开始。|立即上网，立即知道！\"}\n",
      "multi-modal tagging model forward cost time: 0.017578125 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '配音', '平静', '动态', '极端特写', '场景-其他', '静态', '家', '混剪', '单人口播', '室内', '情景演绎', '城市景观', '特写', '填充', '远景', '教辅材料', '办公室'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.89', '0.87', '0.64', '0.58', '0.55', '0.22', '0.21', '0.10', '0.09', '0.07', '0.04', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/054d69d71434a3700c742728ec64a10c.mp4\n",
      "{\"video_ocr\": \"晶晶|考试考的怎么样|哎|你怎么来了|你不说你最近 物理成绩怠上不去吗|我今天又给你 买了一些学习资料|光刷题有什么用|你看我们班现在谁还刷题|你爸也是为你好|您别担心|您可以给孩子报一个|高途课堂 全科名师班|语数英物四科16节|只需要9块钱|我家亮亮 一直在上这个课程|以前物理也不好|现在考试考了103分呢|这门课程这么好啊|对啊|上面都是 北大精华毕业的老师|带队授课|他们将物理归纳为|电、声、光 5个知识点|让孩子充分掌握这门课程|这我家晶晶能听得懂吗|人家亮亮早就跟我说|课后啊 还有辅导老师1对1答疑|整个课程支持 3年内无限次回放|这比刷题强太多7|邢就好|那你给孩子在哪报的名呢|点击视频下方 查看详情|就可以报名啦|在每题列出的四个选璞中，只有确 D.第四象限|则复数z在复平面上所对应的点位了|浙江卫视|UP WAY|30\", \"video_asr\": \"你考试考的怎么样。|哎，晶晶晶晶。|把我怎么来了？你们说你最近物理成绩总上不去吗？你看我今天又给你买了些学习资料，光刷题有什么用？看我们班现在谁还刷题呀。|金姐，你爸也是为你好，您别担心，您可以给孩子报一个高途课堂全科名师班，语数英物四科十六节，只需要九块钱，我家亮亮一直在上这个课程。|前物理也不好，现在考试考了一百零三分呢，怎么可是这么多，对呀，上面都是北大清华毕业的老师带队授课，他们将物理归纳为电力，声光，热。|合知识点，让孩子充分掌握，这门课程中，我家晶晶能够听得懂吗？把人家亮亮早就和我说他在上这个课程了，课后啊，还有辅导老师一对一答疑课程，支持三年内无限次回放，这比刷题强太多了，那就好，那你孩子哪报的名啊？点击视频下方查看详情就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016065359115600586 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '多人情景剧', '推广页', '静态', '平静', '亲子', '家庭伦理', '室外', '全景', '喜悦', '悲伤', '特写', '路人', '动态', '夫妻&恋人&相亲', '极端特写', '朋友&同事(平级)', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.89', '0.81', '0.50', '0.04', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0558f56a87ac7774c87e8cd5898cd8b4.mp4\n",
      "{\"video_ocr\": \"我叫晓英|离婚三年了|前夫脾气暴躁|总是对我发泄|离婚之后一直一个人生活|身边也没个说心里话的人|现在很想找一个男人依靠|我平时喜欢做饭|社会关系比较简单|我也不看重物质条件|因为这些我自己都有|觉得我还不错的话|就来伊对找我聊聊吧|点击下方链接下载伊对吧 介绍给单身的朋友|点击下载|伊对\", \"video_asr\": \"我叫小英，离婚三年了，前夫脾气暴躁，总是对我发泄，离婚之后一直一个人生活，身边也没个说心里话的人。|现在很想找一个男人，依靠我平时喜欢做饭，社会关系比较简单，我也不看重物质条件。|因为这些我自己都有，觉得还不错的话，就来一对找我聊聊吧，点击下方按钮下载一对试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016888141632080078 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '静态', '平静', '推广页', '配音', '单人口播', '场景-其他', '情景演绎', '特写', '室内', '动态', '多人情景剧', '极端特写', '填充', '家', '喜悦', '朋友&同事(平级)', '室外'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.96', '0.76', '0.61', '0.36', '0.14', '0.13', '0.08', '0.07', '0.05', '0.05', '0.03', '0.02', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0559f2e1c5e8d475865f53369a7a2fa7.mp4\n",
      "{\"video_ocr\": \"抢到了 抢到了|儿子啊|我给你抢到了|高途课堂全科名师班|你现在可以去上课了|大家都在抢什么呢|让我们去看一看吧|9块钱|能买到语数英物16节课|都是北大清华毕业名师|带队直播授课|还有老师总结的|数学503个必考知识点|176个常考易错点|42个几何模型|以及9大代数考法|让我们懂得解题技巧|做题啊几步就能出结果|而且啊|课后有专属辅导老师|根据每一个学生的情况|进行详细的分析|一对一答疑|直到孩子学通学透|整个课程支持3年无限次回放|再也不用担心孩子的成绩了|16节直播课才要9块钱|这么好的课程|你们可千万不要错过|赶紧点击视频下方查看详情|报名吧|新学员9元专享 立即报名|16节精讲直播互动课|3年无限次回放复习 ￥|小升初 请选择孩子9月升入年级|【4科16节课 上课时间：8月21日|WALT|初升高 升初三|升高二|支付宝|暑期高中全科名师班|IY'|190|语数英物4科全面提升|新生特惠|499|课程信息 验证码*请输入您的验证码|B钱|l机|清北毕业名师团队授课|手机号* 手机号将是您听课的唯一凭证|获取验还码|16节名师直播课+1对1答疑+3年无限回放|支付方式 微信|科名师班|WAGT D|名师有秘籍 领跑新学期|9元16课时|剩余支付时间 班级剩余名额|r9|高付R:Y0|实付款¥9巴优惠490)|pLUrG|DuTG|oOurG|PluTC|Dluro|16节名师直播课+1对1答|9元学4科 集中攻克重难点|中国联通 4G|名师出高徒·网课选高途 4G|防疙须加|防疾频知|抓住暑假关键时期|温故知新，冲刺领跑新学期|高途课堂全科名师课|09:59.7|11年+|省高考状元带队授课 老师平均教龄11年+|仅剩-名|更多|66%|WALO  NEY's|EY'S|MCKEY|M cE|LE|名师特训班|高途课堂名师特训班|11:25|中国联通\", \"video_asr\": \"我抢了几分了，我跟你抢到了高途课堂全科范围，现在跟去上课了，大家都在抢什么呢？让我们去看看吧，是高途课堂全科名师班，九块钱能买到，语数，英物十六节课，都是北大清华毕业名师授课，还有老师总结的数学五百零三个必考。|一百七十六个常考易错点，四十二个几何模型，以及九大代数考法，让我们的解题技巧，做题不能数学奖，而且啊，课后有专属辅导老师根据每一个学生的情况进行详细的分析，一对一答疑，直到孩子学通学什么可支持三年无限次回放。|孩子独立了十六节直播课才要九块钱，这么好的课程，什么可千万不要错过，赶紧点击视频下方查看详情报名吧！|七。\"}\n",
      "multi-modal tagging model forward cost time: 0.016223907470703125 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '多人情景剧', '静态', '喜悦', '室外', '平静', '惊奇', '亲子', '特写', '动态', '路人', '家庭伦理', '愤怒', '朋友&同事(平级)', '全景', '悲伤', '填充', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.84', '0.61', '0.30', '0.19', '0.14', '0.09', '0.08', '0.04', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/055cf75f25ac93fab1c679e4130c15dc.mp4\n",
      "{\"video_ocr\": \"唉呀 又输了|小美啊|我的新英雄皮肤 帅不帅|哇英雄你充点券啦|都什么年代了还充点券|下载QQ浏览器啊|每天每个礼包|都能拿到你手软|哇真的诶|QQ浏览器不但有|和平精英礼包|还有王者荣耀00飞车等热门游戏|各种礼包等你来领|还能领取各种现金红包|赶紧点击链接|Q03之a|QQ浏览器|光子 盛狂欢|5V5|D城IR|福\", \"video_asr\": \"哎，又输了小美啊，我真心英雄皮肤帅不帅冲点券啊，都什么年代了还冲点券下载QQ浏览器啊，每天每个礼包都能拿到你手软哇，真美QQ浏览器游戏礼包还有王者荣耀。|热门游戏各种礼包等你来领，欢乐领取各种现金红包，赶紧点击链接下载QQ浏览器！\"}\n",
      "multi-modal tagging model forward cost time: 0.016597509384155273 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '喜悦', '单人口播', '惊奇', '家', '动态', '平静', '特写', '夫妻&恋人&相亲', '手机电脑录屏', '极端特写', '室内', '拉近', '家庭伦理', '亲子', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.93', '0.77', '0.60', '0.45', '0.22', '0.09', '0.09', '0.05', '0.03', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05622137cabd41740866a253a3bd4eb9.mp4\n",
      "{\"video_ocr\": \"要么到这要么加钱|你|姑娘|可不能惯着他啊|拉货搬家就找货拉拉|价格透明性价比还高|货拉拉那是什么|一款拉货搬家专属的APP啊|上面车多接单快|手机在线下单|方便又快捷|最快10秒接单|快至10分钟上门|低至20元起步|点击视频下方链接|即可下载货拉拉APP|还有这么好用的APP|我之前怎么不知道|没听见吗|还在这干嘛|行你们行啊|￥278 ¥36 推荐|卡系挑租葬点|东爆桥-推荐点|元大海车|只包运输 1-3名搬家小哥 可选拆装防护|添加收货地|7米6|眼乘6人|无忧搬家|量捷提索|载重:4.5吨|长途大车|载重500公斤|品i|6米8 中货车|企业|发物流|企业用车|长宽高:5.2*2.1”2米 就货体积:21.8方|长宽高1.8*1.3\\\"1.2米|长宽高:9.6°2.3*2.5米|包搬包运|请输入收货地址|请验人|货IN2|所有车型|食司|指家商城全场包邮 200元搬家券|同城货运 搬家|20|229 租买货车|5米2|小面包车|提前3小时预约 即装即走|技拉拉·广州|跨城大货车|货拉拉|停区域 停锁车|停镇|6:21|拉货流|面包\", \"video_asr\": \"要么倒车，要么加钱你。|姑娘可不能惯着他拉货搬家就找货拉拉，价格透明，性价比还高，货拉拉纳是什么一款拉货搬家专属的APP啊！上面车多，接单快，手机在线下单方便又快捷，最快十秒接单快至十分钟，上门的质数元起步，点击视频下方链接即可下载不了APP还有这么好用的APP，我之前怎么不知道，没听见吗？他就干吗？行，你不行啊！\"}\n",
      "multi-modal tagging model forward cost time: 0.022092103958129883 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '推广页', '中景', '场景-其他', '静态', '配音', '喜悦', '平静', '多人情景剧', '填充', '特写', '室外', '惊奇', '单人口播', '路人', '动态', '全景', '朋友&同事(平级)', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.93', '0.72', '0.71', '0.24', '0.22', '0.21', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/056a98ffb2df2c712807a3bb6f55f743.mp4\n",
      "{\"video_ocr\": \"我家的芒果果园直发|超级新鲜|三斤十几块钱不到|买五斤送五斤|坏果包赔|想吃我家大芒果的朋友|赶快点击视频下方链接下载拼多多|首页搜索越南大芒果|买还有优惠券可以领取哦 现在购|现金签到 多多延大钱 新人特权 退货包运费·100%成团·优先发货|ins风四件套 四件套纯棉|越南口罩|热门 女装 鞋包 手机|青芒10斤 最近搜索|越南大芒果|爆卖 果园直发|新人专享 查看更多|手机鉴听器 翅膀充电器|越南大芒果玉芒青芒当季新鲜水果批发应季进口新 已拼10万+件|限时秒杀 新衣馆 9块9特卖|电脑|新人价¥1|14人在拼单，可直接参与|医药馆|青皮甜芒果 13.9起￥0.9|食品 母|搜索|9.9|更多搜素方式|开启新功能|更多、|拼小圈|男装|包\", \"video_asr\": \"我家的芒果果园直发超级新鲜，三斤十几块钱不到，买五斤送一斤，快果包赔，想吃我家大芒果的朋友，赶快点击视频下方链接下载拼多多首页搜索越南大芒果，三斤十几块钱不到。\"}\n",
      "multi-modal tagging model forward cost time: 0.022685766220092773 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '配音', '填充', '平静', '场景-其他', '极端特写', '中景', '静态', '室内', '手机电脑录屏', '动态', '单人口播', '情景演绎', '办公室', '喜悦', '全景', '特写', '商品展示', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.89', '0.72', '0.46', '0.45', '0.29', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/058238a2d0d4e94bb75421d9247f4a22.mp4\n",
      "{\"video_ocr\": \"马上开学|别的孩子都已经开始|快速收心|预习新学期了|你的孩子|是不是连上学期的知识点|都还没有彻底消化|计算还要掰手指|大题还是摸不清|差距就是在这个时候|悄悄拉开了|想要孩子快速收心|领跑新学期|那就不要错过|猿辅导 数学名师特训班|清北毕业的老师|带队直播授课|帮助孩子4天快速收心|进入学习状态|独有百亿级大数据|精选各年级热搜题 TOP10|帮助孩子高效吃透|易错重难点|每堂课30多次互动|注意力翻倍集中|让孩子真正学会|并能讲出知识点|现在报名|本来499元9课时的课程|仅俪29元|再加1元|可再得9课时 语文阅读写作课|还有两套|共27件教辅文具|免费包邮送到家|你还在等什么|赶紧点击视频下方|为孩子的新学期 助力吧|14x78=109z|16x79=12b4|14×78=109z 16x79=12b|B|语文特训班|12x55=6b0|12x55=bbD 71×28=1988|28×89=2412|71×28=1988|28×89=2412 33x42=18b|00:01.08|O0:19.57。|00:28.35|01:05.15|01:14.02|00+19.68|00:37.9|01:23.53|33 ×62=|53×24=172 33x62|53×24=172|YUAMPUDA0|12×63=75り|89x74-658b|~9=|88×69=|8x69|惹着导线留霓|惹辅 数学特|12x63=|89× 74x52=|89x74=|74x52=384:|16x79 42×59=|42×59=)78|速算题|88x69= 6072|45×14=6>|18×7z|59x42=|18x1| 59x42=7478|35=|19x35=|42x75=315D|19×35=665|23×53=|8=|72×26=18 81x28=|13×77=|66|66x59=|66×59=2894 18x72=196|84×75=|84x75=670 13×77=l00 )|84x75=670D 28x43=1》0|13×77=lool  59×42=7478|71×28=|4t|46×18=8七8|44x23= 32×67=|44x23=1012|32x67=2144|34×42=|75x83=|58x44=255|74×16=|33×42=1Z8|74x16=1184|34x42=1428|75x83-625|61×51=|33×62=20lr6 61×51=311I|7x26=|72×26=1872 23x53=12l9|43x|55×57=|23×56=1288|45 x63=|45×63=ヵ85|58x46-2668|Hi|33.|28×43=|72×53=|55x57=引135 79×26=7054 28×19=|55×57=3135 43×42=180b|84x69=|小学秋季|限铺明|计次|停止|训班\", \"video_asr\": \"三。|三。|马上开学，别的孩子都已经开始快速收心预习新学期了，你的孩子是不是连上学期的知识点都还没有彻底消化计算，还要掰手指大题还是摸不清，差距就是在这个时候？|想要孩子快速收心领跑新学期，那就不要错过猿辅导数学名师特训班，清北毕业的老师带队直播授课，帮助孩子四天快速收心进入学习状态，独有百亿级大数据，精选各年级热搜题TOP十，帮助孩子高效吃透易错重难点，每堂课三十多次互动。|注意力翻倍集中，让孩子真正学会并能讲出知识点。现在报名本来四百九十九元九课时的课程，仅需二十九元，再加一元。|得九课时语文阅读写作课还有两套，共二十七件教辅文具免费包邮送到家，你还在等什么？赶紧点击视频下方，为孩子的新学期助力吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01624894142150879 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '单人口播', '喜悦', '家', '多人情景剧', '平静', '惊奇', '室内', '场景-其他', '夫妻&恋人&相亲', '动态', '家庭伦理', '极端特写', '愤怒', '配音', '亲子', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.64', '0.47', '0.40', '0.32', '0.16', '0.09', '0.07', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0582f22c42de8dbd938cef9bef053f6a.mp4\n",
      "{\"video_ocr\": \"你知道想要 不费劲的赚钱|可以通过理财 产品以钱生钱吗|我原来啊也是 想多赚点钱|所以进了股市|每天关注股市的变化 关注大盘的变化|结果钱没有赚到|还因为分心 差点丢了工作|不管是买 股票基金 债券|我的同事都 比我挣得多|后来我就去问 他怎么做到的|他说啊 这个理财 产品不能瞎买|要有目标有 技巧的去买|才可以实现 存款增值|后来我接触到了|微淼商学院的 少怕理财课|12天的课 一天只要一块钱|从理1.2天的课|有专业名师 手把手辅导|让你知道怎么买 买多少|什么时候买 才能挣到钱|如果你也想 一起学习提高|赶紧点击视频 下方链接|查看详情 一起报名学习吧|限量抢购|微淼小白训练营手把手带你轻学理财 角金|话合人尹|月光一族|家庭主妇|零基础学习并不难|专为零基础同学设计，不需要数学基础|CONTENTS 部分课程目录|请输入验证码 获取验证码|你要存多少年才能凑够买房的首付? 你攀登奋斗却赶不上爸妈日渐老去?|CROWD|基础为零，希望花了时间就能学会|上班太忙，需要快速掌握理财思路|课程设计师:封贺|WHY LEARN TO MANAGE FINANCE?|提高财商学习富人思维|月薪3000如何开启财务自由之路|开课3天内，对课程不满意，无条件退款－ 请输入手机号|你能承受几次问人要钱遭到的白眼?|从理财小白到拥有 人生第一桶金|没存款，想要拥有自己的小金库|踩坑老手 盲目投资，急需扭转亏多赚少局面|如何辨别投资陷阱 如何从零开始增加非工资收入|盲目投 亏亐多赚少局面|为什么越穷越要理财|可P刀味床任日|财务自由三大核心工具|想提升家庭财力的全职妈妈|选择我们，12天可以学到以下知识|你省吃俭用居然还撑不过一场大病?|如何从穷人思维过渡到富人思维?|升职？跳槽？创业？不懂企业怎么行？|让财务自由更简单 TRAINING CAMP|工作狂人|BASICS|理财小白|*注：抢购后，班主任将全程指导学习|E?|习|间能学会|12天可以学到以下知识|12天吃透12节理财入门课 国债，股票，基金，保险投资技巧全覆盖|适合小白的理财工具有哪些|为什么要学理财?|踩坑|才有选择生活的权利|特价12元 199元|+86|WH|人生穷富的关键|选择我们。|理财|小白理财训练营|存款多的家庭|购后\", \"video_asr\": \"你知道钱要不费劲的赚钱，可以通过理财产品以钱生钱吗？我原来啊，也是想多赚点钱，所以进了股市。|天关注股市的变化，关注大盘的变化，结果钱没有赚到，还因为分心差点丢了工作，不管是买股票，基金，债券，我的同事都比我挣的多。|来，我就去问他怎么做到的，他说，啊，这个理财产品不能瞎买，要有目标，有技巧的去买菜，可以实现存款增值。后来我接触到了微淼商学院的小白理财课，十二天的课，一天只要一块钱，有专业名师手把手辅导，让你知道怎么买，买多少，什么时候买才能赚到钱。如果你也想一起学习提高，赶紧点击视频下方链接查看详情，一起报名学习吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016289710998535156 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '配音', '场景-其他', '城市景观', '单人口播', '手机电脑录屏', '家', '喜悦', '远景', '平静', '办公室', '推广页', '多人情景剧', '夫妻&恋人&相亲', '家庭伦理', '填充', '愤怒', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0585f6832ee125e21e6abfeaceb61b60.mp4\n",
      "{\"video_ocr\": \"\", \"video_asr\": \"为什么压轴题总是做不出来？为什么英语语法记不住？|还有两百天就高考了，我该怎么办了？晓云你怎么了？怎么我这次模拟考退步了二十名？|数学后面的大题基本上全错了，你这是遇到了复习阶段的瓶颈期，你为什么不报个高途课堂全科名师班啊，专攻语数，英物，四科对你的复习很有帮助的呀，什么名师班啊？在哪里上课啊？|组课堂全科名师班，主讲老师啊，都是北大清华毕业的名师团队授课，现在啊，只要九元就能拥有十六节精讲直播互动课呢！|课后还有辅导老师一对一答疑，你不会的题目啊都能解决，那你快带我去报名吧，点击视频下方链接就能报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016147375106811523 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '多人情景剧', '静态', '亲子', '平静', '室外', '家庭伦理', '手机电脑录屏', '动态', '特写', '喜悦', '惊奇', '场景-其他', '全景', '悲伤', '单人口播', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.84', '0.81', '0.59', '0.25', '0.21', '0.18', '0.16', '0.16', '0.08', '0.07', '0.06', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0587fadead32d73551e1056a42f6944b.mp4\n",
      "{\"video_ocr\": \"服务员|来碗面|十块钱|加双份的肉和菜|那可是豪华版要16!|废什么话|赶紧做你面|得先付钱|没钱还敢这么嚣张|扫我的乐花卡吧|您本次消费16元|实际支付10元|乐花卡剩余额度49990元|你这个乐花卡是什么|乐花卡是分期乐推出的|一种有额度的分期消费卡|最高额度5万|直接提现到微信余额|到哪都能用|还可以本月消费下月还|最长能分24期慢慢还|开通之后|只要绑定到微信支付|每天可以减6元|持续优惠30天|最高减180元呢|太厉害吧|那这个乐花卡怎么申请呢|点击视频下方链接|申请开通乐花卡了|选择充值银行卡|如有任何疑问请联系|乐好|乐在有度乐见更好 美国纳斯达克上市企业|怎个国人饰追参|惹中国人简追案|分期乐|交通银行信新卡(986用)|2|¥5|¥5000|任何以官方或个人名义要求 收取手续费 或 转账等行为|零钱明细|充值方式 分期乐-乐花卡(9620)|可充值到零钱通 有收益，可直摸周于黄微|￥0.03|具体额度请以实际审核结果为准|醍宦谈い请意管气直|分期乐-乐花专 (9AM)|我的零钱|B95730|03 叫0506|温馨提示|皆为诈骗|029|ii8ilE|充值\", \"video_asr\": \"不。|和原来碗面十块钱不要双份肉和菜，那可是豪华版要十六啊，为什么画呀？赶紧做一面得先付钱。|没钱还敢这么嚣张。|来扫我乐花卡吧，您本次消费十六元，实际支付十元，乐花卡剩余额度四万九千九百九十元，你这个乐花卡是什么呀？|乐花卡是分期乐推出的一款有额度的分期消费卡，最高额度五万，直接提现到微信余额，走到哪都能。|还能本月消费，下月还最长能分二十四期，慢慢还开通之后只要绑定微信支付，每天减六元，持续优惠，三十天最高检！|太厉害了吧唉，那这个乱花卡怎么申请啊？点击视频下方链接，开通乐花卡了。|眼睛。\"}\n",
      "multi-modal tagging model forward cost time: 0.016425371170043945 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '静态', '推广页', '中景', '喜悦', '手机电脑录屏', '特写', '极端特写', '惊奇', '家庭伦理', '悲伤', '夫妻&恋人&相亲', '平静', '愤怒', '工作职场', '路人', '家', '办公室', '上下级'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.93', '0.85', '0.79', '0.14', '0.09', '0.04', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/058887a3f8adb98b137938c852060068.mp4\n",
      "{\"video_ocr\": \"慢用啊|奶奶|SRMEN 54乘78等于多少啊|MEN|奶奶忙着上菜呢|你先自己算|不会的|晚上开视频|让你爸教你|4乘8等于32|列竖式太难了|你等会|实在不行你先玩会儿|你怎么还在列竖式啊|今年多大了|EMT 10岁了|日CMT|10岁了两位数的乘法|还在列竖式啊|而且还列错了|效率也太低了|你说现在孩子这个题|我也都不会做了|他爸妈啊|都在外地工作|孩子老嚷嚷着|要报补习班|我也不知道 该报哪个好|阿姨|您给孩子报名猿辅导|数学名师特训班呀|清华北大毕业老师 带队教学|我刚给孩子报上|29元9课时|而且|再加1块钱|还能获得9课时的|还包邮赠送两个 大礼盒|30块钱就能带着孩子|学数学英语|双提升多划算|而且用手机就能报名|屏幕前的家长们|赶紧点击视频下方|给你的孩子报名吧|点击按钮 选择年级|元可换购9课时英语课|猿辅导数学名师特训班|猿辅导х最强大脑 一|9课时名师直播课堂 覆盖90%重难点|¥绳|p|猿辅导在线教育|活动特惠|5WE|29\", \"video_asr\": \"好慢用啊。|五十四个七十八的。|奶奶忙着上菜呢，你先自己藏啊，不会的，晚上开视频叫你爸教你。|五十四乘七十八到底等于多少？列竖式太难啦，你等会儿实在不行你先玩一会，哎，你怎么还在列竖式啊？今年多大了？十岁了？十岁了，两位数的乘法还在，这就是我想要咧，过了再给他。|你说现在孩子这个题我也都不会做了，他爸妈呀，都在外地工作，孩子老嚷嚷着要报补习班，我也不能随便办。|好啊，阿姨，您给孩子报名猿辅导数学名师特训班呀，清华北大毕业老师带队教学，我刚给孩子报上二十九元九课时，而且再加一块钱还能获得九课时的英语名师特训班，还包邮赠送。|哥大礼盒三十块钱就能带着孩子学数学，英语，双提升，多划算，而且用手机就能报名。屏幕前的家长们，赶紧点击视频下方给你的孩子报名吧！|真不懂。\"}\n",
      "multi-modal tagging model forward cost time: 0.017076969146728516 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '多人情景剧', '推广页', '静态', '平静', '路人', '亲子', '手机电脑录屏', '特写', '极端特写', '喜悦', '餐厅', '全景', '惊奇', '家庭伦理', '单人口播', '愤怒', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.94', '0.84', '0.80', '0.26', '0.11', '0.08', '0.03', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0588d5ca088e1b410d2cc2f902112d28.mp4\n",
      "{\"video_ocr\": \"诶出来啦|为什么不给我报名|高途课堂 高中提分班|他们都做完题 检查2遍了|我连题都没有答完|妈稳了稳了|目标985/211|您看到了吗|他们跟着 高途课堂清北毕业名师|从高一开始学习高效的|解题方法和秒杀解题技巧|目标都是双一流名校|而我这么努力学习|考个二本都难|你没给孩子报啊|9块钱16节直播课|你试试也不吃亏呀|何况是清北毕业名师 带队教学|这学习呀|没有捷径|但是好老师好方法|能让你马上看出效果|高中数学|439个知识点|167个考点|80个易错点|57个难点失分点|都总结的很细致|学到了就能拿高分|都怪我手速慢|这个课也太火了|我和他妈都没抢到|您是怎么抢到的呀|点击下方链接 就能抢课了|服一挑战|加尽之挑战|极居之热战|U|长热战|取会松战|之挑战|1K|极尽会桃战|祝所有考生|《极限挑战》第六季官方推荐 中小学生在线教育平台|顶A会热战|枫令热战|极A医令热战|高考顺利 金榜题名|2020全科名师班|极眨 挑战|1 挑评|即刻出发 逆袭学霸|名师在线 挑战极限\", \"video_asr\": \"ZZZZ。|出来了，出来了，你为什么不给我报名高徒课堂，高中提问班，他们都做完题检查两遍了，我连题都没有答我。|妈，稳了稳了，目标九八五二幺幺，您看到了吗？他们跟着高徒课堂，清北毕业名师从高一就开始学习高效的解题方法和秒杀解题技巧，目标都是双一流名校，而我这么努力学习，连考个二本都难。|你没给孩子报啊，九块钱十六节直播课，你试试也不吃亏呀，况且还是清北毕业的名师带队教学，这学习呀没有捷径，但是好老师，好方法能让你马上看出效果。高中数学四百三十九个知识点，一百六十七个考点，八十个易错点，五十七个难点，失分点都总结的很细致，学到了就能拿高。|非都怪我手速慢，这个课也太火了，我和她妈都没抢到，您是怎么抢到的呀，点击下方链接就能抢课了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01612067222595215 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '中景', '推广页', '填充', '静态', '亲子', '室外', '愤怒', '全景', '平静', '动态', '悲伤', '喜悦', '特写', '朋友&同事(平级)', '家庭伦理', '路人', '极端特写', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.87', '0.77', '0.73', '0.54', '0.44', '0.40', '0.09', '0.03', '0.03', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/058a4535ba454a988be45db4b05f74f0.mp4\n",
      "{\"video_ocr\": \"爸|这就是我男朋友|什么|你就找了这么一个穷小子|我不同意|叔叔|请您相信我|我会给她更优质的生活的|就以你现在的条件|难道让我的闺女|跟你一起啃窝窝头么|我最近在跟着|快财商学院的|名师爆款直播课|学习如何理财|我在努力了|就你这样的|还有钱|报理财课|你懂得什么叫理财么|这个课是免费的|我之前是不懂|但是快财上的老师|帮我快速整理知识框架|让我这种小白|也能轻松地|学习理财知识|所以我现在敢说|不花钱能有什么好课啊|这个课|教给我怎么用钱生钱|我现在工作的同时|还有额外的收入呢|这真的是一门好课|理财对于我来说|也真的是一次好的机会|你看他都这么努力了|就同意我们在一起吧|好吧好吧|那你这课|是从哪里报的啊|我也想去学习学习|点击视频下方链接|就能报名|视频为演绎情节|快财|直播\", \"video_asr\": \"这。|这就是我男朋友，什么？你就找了这么一个穷小子。我不同意。叔叔，请您相信我，我会给她更优质的生活的。所以你现在的条件，难道我让我的闺女跟你一起团购核桃啊？叔叔，我最近在跟着快财商学院的临时爆款直播课学习如何给才，我在努力了。|修理券呢，还有钱包理财课，你懂得什么叫理财吗？这个课是免费的，我之前是不懂，但是筷子上的老师帮我快速整理知识框架，让我这种小白也能轻松的学习理财知识，所以我现在敢说我懂什么叫理财？不花钱能有什么好课，这个课教给我怎么用钱生钱，我现在工作的同时还有额外的收入呢。|说这真的是一门好课，比赛对于我来说也真的是一次好的机会呗，你看人家都这么努力了，就同意了，我们在一起呗。|好吧好吧，那你这课是从哪里报的？我也想去学习学习，点击视频下方链接就能报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.01619696617126465 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '填充', '推广页', '静态', '平静', '惊奇', '愤怒', '路人', '全景', '手机电脑录屏', '动态', '夫妻&恋人&相亲', '室外', '喜悦', '极端特写', '厌恶', '外卖', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.95', '0.89', '0.87', '0.67', '0.31', '0.10', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05908188114b38e1643fc9a3e6cab86e.mp4\n",
      "{\"video_ocr\": \"哇|细嫩润滑|晶莹剔透|冰清玉洁|光彩照人|流氓|你这下手也太狠了吧|我还以为|好漂亮的翡翠|不好意思啊|没事 美人如玉嘛|都好看|不过你这件翡翠吊坠真的是太漂亮了|我都看入迷了|我一直想给我老婆也买一件呢|不过|你这个这么好的品质应该很贵吧|好品质的翡翠端庄典雅|确实能凸显出女性的卓然气质|当礼物再好不过|但一定很贵你就错了|我这件就是在对庄翡翠市场APP上|这是一款在线购买翡翠玉石 的电商APP|阳美翡翠的电商APP|上面的翡翠从缅甸源头 直接供给终端用户|货真价实|不怕买贵|那有保证吗|对庄支持第三方鉴定担保交易|对庄|P/2 假一赔一百万|7天无理由退货|可以全网比价|你就放心吧|那太好了|在哪下载啊|点击屏幕下方立即下载吧|￥1288新人大礼包 去使用|中缅边境工厂代购|山道播中 1166观看 源头工厂毛坯手镯定制|苏工和田美玉代购|千年玉雕文化 i直指中 2293观看|￥2000出价6 出价|好品质，所见即所|0元起拍|02货|所有宝贝全部由广东省珠宝玉石及 贵金属检测中心(GTC)进行检测|顺丰速运保价包邮。|由专人收货拆货并送至检测中心出|浪漫不用等 每日必看|热播中|定制浪漫|手镯 串珠/链饰|戒指成品|01款|宝贝先发至鉴定宝，由鉴定师上手|翡翠全品类保真承诺，假一赔一百 万，|自快递签收起7天内，买家可无理由 退换货，退回的顺丰运费和保价费|一个月记录。|精品翡翠挂件一手货源 山直1中 1287观看|精选好货专业把关|美玉无暇手镯定制 值邮|得!冰种飘花福瓜吊坠。|得!冰糯种晴水布袋佛|05运|用由鉴定宝承担。|01上手初体验 上手鉴别图片描述尺寸是否一致|复检商品是否存在纹裂瑕疵等情况，将所检查结果 如实记录。|南花雷台定能源头|好漂亮的老种木那高冰佛公|由鉴定宝提供质量担保，享有对庄|鉴定宝鉴定流程|海南黄花梨高端定制源头...|中缅边境天然翡翠手...|今天17:00结束|中戒花染高端定制H|四会翡翠工厂精品代购|中缅精品世家原石代购|验货，确认实物与照片描述一致。|并出具鉴定证书，确保100%天然真 品。|06退|具证书。全程视频监控，可追溯近|签到有礼 找货专区|山直播中 296元看|18k金伴钻镶嵌如意翡|得!冰种满色平安扣玉环...|海南黄花梨高端定制，|阳美翡翠小精品代购|翠吊坠，玉质莹润，精工...|20小时前|03检|SF|07查|翡翠市场|原石回购|3093观看|5-16|得!冰种飘花玉兰花吊坠...|￥2050|货款由鉴定宝提供担保，确认留货|宝石级翡翠毛坯私人|翡翠毛坯私人...|小IT播 后代购|平洲优质手镯工厂直供|项坠，笑口常开，高冰光...|四会|02打灯复检纹裂瑕疵|让所有人敌心买翡翠|¥3650出价3|免费鉴宝|瑞丽高端手镯代购|得！一对冰种飘花无事牌...|后工厂才可结算。|签定宝服务全程保障|小直摄中 1332观看|项部|得!冰糯种荷叶吊坠。尺...|03 专业鉴定机构出具鉴定权威证书|04保|专业鉴定流程|我的宝贝|翠友圈|¥9900出价11|消息 剩6天11时18分|寻宝 消息|个人中心|中师捷品世家百Z代|寻宝|吊坠/挂件|小查指中 276观看|l直播中 1078观看|￥4400出价12|阳纪态|专关宪退货|签到|立减¥800|啪!|幻0爱的纪念|红0爱的红态 1E爱的纪态|1 ·2爱的纪念 ·爱的纪态忿|1288|发的红态|买翡翠，就要对庄 点击下方链接 立即下载吧|拍卖 新品|直播|鉴定宝服务介绍|3650|分类|0结束|EQ|切换 大图\", \"video_asr\": \"哇，细嫩润滑，晶莹剔透，冰清玉洁刚才。|哇，你这下手也太狠了吧，我还以为。|好漂亮的翡翠，冰清玉洁，光彩照人，不好意思。|没事，美人如玉吗？等会哎，不过你这块翡翠吊坠真的是太漂亮了，我都看入迷了，我一直想给我老婆也买一件了。|对，你这么好的品质，你能和桂好品质的翡翠端庄典雅，确实能突显出女性的庄严气质，当礼物再好不过，但一定很贵就算了，我这件就是在对庄翡翠市场APP上，这是一款在线购买翡翠玉石的电商APP上面的翡翠从缅甸源头直接供给终端用户，获赠价值不怕买贵最有保证吗？对装，这是第三方鉴定担保交易，七天无理由退。|可全网比价已经放心吧，那太好了，在哪下载啊？点击屏幕下方立即下载吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.017155885696411133 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '填充', '多人情景剧', '平静', '特写', '喜悦', '手机电脑录屏', '配音', '惊奇', '极端特写', '夫妻&恋人&相亲', '动态', '场景-其他', '家庭伦理', '朋友&同事(平级)', '单人口播', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.97', '0.81', '0.81', '0.38', '0.35', '0.35', '0.15', '0.12', '0.12', '0.05']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/059a1882f14738c7b44ec97a6849a350.mp4\n",
      "{\"video_ocr\": \"答案是非常有必要！|8-12岁是孩子英语思维|形成的最后一个黄金时期|错过这个阶段的孩子|很可能看到单词不认识|更读不出来|然后就不敢张嘴读单词了|听到其他孩子能轻松读出单词|心理更会出现对自己的不自信|甚至会厌恶英语|从此与别人拉开差距|我是高途课堂金永存老师|在15年的小学英语教学经验中|我总结了|会用单词|打心底爱上英语|跟对老师 别让您的孩子成为试验品|点击视频下方 报名吧|立即报名|点出5一EE酒5|2品EEEE5|小学英语资深主讲老师 釗剑桥英语ズm二7mnT/英语口语考官|剑桥英语入ㄇm，英语口语考官|三年级以|金老师 三年级以前的孩子 有必要学英语吗?|会单司|北大清华毕业师资 平均教龄11年以上|单词|选择比努力更重要|新学员9元专享|第二语言学习关键期 时间不等人，语言学习的关键期一去不复还! 早投入事半功倍;|一1高子天琏期|高途课堂|自姚|自然拼读 8大方法|语法口诀|50种技巧|认识|只需9元|高途课堂名师特训班|10岁之后语言学习能力明 显下隆|名师出高徒·网课选高途|专注英语教学年|Language Exhibits a \\\"CriticalPeriod|金永存|High|了解|7\", \"video_asr\": \"三年级的孩子有必要学英语吗？答案是非常有必要。八到十二岁是孩子英语思维行程的最后一个黄金时期，错过这个阶段的孩子，很可能看到单词不认识，更读不出来，然后就不敢张嘴读单词了，听到其他孩子能轻松读出单词。|更会出现对自己的不自信，甚至会厌恶英语，从此与别人拉开差距。我是高途课堂金永存老师。|十五年的小学英语教学经验中，我总结了自然拼读八大方法，语法口诀，五十多种技巧，让孩子认识单词，了解单词。|用单词打心底爱上英语，选择比努力更重要，跟对老师，别让您的孩子成为试验品，只需九元，点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01648569107055664 sec\n",
      "{'result': [{'labels': ['推广页', '单人口播', '现代', '中景', '教师(教授)', '静态', '平静', '室内', '影棚幕布', '场景-其他', '配音', '教辅材料', '特写', '知识讲解', '课件展示', '拉近', '家', '极端特写', '惊奇', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/059a81ac0be5997fd6e362d31b1a67ef.mp4\n",
      "{\"video_ocr\": \"710王小姐 你的快递|来了来了|请签收|你们公司 福利不错啊|我这都连续 送了3天手机了|什么公|这是疯读极速版|免费包邮送给我的|看小说还送手机|对啊|连续签到7天 就能获得9个碎片|气，站读五章小说又倒在 了地就能得1笞这|母亲更读五章小说 就能得1个碎片 待|她读五章小说，身上 爱的就能得1个碎片|集齐10个P30碎片 免费兑换P30了|快去下载再不看 P30就快被送完啦|免费下载 赢手机|新人福利|两人的对话让她立刻判断清楚了 眼前的情况。|嫁，夏婉儿便向太子慕容桥哭诉|体都撕裘般的疼。|空气迅速回到子安的胸腔， 她大口大口呼吸，驱散了死亡的|要做太子妃的人。 丞相不得已，便逼嫡女夏子|夏子安本就受了刑，这一脚 毫不留情，踹得她一口鲜血吐出|一胎三宝，总裁爹..|原主的父亲，是当朝丞相，|夏婉儿柔柔弱弱地上前，一|一胎|具体活视以翱际规则潍府的人一起逼迫她嫁|下有非分之想，可是，情之一字|具体活动以实际规则8婚:傅先生…|把逼死吗？来人梨花带雨，一副 娇弱的模样，正是夏子安的庶妹|生最宠爱妾室玲珑夫人所出的庶 丞相酒醒后后悔不已，他平|能听到原主夏子安临死前的哀求 和她死前那漫天的血腥。|在一个月前与梁王殿下饮酒，醉 酒之时答应了梁王殿下迎娶夏婉|—一灌入子安的脑子里，慕容桥 方才在院子里发生的事情，|脸内疚地继续道：“姐姐，对不起 我曾答应过你，不会对太子殿|子安摇摇晃晃地站起来，但 是身上的疼痛让她倒抽了一口冷|夏子安虽是嫡女，在丞相府 却从未享受过嫡女的尊荣，她的|她握住双拳，眼底狂怒越发炽 盛。|慕容桥见状，十分心疼，当 即放开子安，改为虚扶着夏婉儿|夏婉儿也哭闹着不肯嫁，因 为，她早与太子情投意合，她是|凭你也配思慕本宫？呸，你这样 的货色，便是送给本宫做妾，本|女夏婉儿，怎么可能真的舍得将|夏子安怒火丛生，慕容桥便 轻蔑地看着她，一脚踹了过来:“|具体活动以实际规则为准|明日签到继续领碎片 签到提醒 看个视频可再领取1枚碎片哦!|每认真阅读五章得1枚，最高可得20+碎片|每|每认真阅读五章得1枚|嫁给植物人之后，沈琉璃莫名 其妙怀孕。她生下三个天…|其妙怀|距结广 开宝箱得碎片 1天0z00|碎片|疯读|Du|TDih|夏婉儿。|一键领取|3枚碎片 总裁爹地|气息。|安代嫁给梁王。|她嫁给残暴的梁王?|剩余59份|宫也不会要你。”|超给力 现代言情|P2.0|30|08:08|嫁给植|碎片明细>|\\\"wEIP30|我的奖品|5/10|点击下方|0/3|2天 只山古|x2 领取|就能\", \"video_asr\": \"七一零王小姐你的快递。|请签收。|你们公司福利不错呀，我这都连续中了三天手机了，什么都。|这是疯读极速版免费包邮送给我的，疯读极速版看小说还送手机，对呀，在疯读极速版上，连续签到七天就能获得九个碎片，五五章小说就能获得一个碎片，集齐十个P三零碎片就能免费兑换P三零了，快去下载，再不看P三零就快被送完了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016115665435791016 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '手机电脑录屏', '多人情景剧', '配音', '平静', '场景-其他', '喜悦', '惊奇', '单人口播', '动态', '室内', '夫妻&恋人&相亲', '拉近', '极端特写', '室外', '全景', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.95', '0.95', '0.89', '0.54', '0.32', '0.16', '0.09', '0.06', '0.04', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05a095dc173c92b967dd13bbc8c4135d.mp4\n",
      "{\"video_ocr\": \"20万20万|我以后再也不会来找你了|你是要把我逼疯吗|我们刚离婚|小宝要上学要花很多钱|我哪有钱给你啊|孩子的上学钱比我的命还重要吗|我得了甲亢|耽误我治疗你心里过意的去吗|离婚之前我不是给你买了|平安i康保·百万医疗慢病版|各类医疗费最高 能报销400万|他们会给你赔付的|我这病也能保么|怎么不能符合投保条件的|原发性高血压|甲亢|等慢性病人群|都可以投保到对应的慢病版计划|(免赔额2万) 保障范围内的医药费|诊疗费等费用100%报销|而且每月仅18.2元起|支持月缴|这么好的医疗险你不用|来找我要钱|这么好的保险|你当时在哪里给我投的|就在视频下方链接点击就能投了|Ⅲ型糖尿病/原发性高血压/甲状腺结节|保障计划|就医绿色通道 5项各一次|理赔说明|买不了保险怎么办?|甲状腺结节、甲亢、甲减 高血脂、痛风、胆囊炎、肾结石...|管理研究中心与社会科学文献出版社|般医疗费用 120种特定疾病|住院前后30天门急诊费用|1、一般医疗保险金和特定疾病医疗保险金年共享免贴额2万元 2、以有社保身份投保，但就诊时来使用社保，始付比伤为60%|HeloRun特权|查看详情|家庭护理(慢病版|全国3亿慢病人群|各类医疗费最高报销400万元|最高额外报销200万元》|医药费、诊疗费、ICU等住院医疗费用|下载平安健康APP|PINGAN|特定疾病医疗保险金 200万元|不限次 私人健康顾问 (慢病版|2型糖尿病|117种特定疾病患者 符合《健康告知》要求可投保|投保前已有的慢性病既往症也能报销|开启健康信用|获得费率优惠|恶性肿瘤二诊|产品特色|肾透析费、抗排异治疗费等指定门诊费用|投保成功|般医疗保险金|报告 (2018)）中关村新智源健康管理研究院、中南大学健康|立即投保|保障范围内，报销比例100%|康保百万医疗(慢病版)|1次|Pris|科学|18.20元起|科学管控慢病 享最高30%费率优惠|最高400万医疗保障|平安健康险|一|投保需如实健康告知，保障内容以保险合同为准|投保需如实健康告知，保障内容以保险合同为准|还能买保险吗|平安 健康|买保险\", \"video_asr\": \"等了甲亢还能买保险吗？十万我以后再也不来找你了，你知道我逼疯吗？我们刚离婚，小宝要上学，花很多钱，我哪有钱给你啊。|学校有的命还重要吗？我等了甲亢等我治疗，你的观点去吗？离婚之前我不是给你买了平安I康保百万医疗慢病版吗？各类医疗费最高能报销四百万，他们会给你赔付的，我这病也能保吗？怎么不能？符合投保条件的二型糖尿病，原发性高血压，甲亢等慢性病人群都可以投保到对应的慢病版计划保障范围之内，一药费诊疗费用的。|这一百报销，而且每月仅十八点二元起，支持月缴这么好的医疗险，你不用来找我要钱，这么好的保险你当时在哪里？有头的就在视频下方链接，点击就能投了！|N。\"}\n",
      "multi-modal tagging model forward cost time: 0.015971899032592773 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '静态', '多人情景剧', '推广页', '单人口播', '夫妻&恋人&相亲', '平静', '家', '特写', '配音', '喜悦', '悲伤', '动态', '场景-其他', '极端特写', '愤怒', '家庭伦理', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.96', '0.95', '0.77', '0.75', '0.73', '0.52', '0.47', '0.43', '0.36', '0.35', '0.28', '0.09', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05b667f4e19ecfb348c195c3cf4a6e7b.mp4\n",
      "{\"video_ocr\": \"通过|的形式|40个动画人物|让1-6年级的孩子|对英语学习产生兴趣|30张思维导图|10种高效背单词的方法|快速记忆小学核心|课程由|“超级拼读发明人|Sam老师亲自授课|课后还有|985毕业名师|1对1答疑解惑|4天时间|给孩子一个|蜕变的机会|9元报名|学完全额退款|家长们|赶紧点击视频下方|查看详情报名吧|领取超多学习资料 立即抢课|小学核心800词汇吗|出一本书的单词吗|跟谁学|小学英语单词速记营|“动漫+课程”|轻松学习|800词汇|你相信4天|孩子就能挑战|你相信5分钟|截图保存图片|102个动画故事|扫码入群|按以下步骤可领超多学习资料 提示报名成功|跟谁学|在线学习更高效|在线学习更高效|”\", \"video_asr\": \"你相信四天孩子就能挑战小学核心八百词汇吗？你相信五分钟孩子就能默写出一本书的单词吗？跟谁学小学英语单词？|通过动漫加课程的形式，运用一百零二个动画故事，四十个动画人物形象，让一到六年级的孩子对英语学习产生兴趣。|再通过三十张思维导图，轻松学习十种高效背单词的方法，快速记忆小学核心八百词汇。|课程由超级拼读发明人在老师亲自授课，课后还有九八五毕业名师一对一答疑解惑，四天时间，给孩子一个蜕变的机会，九元报名，学完全额退款。|家长们，赶紧点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016969919204711914 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '填充', '推广页', '平静', '静态', '配音', '教师(教授)', '场景-其他', '影棚幕布', '动态', '室内', '教辅材料', '拉近', '特写', '过渡页', '喜悦', '极端特写', '拉远'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.70', '0.63', '0.21', '0.15', '0.05', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05bcbaf565bda6fb62801b0cbb228dce.mp4\n",
      "{\"video_ocr\": \"准高一 如何用3年时间考上双一流|不怕初中分数好|就怕高中当陪跑|如够你以为凭借初中能达到115的分数|在高中就能保持你的优势学科地位|那么以10年的教学经验告诉你|在高中极有可能|是初中成绩的一半|原因是什么呢|初中啊 你只在乎记背|而高中真正考察的是方法|我是张冰瑶|跟谁学高中英语首席抢分名师|高中英语必备800词|167个考点|57个难点失分点|利用思维模型的方法|疑问句|抓住核心重点知识|真的达到事半功倍|你必须让知识越学越少|题越做越精|错误的方法只会让你越来越累|被落下的越来越多|别再犹豫了|赶紧来报名吧|立即报名|陈述句|特殊疑问句|肯定祈使句|what引导|how引导|数学 重难点题型高分突破|十年一线教学经验 看双一流|张冰瑶|简单句六大句型|正六大句型 简单句的四|简单句的四个类型|B品四类型|there be句型 一 There stands a tree in front of our house.|主语从句|祈使句|感叹句|高中数英培优特训营|肯定句一They like skating. 否定句 He didn't go shopping yesterday|选择疑问句，一Does your sister work in a factory or in a company How many books are there in the room?|Practise speaking English every day.|What cold water this is!|How fine a day it is!|宾语从句一l don't know where I left my key|点击下方 查看详情|在线学习更高效|独立成分 主语+谓语|线|主语+谓语+宾语+宾语补足语一Ifound the cage empty|新学员9元专享|16个语法点|80个易错点|Are you interested in the music?|You can't swim.can you?|What hardworking students I am teaching!|How bravely they fought the enemy!|英语 单词+语法高效记忆|在句子中，和其它成分无语法关系的词、词组、句子 This,I think，is your teacher 胃语—The old man is coming|主语+谓+间接宾语+直接宾语—The professor gave me a book. 主语+系动词+表语—Our city is very beautiful|主语+谓语+宾语 We should study science.|He is six years old,isn't he?|What a clever boy my son is!|How hot and wet the weahter is today!|The teacher came and saw me.|We aren't students.|名词性从句|Oul TItCTCSLea in thc musIC|Ht M GMeweaheer is today!|主语从句— How the boy climbed up to a tree isn't clear.|w en he will be back.|Don't be afraid of making mistakes.|What a size it is!|How time flies!|考点梳理|跟谁学\", \"video_asr\": \"如怕初中分数好，就怕高中当陪，如果你以为凭借初中能达到一百一十五的分数，在高中就能保持你的优势学科地位，那么以十年的教学经验告诉你，在高中及有可能是初中成绩的一半，原因是什么呢？初中啊，你只在乎器贝儿，高中真正考察的是方。|我是张明瑶，跟谁学高中英语，首席抢分名师，高中英语必背八百词，十六个语法点，一百六十七个考点，八十个易错点，五十七个难点十分点。利用思维模型的方法抓住核心重点知识，真的达到事半功倍，你必须让知识越学越少，听越做越轻。|错误的方法只会让你越来越累呗，拉下的越来越多，别再犹豫了，赶紧来报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016064882278442383 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '静态', '平静', '中景', '特写', '场景-其他', '室内', '教师(教授)', '配音', '极端特写', '动态', '宫格', '拉近', '情景演绎', '手机电脑录屏', '家', '过渡页', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.81', '0.14', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05c112c04410207423237142018935ad.mp4\n",
      "{\"video_ocr\": \"Igotashirt (我有件衬衫)|lgotsomepants (我有裤子)|lgota dress|Avery prettydress (非常漂亮的裙子)|斑马A课英语体验课|是猿辅导在线教育旗下|专门针对2-8岁孩子设立|采用儿歌、动画的形式|提升孩子对英语的学习兴趣|现在报名10节课仅需49元|还免费包邮赠送|随材大礼盒|立即报名|斑马A|课 德辅导在线教出司|猿能|猿输一|完斩|猿出m|导线我|绿|铺导在线快有|猿辅导 在线教育出品|辅导在教育|猿辅导在线数育 出品|猿辅导在线多有出|猿辅导开线数品|在线数育|猿辅导在出品|climb|Find the rcle|价限时仅套29元|线教育|在绿教影诚品|马用户专属优惠|pumpkin|赠送随材礼盒(包邮)|具体以收货为准 S1-S3不同年龄段礼盒内容不同|?、|49元10节课|PARk|系统课课程指南|DAY 2: The Giants Sweater I.Match and say|gonfly|2Paste and say|dragonfiy|ABC|字国|体验课|sweater|BOOK|一8|az TH|NS\", \"video_asr\": \"GOOD GOOD GOOD FATHER BECAUSE OF。|斑马AI课英语体验课是猿辅导在线教育旗下专门针对二到八岁孩子设立，采用儿歌，动画的形式提升孩子对英语的学习兴趣，现在报名十节课只需四十九元，还免费包邮，赠送随材大礼盒，赶快点击下方给孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01632523536682129 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '配音', '平静', '影棚幕布', '全景', '教辅材料', '课件展示', '场景-其他', '情景演绎', '喜悦', '极端特写', '室内', '知识讲解', '动画', '动态', '单人口播', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.95', '0.86', '0.74', '0.70', '0.24', '0.14', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05c32405b0da3c6a5a04903eb5385c57.mp4\n",
      "{\"video_ocr\": \"妈|你别生气了|我这个课 真的只花了29块钱|妈妈你别生气啦|哎涵涵妈妈|你们这是怎么了|这孩子气死我了|语文学不好就算了|还学会撒谎了|非告诉我这个是她|花29块钱报|作业帮直播课送的|这个大礼包啊|现在29块钱确实买不来|现在这个礼包啊|不能买只能送|我都看了|这里面有优秀作文集|精选古诗40首|成长笔记|基础知识手册|这大大小小加在一起|有十几本呢|这个是语文 阅读写作提分班|清北毕业名师带队教学|总结6个 重难题型答题模板|74个必备 阅读写作大招|12种语文思维能力|和5种语文学习方法|课后还有辅导 老师1对1答疑呢|家长朋友们|如果你家孩子|阅读理解总是丢分|作文写得 像流水账的问题|别再犹豫了|赶快点击视频下方|给你的孩子报名吧|秀作文集|0首|作业直播课|学酒文+4用14年疑|小子7心店|上课内容与收到礼盒请以实际为准|名师有大招 解题更高效|￥29=20节名师课|语文阅读|中照女师|成长|笔记\", \"video_asr\": \"你别生气了，我这个课真的。|花了二十九慢慢变。|哈哈，妈妈，你们这是怎么了？这孩子气死我了，唉，语文学不好就算了，还学会撒谎的，非告诉这俩是她花二十九块钱报作业，帮直播课送的。|为这个大礼包啊，现在二十九块钱确实买不来，现在这个礼包啊，不能买，只能送啊，我都看了，这里面有优秀作文集，精选古诗四十首。|成长笔记，基础知识手册，这大大小小加在一起有十几本呢，这个是语文阅读写作提分班清北毕业名师带队教学总结的六个重难题型答题。|七十四个必备阅读写作大招，十二种语文思维能力和五种语文学习方法，课后还有辅导老师一对一答疑呢！|家长朋友们，如果你家孩子阅读理解总是丢分，作文写的像流水账的问题，别再犹豫了，赶快点击视频下方给你的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016881227493286133 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '多人情景剧', '特写', '静态', '亲子', '全景', '家庭伦理', '单人口播', '动态', '平静', '喜悦', '家', '室外', '朋友&同事(平级)', '极端特写', '情景演绎', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.95', '0.92', '0.56', '0.51', '0.45', '0.31', '0.28', '0.13', '0.12', '0.10', '0.08', '0.04', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05cf75bcb6c9b4f16b1806e56ef885d6.mp4\n",
      "{\"video_ocr\": \"￥|小朋友|新学期开学|有什么愿望吗|我就希望|我妈给我讲|数学题的时候|不要再吼我了|一吼我就懵|什么也记不住|还有|我已经有|8本练习册了|我希望我妈|赶紧给我报名|学而思网校|秋季数学特训班|我同桌乐乐|早就报了|这次数学小测|他考的分可高了|说现在|教他的老师是|清华毕业的|光速算技巧|他都学会了5种了|还有3种|应用题解题技巧|我也想学|妈妈听到了吗|都说了好几天了|真的吗|那我也要报|就不用做那|怎么报啊|妈妈们别再犹豫了|点击视频下方链接|就可以给孩子报名了|现在报名|只需要9块钱|还包邮赠送|全套的学习礼盒|相信这一次|一定能帮助孩子|解决学数学难的问题|390好课+名师+教辅=轻松学|学习效果因人而异)|每天进步一点点一|小初高|具体教辅礼盒以收到实物为准)|10课时名师互动直播 4天班主任跟踪答疑|专攻重难点，紧扣考纲高效提分|17年清华毕业教研打磨|哇!|学币而息网权|哈佛大学教育学|特惠|全国 包邮|立省|杨佳\", \"video_asr\": \"小朋友出生七开学有什么样的呢？我就希望我妈给我讲数学题的时候不要再吼我了，一吼我就蒙，什么也记不住。还有我已经有八本练习册了，不要再买了。|朋友，新学期开学有什么用了吗？|我希望我妈赶紧给我报名学而思网校秋季数学特训班，我同桌乐乐早就报了这次数学小测，她考的分可高了，说现在交大的老师是清华毕业的，光束算技巧他都会冷五种了。|还有三种应用题解题技巧，我也想学，妈妈听到了吗？赶紧给我报名了，都说了好几天了。|那我也要报工作，你发的东西错了。|怎么办？在哪报呀？棒棒们别再犹豫了，点击视频下方链接就可给孩子报名啦！现在报名啊，只需要九块钱，还包邮赠送全套的学习礼盒，相信这一次一定能帮助孩子！|姐姐学数学难的问题。\"}\n",
      "multi-modal tagging model forward cost time: 0.01610541343688965 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '填充', '中景', '亲子', '静态', '平静', '路人', '特写', '家庭伦理', '全景', '室外', '喜悦', '动态', '愤怒', '极端特写', '(马路边的)人行道', '惊奇', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.96', '0.87', '0.87', '0.64', '0.36', '0.24', '0.10', '0.09', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05dd2f3e2fb61ddbb27f3546b26fe2d5.mp4\n",
      "{\"video_ocr\": \"危险时刻路过的小伙子 丢掉车子飞奔抱起了孩子|差点酿成悲剧 究竟是车主的失误?|诶|你这人怎么开车的呀?|谢谢谢谢|你可真是我的救命恩人|这要是压到|我都不知道该怎么办才好|你这外卖全撒了吧|我赔给你|喂老婆|我刚在路上救了个小孩|外卖全撒了|我还得回去再取一下|你自己都救不了还救别人?|许子颜|你能不能有点出息啊?|你老婆我孩子都生不起了|这日子别过了|真不好意思|小伙子|你把手机给我一下|小兄弟|我给你在手机上 报了一个|快财商学院|你在上面学学理财|每天打开手机|就会有额外的收入|理财学好了|收入可能比你送外卖都高呢!|可是姐.....|我读书少|这理财能学会吗?|咋不能|我身边的那些 退休的老头老太太|车主心急抢占车位 险些毁了一个家庭!|外卖小哥见义勇为 狂飙救下婴儿|快射|外卖 头才果也|买水果也州|熙7果也快|买小|来也快|果也|女7|オ水曳|7禾飞|爆款理财小白 训练营|果3快|08:56:47:28|09:02:51:08|投资有风险，选择需谨慎，风险贵责任由购买者自行承担|还是家长的失责?|2020.9.4|美团|美飞也快|Mn0|直播\", \"video_asr\": \"去呗。|哎，我这是怎么开始的啊，哎呀，谢谢，谢谢，你可真是我的救命恩人啊，我全家的我都不知该怎么办。|残了嘿嘿谢谢。|你就外卖全散了吧，我赔给你。|白老婆啊，我刚刚在路上救了个小孩，外卖全洒了，我还得回去再取一下，你自己都说不了还叫别人。|你能不能有点出息啊，你老婆我孩子都生不起了，这日子别过了。|真不好意思啊，小伙子你把手机给我一下。|小兄弟，我给你在手机上报了一个快财商学院，你在上面学学理财，每天打开手机就会有额外的收入，理财学好了。|叔叔可能比你送外卖都高，那可是姐，我读书少，这里才能学会吗？嗨，咋不能，我身边的那些退休的老头老太太都在。\"}\n",
      "multi-modal tagging model forward cost time: 0.0160219669342041 sec\n",
      "{'result': [{'labels': ['现代', '填充', '多人情景剧', '中景', '停车场', '外卖', '动态', '手机电脑录屏', '全景', '愤怒', '路人', '平静', '朋友&同事(平级)', '(马路边的)人行道', '夫妻&恋人&相亲', '室外', '远景', '惊奇', '汽车内', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.12', '0.04', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05ee76bd52b4b30a92d75f1fdb1b38ec.mp4\n",
      "{\"video_ocr\": \"你要悄悄拔尖|然后惊艳所有人|暑假是拉开成绩的 关键时期|再不规划就真的晚了，|你想开学超越所有人|完成逆袭吗|报了那么多辅导班|成绩却还是不见提升?|那是你的孩子没跟对老师|没掌握正确的学习方法|他知道高中英语 892个核心词汇|都有哪些吗？|他知道语文阅读中|如何3步完成阅读答题吗？|他知道高考数学必考|42大几何模型吗？|他知道掌握物理口诀法|能秒杀 10大难点失分点吗？|高途课堂推出的|暑期高中全科名师班|16节北大清华毕业老师 直播课|让你家高中生 轻松掌握这些答题技巧|1分钟一道小题|暑假回来的第一个月考|看到这条视频|点击屏幕下方链接|直接领取490元的优惠券|现在只需9块钱就能上|让孩子试试也不亏啊|名额有限|快为你的孩子抢课吧|新学员9元专享 立即报名|第次司日考:|第次目考|p 61 57|形攒底： P61 54|P61 5 898|7 61|78|7>，|2>|高途课堂|期末考试式|形摸底:|筝+次换拟考:|第次同者:|66|8?|第次换拟考:16|/02|高考标:|末考试|高端板|11|期末考试 109|高考薪日椋:|高考目标:1公|高考日标：1 0|英语| 数英语物理|被  数荚语 174|盘 数 英语|物理 4!|57 被款 英语|语摄英语物|英语 物理|被 教| 教学|盘款|招 数学|109|I14|北大清华毕业师资 平均教龄11年以上|语刘|税找底|形攒底|名师特训班|视频为演绎情节|最党|浙江卫视|乃|ぴ竹wK|的ル|引m\", \"video_asr\": \"你要悄悄拔尖，然后惊艳所有人，暑假是拉开成绩的关键时期，再不规划就真的晚了，你想开学超越所有人完成逆袭吗？报了那么多辅导班，成绩却还是不见提升，那是你的孩子没跟对老师没掌握正确的学习方法。|知道高中英语八百九十二个核心词汇都有哪些吗？他知道语文阅读中如何三步完成阅读答题吗？他知道高考数学必考四十二大题和模型吗？他知道。|掌握物理口诀法，能秒杀十大难点失分点，高途课堂推出的暑期高中全科名师班，十六节北大清华毕业老师直播课，让你家高中生！|轻松掌握这些答题技巧，一分钟一道小题，三分钟一道大暑假回来的第一个月考，惊艳所有人！|这条视频点击屏幕下方链接，直接领取四百九十元的优惠券，现在只需九块钱就能上，让孩子试试傀儡，因为有线，还为你的孩子！|老板。\"}\n",
      "multi-modal tagging model forward cost time: 0.0161740779876709 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '单人口播', '配音', '室内', '推广页', '手写解题', '静态', '平静', '重点圈画', '极端特写', '办公室', '喜悦', '场景-其他', '拉近', '课件展示', '拉远', '单人情景剧', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.98', '0.95', '0.53', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05f4708760b327c44f2c9d165b488f95.mp4\n",
      "{\"video_ocr\": \"(港澳台地区不包邮)|We made some banana bread. 我们做了一些香蕉面包。|Wemade some orange juice. 我们做了一些橙汁。|We made some apple pie.|Let's alleat together. 一起来吃吧。|学思维学英语2-8岁上斑马|报名就送超值教材礼包 立即报名|49元10节课|斑马AI课|送\", \"video_asr\": \"你没想到的是。|菲菲上来东西。|不是。\"}\n",
      "multi-modal tagging model forward cost time: 0.016564130783081055 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '情景演绎', '影棚幕布', '才艺展示', '全景', '中景', '喜悦', '场景-其他', '特写', '配音', '平静', '家', '单人口播', '动画', '单人情景剧', '室内', '多人情景剧', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.90', '0.79', '0.64', '0.04', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05f59f9c4fa2cc36b96eabab20a8a14a.mp4\n",
      "{\"video_ocr\": \"能买就买|不怕花钱|游戏，买个星际制霸|就是要高端大气|电脑，给我升到最高级|网吧电脑|什么，还差员工?|+462|网管、保安、清洁工|都麻溜给我动起来|客人来了，还不快去接待|隔壁还有电竞队|买了，给我疯狂升级|爷不差钱|只要够下本|客人怎么会不来~|网吧模拟器|你能经营出什么样的网吧|快来试试吧|小型盆栽 买来的盆栽，虽然很便|挂壁空调|高档咖啡桌 高档咖啡桌，现磨咖啡|优质自助零食 不错的售货机，基本不|飞龙骑脸怎么会输?! IM|XCUM|HEM2图大 91T0|服务人员可以让你的网吧变得 Lv.1|2分39秒|坏的再严重的电脑我也能修理|3分46秒|我看看是不是还有人在偷东西|奖励会购买成功 每分钟收入:+5|奖励会员:+20|当前等级|游玩时间:|随机增加一个客户的上网花费|每隔240秒获得一次免费修理机会|自动赶走在网吧捣乱的人|电竞战队 100000解锁|PAPA|宜。|，更像个大功率风扇。 其他网吧淘汰下来的产|能勾起顾客的消费欲望|会出现卡货或者吞币的 情况。|级|可升级 寂静岭|080ti， 能出的旋|110s|和性能衰现方 游戏道真度|+5|下一级效果 个客户的上网|龙费|获得一次免费|网吧捣乱的人|已购买 使用中|客人每次消费后获得200|92s 游戏收益:|支术创|持续155秒，冷却245秒|花员|总共1次|修机会|持续300秒，冷却100秒|会员数达到 u1P/350|需要会员数量大于507 配送中...|咖啡区|惊世泰坦RTX|主动驱赶不速之客|发现新游戏 抢车位 快到网吧管理中购买吧!|发现新设施奢华饮料机|低配电脑到货了|时尚靓丽到货了|瓷砖地板到货了|发现新设施落地风扇|142456697 oUE/2147498536|142452262|142201669|9999|发现新设施低配电脑|11293|已头 饮料区|6号桌子|GTA 拳皇97|千/rh|点击|点击腿选|大厅 酒店 快速配送|包间|欧耶!我们终于也有自己的电竞|为了传说中的奖杯，大家都要加|未发现|7400|彩虹六号|下次再来。|tPU,R有16|影档|UIP 350|模拟人生|收银员|欧耶！我们终于也|为了传说中的奖杯，|油啊|升级成功|撸大思跑分|自动赶走在网升级员工成功|设施购买成功|品，接上电也勉强能用|更好|随机增加一个|每隔140秒获|自动赶走在网|5250|390000|配送中|优厨自助饮 普通自助饮料|自制简单饮料|05:04|休女|烤争|三国无双 全面战争|Esports|ESpon|星球争霸|#死之间|AOE|IPFA|PAPA CAFE|战Nあ斗万:500|下班了下班了!|装修 游戏 员工|显卡|当前无比真|磁悬|5040000|简帽的饮料机|按住加速|网吧管理|战队了。|使田|四海兄弟|Z00|包1可|载判|鬼泣 傲视三国|01812|修埋工|CS CSHLE|显卡，其 称军命|BEAGHHAND|37:30|半条命|20r-|2019年|2147498508|UIR 2147498511|EXP|个人好像在看 新闻。|性能降低|电脑计|电脑开级|电升饭|142177902|141993245|141889143|141887894|141678152|146873426|14G773472|141647378|D12/10|由时5 M/T|合庆|中时|非桌|拾器|拾器时代|2、|服务员|工作中|休息中|保安|决战荣耀之巅|早期水果电脑|新硬件安装!|模拟器|店外宣传|包宿5元|卡皇。|补足资历|富二代开网吧|教练|逆转裁判|NEW|优质桌 舒适桌|RARA GARE|打赏|1317|任务 仓库|模扎|主务5|普通桌|廉价桌|97|通宵 5元\", \"video_asr\": \"能买就买，能生就生，不怕花钱游戏，买个星期至八要的就是高端党，电脑给我升到最高级，什么还差员工，往往保安，清洁工都给我妈了动起来，客人来了还不快去接待，隔壁还有电竞队买了给我疯狂升级也不差钱。|只要下个本科人怎么会不来网吧模拟器，你能进一出什么样的网吧？快来试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.02169632911682129 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '手机电脑录屏', '静态', '配音', '中景', '推广页', '喜悦', '单人口播', '填充', '平静', '惊奇', '室内', '游戏画面', '办公室', '工作职场', '特写', '愤怒', '混剪', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.62', '0.23', '0.16', '0.07', '0.06', '0.05', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/05f850b3d045e9b353729594743a677a.mp4\n",
      "{\"video_ocr\": \"嘿 哥们|上车|奥|别再被每个月话费追着跑了|赶紧点击下方链接|下载打开拼多多|搜索立即领取|只要9.9就可以领取百元话费啦|羊毛外套女 新飞茶吧机|立即立取平衡车 李嘉琪气垫|￥5499 ￥6400|限量2件 05月17日23:00开始|日常价|饭锅/压力锅[6月2日发完】 App专享|平台不承担责任，介意者慎拍!充值完成后，手机会收到运营|23.33元 抢|商发送的短信通知，停机手机收不到短信，如遇系统繁忙手机|力架|已拼228件 已拼10万+件|话费侠亮 发完】|新人特权|100%成团·优先发货.极速退款·闪电退货|168元 83元|提醒我|[说明] 请在充值时仔细核对要充值的手机号码，如因用户|榴莲饼|李佳琪推荐洗发水|牌电视【6月2日发完】|限时秒杀|Aap0|选定 确认|[品牌款式丰富，随机发货】 全自动面|苹果xr手机膜 绞大蒜机 平衡车|立镜 李佳琦推荐 学生|￥9.9 ￥2700 ￥9.9|加绒被套四件套|精选商品限时秒杀|也有可能收不到短信或者短信延迟。因为线上充值，为此无|提供的手机号码有误导致充值失败，将视为此用户放弃资格，|最近搜索 榴莲特价|立即立取|更多搜索方式|查看更多>|￥9.9 ￥559|离家|里脊|移动联通电信话费充值100元【6月2日发完 商品售完时未能拼单者视为抢购失败，将发起退款|搜索店铺 拍照搜同款|礼记|送3M视频线】400合一游|小台农新鲜水果鸡蛋芒现摘|商品详情|切菜器|[品牌款式丰富，随机发货] Kindle等|周日23:00开抢|精选好货|【多汁超甜】广东覃斗芒果|移动、联通、电信通用|lijilingqu|限时秒杀-现货直发|¥9.9￥40限量2件|终之川能1欢11的I [6月2日发完】|戏儿童电子游戏机掌机SUP|预售 App专享|¥9.9￥104|00:34 .nll4G 搜索|理解 丽江|立即领|移动|联心\", \"video_asr\": \"喂喂。|快充充充。|别再被每个人换位追着跑，赶紧点击下方链接下载，打开拼多多搜索立即领取，只要九块九就可以领取百元话费了！\"}\n",
      "multi-modal tagging model forward cost time: 0.018533706665039062 sec\n",
      "{'result': [{'labels': ['现代', '中景', '喜悦', '多人情景剧', '推广页', '静态', '平静', '手机电脑录屏', '动态', '室外', '特写', '场景-其他', '路人', '惊奇', '单人口播', '全景', '室内', '拉近', '配音', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.65', '0.57', '0.53', '0.46', '0.16', '0.12', '0.10', '0.07', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/060df31cc1dcde1d95f9e788c57493a5.mp4\n",
      "{\"video_ocr\": \"易安 你带我走吧|我不想再回来了|你不要碰易安哥哥|思语 你|我已经怀了易安的孩子|怎么可能|要不说你蠢呢|我特地让易安接近你的|为了 就是你的遗产|傅承景的死|也是我们策划的|没想到他这么在乎你|居然真的肯为你死|我的好姐姐|你不死|财产怎么能顺理成章的|到我们手里|易安 你不能这么对我|你不可以|你|少奶奶|你怎么能在众媒体面前|羞辱傅家|我这是重生了|少爷是南城傅家的|继承人|一表人才不说|我知错了|傅承景现在人在哪|少爷在外面守了你一天一夜|傅承景|你若不愿 我们可以|老公|这一世|我定不负你|下载疯读小说 免费秦金文|《诱爱成婚：傅先生宠妻无度》|疯读|疯读 疯读免费小说|疯读免费小说|故事纯属虚构\", \"video_asr\": \"滚离我远点。|好。|爷爷你带我走吧，不想再回来了，你不要碰一样，哥哥思雨你我就回来腌着孩子，怎么可能要不说你蠢呢，是我特意让医院接近你的。|为的就是你的财富，成景的死也是策划的，没想到她这么在乎，居然真的可以一死，果然还是。|你不死财产怎么设置成知道我不熟啊，你不能这么对我。|你不可以。|上不了。|你怎么能在众媒体面前羞辱富家？我这是重生了。|少爷是南城附加的继承人，一表人才，不说，我知错了啊，破城子现在人在哪，少在外面受了一天一夜。|ZZZZ。|不，陈姐，你若不愿我，我知错了，你老公这一世我定不负你。\"}\n",
      "multi-modal tagging model forward cost time: 0.01603865623474121 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '静态', '特写', '动态', '愤怒', '夫妻&恋人&相亲', '推广页', '惊奇', '极端特写', '厌恶', '喜悦', '室内', '家', '朋友&同事(平级)', '悲伤', '拉近', '工作职场', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.91', '0.80', '0.78', '0.42', '0.39', '0.13', '0.07', '0.06', '0.05', '0.03', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0611449adab4d8105014ac6bd7003d4e.mp4\n",
      "{\"video_ocr\": \"切才15级|呦运气不错嘛35级了|我去都封顶了|你竟然是全服第一|还有满星屠龙宝刀|只能说我精通合成|才有如此顶级装备|游戏中元素是一个|非常值得你去注意的东西|这是一个隐藏的玩法|可以改变命运|不同的元素能够|为你带来不同效果|只要是装备搭配合理|能力足够强大|就能征服榜首|快来微信小游戏和我pk吧|美际匕z20|4 黄梅季的咸鱼...|转盘 任务 免费福利|54290 39352|18|39718 385|139630|世界好友|加载中|汪大明 304252|224|元素合成秘籍|50|知世俗而不俗...|Lv.40 49713609/60003231|3v3|幸运|防御|5305056|梦如人生 22018|68818|161234|元素英雄楗|25981|★女食h食|立即进入|头像 用户名 武器|黄艳萍|合成 战斗 排行|攻击|商城|装备|开启\", \"video_asr\": \"才十五级。|哟，运气不错嘛，三十五级了。|我去都封顶了，你竟然是全服的。|每消除报道可能是我比较京东合成才能有如此顶级装备，游戏中别人做出一个非常实用的实例的走出一个隐藏玩法，可以改变命运，努力带来。|这表示装备搭配。|是什么今天的。|小游戏来给我劈了。\"}\n",
      "multi-modal tagging model forward cost time: 0.02236175537109375 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '喜悦', '手机电脑录屏', '惊奇', '朋友&同事(平级)', '平静', '动态', '特写', '单人口播', '极端特写', '场景-其他', '悲伤', '全景', '配音', '路人', '(马路边的)人行道'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.94', '0.81', '0.57', '0.45', '0.04', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0611b7b104468079f1840c75553c2a4d.mp4\n",
      "{\"video_ocr\": \"恭喜你获得了|一个超能力|什么?|我可以借你一笔|梦想基金|不要利息|一个月能还就行|耶!|你疯了你|老婆你听我解释|我刚才就是做了一个梦|梦见有人借钱不要利息|你说的是分期乐吧|最高可以借5万|分期乐?|对啊|正规品牌的大软件|呐你看|最快一分钟就能放款|而且借一万内|最长还能免息6个月呢|我梦想成真了|你是我不梦卖动了呢|你是不是也心动了呢|赶快点击视频下方链接|来测测你有多少额度吧|B95730|50000 三步急速借款|随活额度 马上提现|美国纳斯达克上市企业，股票代码:LX|n额度高 免息6个月|羹赢5个月|填写资料 2|诸输入手机号码 请输入短信验证码|国内正规的金融服务大平台|额度播环用|最快1分钟出额度|深圳市分期乐网盛科技有限公司|借1万元内免息6个月|三歩急達惜款|万元内免Q6个R|获取验证码|测测你能借多少|普100-10000元|出资来源:贷款资金由正规金融持牌放款机构提供 李1CP备 14041530号-1|VUR lrmP|审批快|手机申请自动审批 最长可分36期|鬈 ,|任何以官方或个人名义要求 y取y绝天或 转账等行为|LOVE YOUR MaMA|)还款灵活|最高可借5万，借1万元内|“请自动审批|SIEASURET|上市集团乐信旗下企业|li 4G|li 4G|如有任何疑问请联系|乐乐在t好|乐在有度乐见更好|理性消费乐在有度 额度以实际批审为准|怀款司话 理性消费乐在有度|高借5万|皆为诈骗|收取手续费 或 转账等行为|温馨提示|16:31|三O 额质\", \"video_asr\": \"恭喜你获得了一个超能力，我可以借你一笔梦想基金，不要利息一个月能还就行。|不要利息为你疯了你。|你听我解释，我看来就是做了一个梦，梦见人借钱，不要利息，你说的是分期乐吧，最高可以借五万，你青龙，对呀，正规品牌的大软件。|啊，你看最快呀，一分钟就能放款，而且啊，借一万内最长还能免息六个月了，我想深圳吗？你是不是也心动了呢？赶紧点击视频下方链接来测测你有多少额度吧！|北京。\"}\n",
      "multi-modal tagging model forward cost time: 0.016539812088012695 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '手机电脑录屏', '多人情景剧', '家', '静态', '单人口播', '特写', '喜悦', '拉近', '家庭伦理', '极端特写', '配音', '夫妻&恋人&相亲', '拉远', '混剪', '室内', '惊奇', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.63', '0.46', '0.35', '0.21', '0.14', '0.10', '0.07', '0.05', '0.04', '0.04', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0618e271b1c4902649bafe68d0ff6491.mp4\n",
      "{\"video_ocr\": \"阅读理解别再从头读到尾 阅读|逐字翻译了|这样做不仅耗时长|正确率还不高|运用我的跳读大法|一篇700多字的文章|你只需要读 其中的158个字|即可轻松秒题|我是高途课堂王煜嘉老师|累计授课达到了 11000+小时|总结出了高考|阅读理解跳读大招|A+A1作之满6结构|让孩子跟着专业的人学习|还在等什么? 点击视频下方|高中美:|888核心词汇|提分真的很简单|只需9元|报名吧|累计线上培训学员万余人|多名学生2个月提分刀分|世纪品榜|VTENTS|第一分人打作我|ENT|INTS|华少|12|全国百佳教师带队教学 平均教龄11年|王煜嘉|高途课堂 名师特训班|新用户专享 立即体验 浙江卫视指定在线教育品牌|园浙江卫视|日渐消瘦|人与自我|夕r ¥9|仅需\", \"video_asr\": \"剧毒理解，别再从头读到尾逐字翻译了，这样做不仅耗时长，正确率还不高，运用我的跳读大法一天。|百度的文章，你只需要读其中的一百五十八个字即可轻松秒题。我是高途课堂王云佳老师，累计授课达到了一万一千加小时，总结出了高考八百八十八个核心词汇，阅读理解，跳读大招，同时A加A作文满分结构，提分，真的。|简单只需九元，让孩子跟着专业的人学！|还等什么，点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016001224517822266 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '填充', '中景', '静态', '场景-其他', '平静', '配音', '教师(教授)', '室内', '教辅材料', '家', '极端特写', '特写', '手写解题', '情景演绎', '办公室', '课件展示', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.84', '0.74', '0.64', '0.46', '0.37', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/061c42fca16979d174d6e66850c8e691.mp4\n",
      "{\"video_ocr\": \"破电驴子 太丢人了|在这等我|合同签了吗|闭嘴|要不是你这破车害我迟到|能签不成吗|就这种人还想跟我签合同|做梦|等一下|二少爷|我终于找到您了|你认错人了吧|对对对.|不好意思|柳小姐|刚才是我失礼了|咱们进去再谈谈吧|岳风|点击屏幕下方了解更多精彩内容|柳家赘婿|岳风妻子|吴得道|更多精彩内容 点击屏幕下方了解详情|神秘 富豪甘 做上门女 婿|《神级狂婿》|《|紫玉公司总经理|内幕 惊天…|小说|全本免费|其#|柳萱\", \"video_asr\": \"不是吧。|破电驴子太丢人了。|在这等我。|ZZZZ。|合同签了吧，要不是破车害我迟到能签不成吗？|这个。|我梦。|我。|等一下，二少爷，我终于找到您了。|你认错人了吧。|对对，不好意思啊，就小姐刚才是我失礼了，咱们进去再谈谈吧。|乐叔。|二少年。\"}\n",
      "multi-modal tagging model forward cost time: 0.022255420684814453 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '填充', '现代', '多人情景剧', '愤怒', '静态', '动态', '夫妻&恋人&相亲', '全景', '特写', '室外', '平静', '悲伤', '极端特写', '惊奇', '拉近', '喜悦', '室内', '厌恶'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.96', '0.93', '0.84', '0.58', '0.48', '0.21', '0.16', '0.10', '0.08', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06209ebb68af10169345f38ffbaf6033.mp4\n",
      "{\"video_ocr\": \"从2017年起“人工智能”成为|中国媒体十大流行语|学习Python|让用户打开人工智能的大门|让人生实现更多可能|乐绿得以情景对话式教学|有趣奸玩|更有社群助教提供答疑辅导|零基础即可轻松入门|现惠89轻体脸4人门课|现只需8.9 轻松体验4节入门课|风变编程\", \"video_asr\": \"二零一七年起，人工智能已成为中国媒体十大流行语，学习拍的，让用户打开人工智能的大门，让人生世界更多可能。风变编程以情景对话式教学，有趣好玩，更有社群助教提供答疑辅导，零基础即可轻松入门，限制十八块九即可轻松体验四节的课。\"}\n",
      "multi-modal tagging model forward cost time: 0.01629018783569336 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '单人口播', '平静', '推广页', '办公室', '特写', '多人情景剧', '动态', '手机电脑录屏', '喜悦', '情景演绎', '工作职场', '朋友&同事(平级)', '极端特写', '配音', '咖啡厅', '场景-其他', '大厅'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.76', '0.65', '0.32', '0.22', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0628ea304ceafae67f41bff67f893f9f.mp4\n",
      "{\"video_ocr\": \"我就说一句|学历低 找工作难|找到工作晋升难|怎么改变呢|现在上班族|都可自考本科|不耽误工作|在职就能学|手机在线学|你想想|你把每天刷手机|的时间用在学习上|你跟你周围的人|能一样吗|而都是985尼211|重点大学|每天仅需|学习30分钟|热门专业可以|自由选择|随便挑|你可能要问了|这么好的事|不会是假的吧|学历存档|信息全网可查|还顾虑高什么|升职考教师|考公务员|考证都能|点击视频|即可报名|在职本科 手机学习 每天仅需30分钟|轻松拿双证本科”|2018年纽交所上市教育品牌·STG|985/211重点大学备考课|看看你会被哪所大学录取?|SUNLANDS|尚德机构|1|2V\", \"video_asr\": \"我就说一句，学历低，找工作难，找到工作晋升难，怎么改变呢？现在上班族都可自考本科，不耽误工作，在职就能学习手机在线学习，你想想你爸每天刷手机的时间用在学习上，你跟你周围的人能一样吗？而且都是九八五二一一重点大学，每天继续学习三十分钟，热门专业可以自由选择，随便挑。你可能要问了，这么好的事不会是假的吧？学历？|倒信息全网可查，还顾虑什么？升职考教师，考公务员，考证都能用，点击视频即可报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.016129732131958008 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '推广页', '静态', '室内', '喜悦', '办公室', '多人情景剧', '平静', '动态', '家', '室外', '配音', '拉近', '特写', '(马路边的)人行道', '远景', '手机电脑录屏', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/062f78c4c434b02cdf700fed4dc92a90.mp4\n",
      "{\"video_ocr\": \"为什么上了高中之后|很多孩子突然跟不上了|其实|大多是数学拉了后腿|小题全靠蒙|大题没思路|有时候光数学这一科|就能拉开30-50分的差距|其实高中数学真的很简单|如果只靠自己揣摩|不仅会走很多弯路|还会消磨趣!|为何不让孩子一开始|就跟着专业人学呢?|我是高途课堂张宇老师|专注高中数学一线教育 9年|总结了高中数学|52个|带你轻松涨分!|只需9元 还在等什么|点击视频下方|8.如图，点Ν为正方 战段ED的中点，则|RAPT|高中数学资深主讲老师|6大 知识k|知识板块|3，|提分技巧|22へ|A.BM=EN，且直 B.BM士EN|新用户专享 立即体验 浙江卫视指定在线教育品牌|，并成为中国古典小|[查看详情]|每教|全国百佳教师带队教学 平均教龄11年|浙红卫视|高途课堂|名师特训班|[0.1.2)|D.10.1.2）|且直|LAUNCH|张宇|ICH|华少|BMf=EN，|WONDERFUL DAY|DAY|标Ⅲ) 四个|为四|解|￥9|仅需\", \"video_asr\": \"为什么上了高中之后，很多孩子突然跟不上了？其实大多是数学拉了后腿，小题全靠蒙，大题没思路。|时候光数学这一科就能拉开三十到五十分的差距，其实高中数学真的很简单，如果只靠自己揣摩，不仅会走很多弯路，还会消磨兴趣，为何不让孩子一开始就跟着专业的人学呢？我是高途课堂张宇老师，专注高中数学一线教育九年。|总结了高中数学六大知识板块，五十二个核心知识点，以及三十二提分技巧，带你轻松涨分。|只需九元，还在等什么，点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01638650894165039 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '单人口播', '平静', '教师(教授)', '家', '多人情景剧', '室内', '手写解题', '亲子', '愤怒', '配音', '家庭伦理', '情景演绎', '特写', '极端特写', '学校', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.70', '0.59', '0.55', '0.53', '0.19', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06308a1c2a1d6708c9d98b89d1d30a3e.mp4\n",
      "{\"video_ocr\": \"哎哟我的天呐|这高途课堂真是疯了啊|16节直播课只要9块钱|喂小林啊|你还没给孩子报|高途课堂的高中全科名师班的课呀|你赶紧报啊|9块钱16节课|平均教龄11年的|北大清华毕业的老师带队|给孩子上课|这成绩还怕提不上去|闺女闺女快来啊|高途课堂又开课了|就是那个教我各种提分技巧|和解题技巧|课后还有辅导老师|1对1辅导答疑的|高途课堂吗|对啊|你快给我报名啊|早就报了|你呢|快点击视频下方链接报名吧|名师出高徒.网课选高途\", \"video_asr\": \"哎呦，我的天呐，这高途课堂真是方法十六节直播。|只要九块钱还响铃啊，你还没给孩子报高途课堂的高中全科名师班上课呀，你赶紧报啊，九块钱十六节课，平均教龄十一年的北大清华毕业的老师带队给孩子上课，这长夜害怕提不上去的。|高途课堂又开课啦，就是那个交给我各种提分技巧和解题技巧，课后辅导老师一对一辅导答疑的高途课堂吗？对呀！|快给我报名了，早就报了你呢，快点点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01839733123779297 sec\n",
      "{'result': [{'labels': ['现代', '家', '中景', '静态', '推广页', '多人情景剧', '单人口播', '喜悦', '亲子', '手机电脑录屏', '平静', '家庭伦理', '愤怒', '极端特写', '夫妻&恋人&相亲', '特写', '手写解题', '惊奇', '动态', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.75', '0.54', '0.50', '0.28', '0.05', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/063e5bb70c18805b560f883ab7004048.mp4\n",
      "{\"video_ocr\": \"几千块钱的练习册|都愿意给孩子买|却不愿意|花9块钱|给孩子一个|改变未来的机会吗|每天逼着孩子|一套又一套的刷题|掌握清华北大|毕业的名师|总结的学习方法|可以让孩子|告别题海战术|三题海5|还在带着孩子|征战各大辅导班吗|你在路上耽误的时间|肓途课堂的老!|都已经带着孩子|攻克完|一整套重难点题型了|不用怀疑|高途课堂全科名师班|寻三君堂全科名师适|语数英物|四科旧节直播课|毕业的名师带队教学|平均教龄11年|教的都是|和解题技巧|课后还有辅导老师|一对一答疑|无论孩子基础如何|老师都会定制|都会定|专属学习计划|不要扰豫了!|赶紧|点击视频下方链接|报名吧!|仅需|新用户专享立即体验 浙江卫视指定在线教育品牌|高途课堂|浙红卫视|名师特训班|全国百佳教师带队教学 平均教龄11年|¥9.00|华少|¥9\", \"video_asr\": \"几千块钱的练习册都愿意给孩子们，却不愿意花九块钱给孩子一个改变未来的机会，每天逼着孩子一套又一套的刷题，却不知道掌握请清华北大毕业的名师总结的学习方法，可以让孩子告别题海战术，孩子带着孩子征战各大辅导班，你在路上耽误的时间，高途课堂老师都已经带着孩子攻克完一整套。|南北提醒了，不用怀疑，高途课堂全科名师班，语数，英物，四科十六节直播课都是清华北大毕业的名师带队教学，平均教龄十一年，教的都是高效的学习方法。|点击进行，课后还有辅导老师一对一带，无论孩子基础如何，老师都会定制专属学习吧！不要犹豫了，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016180753707885742 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '平静', '单人口播', '静态', '室内', '配音', '场景-其他', '特写', '手写解题', '极端特写', '喜悦', '拉近', '家', '动态', '教师(教授)', '情景演绎', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.36', '0.23', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/063f90aa13325a57bd710c3184badc41.mp4\n",
      "{\"video_ocr\": \"6月份体会到钱生钱的乐趣|7月份房租和信用卡还款ス再是烦恼|299元 现花oヵ费学|299才现花0ヵ￥)费学|299元视花0才免费学|现花0ゎ￥)费学 29礼|2利 现花oヵ免)费学|视频为演绎情节|视花o才￥费学|6月份报名a快财商喾院理财小白训练营|学理财上快财|8月份 工作只是生活的一部台|6月伦报名性财商学院现财小百训练营|现花o元费学|5月工资|2200元|1500元)|存款|快财商学院|日常开支|最后剩下|800)|直播|信用卡|0元|快灯|房租\", \"video_asr\": \"今天给你们看一个账本，五月份工资只有五千，房租两千二，信用卡要还一千五。|日常开支还要八百，最后只剩五百，存款只有零，一分钱都剩不下。|六月份报名快财商学院小白理财训练营，学习课程当日体会到钱生钱的乐趣，七月房租和信用卡还款不再是种烦恼。|八月工作只是生活的一部分，两百九十九元的快财商学院小白理财训练营，现在零元就可以免费学，赶快点击下方链接了解详情，立即报名吧！|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.015882492065429688 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '配音', '推广页', '平静', '填充', '重点圈画', '幻灯片轮播', '手写解题', '课件展示', '手机电脑录屏', '混剪', '室内', '拉远', '动画', '中景', '特写', '知识讲解', '(马路边的)人行道', '学校'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0652ee4527879730144fe57b41e4c900.mp4\n",
      "{\"video_ocr\": \"写出元|写出高分价|一篇作文|最吸引老师的|无非是一个好开头|比如写冬景作文|你就不要写|冬天来了|天气变冷了|这样普通的开头|不如试一下|对比开头法|冬天虽然不像|万紫千红的春天|那么美丽|不像绿树成荫的夏天|处处充满生机|不像果实累累的秋天|那么吸引人|但它却银装素裹|动人心扉|我是张驰老师|毕业于北京大学中文系|在学而思网校|秋季语文特训班|我将会教给孩子|36大阅读写作方法大招|24大语文核心知识点|10大知识版块|课后还有班主任老师|一对一辅导答疑|现在报名只要9元|还在等什么|点击视频下方|查看详情|给孩子报名吧|家长们注意啦!|快|快来看看怎么让孩子|毕业于北豪大学|每天赚步一点点一|立即报名 元/10课时|专攻重难点，阅读写作高效提分 官方价:399元|生步 乍突破班|小达老师 北京大学|珍贵教辅 全国包邮|来计划|教龄3年|小初高|老师\", \"video_asr\": \"家长们注意了，快来看看怎么让孩子写出高分作文，一篇作文最吸引老师的无非是一个好开头，比如写冬景作文，你就不要写冬天来了，天气变冷了，这样普通的开头，不如试一下对比开头法，冬天虽然不像万紫千红的春。|那么美丽，不像绿树成阴的夏天处处充满生机，不像果实累累的秋天那么吸引人，但他却银装素裹，动人心扉。我是张池老师，毕业于北京大学中文系，在学而思网校秋季语文特训班，我将会交给孩子三十六大阅读。|写作方法大招二十，四大语文核心知识点，十大知识板块，课后还有班主任老师一对一辅导答疑。|现在报名只要九元，还在等什么，点击视频下方查看详情给孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015833377838134766 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '静态', '平静', '教师(教授)', '特写', '室内', '场景-其他', '极端特写', '知识讲解', '混剪', '配音', '教辅材料', '家', '家庭伦理', '办公室', '喜悦', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.74', '0.32', '0.15', '0.05', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/065e17e016bd6e0cd1d74aeaa4ad2b0b.mp4\n",
      "{\"video_ocr\": \"我写完了|比之前还快了5秒|而且呀 全对|真厉害|乐乐爸|之前乐乐速算没这么快|怎么进步这么大啊|对啊|肯定是花大价钱|报了补习班了|那你可就错了|我们没报什么补习班|我们是在家学的|那一对一在家 辅导更贵了|这是爸爸花 29块钱给我买的|数学养|猿铺导|B|数学名师特训班|有9课时呢|清北毕业名师|没错 总结了|20个必备计算技巧|25个常考应用题巧解法|50+经典习题精练|29块钱能学这么多|真的这么便宜吗|真的只要29块钱|而且现在|还有15件教辅文具|包邮送到家|只要多加一块钱|再得9课时 语文阅读写作课|那这课怎么报名啊|我也想让妈妈给我报名|点击视频下方链接|就可以报名啦|德辅导，|痕辆导在线钱|秉辅导|狼辅导anm|加1元可换购9课时语文课|27件原创教辅 礼盒全国包邮|带队授课|狼拥导|人令。|YUANFUDA0|CAPSTEC|Hi|语文特训班|包邮|癌辆看|猿辅导在线教育|全国\", \"video_asr\": \"我写完了，比之前快了五秒，而且啊，全对是吧，唉，算了吧。|智商上的付款没这么快，怎么定不这么大一对啊，这是花大价钱报了补习班了，看看你可就错了，我们没报什么补习班，我们是在家学的。|在家辅导更贵的，但是八八花二十九块钱给我买的呀，辅导数学名师特训班有九课时吧，清北毕业名师带队授课，没错，总结了二十个必备计算技巧，二十五个长款，有紫色解绑和五十家经典习题，经典二十九块的学这么多真的这么便宜了，真的只要二十九块钱，而且现在还有十五件教辅文具。|包邮送到家，找作家一块钱再得九课时语文阅读写作课。|帮你啊，我也想让我妈妈给我。|你点击视频下方链接就可以报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01598191261291504 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '平静', '静态', '多人情景剧', '亲子', '极端特写', '喜悦', '家庭伦理', '教辅材料', '动态', '室外', '路人', '特写', '室内', '全景', '惊奇', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.88', '0.87', '0.82', '0.11', '0.07', '0.07', '0.07', '0.06', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/065ee519863350692c7254bacfcfdae0.mp4\n",
      "{\"video_ocr\": \"不就8万吗|你就把车抵押出去|通过玖富万卡申请|最高有20万的借款额度|玖富万卡|A自动审核|最快3分钟放款|最长可分24期慢慢还|点击视频下方链接|申请你的额度吧|最高可借20万元|三步极速借款 敢活额度 快速摄取|测测你能借多少|流程短 填写手机号注册-下载APP|放心贷 银行等正规机构放款，可靠有保|20w|申请贷款查看额度|审批快|还款灵活 长可分241|注册即表示您同意《平台用户注册协议》|ONECARD|道亏查科|增写货料|障|实际额度以审批结果为准，请珍视信用，按时还款，勿过度举债|具体额度费手等根据实际审挑大雄 息费建免优惠以具体枯动规则为准|200000\", \"video_asr\": \"我就八万吗？你就把车抵押出去，通过九放款申请，有最高二十万的借款额度AI自动审核，最快三分钟放款，最长可分二十四期，慢慢还。点击视频下方链接申请你的额度吧，最高二十万来，请你不用咯，这次你有多少额度吧？\"}\n",
      "multi-modal tagging model forward cost time: 0.015932798385620117 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '手机电脑录屏', '静态', '场景-其他', '多人情景剧', '平静', '配音', '朋友&同事(平级)', '单人口播', '特写', '喜悦', '动态', '愤怒', '室外', '路人', '惊奇', '夫妻&恋人&相亲', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.93', '0.86', '0.37', '0.34', '0.34', '0.08', '0.05', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0665440dbc4dd131cc2c6b89a9b7211d.mp4\n",
      "{\"video_ocr\": \"现在很名人都知道|用手机短视频能赚钱|但是大部分人|只能赚到几毛钱|要么就是提现不了|说自己是什么大品牌|到头来还是欺骗用户|而我们研发的|快手极速版承诺|同样是看视频|我们给的金币更多|提现的也更多|金币能直接兑换成现金|提现到微信|刷刷视频|就能赚到这么多|每天都可以提现|何乐而不为呢|赶紧点击视频下方|下载吧|上快手极速版|刷视频就能领红包|娱乐赚钱两不误|你还在等什么呢|福|+0.36元 新用户现金红包|预计10分钟内到账微信钱包 继续看视频|48.8|赶紧点击 视频下方链接|每天副3分钟视频，哪花钱就有了，徽还不知量?|我的收益 收益说明|提现|微信支付:商家付款入账通知 现在|1.若连续30天未登录，未提现 2.金币预计会延迟1分钟到账，部分流水会合并展示 金币明细|2.金币预计会延迟1分钟到 部分追水会合并展示|首次专享 3元|了解详情|2020.4.2 好友赠送新用户红包|+1元|20元|审核中|微信 支付宝|9465金币|48|1.3|详情见活动规则，奖励金额视任务完成情况而定|提现成功!|选择提现金额|2020|好友\", \"video_asr\": \"现在很多人都知道用手机短视频能赚钱，但是大部分人只能赚到几毛钱，要么就是提现不了，说自己是什么大品牌，到头来还是欺骗用户。而我们研发的快手极速版承诺同样是看视频，我们给的金币更多，提现的也更多。|金币能直接兑换成现金，提现到微信，刷刷视频就能赚这么多，每天都可以提现，何乐而不为呢？赶紧点击视频下方下载了上千首出版刷视频就能领红包，娱乐赚钱两不误，你还在等什么呢？赶紧点击视频下方链接下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01618361473083496 sec\n",
      "{'result': [{'labels': ['现代', '静态', '多人情景剧', '中景', '推广页', '平静', '特写', '动态', '工作职场', '惊奇', '手机电脑录屏', '喜悦', '场景-其他', '上下级', '朋友&同事(平级)', '愤怒', '配音', '室外', '拉近', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.95', '0.69', '0.43', '0.41', '0.35', '0.30', '0.26', '0.12', '0.06', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/067ede3798d36b64b4b569208ffc4cd6.mp4\n",
      "{\"video_ocr\": \"您教孩子用的什么方法呀|唉 我哪会教孩子啊|一到做阅读写作文大人孩子都犯难|那您是怎样教育孩子的呢|我也不会|但是我给孩子报了|学而思网校秋季语文特训班|做题写作文都是独立完成|一点都不操心|真的有那么好吗|是真的|是由北大毕业名师授课|写作和阅读逐一攻破|市场价399现在只要9元|更有教辅大礼包送到家|赶紧点击视频下方链接|给你的孩子报名吧|全国 包邮|专攻重难点，阅读写作高效提分 官方价:399元 元/10课时|小达老师 北京大学 秋季语文特训班|学而思网校 小初高|一每天进步一点点一|立即报名|9\", \"video_asr\": \"您好，您家孩子用的什么方法啊？哎，我哪会教孩子，一到做阅读，写作文，大人孩子都犯难，那您是怎么教育孩子的呢？我也不会，但是啊，我给孩子报了学而思网校秋季语文特训班，做题，写作文都是独立完成，一点都不靠近学而思网校秋季语文特训班真的有那么好吗？是真的，学而思网校秋季语文特训班是由北大毕业名师授课。|阅读族攻破市场价三百九十九，现在只要九元，你就大礼包送到家，赶紧点击视频下方链接给你的孩子报名吧！|在。\"}\n",
      "multi-modal tagging model forward cost time: 0.016089200973510742 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '喜悦', '平静', '单人口播', '路人', '动态', '惊奇', '室外', '朋友&同事(平级)', '特写', '手机电脑录屏', '全景', '拉近', '极端特写', '采访', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.88', '0.76', '0.70', '0.19', '0.09', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/068462a2dd49dd4a32330afab6ffa16a.mp4\n",
      "{\"video_ocr\": \"免费领取 5天哈佛早教训练营 50万精英家庭的选择|抓起亲了|棒棒的!|真好玩呐|上点点橙|点点橙早教盒 玩具早教轻松有效|有效|貂轻松有效|玩具早教 轻松有效|点点橙 ABC积木\", \"video_asr\": \"抓起来了棒棒的。|上点点橙，免费领取五天哈佛早教训练营！\"}\n",
      "multi-modal tagging model forward cost time: 0.016867399215698242 sec\n",
      "{'result': [{'labels': ['家', '现代', '填充', '亲子', '家庭伦理', '多人情景剧', '动态', '中景', '静态', '配音', '夫妻&恋人&相亲', '情景演绎', '极端特写', '喜悦', '全景', '悲伤', '平静', '手写解题', '拉近', '推广页'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.05', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06aa3c8795b45c291b405cacaa35811d.mp4\n",
      "{\"video_ocr\": \"如果你真心想让你的 生活品质提高|家庭变得富有|那么你需要拓宽你的 收入来源|有很多人说我愿意但 是没方向|那么我来告诉你|现在有一种在家说书 配音|就能赚钱的副业选择|为什么不能给自己一 个机会|试一试把配音变现|现在有一个机会摆在 你面前|只需点击视频下方|支付1元|就可以领取一份潭州 教育官方推出的|原价259元的播音配 音精品课|专业大咖帮你纠正发 音美化声线|课后助教老师随时答 疑|一部手机随时随地上|不占用工作时间|把握这次机会|让O基础小白的你|也能快速掌握播音配 音技巧|在家配音说书|Tanhnt|ThanzhouEDlu|Tanzhou日0U\", \"video_asr\": \"如果你真心想让你的生活品质提高，家庭变的副总，那个你需要拓宽你的收入来源，有很多人说我愿意，但是没方向，那么我来告诉你，现在有一种在家瘦出配音就能赚钱的副业选择，为什么不能给自己一个机会试一试打配音面前？现在有一个机会摆在你面前，只需点击视频下方支付一元，就可以领取一份财政教育官方推出的原价两百五十九元的播音配音精品课。|专业大大帮你纠正发音，美化声线，课后助教老师随时答疑一部手机，随时随地上课，不占用工作时间，把握这次机会，让零基础小白的你也能快速掌握播音配音技巧。|在家可以球球。\"}\n",
      "multi-modal tagging model forward cost time: 0.01648426055908203 sec\n",
      "{'result': [{'labels': ['现代', '中景', '平静', '推广页', '静态', '动态', '家', '填充', '手机电脑录屏', '混剪', '情景演绎', '配音', '多人情景剧', '单人口播', '办公室', '室内', '特写', '极端特写', '喜悦', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.88', '0.51', '0.49', '0.25', '0.23', '0.14', '0.11', '0.10', '0.10', '0.08', '0.07', '0.06', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06b6b2ba30fdd1ed6aebf2809cc79e3f.mp4\n",
      "{\"video_ocr\": \"老婆|我打听到了|打听到什么啦|老张给他女儿报的是|作业帮直播课|难怪数学那么好啊|我都把礼盒都拿来了|就是这个|新学期数学名师提分班|30元18节课|还有...|这能行吗?|当然行啦|这都是清北毕业名师|带队教学|这师资力量有保障|培养孩子|数学思维的同时|还教授各类解题大招|速算抄训|和速算技巧|这么好呢|那你没问问这么报名啊|点击视频下方链接|就可以报名了|快速掌握重难点 轻松领跑新学期|名师有大将|00|上课内容与收到礼盒以实际为准|课|中国文排|解题更高效|立即抢购\", \"video_asr\": \"老婆我打听到了那什么了，老张给他女儿报的是作业帮直播课，难怪数学那么好啊，看我都把礼盒都拿来了，就是这个作业帮直播课，新学期数学名师提分班，三十元，十八节课还用能行吗？当然行了，这都是清北毕业名师带队教学，这师资力量。|有保障啊，培养孩子数学思维的同时，来销售各类解题大招和速算技巧呢，这么好呢？那你们问问怎么报名啊？我当然问了，点击视频下方链接就可以报名了。|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016396045684814453 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '中景', '家庭伦理', '静态', '平静', '多人情景剧', '亲子', '教辅材料', '家', '极端特写', '喜悦', '配音', '动态', '室内', '愤怒', '全景', '单人口播', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.90', '0.90', '0.15', '0.10', '0.07', '0.06', '0.05', '0.02', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06c5bd66fc0a7960ef211415832151d4.mp4\n",
      "{\"video_ocr\": \"小美快帮我签到|忙着呢签什么到啊|哇|这什么呀|拼多多签到|签到就能领现金|怎么领啊|下载拼多多|点击首页现金签到|立马就有红包到账啦|分享好友还能领取更多呢|我也想试试|怎么下载呀|点击下方链接就可以下载啦|拼多多春节不打烊|赶快下载拼多多签到领现金吧|ale|让你红包领到手软哦|新人礼包|新人特权 退货包运费·100%成团·优先发货|当月累计签到15天领奖励(价值10元) 0/15|钻毛神器|限时秒杀 断码清仓|您有现金未取出|新人1元购|签到成功，获得5.12元|还有额外微信红包|苹果x手机壳商务超|加厚金貂绒珊瑚绒四件套保暖 火滴全面屏千元学生|百亿补贴|好货春回家 PK赢红I包|继续领取其他福利|手机|新人专享大礼爆款低至1元|连续签到3天|立享多重优惠|大号儿童工程车 苹果系列磨秒手机壳 新人价|食品 母婴|15:50|短毛绒四件套 热门 男装|手机 鞋包 食品|鞋包|现金排行榜 提现|正品保障|侣冬季寝室宿舍防滑学生拖鞋 出i1l|9块9特卖|全球购|301|全新升级|电器|新人价¥1|男装|已开启提醒|热门|￥0|12|特价￥66|立即购买|汽车|推荐\", \"video_asr\": \"小美在王签到，忙着呢，侵蚀着读。|哇，这什么呀，拼多多签到签到就能领现金怎么领了？下载拼多多点击首页，千金签到立马就能到账了，分享朋友还能领取更多的，我也想试试怎么下载啊，点击下方链接就可以下载了，拼多多春节不打烊，赶快下载拼多多签到领钱钱吧，让你红包拿到手软哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016136884689331055 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '推广页', '中景', '静态', '多人情景剧', '喜悦', '配音', '场景-其他', '平静', '家', '单人口播', '极端特写', '特写', '夫妻&恋人&相亲', '惊奇', '愤怒', '家庭伦理', '动态', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.90', '0.87', '0.76', '0.42', '0.23', '0.11', '0.04', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06c7c304a1e25cd2227665b09beb1d02.mp4\n",
      "{\"video_ocr\": \"喂您好|实在抱歉|您购买的斑马A课|英语体验课|已经下架了|现在没有了|别的课程您也可以了解一下|49元10节课的|斑马A课英语体验课|对|不卖了|全都给我下架|调整一下|准备涨价|现在10节课49块钱|1节课还不到5块钱呢|北美外教 动画教学|还送大礼包|我儿子亏死了|您这是干什么呀|就是我定的|儿子|太便宜了|你挣不到钱呐|我们做教育的|就是要不计成本|你知道吗|家长们带着孩子上过课后|都说情景式动画教学的形式|效果特别好|孩子坐得住也学得进去|而且孩子用手机平板就能学|给他们省了不少麻烦|这么好的课我要让孩子都学得到|不行啊这|听我的|再加5000个名额|还包邮赠送随材礼盒|家长们别犹豫了|点击视频下方链接|抢课报名吧|立即报名|猿辅导|在线教育 出品|猿辅导在线教育|猿辅导在线教得 出品|23122231231 刘须|3193212 肖续|猿辅导在出线|在线|在线教育|猿辆寻|痃铺导|猿冥号|具体以收货为准 1-S3不同年龄版礼金内容不同|段礼盒内容不同|具体|斑马Al课|3121T209088|3122215146刈艳|S1-S3 不|重熏|S1-53不同年龄段礼盒内容不同|不同年龄段礼盒内容不同|虽兹|呈羊|出品|预育|138****6789|188****1237|赠送随材礼盒(包邮)|189**水*6778 程限小风1婚50 188****7923 程程小K1梦天0大sAt hoe”|138**米*2345 程程小2V传00天p|136****6789 程程小区5号青1学|李熏熏|188****7923 程程小区1号楼1单元|135****5688 程理小区5号情月 178****5129 程程小区2号楼师|178****5656|1S|62631231331|徐宁 12349876313张徐徐|67873210097李勋 34365789904王晓琦|猿5异92123778张一二|12323678990张怡宁 12873211商宁|189****6211程程小区3可楼5前|程程小区]号楼50元n0|13452678123 李燕燕|12111212111王一天 23331090887\\\\任晨晨|23231324890 刘霞 23566533890张康|159米***6565|32122123778张|188****4297程严小区59绩1单|136****4126|188****4547 程程N1秒他3包 178****5656 理帮小区K1梦师30庆|1.35****5688|21223133212肖续|33122290877李毅|32456668900肖宁|刘霞|商宁 189%|01011910005|WVS摄|51|60|49|T2\", \"video_asr\": \"呸呸呸呸呸。|喂，您好，实在抱歉，您购买扫码AI课英语体验课已经下架了，现在没有了别的课程，你也可以了解一下，四十九元十节课的斑马AI课英语体验课已经下架了，实在抱歉，对不卖了，全给我下架，调整一下，准备涨价，现在十节课四十九块钱，一节课还不到五块钱呢，别被你败，也就花掉还送大礼包，我就亏死了，准备赶紧涨价。|这是干什么呀？这斑马AI课有一天课四十九元，十节课就是稳定，而儿子啊，太便宜了，你就爆钱了吗？我们做教育的就是要不计成本，你知道吗？家长们带着孩子上课，课后都说刑警是动画教学的形式，效果特别好。|你坐的住这学习兴趣，而且孩子用手机，平板就能学，给他们省了不少麻烦，这么好的课我要让孩子都学得到不行，夏天玩得满满，来课英语体验课四十九元，十节课再加五千个名额，还包邮赠送水彩礼盒，家长们别犹豫了，点击视频下方链接抢课报名吧！|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016115665435791016 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '推广页', '中景', '朋友&同事(平级)', '动态', '工作职场', '特写', '平静', '办公室', '家庭伦理', '极端特写', '手机电脑录屏', '愤怒', '拉近', '亲子', '上下级', '悲伤', '静态', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.99', '0.96', '0.95', '0.93', '0.88', '0.78', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06c7f68ef045fed490fa1fc974e2b517.mp4\n",
      "{\"video_ocr\": \"你就是爷爷非让我娶的丫头|看来爷爷真是老糊涂了|走开|对不起|我给你收拾|出去|顽皮|丫头|怎么了|我发现我喜欢上你了|霍先生|你一直都是我的|点众 阅读|《霍先生，你是我的言不由衷》|物出能并工|艺小咀|液恩小啦|M\", \"video_asr\": \"你就是爷爷非让我娶的丫头。|看来爷爷真是老糊涂了。|走开。|对不起，我跟你说什么。|我找不到。|好的原因去阻挡这一切的情意。|这感觉太奇异。|我感觉能说明。|顽皮。|爱情的。|姐姐会发生也不一定。|热的清洁是好性感哦。|我发现我喜欢上你了。|可是我刚刚。|霍先生，你一直都是我的。|这是个不知道。|不像我。\"}\n",
      "multi-modal tagging model forward cost time: 0.016181230545043945 sec\n",
      "{'result': [{'labels': ['现代', '特写', '中景', '多人情景剧', '推广页', '静态', '愤怒', '动态', '室外', '夫妻&恋人&相亲', '朋友&同事(平级)', '平静', '亲戚(亲情)', '拉近', '单人口播', '室内', '悲伤', '惊奇', '极端特写', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.95', '0.71', '0.48', '0.21', '0.18', '0.09', '0.09', '0.04', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06d16ce1129620efe159cf80828a01d7.mp4\n",
      "{\"video_ocr\": \"Cabbage 包菜|Bread 面包|英语听说启蒙 亲子阅读专家|Noodles 面条|rice米饭|伴鱼绘本|Corn玉米\", \"video_asr\": \"TAB TAB TAB RISE RISE RISE MODELS MODELS MEASURES BAD BAD BAD GOGOGO伴鱼绘本，英语听说启蒙，亲子阅读专家。\"}\n",
      "multi-modal tagging model forward cost time: 0.016771316528320312 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '配音', '填充', '平静', '课件展示', '动画', '静态', '中景', '喜悦', '知识讲解', '重点圈画', '宫格', '教辅材料', '才艺展示', '幻灯片轮播', '动态', '手机电脑录屏', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.77', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06d4c209c95679a7c9055438ac972a0c.mp4\n",
      "{\"video_ocr\": \"哎|你这也不是iPhone11啊|i4g|讨厌|那么贵|我哪买得起啊|买得起|你在分期乐上|最高有5万额度呢|5国|对呀|额度不用不收费|36期慢慢还|最长可分￥ 慢慢还|最长可分36期慢慢还|真的|手机号给我|呐 纯线上申请|无需面审|最快1分钟就能放款|这就是你的额度|太好了|我有最高5万额度|我有钱买新手机啦|朋友们|你们也赶紧点击视频 下方链接|查看自己有多少额度吧|封0吞|d宝|1云1E1E|1a于 封0之？|三国红于|M一|分期乐|美国纳斯达克上市（股票代码：LX)|额度以实际审核为准|随借随还|最高额度(元)|1云E·021|Frendly|FrandL|Fnendk|ARI|mMMw|50000|andls|最长\", \"video_asr\": \"嗯哎，你这也不是IPHONE十一啊。|房价那么贵，我哪买得起买得起，你在分期乐上最高有五万额度，最高有五万额度，对呀，大家在分期乐上最高都有五万额度呢，额度不用不收费，最长可分三十六期，慢慢还真的手机号给我。|那纯线上申请，无需面审，最快一分钟就能放款，这就是你的额度太好了，我要再转五万额度，我有钱买新手机了。|朋友们，你们也赶紧点击视频下方链接查看自己有多少额度吧！|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016033649444580078 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '喜悦', '路人', '惊奇', '愤怒', '单人口播', '手机电脑录屏', '平静', '场景-其他', '室内', '全景', '特写', '家', '悲伤', '动态', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.85', '0.37', '0.18', '0.09', '0.07', '0.06', '0.06', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06e5bb075d1006ae408de4173ebbff40.mp4\n",
      "{\"video_ocr\": \"高考语文150分|但是很多同学|只能考100分左右|已经成为大部分人的弱项|并且随着考试难度增大|区分度越来越大|其实啊|语文是性价比|非常高的一门学科|掌握正确的方法|提高20-30分完全有可能|你不知道|是因为你没有体验过|我是马昕|北大中文系博士 执教10年|培养出多名|高考语文单科状元|江湖人称|语文状元制造机|我会用多年|培养语文学霸的经验|结合判卷人思路|带你正确学习语文|并且有效提分|那么到底有没有|这样神奇的课程呢|欢迎点击视频下方体验|阿用你使动。所谓镜动，是指谓汤厨表示的功作行为不,宝通|说了，因为他是医主，有定委选。|微送拍限存考，睡掉有会不得了。微在梦看出案了，两开地家时 X00元钱庄在枕头下，只带走子21本阳书，送耐作着这科写道|战压在枕头下，只领走了21本租书。这时作着这样可道: 春芳时，地珂有些会不得了，元春芳看出来了，离开边家时|高途课堂|浙江卫视|5|马听|周素用喜潮形式表现出末《|一次是从西方来的水 配t|您花源记》)这里的“子是的不网用法，|应具备哪些要亲 动is 选择地卉晨a76e|成功的访该应美冬写叠针、 为一个优秀的来动者庄品|阳光随看n库奶干:|到了爱与被爱的歌卡|遂成为一种“含汕的奖|的意界。对这一美理|德”的意思，对这一类惠 就水也，而能江河。(功学》)|长。为了解她上T的，确定访谈重点;|中央电视台(面时面)装 尽”病区护士长，东雕 宠的淡重点:|事。想想看，账剧！|—这是一道或嘴门|想刺价价在于目的和方法1 同和效果的不盾又 这些矛盾的惠刷D|这的问题”“是生再还 谁的翻译更|得”字相对 危，浪收面|马个“得”字相对而言的， 力弧危，战|倾听后，访谈着门|张贴试玉花八门时告s;|大厅门城路收的|人民教育出版社 课程教材研究所|必修|便非常生气，店不包|了银受访着。设计iy|地打装旧的患地2|集因结果在童料之外的前额 呵可爱存在于同一人物外上# 正国为喜菌和悲剧，希售|的“卑鄙指“建血，|这里的“卑哪”指“地新 中加点词的意又和用渎、了解名调活用作动词的脱律|在美丽的优俱中特|手中，让绝们在美霸动龙A|已敏颜的人，这些人是段们学习的榜样。比卿，靓批不格看转宝|美的人，这些人是我们学习的榜样。比如，那丝不惜牺牲宝多 无私绝为社会服务的“青年想愿者”:豚丝在推折、医破和|一个变要的友现时代，发现了什么?固为求知献的觉醒，发现了 你本质的衣而开始星现在一个人的秘神祝野之中了。所以，我把中|费的发呢时代。发现了什么？四为求知欲的觉醒，发现了一个|新用户专享 立即体验 浙江卫视指定在线教育品牌|任，实期徒有一起拖负，世R|为此处应如何翻诗呢 名言:Translators we t|[面东地积累和技理|贵上有重点地积黎和桃了|他德在门外贴条子|，爱做好起译儿L手是|文化线产，他有助子视高鞋 进行一看械理|的伤谈离不开死分的商名|不清千我状，有要队|和学语文谏程教材研究开发中心 么京大学中文系 语文教育研究所|g、“错误是在德的打 鼎格，却是十分微商化大|英文中“he 真有多 高。不网的中国诉考官场|俑文章的关锤。|是读懂文敬的失语。一制天|编着|天下之乐回乐。\\\"((击阳棱记》）了解其中始两个“之 和两个 示同语或句子之同的结构笑系，有助千唤切地硬籽文仓。|时是有点经版的，一个是觉得名么会超到没命去，后来她不休适持。 红靠心。我这样一说的话。他就腰不高兴了，好像大家有点儿铭|每个人的感受往挂不同，能写出自己的技特感受，文麻也统有了新 我同写中学到代的文字:|中学时代的文字:|“先帝不以臣卑属。(A联私天对《扎雀床南飞)中的“相”字通行械理。|策件，无私地为社会服务的“青年志思智”:那些在控折、国难和 居些花腰遭命运打击、备会生活限辛之居仍然乐规向上，宽厚善食|兴趣和条件有选择免看的了国遭命运打击、备尝生活艰辛之后仍松乐观向上、充即兽良|坚国—它养会一2之，所发现的是人生画面上最重姿的几笔，质言之，可以说是|这道门怎能不坚器一02所发现的元人生画面上最变委的儿笔，质言之，可以说是发! 国，发现了一个开性会吞。因为自我意识的觉殿，发现了自|一个人，因此，在是是国价|指一个人因此，在阳美是所为寿那也以卖与战别者， “等，多数则发生了变化超的何以及它在剑于中的位重来科识它的用法，从学过的文言;|何确定访设目的和片更时可试委去肯定委去的，青定是委去的，没程说，|学挺有“人经，看购变化，造化使借机向他透露了自己的然干秘密。正是在上中学账|新装的大门级手花有”想点造化使信机向地造意了自己的给干轻密，五是在上中生即个|铁、书、属、通、感、谢 法的掌摄可以使期北技的方法来虹纳，对真体语言环境中|方，既越委夫青定类去的，青定是要去的，没呼说 小封模也有优辣吗?|触尽晚天一张单程意吧，如果息够再有双程靠回来那就更评。 急没有玩豫吗?|n一起t中，中学时代是最重安的，其重费性鞋往被体计符不够，这御也友 每者太懵懂。过来人又太健恋。一个人出童年进入少年，身体和心|山学时代是最重要的，其重受性往往被信计得不够。这制也在情 借怪，过米人又太徒忘。一个人由变年进入少年，身张和心灵|样斯愚:或者要 来马上在日前索理 在于将章吉词德性格中内在|不”来看，|单奇词古大多数，其中有奶 除、珠、始、超通、发、就妖图、过、银、间、|活动，要思考双下咒是。|明文育虚词的数品要少得多，健时理解文言文内移来说，文请|金呢?|莞。人们常把“合汕的奖 的审美特低。这是因为，理 王体的清精的冲击和积|音近而借用。阴1:|司或音近而借用。阅如:1 这里通“慧”，意级”|个故事还会更人|复由的，每“主语俊离清包么样\\\"的堂退，阿如:“外适确面4 胸“年”疑“键………自相争斗”的意思|，我会好好地全好术术，地说你数心。口气是狼经定的，便|武从额洛上秋助了范春芳，还是跑泰芳从精神上教动了我?|这所体我，可以一借取力 土文化和现支社的Wg|注重创新|奇之\\\"(你钟水》)这里的“奇是*认为他报奇特”的意思|社走的那天睫上，巧6号晚上，我越东看的时线、(她说妈妈件不会|$所天晚上，16号她上，我格东雨时时提，(她说)妈妈你不会|a仪，假少会想到向对方学习什么，这一蓝却看出，证贴这个股济上 学生的放事，人们巴经读过极彩了，这一简短让读着心灵一资。阴为|放事，人们已经读过银多了，这一智即让读着心灵一震。因为|为是借用一个字来代导一何会于其址。((游褒秤山记》)|谈中启发、引视 实问题要具体，有什对性，措醉要恰当，表述要清楚 可季宝见，与同学讨论、交意。|如何在这段访淡中自度，残腰具体，有针对性，攒辞耍恰当，表述要清苑: 现，与同学讨论、交流。|时代，认为中学时代“可以说是发规了人生” 是怎样认识的！结摄新的作文。例如，《我阁望有个后妈)(我想生病)。这些题目|述方式，这个故事剑认为中学时代 “可以说是发现了人生” 新，同学们是怎脚礼除的作文。剑如，(我需望有个后妈)(我想生病》。这些题目|自她虚珂可以表示特定的语法关系，如“着“所\\\"等同与动调|轩送么可始，我说一切年洗夫和疑，我说视然小装速么说了，没命 小打黑的，我就柚由不永，饭加如出不来的语，你就照看小孩吧，我|我想起点手，够超那钟楼络将的钟声，镇人励息、银人上适、保 看出，所谓立意新、就是指文尊的概点战看同辅的角度与众不同。|阿沉立意新，就是指文章的灵点成看何慧的角度与众不同|再着、疾思如仇、追求直弹|莱纳特)中有一段丢2的a，|是文言文中的餐忍现象，简|(师说》)|陈值不本、糖待他人的人.……站施人可能花不是社会名流，也不 但我们在心望数佩他(她)们门。请对他们进行一次访说，了|下幸、善待他人的人………这些人可能既不是社会名流，也不|(周国平《(发思的时|生看。(重耳之亡》)|备好访谈提鲫，访谈提妈有如下要求: 防面，走进他(她)车富多彩的心灵世界。|铁提纲，访谈提纲有如下要求:|对于谢冕，这是一个“梦幻时代”。他用诗人的恩维与语有槽 更有速憾。对于周圆罕，这是个“发现的时代”。他用管学|人他好找中U摄声艺本中的捷气，是个不可度跳的 无自由，不柳格它看人输之口，穿看r不y明以|汉婚大数相简，也可以分为名司，动珂，那有闻，教网，壁网 作动同，例如:“倒此般以神名。\\\"l无钟山记))这里的\\\"么|文，写她教励了一个生结菲震国魂的小学生脱春芳。面尚自已|呵盲的|各么游这个折话呢？ 不会的没宝帅|下论暴费，不论又而，寥岁年牛，及时而中蛙，本身就是一种恒久的|香，不论风的，岁岁年年，及时而字饭，本身就是一种恒久的|普通高中课程标准实验教科书|宁说是文人整客的面更点专 井将它与新教相提井论<了|高园块文京文的力，或|全国百佳教师带队教学 平均教龄11年|先了,Trwnve |同多义|的(门的是之 木门城玛破了|处的“放屁！|小中天下人忧和已忧、天下人乐和己乐，优和乐儿星关系，进|钟定时敬场。那声音是温馨的、安评的，既抚慰衣们，又区唤|北大中立照聘士 们年在线教育经验|(选自依积慧《前线”目记|(选自张积是(“前线”日记》|愿译。|的本。 批的觉醒，发现了一个开性世界。四为自真意识的先履，发现了自|上教励了自己。这一敌到立题，让旗着得到了新的教益|的义项，魔结合上下X在得|华少|年柠审通过|教材审龙看货会|尽之际”案|考级下风题:|怕有后妈，这一位却爆望有个后妈，不新鲜吗？ 原来纱是大爱父|妈，这一位却望有个后妈，不新鲜吗?原来她是太爱父亲|语文|名师特训班|手、有的学着称之为t 仅是机城的请言转妆 【桃战推的工作|学教材审定微员会|”“活下去还是下活|名著导读|与探讨|异义|去。|￥9\", \"video_asr\": \"高考语文一百五十分，但是很多同学只能考一百分左右，已经成为大部分人的弱项，并且随着考试难度增大，区分度越来越大。其实啊，语文是性价比非常高的一门学科，掌握正确的方法，提高二十到三十分完全有可能你不知道，是因为你没有体验过。|却的方法，我是马欣，北大中文系博士，执教十年，培养出多名高考语文单科状元，江湖人称语文状元制造机。|我会用多年培养语文学霸的经验，结合判断人思路，带你正确学习语文，并且有效提分。那么到底有没有这样神奇的课程呢？欢迎点击视频下方体验。\"}\n",
      "multi-modal tagging model forward cost time: 0.016358375549316406 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '教师(教授)', '静态', '影棚幕布', '配音', '平静', '填充', '室内', '场景-其他', '过渡页', '宫格', '教辅材料', '幻灯片轮播', '特写', '喜悦', '知识讲解', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.86', '0.44', '0.17', '0.14', '0.06', '0.05', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06fdb7fc275a0980e63e516abfdc0f6b.mp4\n",
      "{\"video_ocr\": \"老板娘我求你个事儿|我能不能先预支|1万块钱工资啊|别求我|你自己有十几万的额度|你求我干什么|老板娘|我要是有十几万|我还找你借钱干嘛|说你两句|还急眼了|把你手机给我|自己看|15万!|我哪来那么多钱啊 150000元|360借条啊|最高能借20万|1分钟申请|最快5分钟到账|而且不使用|是不收费的|如果你也想拥有|属于自己的|那就点击视频 下方链接|申请吧|恭喜您成功领取免息券|点击视频下方立即申请|￥=6日借条|￥ ヨ6口借条|￥彐6G 借条|¥ヨ60借条|￥G惜条|我嗽菜那么黟锓满|我哪莱么娶锟瞒|立即领取 活动规剧2>|活动规则>:|贷款额度、放款时间等以实际审批为准 贷款有风险，借款需谨慎，请根据个人能力合理贷款|贷款有风险，借款需谨慎，请根据个人能力合理贷款|请根据个人能力合理贷款|30天息费优惠券 借一年，慢僵还|选择360借条的四个理由|贷款有风险，借款需|全民免息狂欢 最长|137829|18:104008|7 返回|开始\", \"video_asr\": \"老板娘，我求你个事，我能不能先预支一万块钱工资啊，别求我，你自己有十几万的额度，求我干什么？老板娘，我有是有十几万我还找你借钱干吗？|说你两句还急眼了，把手机给我。|自己看十五万，我哪来那么多钱啊，三六零借条啊，最高能借二十万，一分钟申请，最快五分钟到账，而且不使用是不收费的。如果你也想拥有属于自己的三六零借条额度，那就点击视频下方链接申请吧！|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016130924224853516 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '平静', '汽车内', '多人情景剧', '惊奇', '喜悦', '动态', '单人口播', '愤怒', '拉近', '手机电脑录屏', '悲伤', '特写', '办公室', '配音', '极端特写', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.90', '0.71', '0.52', '0.37', '0.26', '0.20', '0.19', '0.12', '0.11', '0.10', '0.03', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/06feb2b30337870e4aa52534d7309a38.mp4\n",
      "{\"video_ocr\": \"我要出差两天才回来|你要自己照顾好自己|回来不能让我看到你瘦了|拜拜~|哇~谢谢宝贝|怎么买了这么多啊是不是很贵呀|这么快送到了呀|不贵呢|京东超市周年床额券|满1q9100荒味|满199云减100元 大神牌|你有什么需要的跟我说|好~|三只松鼠坚果炒|百草味 肉干肉脯 白芝麻猪肉脯1...|爆款直降 箭牌|三只松鼠办公室休|小吃经典长沙特... 21.80|益达木糖醇无糖 口香糖大瓶装...|百草味 香辣土豆 片210g开袋即...|158g休闲零食...|品铺子高端零|三只松鼠|饼干蛋糕 糖果巧弓|新品三只松鼠_爆 俏香阁 休闲零食|坚果蜜饯|三只松鼠休闲零食|芝肉弹熏煮香肠. 小吃 干脆面 日...|果零食干果坚... 三只|益达|可草味|休闲零食|158g休闲|百草味 原味红薯 ￥12.90|3/8 品类精选|餐口袋|坚果礼|美|条50g|百草味 零食特产|鸭脖子170g/袋...|百草味网红休闲|09C|￥159|热门品牌|货孕妇坚果干... 食综合果仁 每...|肉干熟食|零食榛子味一...|—…|为你推荐|优惠不止5折|¥16 .80|潮礼抢先购|京东超市 13|拉面丸子|薯脆|中秋荟聚 因味团圆|良品铺子|WRIGLEY|货孕|口香|我想吃零食|京东\", \"video_asr\": \"我要出差两天才回来，你要自己照顾好自己回来不能让我看到你瘦了哦，拜拜。|像吃零食哇，谢谢宝贝的，买了这么多啊，是不是很贵呀，这么快送到了呀，不贵呢。|京东超市周年庆大鹅券满一百九十九减一百，有什么需要的跟我说好。\"}\n",
      "multi-modal tagging model forward cost time: 0.016396522521972656 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '配音', '平静', '场景-其他', '喜悦', '手机电脑录屏', '特写', '室内', '动态', '惊奇', '悲伤', '拉近', '夫妻&恋人&相亲', '亲子', '极端特写', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.90', '0.87', '0.87', '0.30', '0.18', '0.06', '0.06', '0.05', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/070fd7493f1e28a22e7de73b2a4ed979.mp4\n",
      "{\"video_ocr\": \"这里是快手极速版|我想玩说唱|给你耳朵来一棒|最近很多老铁都在网上唠嗑儿|快手极速版最近怎么如此的火|我说那是当然|领红包的软件|谁不眼馋|只可惜听我介绍的人|不以为然|很多人都质疑我|说提现是骗局|三毛六分就提现你的担心真是多余|不信你现在下载随便刷刷视频|领红包提现微信你看行不行|别着急它的福利我还没说全|现在邀请新人加入|就得三十六元|这么好的软件我不信你不动心|-EO|婉吹\", \"video_asr\": \"哟哟哟，这里是快手极速版，我想玩说唱给你耳朵来一半。最近很多老铁都在网上唠嗑，快手极速版最近怎么如此？在和我说那是当然，领红包的软件谁不眼馋？只可惜经过介绍的人不以为的，很多人的质疑，我说提现都是骗局，三毛六就提现，你的担心真是多余，不信你现在下的随便刷刷视频。|领红包提现微信，你看行不行？别着急，他的福利我还没说全，现在邀请新人加入就得三十六元，这么好的软件我不信你不动心。|ENS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016630172729492188 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '全景', '单人口播', '平静', '配音', '静态', '动态', '室外', '室内', '多人情景剧', '商品展示', '夫妻&恋人&相亲', '特写', '愤怒', '场景-其他', '拉近', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.91', '0.50', '0.31', '0.12', '0.07', '0.04', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0710422e267d09f7bae02cef0c80c963.mp4\n",
      "{\"video_ocr\": \"师傅 就到这里吧|好的|师傅|麻烦你帮我取一下行李|没有问题|即刻天气温馨提示|10分钟后有雨|美女 你的包|谢谢师傅|没事 应该的|等一下|这天再过10分钟|就下雨了|这伞你拿着吧|这天好好的|怎么会下雨呢 不用了|说了你还不信|来看我这即刻天气|即刻天气|一款分钟级的天气预报|可随时查看天气情况|精确到几点几分下雨|还有语音播报提醒|免费下载使用|别淋了雨 感冒了|谢谢|这即刻夫气也太准了|让我也下载一个|你也点击视频下方链接|未来24小时天气|09:00|12: 风力|严重|儿童、老年人及心脏病、呼吸疾病患|能见度 东北风|西北风2级 湿度99%紫外线 无|中雨转小|5° -1°|300|实况天气10:22发布 很冷，需穿厚外套厚裤子保暖以免感冒|雨 紫外线|40|轻度污染|者避免长时间、高强度户外运动|气压 10300.0m|3级|小雨>|x07:43 出17:36|中度|进沙国|空气良|12/17 明天|≡ 西安|150|日出07:10 日落17:05|89|中国发布值|空气湿度 体感温度|现在|15天预报|948 hpa|9·|西北风|西南风西北风西北风 1-3级 1-3级 1-3级1-3级|西北风西南风西北风西北风东北风西南风西北风|西南风西北风西北风|东北风|西南风西北风西南风西北风西北风东北风西南风|5天空气质量|75|日落18:45 日出07:05|5.80|99%|93空气 71|健康|164二氧化硫 SO2|雷阵雨|1-3级 1-3级 1-3级 1-3级 0-2级0-2级 1-3级 1-3级 1-3级 1-3级 0-2级 0-2级 1-3级 1-3级 1-3级 1-3级 0-2级 0-2级|CO|雾霾 多云|多云 雾霾 雷阵雨 冰雹|一氧化碳 臭氧|细颗粒物|91空气 69|今天 明天 后天 周二|周三 周四 今天 明天 后天 周二 周三 周四  今天 明天|周二|后天 周二|164|三西安|PM10|多云 雾霾|今天 明天|“60|周昨天|小雨小雨|7~10°|112|良轻轻度|包 良良轻 轻度|BH|BYD|PM2.5|J.|O3\", \"video_asr\": \"师父就到这里吧好的。|师傅，麻烦你帮我取一下行李，没有问题。|即刻天气温馨提示，十分钟后有雨。|没事。|美女你的包谢谢啊，没事硬硬的等一下。|今天啊，再过十分钟就下雨了，这伞你拿着，天好好的怎么会下雨呢？|说了你还不信。|来看我这即刻天气。|即刻天气，一款分钟级的天气预报，可随时查看天气情况，精确到几点几分下雨，还有语音播报提醒，免费下载使用。|这伞你就拿着吧，别淋了雨感冒啊。|这即刻天气也太准了吧，让我也下载一个吧。|恋点击视频下方链接，免费下载一个吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016646862030029297 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '动态', '全景', '室外', '惊奇', '平静', '喜悦', '极端特写', '手机电脑录屏', '悲伤', '路人', '特写', '朋友&同事(平级)', '夫妻&恋人&相亲', '拉近', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.86', '0.85', '0.82', '0.73', '0.43', '0.22', '0.15', '0.14', '0.07', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07109009e159e9dee7ed0f146879652c.mp4\n",
      "{\"video_ocr\": \"啊啊终于抢到啦|喂媳妇儿|我给儿子抢的|斑马A课思维体验课|送的教辅大礼盒到了|你赶紧去拿一下啊|诶好|大家都在抢什么呢|我们去看看吧|是猿辅导在线教育|推出的斑马A课|思维体验课呀|49元就能买10节课|都是由10年以上经验|专门研究孩子|数学启蒙的|教研团队打造的|专为3-6岁孩子|定制的思维启蒙课|课程啊都是采用|动画儿歌和游戏|互动的形式教学|孩子呀坐的住|也学的进去|根据孩子年龄|分级教学|真的是什么基础都能学|课程呢还支持|无限次的回放|直到孩子学会为止|课后还有辅导老师|一对一的答疑|系统培养孩子|6大知识模块|9大思维能力|16种思维方法|再也不用担心|孩子数学计算问题了|10节课只要49元|还免费包邮送教辅礼盒|这么好的课程|家长们千万不要错过|赶紧点击视频下方链接|查看详情吧|斑马A课|课极有励 孩子效|3-6岁幼小数学必备 49|2-8岁上斑马学思维 学数学|玟1马A！课|猿辅导在线教育 出品|猿辅导|选择宝贝年龄\", \"video_asr\": \"终于抢到了，哎，媳妇，我给儿子抢的斑马AI课，思维体验课送的教辅大礼盒到了，你赶紧去拿一下，哎，好，大家都在抢什么呢？我们去看看吧。|哎呀，是猿辅导在线教育推出的斑马AI课思维体验课呀，四十九块钱就能买十节课，都是有十年以上经验，专门研究孩子数学启蒙的教研团队打造的专为三到六岁孩子定制的思维启蒙课课程啊，都是采用动画，儿歌和游戏。|冻的形式教学，孩子啊，坐得住也学得进去，根据孩子年龄分级教学，真的是什么基础都能学，课程还支持无限次的回放，直到孩子学会为止。课后啊还有辅导老师一对一的答疑，系统的培养孩子六大知识模块，九大思维能力，十六种思维方法，再也不用担心孩子数学计算问题了，十节课只要四十九元，还免费包邮送！|教辅礼盒这么好的课程，家长们千万不要错过，赶紧点击视频下方链接查看详情吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016690492630004883 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '多人情景剧', '平静', '室外', '静态', '喜悦', '亲子', '特写', '动态', '朋友&同事(平级)', '全景', '路人', '家庭伦理', '手机电脑录屏', '单人口播', '惊奇', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.89', '0.66', '0.37', '0.18', '0.05', '0.04', '0.04', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/071b5b6164642fc03e2e04b4e0fc17a7.mp4\n",
      "{\"video_ocr\": \"找不到工作|整天就知道玩电脑|姐我找到工作啦|配音演员呢|可是你哪比得上|ペ家播音专业|我的坐骑可是纯天然无污染|我现在就打她回来煲鱼头汤|天啊你什么时候|变得这么厉害的|就是这个潭州教育播音课|播音竞 就是这个|专业老师传授控制发音技巧|特别耐心|而且|课后还有助教跟踪辅导答疑|只要不会随时问|教学效果特别好|不错不错|在哪里可以报名|点击下方链接就可以|报名潭州育挢音课啦|潭州教育|丽语/福影/主特/最名 每魔8点 在线教学|朋请/摄音/主特/配音|填写信赢领取价值(99元诚程+海量学习科 您想学习楼音主持的日的是?|您的车时是? (用子分班》|零基础学福音主持|您的年补是? (用于分细)|你的手机 《已超级加密）|新视旁白|填写信息领取价值699元|立期领取学习资科+试听课|TANZHOUEDU.COM|每清/播8/主持/配备|电台主播|道州教|漫|海量学习资料|招募令\", \"video_asr\": \"找不到工作，整天就知道玩电脑，只要找到工作了拍演员呢，可是你哪比得上人家播音专业啊，我的坐骑可是纯天然无染哦，现在就打她回来煲鱼头烫片，那你什么时候变得这么厉害呀？就是这个弹着教育播音课啊，专业老师传授控制发音技巧，特别耐心，而且课后。|有助教跟踪辅导答疑，只要不会随时问，教学效果特别好，不错不错，在哪里可以报名呀？点击下方链接就可以报名盘州教育播音课咯！\"}\n",
      "multi-modal tagging model forward cost time: 0.016280174255371094 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '平静', '单人口播', '特写', '手机电脑录屏', '动态', '配音', '喜悦', '多人情景剧', '家', '朋友&同事(平级)', '室内', '情景演绎', '办公室', '极端特写', '室外', '场景-其他'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.87', '0.81', '0.53', '0.48', '0.42', '0.23', '0.07', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/071d0d68ae11105a8dfb668bfdf6a981.mp4\n",
      "{\"video_ocr\": \"妈妈妈妈|快给我打开英语视频课|好好好别着急|给|你儿子怎么这么好学|能主动学英语|你可以试试牛津树双语视频课呀|中英双语讲解|我家孩千看7一玖就停不下来了|现在1元解锁课程|我就给他报名了|每天他就像看动画片一样的学英语|可感兴趣了|这么好的课程我孩子也能报名吗|当然可以7了它适合3-6岁的宝宝|关注“小早英语”公众号|赶紧给你家孩子报名吧|3-6岁儿童启像课|视频为演绎情节|视绎情节|CP|情节|小早启蒙\", \"video_asr\": \"妈妈会给我打开一视频课，好好好，别着急，你儿子怎么这么好学，能主动学英语，你可以试试年轻树双语视频课呀，天津商业展现在家孩子呀，看了一次就停不下来了，现在医院接受课程，我呀就给他报名了，每天啊，我就像看动画片一样的学习英语。|可感兴趣了，这么好的课程，我孩子也能报名吗？当然可以了，它适合三到六岁的宝宝，关注小小英语公众号，赶紧给你家孩子报名吧！|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.016216754913330078 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '平静', '家', '亲子', '填充', '特写', '朋友&同事(平级)', '动态', '喜悦', '家庭伦理', '室内', '惊奇', '夫妻&恋人&相亲', '极端特写', '单人口播', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.94', '0.85', '0.65', '0.51', '0.41', '0.05', '0.04', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0732dd11a2400fdeda96a66452511e74.mp4\n",
      "{\"video_ocr\": \"妈妈|你看|这是9宫格的数独棋|每一行每一列|都没有相同的数字|这是16宫格的|我也拼好了|我只花了5分钟 就拼完了|我是不是很厉害呀|因为我上了|斑马AI课思维体验课|我家孩子上了|一段时间后|一|就爱上了数学|现在不光会100以内 的加减法|说话做事情也 越来越有条理了|没想到10节课只要49元|还是猿辅导在线教育出品|0基础的孩子也能学|课程支持无限次回放|不怕孩子学不会|你也赶紧点击下方详情|报名吧|还包邮赠送 精美教辅大礼盒呢|斑马A课|思维|2-8岁上斑马 学思维 学英语|斑马\", \"video_asr\": \"妈妈妈妈，你看这是九宫格的速度题，每一行每一列都没有相同个数中，这是十六万多的，我也片。|这是三十六红疙瘩，我只花了五分钟人听完了，我是不是很厉害呀，因为我上单打AP。|体验课，我家孩子啊，上了斑马AI课思维体验课一段时间后啊就爱上了数学，现在啊，不光会一百以内的加减法，说话做事啊也越来越有条理。|没想到十节课只要四十九元，还是原辅导在线教育出品，零基础的孩子也能学课程，这只无限次回放，不怕孩子学不会你也赶紧！|其相貌详情报名吧，还包邮赠送精美大礼盒呢！\"}\n",
      "multi-modal tagging model forward cost time: 0.01908731460571289 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '家', '静态', '亲子', '多人情景剧', '家庭伦理', '平静', '单人口播', '教辅材料', '喜悦', '特写', '动态', '配音', '手机电脑录屏', '学校', '场景-其他', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.96', '0.81', '0.71', '0.21', '0.10', '0.06', '0.05', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/073fc337140af2c9b4a42685a2e3ce8c.mp4\n",
      "{\"video_ocr\": \"这种人是怎么混进来的|不知道我们公司|最低要求是本科学历吗|不就是个本科学历嘛|有什么了不起|这就是我两年前|面试的窘境|在能力与理想|相匹配之前|所有舒适都是绊脚石|啊|于是我做了详细的|时间规划|加入了1元本科特训课|无需到校考勤|一个手机就能学|现在|我实现了弯道超车|把生活过成|别人羡慕的模样|别再抱怨生活的同时|又无动于衷了|开始改变吧|踏出舒适圈|上班族在职本科特训营 一套全|视频内容为剧情演绎|查看课程内容|1638000|内训资料讲义|演绎|本科挑战月薪过万|国际上市在线教育品牌|高等教育在职本科 高等教育本科学历|20715|算一笔账|尚德机构|名师答疑|互动课堂|简历|￥1\", \"video_asr\": \"这种人是怎么混进来的？不知道我们公司最低要求是本科学历吗？不就是一个本科学历有什么了不起的？这就是我两年前面试的窘境。|在能力与理想相匹配之前，所有舒适都是绊脚石。|于是我做了详细的时间规划，加入了一元本科特训课，无需到校考勤，一个手机就能学。现在我实现了弯道超车，把生活过成了别人羡慕的模样，别再抱怨，生活的同时就无动于衷了，开始改变吧，踏出舒适圈，从一元本科特训课开始。\"}\n",
      "multi-modal tagging model forward cost time: 0.01638007164001465 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '极端特写', '平静', '手机电脑录屏', '动态', '配音', '特写', '喜悦', '单人口播', '拉近', '多人情景剧', '办公室', '情景演绎', '室内', '愤怒', '全景', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.96', '0.91', '0.67', '0.65', '0.52', '0.29', '0.28', '0.24', '0.13', '0.03', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/073fdf49a1e6bce097745df0061378f7.mp4\n",
      "{\"video_ocr\": \"您这边一共消费|5200元|请问 谁买单|扫我安逸花|哎.姐|这我能用吗|当然|安逸花啊|有最高额度20万|最快1分钟放款|而且最长还能分|12期还款呢|15万|大姐|我怎么申请啊|你啊|点击下方链接|就能在线申请|你的额度了|马上消费|花|安逸|女逸花|立匹花|与费|马上 额度140000元。|马」淆贾|马满哥|pARES|利急低 选捧安逸花的六个理由|【安逸花】消费5200元，剩余|安选花申请|贷款额度、放款时间等以实际审批为准 贷款有风险，借款需谨慎，请根据个人能力合理贷款|安值捉|超高链置(元)|16:36|0 C|O CA|200，000|现在|女巴化|额度高\", \"video_asr\": \"我不做大哥。|您这边一共消费五千二百元，请问谁买单的安逸花消费五千二百元，剩余额度十四万元。姐，这我们用吗？当然安逸花。|最高额度二十万，最快一分钟放款，而且最长还能分十二期还款呢，十五万。而且我怎么申请呢？你呀，点击下方链接就能在线申请你的额度啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016674280166625977 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '平静', '动态', '喜悦', '路人', '单人口播', '惊奇', '特写', '夫妻&恋人&相亲', '室外', '手机电脑录屏', '全景', '填充', '朋友&同事(平级)', '愤怒', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.90', '0.77', '0.70', '0.47', '0.17', '0.12', '0.04', '0.04', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0744231775210a1f1a55dc54a4a3238e.mp4\n",
      "{\"video_ocr\": \"不要再让孩子背|高中英语3500词了!|不要再整天让孩子刷题做卷子了!|我是高途课堂闫铭老师|跟我学|单词不用全背会|你只需要背铭哥的|就足够|英语阅读也不用全读完|任何题目三步搞定|只需要9元|给孩子一个成为学霸的机会!|查看详情抓紧报名|立即报名|2.0词|只需9元|省高考状元带队授课 老师平均教龄11年+|834高频单词|阅读三板斧|高途课堂名师特训班 新学员9元专享|授课风格幽默|年教学经验 原省级重点中学教师|名师出高徒.网课选高途|高途课堂|随身速记 2019|随身|3500词 高中英语|19 规范解析|试题 调研|闫铭|、\", \"video_asr\": \"不要再让孩子背高中英语三千五百词了，不要再整天让孩子刷题做卷子了，我是高途课堂严明老师跟我学。|不用全背会，你只需要背明哥的八十四高频单词就足够，英语阅读也不用全读完，你只需要掌握明哥的阅读三板斧。|任何题目三步搞定，只需要九元，给孩子一个成为学霸的机会，查看详情抓紧报名！\"}\n",
      "multi-modal tagging model forward cost time: 0.02254962921142578 sec\n",
      "{'result': [{'labels': ['推广页', '单人口播', '现代', '教师(教授)', '中景', '静态', '室内', '配音', '平静', '场景-其他', '特写', '教辅材料', '影棚幕布', '学校', '知识讲解', '填充', '情景演绎', '动态', '拉近', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.63', '0.30', '0.16', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0748d519d66f95fc46f32dd88fd42f9d.mp4\n",
      "{\"video_ocr\": \"你想不想拥有这样动听的声线 迷人的嗓音|摆脱聚会KTV不会唱歌的尴尬|现在点击视频下方链接|只需9.9元就可以成功报名潭州声乐课|0基础也可以学哦|视频内容为演绎情节\", \"video_asr\": \"我喜欢这样跟着你，随便你带我到哪里。|慢慢慢慢慢慢，你想不想拥有这样动听的声线，迷人的嗓音，摆脱聚会KTV不会唱歌的尴尬？现在点击视频下方链接，只需九点九元就可以成功报名谭忠声乐课，零基础也可以学哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016483545303344727 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '动态', '平静', '推广页', '特写', '多人情景剧', '单人口播', '喜悦', '家', '手机电脑录屏', '家庭伦理', '室内', '办公室', '愤怒', '极端特写', '情景演绎', '惊奇', '咖啡厅'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.67', '0.51', '0.47', '0.45', '0.27', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07518005aa1eded08d92f58a0de726c0.mp4\n",
      "{\"video_ocr\": \"回想杷这口红颜色 纹|纹在嘴唇上|新款细限回吓|新款小细跟口红|细管回红 每一步都很出色|丁2立减3|二室立减30|PERFECT DIARY 完美日记|会首圆园|完重国记2|限战抢日|点击 查看\", \"video_asr\": \"真想把这口红颜色纹在嘴唇上，完美日记新款小希跟口红姐妹们拼吗？\"}\n",
      "multi-modal tagging model forward cost time: 0.0164797306060791 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '平静', '手机电脑录屏', '中景', '配音', '极端特写', '填充', '静态', '场景-其他', '单人口播', '特写', '动态', '喜悦', '全景', '混剪', '室内', '情景演绎', '室外', '重点圈画'], 'scores': ['1.00', '1.00', '0.45', '0.44', '0.42', '0.39', '0.38', '0.30', '0.25', '0.19', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0752cffd24ba744674149a6599355986.mp4\n",
      "{\"video_ocr\": \"给你报还不行吗?|不就9块钱吗?|我们班好多同学|跟着高途课堂的名师|顶尽三松战|极会热城|水限专热战|高途课堂 名师出高徒.网课选高途\", \"video_asr\": \"给你报还不行吗？不就九块钱吗？我们班好多同学跟着高中课堂，名师从高。\"}\n",
      "multi-modal tagging model forward cost time: 0.016573429107666016 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '手机电脑录屏', '单人口播', '多人情景剧', '配音', '特写', '极端特写', '喜悦', '动态', '惊奇', '室外', '平静', '愤怒', '全景', '填充', '亲子', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.93', '0.82', '0.80', '0.61', '0.44', '0.32', '0.30', '0.18', '0.08', '0.04', '0.04', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/075c2927db605623001b925e214f4ca5.mp4\n",
      "{\"video_ocr\": \"林阳|你怎么在这|是不是奶奶|让你来当替罪羊的|奶奶真是太无情了|小颜|我可以帮你|帮我|就是因为嫁给你这个没用的人|我才当了替罪羊|你们苏家害的我爷爷生死未卜|居然还敢来|我可以|治好老爷子的病|林阳你胡说什么|我都到门口了|为什么不让我进去试试呢|爷爷|难道这就是传说中的一线神针|林阳你到底是谁|《超级赘婿》 立即追书热门小说|《超级赘骄》之一线神针|下载小说阅读吧 精彩不断|本故事纯属虚构|阅读吧\", \"video_asr\": \"林洋你怎么在这，是不是奶奶让你来当替罪羊的？|唉，奶奶真是太无情了。|秋燕。|我可以帮你帮，我就是因为嫁给你这个没用的人，我才当了替罪羊。|你们苏家害得我爷爷生死未卜，居然还敢来。|我可以治好老爷子的病，羚羊你胡说什么啊，我都到门口了，为什么不让我进你是什么？|等等等等。|嗯，爷爷。|难道这就是传说中的一见全身？|你到底是谁？|我。\"}\n",
      "multi-modal tagging model forward cost time: 0.016581296920776367 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '特写', '家', '静态', '动态', '夫妻&恋人&相亲', '愤怒', '平静', '亲子', '惊奇', '家庭伦理', '极端特写', '悲伤', '厌恶', '拉近', '配音', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.94', '0.87', '0.83', '0.70', '0.58', '0.14', '0.08', '0.08', '0.06', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/075cf866a923a92962d89b12ad126b08.mp4\n",
      "{\"video_ocr\": \"高一” 到底有多重要呢|就好像是修建一座高塔|地基夯得多深多牢|直接决定了塔能建多高|高一学得好|基础打得牢|高二高写就能迎头直上|高途课堂 高中全科名师班|9元16节 精讲直播互动课|由北大清华毕业 名师带队教学|紧扣考纲系统学习|语数英物4科全面提升|名师总结出的 4种高效解题方法|帮孩子在高中起步阶段|就掌握实用答题技巧|课程还包括 7大数学思维专题训练|10篇英语阅读高频考题|和4类语法经典题型讲解|孩子不偏科 人生选择才更多|屏幕前的各位家长|快点击视频下方链接 查看详情|为您的孩子报名吧|全国百佳教师带队教学 平均教龄11年 新用户专享|10年 17天 主讲老师每堂课精心|录取率不足2%|最新定制课程，紧扣考纲系统学习m|一全程伴学，提升学习效果|上课提醒|厦浙江卫视|高途课堂|云浙江卫视 中资深清北师教学，快速全面提升|2|90%|3轮严格第选师资|备课17天 平均教龄10年以上|北京大学|预习资料|8年教|学习报告展示|史心语|立即体验 浙江卫视指定在线教育品牌|高途课堂科特训营|主讲|学情沟通|3年医放|口华少|名师特训班|¥9|课后|秒杀课|仅需\", \"video_asr\": \"高一到底有多重要呢？就好像是修建一座高塔，低级亢的多深多牢，直接决定了塔能建多高，高一学得好，基础打得牢，高二高三就能够迎头直上高途课堂，高中全科名师班九元十六节精讲直播互动课，有北大清华毕业名师带队教学，紧扣考纲，系统学习。|语数，英物四科全面提升，名师总结出的四种高效解题方法，帮孩子在高中起步阶段就掌握实用答题技巧。|课程还包括七大数学思维专题训练，十篇英语阅读高频考题和四类语法经典题型讲解，孩子不偏科，人生选择才更多。|屏幕前的各位家长，快点击视频下方链接查看详情，为您的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01625990867614746 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '单人口播', '静态', '平静', '场景-其他', '配音', '特写', '室外', '手机电脑录屏', '室内', '转场', '动态', '教师(教授)', '极端特写', '多人情景剧', '重点圈画', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.70', '0.69', '0.16', '0.16', '0.14', '0.07', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/075eee994f173fdc9e6a56eb79f42e18.mp4\n",
      "{\"video_ocr\": \"你不能坐在着|啊这有人呀|你穿着地摊货挨着我脏不脏呀|不会呀刚洗的怎么会脏呢|算了|同学们|明天早上我要签约紫玉|就不陪你们了|我送你吧|你们怎么什么人都招|不好意思 不好意思|我这就开除他|你有这权力吗|喂 小姨你下来一趟|怎么了|你们刚刚招聘的保安|弄花了我的车|还耍赖|我不管你是谁招聘来的|我以副经理的身份通知你可以走了|是吗|对不起总裁路上堵车了|韩秘书你是不是弄错了|这是岳家族长亲自任命的新任总裁岳风|风….|“ …..|差辈了|我跟李沁是同学|你是她小姨|承受不起呀|欲知后事请|豫A7LI7E\", \"video_asr\": \"不能说来着。|你穿地摊货挨着我。|对呀，刚洗的。|算了，同志们，明天早上我要签约，紫玉就陪你们了。|我送你吧。|你们怎么什么人都招啊，我也是我也是。|这就开除她，你有这权利吗？|怎么了。|你们刚刚招聘的保安都翻了五车。|这耍赖，我不管你是谁招聘来的，我以副经理的身份通知你可以走了。|师父。|就是总裁要堵车了，我就知道错了。|这是岳家族长亲自任命的新任总裁曲越峰。|卓查分了给你信息。|实在想也承受不起。\"}\n",
      "multi-modal tagging model forward cost time: 0.02235555648803711 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '动态', '推广页', '静态', '室外', '喜悦', '特写', '全景', '惊奇', '夫妻&恋人&相亲', '路人', '悲伤', '平静', '亲戚(亲情)', '极端特写', '愤怒', '厌恶', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.98', '0.94', '0.51', '0.39', '0.34', '0.24', '0.21', '0.05', '0.05', '0.04', '0.02', '0.01', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0764c067cddfe6841e224e2b1210dd37.mp4\n",
      "{\"video_ocr\": \"\", \"video_asr\": \"十一个合同哪能那么便宜。|啊。|给脸不要脸。|滚。|老婆走了。|要不是你贸然答应我怎么会。|你们没一个好东西。|敢惹我老婆。|找死跌下给我毁了剑打起来是。|喂，你到底得罪了什么人什么。|快。|在不在。|哇。|好。|叶女士，你要干什么？我老公在这求您高抬贵手，饶过我吧。|那十一的活动没钱。|并且让利百分之五十。|这。\"}\n",
      "multi-modal tagging model forward cost time: 0.021778106689453125 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '动态', '愤怒', '静态', '特写', '平静', '拉近', '夫妻&恋人&相亲', '极端特写', '室外', '惊奇', '全景', '悲伤', '单人口播', '工作职场', '上下级', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.86', '0.76', '0.60', '0.46', '0.13', '0.09', '0.04', '0.03', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0765bf893f8ff09d5e9fe61483786abd.mp4\n",
      "{\"video_ocr\": \"刷宝|一款刷视频 就能赚钱的视频软件|咔|演技呢|懂不懂什么叫演技|你先体验一下什么叫刷宝吧|大姐|现在能拍了吗|没空|我现在在刷宝上刷元宝呢|刷宝APP|刷视频就能赚元宝|扫除x2|吹散|令天去吃火锅 元宝可以自动兑换成现金|我就点7个微辣|刷宝主播|每天签到就能得元宝|现在邀请好友|还有37块6的奖励哦|记住还可以 直接提现到微信哦|邀请好友赚37. 6元|看直播参|完善身份信息|当前元宝(个)|新人礼包 余额流水|微信 Little fairy|@牡可与萌豆|@司二缸司|16万|立即提现至微信 去邀请|去完成 10秒筒单问巻，即可轻松赚500元|+0.38|点击登录> 领更多福利哦~|专属任务|Littlefairy0412|我的钱包|提现详情|你今天还未邀请好友哦|刷宝抽大奖|查看更多>|看到底3，去赚钱吧|微信我-ベ包·零ベ-零钱明细|2天|チ再领取有机会得更多哦~|每日签到，轻松赚元宝|邈请个好友 成功邀请有个好友，可额外获得1元奖|提现申请已提交 预计到账时间:08月28日15:25|立即签到|娘写个性签名更客易获得别人的关注哦 0获赞|收益说明|716|232|超精美礼品100%中奖|支付宝-我的-余额-明细|你笑起来好看 省页|学方吉的重要性|看视频|聚宝盆|@爱大笑的Sun 关注|@逗哥新靚|看视频ぇ宝，看得越タ赚得越②哦|展开|我知道|欢迎来刷宝，送您一份新人奖励！搓搓|汇率受广告收益及您的活值影响|百变风格 等你来看|分享你的第一个刷宝视频|理财金余额:0.00e|1618|去领取|元宝互动 提活值>|宝!|2019-08-2715:23:05|到账查询|+288|粉丝|客服小姐姐会加紧处理的，请耐心等特哟~|外婆|你什么时候来吖?|垫精看个智卖|开箱领元宝|去抽奖|拍摄视频|+58|去看看|猜大小|999|任务\", \"video_asr\": \"刷宝，一款刷视频就能赚钱的视频软件，演技呢都懂什么叫演技？|你先结一下什么叫双龙吧。|小姐现在能拍了吗？没空，我现在在刷宝上刷元宝呢。|刷宝APP刷视频就能赚元宝，元宝可以自动兑换成现金，每天签到就能得元宝，现在邀请好友还有三十七块六的奖励哦！|记住，还可以直接提现到微信哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016646623611450195 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '手机电脑录屏', '单人口播', '喜悦', '多人情景剧', '平静', '家', '配音', '室外', '惊奇', '路人', '愤怒', '拉近', '室内', '全景', '办公室', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.32', '0.15', '0.08', '0.03', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07714feca99539b3934a9eaed1a24f04.mp4\n",
      "{\"video_ocr\": \"妈妈|你火了|我怎么火啦|现在我们班小朋友|家长都想问你|怎么报名 斑马Al课|思维动画课|对呀 自从你给我|就可以报名啦|孩子设计的|?|19+5   3+19|19+5 3+19|每天十五分钟 短时高频教学|轻轻松松让孩子 掌握|逻辑推理观察 等能力|49元10节体验课|包邮赠送 超豪华教具礼盒|赶紧点击视频下方 查看详情报名吧|报名了|我现在100以内 加减法|都能快速得出答案|他们都想学呢|我当是怎么回事呢|你去告诉他们|是专门为3-6岁|课程通过 动画 儿歌等|孩子喜欢的形式 教学|t马AL|猿辅导在技教算 说|狼辅导 城马A|猿辅导在线技期 |获线视育|你浦导|限涌导|猿辅帚|招证导在线权间|摆制导在线教购|猿辅导在红教黄 出a|同衣线机|2-8岁上斑马 学思维 学英语|猿辅导在线教育出品|马A1|fX士 与A保|1捕导|ム△?|sS2|H72|思维\", \"video_asr\": \"妈妈妈妈。|你火了，我怎么火啦？现在我们班小朋友家长都想问，你怎么报名斑马AI艾克思维动画课呢？妈妈让克斯动画课，对呀，自从你给我报名了斑马埃克思维动画课，我现在一百以内，加减法都能。|快速得出答案。他们都想学呢？我当是怎么回事呢？你去告诉他们，现在点击视频下方就可以帮忙吗？是斑马AI课。思维动画课是专门为三到六岁孩子设计的课程，通过动画，儿歌等孩子喜欢的形式教学，每天十五分钟短时高频教学。|轻轻松松让孩子掌握逻辑，推理，观察等能力，现在报名四十九元十节体验课，还包邮赠送超豪华教具礼盒！|赶紧点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016408205032348633 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '单人口播', '静态', '平静', '场景-其他', '教辅材料', '配音', '动态', '家', '亲子', '喜悦', '多人情景剧', '特写', '动画', '极端特写', '全景', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.94', '0.93', '0.83', '0.71', '0.61', '0.56', '0.23', '0.11', '0.08', '0.07', '0.03', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07732170d872d045c7d82728f15d387c.mp4\n",
      "{\"video_ocr\": \"高途课堂 高中全科名师班|由省高考状元|北大清华毕业|全国百佳教师|带队教学|平均教龄11年|在上千万道考题中 taml|t总结出高中数学|168个常考点|语文3大作文|印高分黄金法则|英语阅读理解|十字口诀|物理十二通解模型|教孩子用考试型思维|把高中重难点知识|一一拆解|拆解|tam\\\\把握住高中|把握住高中|(就是把握住人生|最重要的方向|9元16节名师直播课|点击视频下方链接|就能报名|快给孩子一个|逆袭的机会|满心欢喜的|有的孩子|考上自己 理想的大学|被迫选了自己|毫不喜欢的专业|匆匆忙忙 进了一所名叫|“社会” 的大学|如果不想让|自己的孩子后悔|那作为父母的我们|更应该为孩子 选择一个|正确的老师|直|角公六 -l-2im|闸公戒|-F2sm|f0n0Y2 aXI-|Y2|00XY2|o9xY2|sY|23×Y2|tonke(H0 tinld|tke(H|sVar欢|(I0|:larad|My|(I40d|(Hm0|slarosd tonu-:(HoO|tonNM:(Hd|s/y|tanv atox= Smd,ceco inAuA sinA'|inAHvnA|InAwaA|imnHoasA|simf|mnAonA|rinAcasA|浙江卫视inA以uA|2HZSmHaA|7HmAA|mHuaA|imAvaA|inf|7H=7SmAaA A以什si|>H=TmAoaA|2A=mHoaA|iA|inHcaA|A什s m计H· (at |m2A: (at|A你s什sin|m冲H·(t|-I-2m m训:(at|m计H;(al|m训A·(l|m2A·lt|花閏|高考后|生 2cosA|A)/lI-ton|作 2casH'|生2caH|作 2c以A|牛 2cas|A/ll-tch|住 2coH|生 20cH|‘2ccsA|1)/lF-ta|浙红卫视|6)|in6)|m5|bi5)|Ei5|属于哪一类|“钮PB训|仅需|降军太|降星忒|所忒|名师特训班|ta|全国百佳教师带队教学 平均教龄11年|ue(nosn|ws u-(nosa|VUsit|￥Vrsh|Wefsi|Wfih|新用户专享立即体验 浙江卫视指定在线教育品牌|tan0 aox=Sndl c4cox|tmn0 atox= Sno ceox|tan0 otx=Snod,ccoo|tmnv Wiox= Smd. ca|tamo Wto= S7md ccox|tano Otio= Sno. csco|tmnu ao= smd.cco|tao ao= Smo.ceco|tan ato=Smd. csc0|tmno Otx=Sd. ceOo|tamni Wiox= Siad,cco|axl(|ap|i(o|Xax|axC|XI+|hmin(c oaxa|tmin(oe|idt|bmnD)|求值值|华少|¥9\", \"video_asr\": \"高考后，你的孩子属于哪一类？有的孩子满心欢喜地考上自己理想的大学，有的孩子被迫选了自己毫不喜欢的专业，还有的孩子匆匆忙忙进了一所名叫社会的大学。如果不想让自己的孩子后悔，但作为父母的我们更应该为孩子选择一个正确的老师。|高途课堂高中全科名师班，由省高考状元北大清华毕业，全国百佳教师带队教学，平均教龄十一年。|上千万道考题中，总结出高中数学一百六十八个常考点，语文三大作文高分黄金法则，英语阅读理解十字口诀，物理十二通解模型。|教孩子用考试形思维把高中重难点知识一一拆解，把握初高中就是把握住人生最重要的方向。|元十六节名师直播课，点击视频下方链接就能报名，快给孩子一个逆袭的机会！\"}\n",
      "multi-modal tagging model forward cost time: 0.016685962677001953 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '单人口播', '平静', '静态', '动态', '学校', '配音', '教师(教授)', '场景-其他', '极端特写', '室外', '远景', '室内', '影棚幕布', '混剪', '(马路边的)人行道', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.65', '0.61', '0.51', '0.14', '0.08', '0.06', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07734d3cc5adac0ef250e9b3b84827ec.mp4\n",
      "{\"video_ocr\": \"50岁以上的朋友注意了|赶紧把你戴的那些 劣质老花镜丢掉吧|传统的老花镜 只能看近不能看远|经常来来回回地摘戴 很不方便|戴久了眼睛又累又酸|试试这幅超视立 新型老花镜吧|我们这款 新型老花镜的镜片|近看远看都不耽误|久戴不累|清晰舒适不晕眼|不仅可以戴着它看书|看手机|开车走路也不耽误|造型大方男女通用|老花近视远视散光|只要一副就解决|点击视频下方链接|即可购买同款花镜了|щper Vision|眼花，就戴超视立|超视立 视光中心|S\", \"video_asr\": \"五十岁以上的朋友注意了，赶紧把你带的那些劣质老花镜丢掉吧！|传统的老花镜只能看近不能看远，经常来来回回的摘戴很不方便，戴久了眼睛又累又酸，是这副超视力新型老花镜吧！|这款新型老花镜的镜片，近看远看都不耽误，九代不累，清晰舒适，不晕，眼不紧，可以带着它看书。|看手机。|开车走路也不耽误。|造型大方。|男女通用，老花近视，远视散光，只要一附近解决，点击视频下方链接即可购买同款划进来。\"}\n",
      "multi-modal tagging model forward cost time: 0.016739845275878906 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '喜悦', '平静', '多人情景剧', '单人口播', '特写', '室内', '办公室', '手机电脑录屏', '家', '朋友&同事(平级)', '惊奇', '填充', '夫妻&恋人&相亲', '动态', '单人情景剧', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.93', '0.88', '0.87', '0.73', '0.18', '0.03', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/078e4200a35a79db4ba9d68f8c70d70a.mp4\n",
      "{\"video_ocr\": \"1招 让你的写人作文|脱颖而出|很多同学|在写人物作文的时候|首先就是从头到脚|把人物的外貌描写一遍|看看字数够不够|够了就停笔|不够就再夸一顿|字数不够再夸|夸到字数够为止|NO NO NO~|今天教你一个 写人的方法|特征法写人|例如 《我的大嗓门爸爸》|我的爸爸是个大嗓门|说起话来又快又响|我有时 很喜欢他的大嗓门|有时也会有一些害怕|这就是抓住人物身上|最具特点的东西|那么这个特点|就是构成你文章|最大的亮点|作业帮直播课|语文阅读写作提分班|帮助孩子全面提升|阅读写作能力|掌握写作方法|让孩子不再死记硬背|就能写出高分作文|20节课只要29元|还包邮赠送|12件套大礼包|屏幕前的家长们|还等什么|赶快点击 视频下方链接|报名吧|名师有大招 解题更高效|中国女排 00|20节重难点提分课 清华北大毕业名师带队教学|上课内容与收到礼盒请以实际为准|解团更离约|中国国家女子排球队官方教育品牌|全国包邮送教辅大礼包 立即报名|快速提升阅读写作能力|特惠|元|29\", \"video_asr\": \"一招让你的写人作文脱颖而出。很多同学在写人物作文的时候，首先就是从头到脚把人物的外貌描写一遍，看看字数够不够。|够了就停笔，不够就再夸一顿，字数不够再夸，夸到字数够为止。NO NO NO。今天教你一个写人的方法特征，把血人。例如我的大嗓门爸爸，我的爸爸是个大嗓门，说起话来又快又响，我有时很喜欢她的大嗓门，有时也会有一些害怕，这就是抓住人物。|身上最具特点的东西，那么这个特点就是构成你文章脱颖而出的最大亮点，所以帮直播课语文阅读写作提分班，帮助孩子全面提升阅读写作能力，掌握写作方法，让孩子不再死记硬背就能写出高分作文，二十节课只要二十九元，还包邮赠送十二件套大礼包，屏幕前的家长们还等什么，赶快点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01666259765625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '平静', '室外', '公园', '家庭伦理', '喜悦', '单人口播', '亲子', '动态', '教辅材料', '特写', '愤怒', '路人', '全景', '极端特写', '商品展示'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.95', '0.81', '0.66', '0.60', '0.54', '0.43', '0.22', '0.20', '0.19', '0.08', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0793a4a463794dd3397a08f0c5af0a7e.mp4\n",
      "{\"video_ocr\": \"就你还要什么爱情|那离婚的女人|可难嫁了|他们说|不配拥有爱情|像你这样的|在我众多相亲对象里边|也就是个中等水平|女人的30岁|是个尴尬的年纪|看到阿曼达一个人在回家|强势的女人|找不到合适的人|为什么要管别人怎么说|你们两个|明天不用来上班了|不管处在什么状况下的女人|都应该拥有权利追求爱情|来伊对|每个女人|都值得被爱|介绍给单身朋友|重9|福\", \"video_asr\": \"怎么都没看见呢。|哎呀。|就你还有什么爱情，结婚的女人不可难嫁了，今天他们说离过婚的女人不配拥有，爱情是需要你这样的，像我中的相亲对象里边也就是个中等水平啊，你让他们说不行了，三十岁是个尴尬的年纪。|看来慢了一个人在回家。|他们说强势的女人。|找不到合适的人。|为什么要管别人怎么说？|啪啪啪啪啪啪啪啪啪啪。|你们两个明天不用来上班了。|不管处在什么状况下的女人，都应该拥有权利追求爱情来伊对每个女人都值得被爱。\"}\n",
      "multi-modal tagging model forward cost time: 0.016695022583007812 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '多人情景剧', '静态', '手机电脑录屏', '喜悦', '配音', '平静', '惊奇', '家', '场景-其他', '朋友&同事(平级)', '夫妻&恋人&相亲', '悲伤', '全景', '单人口播', '动态', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.98', '0.96', '0.51', '0.33', '0.22', '0.20', '0.19', '0.15', '0.13', '0.10', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07ab0596c738bfa1ab9219eb77ff6b08.mp4\n",
      "{\"video_ocr\": \"叶子叶子长在树枝上|刷牙 我们天天都要刷牙|立即报名 年中大促|课程限今天加送启蒙大礼盒|小猴语文 适合3-7岁孩子|叶|趣味启蒙课|10节课\", \"video_asr\": \"十字交叉的树枝上长出了一片叶子。|叶子，叶子长在树枝上，小朋友正拿着牙刷在刷牙呢。|牙刷牙。|我们天天都要刷牙。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016247272491455078 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '配音', '填充', '课件展示', '动画', '静态', '平静', '喜悦', '才艺展示', '情景演绎', '手机电脑录屏', '特写', '绘画展示', '中景', '教辅材料', '室内', '商品展示', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.92', '0.19', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07b7bef2e4dcde601a39dff68b14ad3d.mp4\n",
      "{\"video_ocr\": \"想吃为什么不自己定|我就剩几毛钱了|我每天生活费就10块|我刚又提现了100块|就当我请你吧|你是怎么赚的呀|手机给我|你刷两下试试|这是什么呀|大平台正规靠谱|你手机刚下载|前几天奖励翻倍|10的更多|我手机里怎么|多了这么多钱啊|这些呀|快手极速版赚的|那这个快手极速版|是怎么下载到|我手机里头的呢|有唯一的下载链接|就在视频下方了|通过这个链接还可以|领取凡十块钱|的新人红包|快看看 你能领取多少钱吧|立即下载|LAKERS|928|9|就是这个|每刷一个|几秒钟的小视频|都会有现金奖励|刷的越多|可以直接提现到|微信和支付宝|白给的钱|为什么不要|点击下方链接|看视频 现金赚不停|欣奶家ChicKen 韩国炸鸡|欣妍|就会发现身边一些不正常的 云小桑M云小|有一个男孩子愿意为我点赞|司杰伦拍同框# 没人看清我穿的什么#一|约会好？#我要上热门##热门#＃#感谢快 今天发天气这么好，我们是私奔好，还是|有趣的友情胜过敷衍的爱情#我要上热门#|我这个人比较肤浅，谁嘴甜我回复谁#我 要上热门##热门#＃感谢快手我要上热门|+关注|，入我倒要看看我的心跑哪|日常 今日签到加166金币|日常任务 首次邀请再送3元 限时福利|结著|转入零钱通 给自己加加薪|已存入零钱|7月29日 20:31|7月26日 19:23|7月30日 21:06|¥0.38|¥98.72|天天签到领金币|邀请好友赚36元|我的零钱 ￥218.38|10元|提取 预计10分钟|+50.00|看视频赚金币 医天天里|打开签到提醒+100金币|充值|741|金币每日凌晨自动兑换成现金|您有一个红包待领取|零钱明细|7月31日18:41|343 分享|1773|MELLO 2202|去赚钱|签到提醒|等到缅分享到微信群领100金币|浏览器|1I|#感谢快|5/6|电话|云小朵M|#热门##感谢快手我要上热门# (作品原声-云小朵M 云小朵|云小朵M的作品原声|朵M  云小朵M的作品原声|了v朵M的作品原声-云小朵M|里去了|把你位置报出来|云小朵M 云小杀|名宇叫做想妮乐铁汁|复制链接|1.1w|782|797|奖励金额以活动规则为准|.可期-KSGirls-陈逗逗(励金额以活动规则准|- KSGirls-陈逗逗(新歌-卖励金额以活动规则方准|晚上7:25 |7月15日 周三上午|可提现|提现到微信:往事随风|边百视城边哺街，多看多嘶|边看视频边钱，多看多赚|联系人|微信|账单详情 全部账单|尊敬的客户:您于2020年07月31日10点54|手我要上热门#|梅美baby 把你位置报出来，我倒要看看我的心跑哪|活动规则 666|审核中|每次100金币，每天1000金币0/10|选择提现金额|832|158.8元|￥5|成功到账50元。|金币收益>|欣姆家C 韩国|欣好家|借利 金币|邀请好友赚36元 多邀多赚 好友看视频你赚钱，点击查看|2w|20年7月|信息|26.8w|08-31|详情见活动规则，具体金额视任务完成情况而定 晴 29C|规则说明|统计|现在|9出甘号为己|复制邀请码|去邀请|子，还是|快手极迷放提现|当前状态|大1礼|喜巴|那我就会觉得|倒计时 23:59:54.7|o0s1见c|快ol1 utX|提现秒A26|8，|已连续签到0天|面对面邀请|商品|7月|5天|05-24|今天已签|领红包|祝现|明天可领|10:41|3元\", \"video_asr\": \"你想吃的时候自己掉，我就用几毛钱，我每天收入十块，我刚刚提现了一百块，就当我请你吧。|你是怎么赚的呀，来手机给我。|你刷两下试试。|这是什么呀？这是快手极速版大平台正规TOP，你手机刚下载，前几天奖励翻倍赚的更多，我手机里怎么多这么多钱啊，这些啊都是你刚才那么快手极速版赚的提现秒到账，那这个快手极速版是怎么下载的我手机。|唯一的下载链接就在视频下方了，通过这个链接还可以领取几十块钱的新人红包，快装盘你能领取多少钱啊？就是这个快手极速版，每刷一个几秒钟的小视频都会有现金奖励，刷的越多赚的越多可以直接提现到微信和支付宝，白给的钱。为什么不要点击下方链接就能下载？|是吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016057491302490234 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '手机电脑录屏', '中景', '静态', '多人情景剧', '场景-其他', '配音', '喜悦', '愤怒', '红包', '惊奇', '平静', '家', '单人口播', '夫妻&恋人&相亲', '全景', '门口', '路人', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.90', '0.88', '0.34', '0.08', '0.06', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07b9ae67009d79329be5fbb0be76e50a.mp4\n",
      "{\"video_ocr\": \"您没事吧|秦山|记住我让你查的事|滚出临州|不要让我再见到你|我还会回来的|你给我等着|我回来了|主人 鬼师回来了|让他消失|你是谁派来的|精彩继菜看|点众阅读，热门小说追更神器 《荣耀战神》|也只能在我的脚下|你们眼中的神|1|FOXRACING|五年戎马|荣耀归来|颤 抖|匍匐\", \"video_asr\": \"当然。|你没事吧。|青山。|记住我让你查的事。|滚出林州。|让我再见到我还会回。|给我等着。|我。|回来了。|对，鬼师傅让他消失。|啥。|嗯嗯。|嗯嗯。|你是谁派来的？\"}\n",
      "multi-modal tagging model forward cost time: 0.01621079444885254 sec\n",
      "{'result': [{'labels': ['中景', '填充', '推广页', '多人情景剧', '静态', '现代', '特写', '室内', '平静', '夫妻&恋人&相亲', '惊奇', '悲伤', '动态', '愤怒', '全景', '朋友&同事(平级)', '室外', '厌恶', '喜悦', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.89', '0.88', '0.84', '0.82', '0.76', '0.64', '0.37', '0.29', '0.27', '0.13', '0.09', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07c239abd84c41a38e33d1bbe07c0d1c.mp4\n",
      "{\"video_ocr\": \"李总|不好啦|怎么了?|我们的价格系统好像出了BUG|10节斑马AI课思维体验课|才只要49块钱|现在订单已经爆满了!|这是我要求降的价格|我们斑马AI课趣味思维课|可是请十多年经验的专业教研团队|专为3-6岁孩子设计的课程|用领先的A动画互动技术|让孩子趣味学习|课程内容质量高|研发成本也很高啊|先不要管成本!|我们当初做教育的初衷你忘了吗?|听我的|现在报名不仅49元10节体验课|价值108元的教具大礼包|也免费包邮赠送！|再送30节国庆TV课|语文思维英语各10节|让孩子3科一起学!|李总我明白了|我这就去办|可以将抽象的数学知识具象化|希望能帮助更多的孩子建立|数理思维能力|点击视频下方链接|你也可以给孩子报名了|斑马A课|猿辅导在nmw|猿辅导在感|猿辅导 在线教育|导|2-8岁上斑马学思维 学英语|猿辅导在线教育出品\", \"video_asr\": \"李总，不好了，怎么了？我们价格性能好像出现了八十节方法AI课，思维体验课只要四十九块钱，现在订单已经爆满了。|是我要求教的家里总，我们笨吗AI课，趣味思维课，可申请十多年经验的专业教研团队，专为三到六岁孩子设计的课程。|领先的AI动画互动技术，让孩子趣味学习，课程内容质量高，研发成本也很高呀！先不要管成本，我们做教育的初衷懒得你忘了吗？|听我的，现在报名不仅四九元十节体验课，价值一百零八元的教具大礼包也免费包邮赠送，在送三十节国庆七雷克语文思维一个世界。|让孩子三科一起学行了，我明白了，我就不明白他哪个趣味思维课可以将抽象的数学知识俊讲话。|希望能帮助更多的孩子建立数理思维能力，点击视频下方链接也可以给孩子报名呢！\"}\n",
      "multi-modal tagging model forward cost time: 0.01624321937561035 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '多人情景剧', '静态', '平静', '亲子', '家', '特写', '家庭伦理', '喜悦', '教辅材料', '单人口播', '极端特写', '手机电脑录屏', '愤怒', '动态', '办公室', '场景-其他'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.95', '0.93', '0.78', '0.69', '0.67', '0.35', '0.34', '0.32', '0.05', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07c3f9a4f5370e49fd61b19d6d52f2ae.mp4\n",
      "{\"video_ocr\": \"小爷我叫顾潇墨|如果你是她的孩子|想必也该如此|小包子|你没事吧|五年了|今天不但自己送上门|还给我送了个儿子|既然再次相遇|本少不会再把你丢了|《天才酷宝总裁宠妻太强悍》|酷卖湖凤宠强悍》|米读极速版|本故事纯属虚构|《天\", \"video_asr\": \"小爷，我的故乡。|如果你是他的孩子。|小丽在。|小伙子没事吧。|今天不知道自己做什么。|他给我送轩轩再次相遇。|感受不会做题。\"}\n",
      "multi-modal tagging model forward cost time: 0.016576290130615234 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '多人情景剧', '推广页', '平静', '家庭伦理', '夫妻&恋人&相亲', '亲子', '拉近', '办公室', '喜悦', '特写', '室外', '家', '愤怒', '悲伤', '动态', '惊奇', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07c8d3a9c02d0e3f11fc20937f7c2629.mp4\n",
      "{\"video_ocr\": \"喂李哥|您的快递到了|依依|爷爷我们又见面了|爷爷买了好多好吃的|在办公室去拿|谢谢爷爷|小王啊|怎么又带孩子送快递啊|她妈妈出去干活了|小孩一个人在家|我也不放心啊|中秋回家么|不回了|休息一天啊|少赚不少呢|我可舍不得回去|你有两年没回家了吧|来把手机给我|我给你报名了|快财商学院|别光顾着赚钱|在上面学学理财|理财|可别闹了|我哪学的会啊|我身边50多岁的人|都在学习理财|有时候|理财收益|比退休金还高呢|那这不便宜吧|现在点击视频下方链接 就能免费报名了|￥√=X一早一 底山原苹|液山|i.外有飞|iy下|t0”|第苹果|可美一包一|山透塬苹果|法山源苹果|大山甚源年片|大该山 源苹异|大床山盐源苹异|投资有风险，选择需谨慎，风险责任由购买者自行承担|理想生活上天猫|理想天持|村东升|闲活|学理财上快财|再蓝诉重茶创等幕|TM1A|感谢您惠顾|视频为演绎情节|400-706-828|Beijing 万.在线|万在|万.神|402|413|￥√=ョ|￥飞|00C0|x阳很|ing|直播|GT|E|天#\", \"video_asr\": \"喂，李哥你的快递到了。|你的我的。|嗯，爷爷买了好多好吃的在办公室啊，谢谢爷爷，嗯，小巴。|怎么又带孩子送快递啊，他妈妈出去干活了，小孩一个人在家我也不放心呢，中秋回家吗？|不回了，休息一天啊，少赚不少呢，我可舍不得回去，你有两年没回家了。|快手这个。|我给你报名了快财商学院，林锋不赚钱，在上面学学理财，理财可别闹了，我哪学的会呀。|我身边五十多岁的人都在学习理财，有时候理财的收益比退休金还高的。|那这个不便宜吧，现在点击视频下方链接就能免费报名了！\"}\n",
      "multi-modal tagging model forward cost time: 0.01662421226501465 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '喜悦', '平静', '亲子', '愤怒', '全景', '惊奇', '悲伤', '特写', '路人', '手机电脑录屏', '动态', '家庭伦理', '极端特写', '朋友&同事(平级)', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.73', '0.40', '0.26', '0.20', '0.19', '0.12', '0.07', '0.06', '0.04', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07ca0494e6959d470aea6c58e6eb1b14.mp4\n",
      "{\"video_ocr\": \"郎朗看诺诺用小叶子练琴|哎呦|这么厉害呀|太厉害了|连郎朗都称赞的诺诺 原来是这样练琴的|当你弹对的时候|这个音符会变成绿色|这个乐谱就卡在这里不动了|光标就停在你弹错的这个音上|你有5秒的时间自己去找键位|然后屏幕下方就会出现小键盘|告诉你正确的音怎么弹|49元体验两周|这么智能的APR|郎朗推荐 ￥49元/2周|款给爱愿丝|就给爱服线|T|AI纠错 消灭错音 -官方价2600年|国民琴童练琴考试必备|错音 不熟练|[德]|【德]L.van. (1770|一乐句 Poco moto q-70|小叶子|ONE|30NE|BOHE|TheONE|弹晚了\", \"video_asr\": \"豆豆。|哎哟，这么厉害啊。|连朗朗都称赞的诺诺原来是这样练琴的，当你团队的时候呢，会衣服会变成绿色，但当你弹错的时候，这个院子就谈到这里不动了。|光标是停在你弹错的这个音上面，你有五秒的时间可以自己调键位，然后屏幕下方呢就会出现这个小键盘，告诉你正确的音怎么弹，三十九元体验两周智能的APP，快来一起来练习吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022882938385009766 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '平静', '手机电脑录屏', '家', '场景-其他', '单人口播', '配音', '特写', '喜悦', '室内', '极端特写', '动态', '宫格', '亲子', '混剪', '家庭伦理', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.96', '0.92', '0.56', '0.34', '0.21', '0.10', '0.10', '0.08', '0.03', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07cb21249621690d1ba7f06ef350a54a.mp4\n",
      "{\"video_ocr\": \"妈妈 你不用陪我了|有VIP陪练|+86 验证码登录|密玛量录|上班结束 满身疲惫|TrE|T kiE|Tr's|注解>\", \"video_asr\": \"你。|下载。|妈妈你不用陪我了，我有VIP陪练。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.01660442352294922 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '平静', '动态', '手机电脑录屏', '惊奇', '室外', '喜悦', '全景', '特写', '拉近', '极端特写', '配音', '路人', '拉远', '愤怒', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.74', '0.67', '0.62', '0.49', '0.49', '0.30', '0.29', '0.23', '0.15', '0.08', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07cb670349f59e1988868031e68967c3.mp4\n",
      "{\"video_ocr\": \"哥几个都到了|就等你了|哦我就不去了|你们几个少喝点|对不起对不起|刚子|你小子不合群啊你|以前我手里还有点|哥几个今天你请|明天我请|现在|你这话说的|好像哥几个谁在乎一样|我在乎|现在的我暂时 融不进这个圈子|说什么呢|一天兄弟一辈子都是兄弟|你这样我不能不管|手机给我|给|我在你手机上给你 报名了快财商学院|你在上面学学理财|每天打开手机就能 获得额外收入|这不会是骗人的吧|兄弟我能骗你吗|我身边50多岁的人 都在学理财|用理财赚的钱 比退休金还多呢|真的呀|这报名费贵不贵啊|点击视频下方链接|0元就能免费报名啦|快财 快财商学院|美困外卖|学理财上快财|huckin cotee|直播\", \"video_asr\": \"哥几个都到了就等你了哦，我就不去了，你们几个少喝点。|就刚才你小子不合群啊，你以前我手里还有点歌姐，今天你请，明天我请。|现在你这话说的好像哥几个谁在不一样，我在乎，现在的我暂时融不进这个群，说什么呢？一天兄弟，一辈子都是兄弟。|你这样我不能露手机给我给。|我在你手机上给你报名了，会在上个月在上面学学理财，每天打开手机就能获得额外收入，只不会是骗人的吧。|兄弟，我能骗你吗？我身边五十多岁的人都在学理财，用理财赚的钱比退休金还多呢，真的呀，这报名费贵不贵呀？|点击视频下方链接，零元就能免费报名了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016323089599609375 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '填充', '静态', '惊奇', '喜悦', '路人', '平静', '动态', '愤怒', '特写', '全景', '悲伤', '朋友&同事(平级)', '极端特写', '夫妻&恋人&相亲', '单人口播', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.84', '0.71', '0.64', '0.46', '0.08', '0.07', '0.05', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07cd9697b15913173f2dbbcbdabf4269.mp4\n",
      "{\"video_ocr\": \"我是高途课堂张展博老师|清华本科及硕士|高中物理常年满分|千题一箴 了法归一|12大通解体系会帮你用很少的时间|学很少的知识考很高的分数|让你彻底告别过去|低效甚至无效的学习方式|F 42他小d款/Bd Ipdv|给自己一个黑马逆袭的机会|查看详情 抓紧报名吧|作为一名父亲 面对孩子学业上的难题|我却难以解决|现在孩子随便一道题就能把我难倒了|在教育上我想找到真正懂孩子|懂学习方法的好老师|浙江卫视指定在线教育品牌 立即体验|了试崩溃|d o|Z-mc 山 。|Z-md d|Z-mg ds -o|B.do|=mC|F.M|=7c|F|Z=m pBd -o|中B.ds-o|F个一h fz·dl|F一g·dl|-m|课程难度大|秒题很累|FM一p|12个通解|F¥年r|Zmc|F.“帮|名师特训班 高途课堂|A4纸学习法|只需9元 E-dl|F4一|50|口.|21|全国百佳教师带队教学平均教龄11年|Fma|F·t|F三7Q|F=y Hd-/Jds+|张展博|Hed]d+/果山|Hd/d+/e|Hda/]小d+/光|高中物理主讲老师 清华大学学士|新用户专享|浙江卫视|人山|￥9|仅需\", \"video_asr\": \"作为一名父亲，面对孩子学习上的难题。|我却难以解决，现在孩子随便导致就能把我难倒了。在教育上，我想找到真正懂孩子，懂学习方法的好老师。我是高途课堂张展博老师，清华本科及硕士，高中物理常年满分，先题意解。|法归一十二大通解体系，会帮你用很少的时间学很少的知识，考很高的分数，我的A四纸学习法，让你彻底告别过去的一笑，甚至无效的学习方式，只需九元，给自己一个黑马逆袭的机会，查看详情抓紧报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015910863876342773 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '推广页', '填充', '静态', '教师(教授)', '平静', '室内', '配音', '情景演绎', '特写', '学校', '场景-其他', '影棚幕布', '极端特写', '家', '课件展示', '动态', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.74', '0.34', '0.23', '0.18', '0.04', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07d1ebb6267b206eeb200ce9ca3b7fe3.mp4\n",
      "{\"video_ocr\": \"立即报名|唉孩子还没睡呢|这次摸底考试没考好|一回家呀就把自己|关在屋子里|不停的刷题呢|哎我得说说他去|这样下去身体不得熬坏了|你就别去啦|这马上就高三了|那不刷题怎么考好大学|刷题就能考高分啊|你得有正确的学习方法|你看昂|我给咱孩子报了|高途课堂全科名师班|都是好老师|每年送不少的孩子|上北大清华|平均教龄|都现在11年以上|省高考状元|给咱们当辅导老师|那得多贵呀|你工资够还是我工资够|不用|这个课块钱|就有16节课|包含|包含语数英物|4科的重难点|和高效的解题技巧|这么好的课呀|那你在哪找的呀|直接点击视频下方链接|就可以报名了|名师出高徒·网课选高途|名师特训班 省高考状元带队授课 老师平均教龄11年+|高途课堂|新学员9元专享|语\", \"video_asr\": \"我。|孩子还没睡呢。|这次摸底考试没考好，一回家呀就把自己关到屋子，连自己的刷题了。|我得说说他去，这样下去身体不得熬坏了你就别去啦，这马上都高三啦，她不刷题怎么考好大学呀，刷题就能考高分啊，得有正确的学习方法啊，你看啊，我给咱们孩子啊。|了，高途课堂全科名师班都是好老师，每年啊，送不少的孩子上北大清华，平均教龄都在十一年以上，这省高考状元给咱们当辅导老师那得多贵呀，你工资够还是我工资够呀，不用这课，九块钱就有十六节课，包含语数，英物。|四科的重难点和高效的解题技巧，这么好的课呀哎，那你从哪找的呀？直接点击视频下方链接就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016475915908813477 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '填充', '推广页', '中景', '静态', '家', '平静', '亲子', '家庭伦理', '喜悦', '特写', '夫妻&恋人&相亲', '动态', '极端特写', '惊奇', '悲伤', '亲戚(亲情)', '单人口播', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.94', '0.42', '0.25', '0.13', '0.12', '0.04', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07d24893efe7f76a8918b206b695ee93.mp4\n",
      "{\"video_ocr\": \"嫂子|给我拿10万块钱呗|怎么|当公司的钱大风刮来的啊|你又要这么多钱干什么|这不女朋友看上辆车|想哄她开心|也没见你对你哥还有我这么好呀|要想借钱|找360借条|我这可是要收利息的|合着360借条不要利息|借4万最长免息30天|就算过了免息期|借万元每天利息最低才2块7起|不信|你输入个人资料|就能在线申请|最高额度20万|最快5分钟放款|我有15万的额度|这下不差钱了|屏幕前的你|也赶紧点击视频下方链接|申请你的额度吧|额度 一次授信，循环|十头#|8|大额借款|赚钱专区|即向所在地警方报案处理。|立即领取 活动规则>>|钱不够?领最高30万额度|财产安全，谨防上当受骗。 如有财产损失，建议您立|借一年，慢慢还 请输入您的手机号|邀请赚赏金 超级额度|请您提高警惕，保护自身|最高20万额度 最长30天免息|150000 00|免息|不会向用户收取任问费用. 360惜条在贷款发放前，|30天息费优惠券|选择360借条的四个理由|向您索要保证金、工本费、解冻费、 账户改费用。|道票360数科爽团旗下消类信贷品牌|4G 具体额度/放款时间以实际审批为准|全民免息狂欢|最长|20万购物额度待领取 可用额度(元)|物额度待领取|13866669356|总额度150000元 1千元借12个月，日费用最低2毛7|普高20万|金融有风险，贷款需谨慎|全了有紧跄，抢资须灌铺 具体贷款额度以实际审批为准|借钱|JKL|PQRS|送最高12期优惠券 更多|认证提额 充值中心|会员专享 捕鱼赢金条 百亿补贴|骗子常以发送工牌、身份证照片方式 假冒360借条工作人员在贷款发放前|信用检测 百亿补贴|.1I 4G|¥lI4G|4G|100%|完成|天天拆现金|Tuv|我的优惠券|1990元|郑重提醒|GHI ABC\", \"video_asr\": \"嫂子给我拿十万块钱呗。|怎么咱公司的钱都是大风刮来的，你有什么多钱干什么？|这个女朋友看这辆车吗？想好了开心也没见你对你哥还有我这么好呀，想借钱找三六零借条呀，我这可要收利息的，合着这三六零借条不要利息啊，三六零借条借四万最长免息三十天，就算过了免息期，借万元每天利息最低才两块七起，不信呢？你输入手机号和身份证。|就能在线申请最高额度二十万，最快五分钟放款。|妈呀。|有十五万的额度啊，这家伙赚钱了，你目前的你也赶紧点击视频下方链接申请你的额度吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016431808471679688 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '平静', '手机电脑录屏', '单人口播', '家', '惊奇', '喜悦', '夫妻&恋人&相亲', '极端特写', '动态', '特写', '家庭伦理', '悲伤', '办公室', '全景', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.88', '0.82', '0.41', '0.32', '0.29', '0.21', '0.04', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07d6b9b73c17ccd4924b4408c768b667.mp4\n",
      "{\"video_ocr\": \"我作业写完了|我要拿iPad|上猿辅导网课|来啦|给|自从报了猿辅导|孩子在家随时随地|都能学|清华北大毕业老师教学|49元两周课|49元|课前热身 在下面|握速算技巧 巧解应用题|)+5-8 )=96 开足学霸|我是学套|)中填入适当的数 ）15 g+l|9+(|)+5=15|72+(|）×5=45|160|小学数学重难点特训班|( J-812 18-1 j…10|）-8-12 18-( )10 )-9|立即报名|猿辅导|60折|8xl|20\", \"video_asr\": \"妈妈，我作业写完了，我要拿爱拍照比较辅导网课来了。|自从报了猿辅导，孩子在家随时随地都能学，清华北大毕业老师教学四十九元两周课。\"}\n",
      "multi-modal tagging model forward cost time: 0.016545534133911133 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '手机电脑录屏', '特写', '平静', '多人情景剧', '极端特写', '单人口播', '配音', '亲子', '家', '动态', '场景-其他', '朋友&同事(平级)', '情景演绎', '惊奇', '教辅材料', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.88', '0.66', '0.60', '0.59', '0.42', '0.23', '0.14', '0.07', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07d88f710849972869b344c9d719828a.mp4\n",
      "{\"video_ocr\": \"n|我们来晚了|你有没有发现|有些单词你明明认识|但换一个场景|就看不懂啦|这就是所谓的|一词|一词多义|比如说|为什么|No restrictions|指的呀是|Noprice|这也是为什么|Smoke Free|其实啊就是|一词多义的单词啊|只有一个基本意思|而其他所有的意思|我在网易有道精品课|开设了7天英语|899的课程|点击屏幕下方报名|我们在直播间|不见不散哦|Smoke Free Area|Free|应该是免费吸烟区|或者|自由吸烟区的意思|其实firee虽然有很多意思|但是它的基本意思都是没有|free之所以可以表示自由的|它之所以可以表示免费的|都是由基本意思的延伸含义|无烟区|免费的|自由的|没有约束|11 禁止吸烟|实战蜕变营|就可以得到|.rethebest|北京市奥运会、残奥会志愿老官方英语培训师 法国空客集团へ-u165031601こ0中国区英语培训|ry|ke|No Smoking|现在只需要9元|杨亮|cheer|doi|奥运会、残奥 官方英语培训师|单买价:899元 22岁以上，高中英语基础|词汇/语法/口语/阅读/写作 用英文经典 5大技能全面逆袭|co|fee|St|tr|中国区英语|客集园 看官方英|适合人群 点击链接立即报名|ha|高中英计|高中英语基础|想要提升英语的|天实战英文硬核蜕变|best|stdoi|奥运会残奥会志|想要提|朋友|中国区|有道精品课|Pi| 对不起! 我们|jaou|9元特惠|ddo|aw|oW\", \"video_asr\": \"高中英语基础想要提升英语的朋友们，对不起，我们来晚。|有没有发现有些单词你明明认识，但换一个场景就看不懂了？这就是所谓的一词多义，比如说为什么SMOKE FOR EVER也是无言区的意思，所以这个词不是免费的或者自由的意思。|那SMOKE FREE AREA应该是免费吸烟区或者自由信任区域。其实福利虽然有很多意思，但是啊，它的基本意思都是没有，所以这所以可以表示自由。|质量是NO RESTRICTION没有约束，他之所以可以表示免费的质量JMO PRIZE没有价格，这也是为什么SMOKE FREE其实啊，就是NO SMOKING禁止吸烟的意思。一词多义的单词只有一个基本意思，而其他所有的意思都是由基本意思的。|深含义，我在网易有道精品课开设了七天英语实战蜕变营，八百九十九的课程，现在只需要九元就可以得到。|点击屏幕下方报名，我们在直播间不见不散哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01617598533630371 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '静态', '教师(教授)', '单人口播', '平静', '室内', '知识讲解', '影棚幕布', '场景-其他', '配音', '室外', '喜悦', '特写', '惊奇', '单人情景剧', '拉近', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.91', '0.49', '0.22', '0.09', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07dc3deedb57ad0208c02ac18d3fc726.mp4\n",
      "{\"video_ocr\": \"可以改变|刚工作时|我也曾为微薄的收入|疯狂加班|视工资为唯一收入|可房租 交通 信用卡账单|让我倍感压力|拼命工作|并没有给我带来|想像中的安全感|看着同期的同事们|按时打卡下班|不必看领导脸色|日子依旧过得光鲜靓丽|我开始怀疑自己|是我不够努力吗|后来我才知道|原来同事早已通过理财|学会了用钱生钱|不再只依靠死工资|尽情的享受生活|在她的推荐下|我报名了微淼商学院的|小白理财训练营|12天的学习让我|学会科学使用基金|国债等理财工具|掌握富人思维|存款也开始不断增加|现在我也推荐你|班主任全程辅导|0基础小白也能学|如果你也想摆脱|死工资的束缚|现在点击视频下方 查看详情|让改变从此刻开始|限量抢购 为什么要学理财?|行病的-个|LOUIS VUITTON|0川Mii1l|B|fCVCCTtSSttY|CCNCCCrrrrffIty|学理财上微淼|理财知识学习 会带来怎样的改变吗?|你相信12天时间|21 18|18|黄防|狄国！|[QUSWiTC|自秦 36NIND|首 36NINani|2:00-20:00|你知道12天的|可以改变你的人生吗?|微淼 商学院|微淼 商学|院|视频为演绎情节|你的人生吗?|百欢乐餐厅|TECHNOLOGY|A兔|微淼|商|学|院|24\", \"video_asr\": \"不。|刚工作时，我也曾为微薄的收入疯狂加班，是公司为唯一收入，可房租，交通，信用卡账单让我倍感压力，拼命工作并没有给我带来想象中的安全感，看着同期的同事们按时打卡下班，不必看领导脸色，日子依旧。|的光鲜亮丽，我开始怀疑自己，是我不够努力吗？后来我才知道，原来同事早已通过理财学会了用钱生钱。|不再是依靠死工资尽情的享受生活，在他的推荐下，我报名了微淼商学院的小白理财训练营，十二天的学习让我学会科学使用基金。|国债等理财工具掌握，富人思维，存款也开始不断增加。现在我也推荐你报名微秒商学院的小白理财训练营，班主任全程辅导，零基础小白也能学。|如果你也想摆脱死工资的束缚，现在点击视频下方查看详情，将改变从此刻开始。\"}\n",
      "multi-modal tagging model forward cost time: 0.016316652297973633 sec\n",
      "{'result': [{'labels': ['填充', '推广页', '中景', '动态', '现代', '配音', '混剪', '情景演绎', '平静', '静态', '室外', '手机电脑录屏', '室内', '单人口播', '拉近', '办公室', '家', '全景', '夫妻&恋人&相亲', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.97', '0.93', '0.93', '0.89', '0.84', '0.80', '0.73', '0.70', '0.66', '0.45']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07dcb83ae9cbc22cdce87b9bd3a6cf86.mp4\n",
      "{\"video_ocr\": \"怎样的女人 最有家庭地位|答案是|经济独立的女人|她叫张萌|和老公结婚3年|每个月老公都 心甘情愿地|把所有工资 交给张萌打理|她不是靠脸蛋|娘家也没有 任何雄厚的背景|秘诀在于|即使是有了 家庭之后|让自己埋没于|柴米油盐中|而是不断地学习|拓展自己|除了工资收入 以外的收入来源|她循序渐进的 学会了|各类理财工具的 使用方法|通过各种巧妙地投资|让存款不断增长|一开始老公|抱着迟疑的态度|但逐渐|随着赚到的Q 越来越多|老公的态度 也逐渐转变|每个月的理财收入|早就超过了|试问还会没有 家庭地位吗|现在微淼商学院|推出 限时特惠课程|通过班主任 全程的细心辅导|帮你找到|适合自己的 投资方式|12天12节理财课|提升家庭幸福感|从现在开始|点击屏幕下方链接|为您和您的家人 报名学习吧|学理财上微淼|微淼商学院|新学院|微淼|微强商|查量P*5100千06RtR日090巧|贵际户*5100于06月08日0900转 入5080.00元，现余额为25620051|就是投资自己|视频为演绎情节|商|学馆|成学|95338|商学家|元、捷要：理财收益。|元，换爱：理财收益|财上微森|学理财 12052888588|财上|最好的投资\", \"video_asr\": \"怎样的女人最有家庭地位？答案是经济独立的女人，她叫张萌，和老公结婚三年，每个月老公都心甘情愿的把所有工资交给张蒙哪里，他不是靠脸蛋，娘家也没有任何雄厚的背景，秘诀在于，即使是有了家庭之后，他也没有让自己埋没于柴米油盐中，而是不断的学习拓展自己，除了。|工资收入以外的收入来源，但学习渐渐的学会了各类理财工具的使用方法，通过各种巧妙的投资让存款不断增长。一开始老公还对张蒙报着迟疑的态度，但逐渐随着赚到的钱越来越多，老公的态度也逐渐转变，现在张某每个月的理财收入早就超过了老公的公司这样的女。|试问还会没有家庭地位吗？现在，微淼商学院推出限时特惠课程，通过班主任全程的细心辅导，帮你找到适合自己的投资方式。|二天十二节理财课，提升家庭幸福感，从现在开始，点击屏幕下方链接，为您和您的家人报名学习吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01636672019958496 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '静态', '多人情景剧', '平静', '办公室', '单人口播', '家庭伦理', '家', '亲子', '惊奇', '喜悦', '工作职场', '室内', '特写', '朋友&同事(平级)', '动态', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.76', '0.66', '0.58', '0.22', '0.20', '0.20', '0.15', '0.08', '0.07', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07de9a3b19d5ba5ac1998df07aef24af.mp4\n",
      "{\"video_ocr\": \"小学生在学习英语过程中|最大的问题就是背单词|单词学得好 英语没烦恼|单词学得差考试总害怕|中小学生的英语课本中|每个单元都有自己的主题|利用思维导图、速记主题和单词的顺序|再通过音标和发音规律拼读单词|再也不用死记硬背了|让背单词比玩游戏还吸引孩子|Sam老师会告诉你在英语的学习过程中|背单词只是第一步|那么接下来还要在阅读和文章之中 速记单词的使用方法和意思|在4天的学习训练营中|我将会带领同学们一起来挑战高效地背单词|大家可以点击屏幕下方的链接|跟着Sam老师一起来高效地学习英语|Sam老师|《超级拼读》创始人|8节名师直播课 8大组合速记法|单词|注册成功后 截图保存图片 扫码入群听课 领取学习资料|11年一线教学经验|让孩子爱学、敢说|掌握8大语法规则|单词速记 训练营|跟谁学|-小学英语-|激发兴趣\", \"video_asr\": \"单词，单词单词，小学生在学习英语过程中最大的问题就是背单词，单词学得好，英语没烦恼，单词学的差，考试总害怕等。中小学生的英语课本中，每个单元都有自己的主题，利用思维导图速记主题和单词的顺序，再通过音标和发音规律。|拼读单词再也不用死记硬背了，让背单词比玩游戏还吸引孩子！|三，老师会告诉你，在英语的学习过程中，背单词只是第一步，那么接下来还要在阅读和文章之中速记单词的使用方法和意思。在四天的学习训练营中，我将会带领同学们一起来挑战高效的背单词，大家可以点击屏幕下方的链接，跟着森马老师一起来高效的学习英语，注册成功后截图，保存图片，扫码入群听课，领取学习资料。\"}\n",
      "multi-modal tagging model forward cost time: 0.01723194122314453 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '单人口播', '推广页', '平静', '教师(教授)', '填充', '室内', '场景-其他', '宫格', '影棚幕布', '配音', '手写解题', '课件展示', '特写', '办公室', '极端特写', '动画', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.87', '0.25', '0.11', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07e54a1b4c905ac04a1e8d016c3f76dd.mp4\n",
      "{\"video_ocr\": \"安婧老师|你说我要是有100万|是不是不用工作|光靠理财收入 就能养活自己了|那我现在给你100万|你一定能赚到钱么|这个...|不好说 估计会亏吧|你看 不学习|就算你有了钱 也不敢理财|想要用钱生钱 那得先懂钱|我说说而已啦|我这一个月几千块|有学理财的必要么|当然有|你要知道|不是有了钱才来理财|而是会理财 才能有更多的钱|月薪三四千|也可以有 自己的理财方案|那老师 你教教我怎么学呗|要我说啊|你可以现在花1块钱|报名小白理财训练营|5天的精品课程|教你可复制的 投资方案|都是10年理财经验的 老师授课|重实操 有干货|还有小白必备的 投资避坑指南|教你培养理财思维|变身理财达人|那这么好|怎么报名啊|点击屏幕下方 查看详情|1块钱赶紧抢课吧\", \"video_asr\": \"韩菁老师，你说我要是有一百万，是不是不用工作，光靠理财收入就能养活自己了，那我现在给你一百万，你一定能赚到钱吗？这个不好说，估计会亏吧，你看步学习，就算你有了钱也不敢给开药，要想用钱生钱，那你得先懂钱。|我说说而已啦，我这一个月几千块有学理财的必要吗？当然有，你要知道，不是有了钱才能理财，而是贵理财才能有更多的钱，月薪三四千也可以有自己的理财方案。对了，教教我怎么学呗，要我说啊，你可以现在花一块钱来报名小白理财训练营。|的精品课程，教你可复制的投资方案，都是十年的理财经验的老师授课中，实操有干货，还有小白必备的投资避坑指南。|教你培养理财思维，变身可以在大人学会用钱生钱，那这么好怎么报名啊？点击屏幕下方查看详情，一块钱就赶紧抢课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016721487045288086 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '静态', '中景', '推广页', '平静', '室内', '喜悦', '特写', '手机电脑录屏', '填充', '家', '情景演绎', '单人情景剧', '场景-其他', '配音', '影棚幕布', '动态', '多人情景剧', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.56', '0.18', '0.12', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07f65742bd53948793bd6c2297c135a0.mp4\n",
      "{\"video_ocr\": \"国际英语教学能力认证|剑桥大学TKT|作业帮直播课|小学英语的中教负责人|在我的课上|我会用思维导图|让孩子知道|国|单词之间的潜在关系|见到uTr|见到u|就能想到nurse、 hurt|就能想到cirde、 skirt|掌握这些小技巧|几分钟就能背会|几十个 单词|轻松挑战英语的满分|13节课只需要9块钱|还包邮赠送教辅大礼包|赶快点击视频下方的链接|报名吧|是谁说我们家浩浩|语文考试作弊的|浩浩妈|没人说浩浩语文作弊|但是他这语文成绩|进步也太大了吧|两个字母|能让你读出干百个单词|你相信吗?|你做不到的话|是因为你没有学习到|正确的方法|跟我学13节课|你也能掌握技巧|轻松读写|小学大部分的复杂单词|我是英语老师王佳宝|skirt|firm|名师有大招解题更高效|英语学小技巧|英语单词语法名师课|剑桥大学TKT国际英语教学能力认证 作业帮直播课小学英语中教负责人|单调本|报名就送超值大礼包 上课内容与收到礼盒请以实际为准|SCHOOL|cooker|5大切式|pork|or|birth|storm|dldier|purse|church|TF|王佳宝|hurt|Mars|large\", \"video_asr\": \"是谁说我们家好语文考试作弊的号好吗？没人说好好语文作弊，当时他这语文成绩进步也太大了吧，两个字母。|能让你读出千百个单词，你相信吗？如果你做不到，是你没有学习到正确的方法，跟我学十三节课。|也能掌握技巧，轻松读写小学大部分的复杂单词，我是英语老师王家宝，坚强大学GAG国际英语教学能力认证。|最帮直播课，小学里的中教负责人，在我的课上，我会用思维导图让孩子知道单词之间现在的关系，见到U二。|能想到MERS MERS见到IR不能想到CIRCUS GIRL掌握这些小技巧，几分钟你就能背会几十个单词。|轻松的挑战英语的满分，十三节课只需要九块钱，还包邮赠送教辅大礼包，赶快点击视频下方的链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016086578369140625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '静态', '多人情景剧', '平静', '喜悦', '惊奇', '单人口播', '全景', '家庭伦理', '愤怒', '室外', '亲子', '手机电脑录屏', '夫妻&恋人&相亲', '室内', '路人', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.74', '0.46', '0.21', '0.15', '0.11', '0.07', '0.06', '0.03', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/07f8a49ff38128dbc87d027ad45d7290.mp4\n",
      "{\"video_ocr\": \"我们这些当保安的|一个月就这么点工资|养家糊口都不够用|今天刚下载了|疯狂点点消|就给了我1◎回块钱|这三四天的饭钱就有了|我们这些送外卖的呀|每天日晒风吹|实在是大辛苦了|今天呀下载了这个|疯狂点点消就领了|10◎块钱现金红包|第一次遇见|连心好的事情|人0|最45的就是生病|既费钱还难受|幸好我下载了|刚下载|就领了 IO0元的|现金红包|这么好的事|你们还在等什%|赶快点击视频下方链摇|下载吧|就是这个疯狂点点消|弃|新人上线就有|66元大红包|不是金币哦|简直不要大良心|玩游戏就能领红包|100.00|恭喜获得，新手礼包|点击2个以上 同色方块，就能 消除它们!|成目标哦！ [以完|已存入零钱，可的入零透族收益)|已存入等钱，可购入零眼通膀收)|回复表情到聊天|0元|用户7d6c提现108.47元成功|用宁 rdoctI108 47元厅功|用户0R2提现147 88元成功|用户了1提现13Z77元成功|多害发财，大言大利|yeah!|消除越多红包出现的概率越大|存入余额|继续赚红包|66.00元|1.05元|无敌 10:52|恭喜您获得|新手礼包|太壳大利|第λ兴|第δ兰|第关|点击下方链接下载|用户8ce9提现148.32元成功|P8ce9擅刃148|ROie )1052|国910.52|Roi0|到账金额以实际为准，请勿充值转账|余额|余额满100元可以提现|确定|恭喜发财Λ|立即提现|分红礼盒今日收益|259.47元|RI0 33)110.52|和9|25\", \"video_asr\": \"我们这些当保安的呀，一个月就这么点工资，养家糊口都不够坐。|今天刚下载了凤凰岭点香，就给了我一百块钱，这三四天的饭钱就有了。我们今天送外卖的呀，每天日晒风吹，实在是太辛苦了，今天呀，下载了这个疯狂点点香，就领了一百块钱现金红包。|一次遇见这么好的事情的人啊，最怕的就是生病，既费钱还难受，病好我下载了疯狂练练，刚下载就领了一百元的现金红包，这么好的事，你们还在等什么？|点击视频下方链接下载吧，就是这个疯狂点点消新上线就六十六元大红包，是六十六元红包不是金币哦，简直不要太良心，赶紧下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01669168472290039 sec\n",
      "{'result': [{'labels': ['现代', '静态', '推广页', '中景', '场景-其他', '平静', '配音', '多人情景剧', '室外', '喜悦', '路人', '全景', '手机电脑录屏', '动态', '单人口播', '惊奇', '红包', '填充', '宫格', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.96', '0.96', '0.55', '0.48', '0.33', '0.30', '0.21', '0.15', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08088c7f657efbac30ffe5e75e7dd490.mp4\n",
      "{\"video_ocr\": \"很多用户|打来电话问|拼多多9块9|充100元话费|到底怎么充|今天|我就手把手教您|打开拼多多|输入立即领取|进入9块9秒杀界面|的商品页|在秒杀时段|点击立即购买|确认您的手机号码|选择支付方式|充值成功|就这么简单|拼多多官方福利|正在大批放量|百万的用户|已经享到优惠|无论您是|联通|电信|移动|都能参与|你也试试吧|小型洗衣机 莲雾 芒果|莲雾|立即领取>|更多预告|￥9.9 ￥270|请输入支付密码|卡即可领券 每月还可免单|海信a5手机 便携式打印机|0D B评R|8|查看更多> 秒杀万人团|正在抢购中 移动联通电信话费充值100元|零钱|再次购买 申请退款|￥9.9 ￥1399|三全国联保顺丰包邮 一舰店 狂欢中|App专享|因用户提供的手|Apple iPad Air3 20|【款式丰富，随机发货] LOFREE/洛|ji ABC DEF|18分钟|争品|商品详情|【说明】请在充值时仔细核对要充值的|微信支付|759|12:00正在疯抢|TCL电视|【说明]请在 要充值的手机号码，如|因用户提供的手机号码有误导致充值失妙|搜索店铺|拍照搜同款|5G|11模型等乐高玩具|请d值-河南联通-100元|限量2件|吉吉|己饥己溺 丽丽|国美甄选大牌家电|￥419起|牌多功能锅|商品售完时未能拼单者视为抢购失败，将发起退款|充值|￥ 9.9 (100元话费)|充话费联通 搜索|限时秒杀|rT|订单编号200401-674815017892453 支付|我的订单|[品牌款式丰|预售 App专享|iPad air3|【品牌款式丰富，随机发货】摩飞等目|芒果|周日23:00开抢|领张5元无门槛|芒果10斤装|价格|)充值吗? 确认给手机号(139|精选推荐|拼多多|最近搜索|正在疯抢 即将开抢|奥等口红|待付款 全部|即可参与活动|客服：小美|款式品牌丰富，随机发货]vivo等品|更多搜索方式|0808|拼多多平台商户 雨户|[买5斤送5斤]越南玉芒新鲜 家装节|移动、联通、电信通用|首次充值确认|¥9.90|嫩玉米粒|立即评价|品牌|斐口红款等机械键盘|衣架裤夹|综合 销量|￥9.9￥101限量2件|越南芒果坏果包赔|距结束31:37:09|有线充电版 国行原封正品|万人团价|PQRS|或者短信延迟。因为线上充值，为此无法开发票。|冰糖心苹果|扫把单个|筛选|积极|激励金|美的全直流变频空|WXYZ|￥956已拼4.9万仟|100元|时:￥9.9(免运费)|×1|无门槛券|123|Pad Air 2019|客服|厨房用品|4G210/|16:22 21%0|08:37 87%|3:57|充值失败，将视为此|返回商家|共10款商品正在疯抢|9.9元 秒杀|话费快充|TUV|已抢2682件|收票|拼多多福利社>|i'li jin|1719万人团价|取消|￥4599 已拼1.1万件 未来官方旗般店|￥376|每周日23:00 每次2件|868|拼多多客服中心|进入APP搜索“立即领取|进入APP搜索“立即领取”|待分享|9块9秒杀|VA AAA7|A9|2PAA|中国联通|下载拼多号|买5斤送5斤|8EA|待发货|待评价|9N\", \"video_asr\": \"很多用户打来电话问拼多多九块九充一百元话费到底怎么冲，今天我就手把手教您打开拼多多，输入立即领取，进入九块九秒杀页面，找到一百元话费的商品页，在秒杀时段点击立即购买，确认您的手机号码，选择支付方式，充值成功就这么简单。|拼多多官方福利正在大批放量，百万的用户已经享受到优惠，无论您是联通，电信，移动都能参与。|也试试吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.0171816349029541 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '配音', '推广页', '静态', '中景', '场景-其他', '平静', '办公室', '喜悦', '特写', '单人口播', '工作职场', '拉近', '填充', '极端特写', '动态', '多人情景剧', '惊奇', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.93', '0.92', '0.73', '0.59', '0.43', '0.42', '0.40', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/080bcac6588f15ee731b6b3a6c05ef39.mp4\n",
      "{\"video_ocr\": \"哇有吃的|撃|这么好吃|讨厌你怎么全给我吃了|4看き|p音き|哈哈这个冰镇的太好吃啦|吃完整个人清凉了好多|喂 哪买的啊|别生气啦|我再给你买好不好|那你上拼多多|拼多多|在上面搜索这个|酸奶西米露罐头|发起拼雏|这也太贵了吧一罐就要21.8|你傻啊那个是一箱的价格|这里面一共有6罐|而这套6神泰人馕味|而且有6种不同的口味|一箱才21.8|拼多多也太便宜了吧|还包邮哇|赶紧买|巴拼66万件 桃爱蚕酸势西米缓头416镶整箱甜品新鲜水|级镇费焱额奶西米攥暖头416蹉整箱甜品新鲜水|挑壹佰酸奶西米露罐头41罐整箱甜品新鲜水 7天通换 48小时友货|获级您桃妻估酸奶费来蠢暖头4活雄整箱甜品新鲜水|21象|21.8 满售407件版你复￥26起|美售4件后恢复¥26起|0999|立即领取 在App打开>|在Ap开》|紫正在拼这个高品 RI0 1224|再售447件后恢皇￥26起|再售44件活族复轻2起|1人在拼争，可直接参与|价廉质优 还不快来看看|独购买|￥28|单脸购买|自选4罐联系客服|口味 A套餐6灌酸奶口味|10人在拼单，|Move cute anjmals|Angel 37分钟前五星好评|仟镇 西米|什锦|看号|是音号|?。号 舌号|酸奶黄桃2酸奶椰果2|love cute &iafandis|Move Cute aitnn|酸奶橘子3酸奶菠萝3|40T8|078|四米g|米东|可直接参与|好渴|不客气了|comiedlral|cariedrol|又现多多|查看更多|16:10|1A|去拼单|ime|V8|三马|确定\", \"video_asr\": \"嗯嗯。|嗯。|嗯嗯。|哎。|好。|嗯，这么好吃。|讨厌。|怎么全老实了，就用这个太老实赚准备人都妻凉了，老婆，哎，咋买了，别生气了嘛，我再给你买的啊，那你说拼多多拼多多，在上面搜索这个酸奶西米。|外套这也太贵了吧，一挂就二十一块八，一傻呀，那个是一箱的价格，这里面一共有六个，而且有六种不同的口味呢，一箱。|二十一块吧，真的座椅太便宜了，还包邮啊。|我可去买给你买去买。|啊。\"}\n",
      "multi-modal tagging model forward cost time: 0.017019271850585938 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '家', '推广页', '商品展示', '极端特写', '夫妻&恋人&相亲', '中景', '喜悦', '家庭伦理', '愤怒', '手机电脑录屏', '平静', '静态', '亲子', '婆媳', '才艺展示', '特写', '场景-其他', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/080dd117b70bab641d4f2752d55a2a3b.mp4\n",
      "{\"video_ocr\": \"你也太强了吧|怎么做到的|魔术挂衣架|这款衣架不仅防滑|而且横竖都能使用|挂头还能360度旋转|拼多多上拼团只要5.8|现在下载打开拼多多|就能找到它啦|你也快去试试吧|家里那么多衣服怎么办|能衣架航可以轻松搞定|子鼓|个装5.9元|TUV|¥6.8|拉\", \"video_asr\": \"太长了吧，怎么做到的魔术挂衣架，这款衣架不仅表面防滑，而且横竖都能使用，挂头还能三百六十度旋转，在拼多多拼团只要五块吧，现在下载打开拼多多搜索模式，怪一下就能找到它了，你也快去试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016312360763549805 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '单人口播', '手机电脑录屏', '配音', '静态', '喜悦', '场景-其他', '家', '多人情景剧', '办公室', '平静', '夫妻&恋人&相亲', '朋友&同事(平级)', '动态', '情景演绎', '室内', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.97', '0.85', '0.54', '0.40', '0.23', '0.17', '0.04', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0842586e745508b5b4394a0c4f01ddb1.mp4\n",
      "{\"video_ocr\": \"我不是故意摔坏手机的|我这个手机啊|是在拼多鸟九块九买的|现在啊还有活动|打开拼多多|搜索立即领取|咱呀九块九再买一个|咱呀九块九再买一 个pPa|限时活动抢购价9.9元|OIPO\", \"video_asr\": \"你根本就不懂我，你不说我怎么懂你啊，你和他不是归人，是玩手机，我这个手机啊是在拼多多九块九买的。|现在啊，还有活动，打开拼多多搜索立即领取，咱呀总会躲在某个区吗？\"}\n",
      "multi-modal tagging model forward cost time: 0.016535043716430664 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '手机电脑录屏', '静态', '喜悦', '特写', '愤怒', '惊奇', '朋友&同事(平级)', '悲伤', '动态', '夫妻&恋人&相亲', '家', '极端特写', '平静', '家庭伦理', '单人口播', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.68', '0.62', '0.52', '0.46', '0.22', '0.13', '0.12', '0.06', '0.03', '0.02', '0.02', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08446aba1b6e7d2e5dee25836cfed1e0.mp4\n",
      "{\"video_ocr\": \"叔叔|您的菜上齐了|哎等一下|你怎么不上学|跑这来端盘子来了|我爸说|反正我也考不上大学|还不如来店里|帮他打杂呢|老板|你怎么不让孩子上学呀|哎 我也不想呀|但是孩子成绩呀|一直提不上去|不如早点打工赚钱呢|孩子成绩提升不上去呀|是没找到正确的学习方法|来|我给你推荐一款|我家孩子一直在上的|这个课|高途课堂|高中全科名师班|里面的主讲名师啊|来自于每年都能输送|清华北大学子的|一线名校|他们中有|中高考研究专家|省级高考状元等|你家孩子听过这个课以后|效果怎么样啊|自从我家孩子听了|这个课以后啊|成绩是一点一点|从班级倒数|提升到了年级前十|你看|这是我家孩子的|录取通知书|是嘛|那这个课在哪报名啊|点击视频下方链接|可以报名啦|9元钱|就有16节课|而且课后啊|还有专属辅导老师|根据每一个孩子的|实际情况|进行详细的分析|一对一答疑|直到学通学透|轻松应对考试|贵重物品妥 如有丢失后|魔重物品妥善保 有丢失后果自|禁止吸烟|高中16节精讲直播课|iA线|炒面...11元|特色额乓|月色|维色店E|全国百佳教师带队教学 平均教龄11年|蒙养好味通|乐通8|福盈|激新发真我|精养好嗓j|情养好吸|亿年教酸|北京大学毕 湖北省高考|记年敬龄|ADMISSION NOTCE|券居特惠 499 3年回放|随时复习|高中4科|湖北省商考状元|本科生录取通知书|￥9|浙红卫视|重食类|高二|新用户专享 立即体验 浙江卫视指定在线教育品牌|名师班|名师特训班|答疑解惑随时复习|共16节课 答疑解惑 防时复习|共16节课 答疑解惑|答疑解惑|.18元，鸭头|18元鸭 .10元 炒年糕|11元 炒饭.|3年无限次回放复习|华少|.9元|炒年糕|.9元 妙面...11元|高途课堂特训营|名师有秘籍 领跑新学期|价格|你糕.1l元|12元|清北毕业名师团队授课 语数英物4科全面提升|1万1辅导|No Smoking.|上啸法短，我罚将对您客运的信件之外的内件物，动|价499-优惠券490 课程指导|=课程指导价499-优惠券490|..6元 秘制|.16元|争元|炒粉丝...1元 R|同平主件|局平王科|立省490元|小ph|到手价|元业龄|9.\", \"video_asr\": \"叔叔，您在大型哎，不是那怎么不上学，跑出来端盘子来了，我爸说反正我也考不上大学，还不如在店里帮忙打杂的。|老板，你怎么不让孩子上学？我也不想的还是孩子成绩啊，一下就容易，反正可以考上大学，如何解决孩子成绩提升不上去啊？是没找到正确的学习方法，你怎么能不让他上学呢？来指给你推荐一款我家孩子一直在上这个课，高中课堂，高中全科名师班里面的主讲名师啊，来自每年都能输送升华版。|把学子的一线名校，他们中有中高考研究专家，省级高考状元等，哎，你家孩子结婚这个课以后效果怎么样？自从我家孩子听到这个课以后啊，成绩是一点点从班级倒数提升到了年级前十，你看这是我家孩子的通知书，是吗？那这个课程你啊，点击视频下方链接就可以报名了，九元钱就有十六节课。|课后还有专属辅导老师根据每一个孩子的实际情况进行详细的分析，一对一答疑，直到学通学透，轻松应对考试。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016412734985351562 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '多人情景剧', '平静', '亲子', '全景', '单人口播', '室内', '极端特写', '愤怒', '手机电脑录屏', '教师(教授)', '家', '教辅材料', '家庭伦理', '学校', '特写', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.86', '0.61', '0.53', '0.50', '0.35', '0.20', '0.15', '0.13', '0.10', '0.10', '0.03', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/084b4d5cb4fed12310b0013ef7909036.mp4\n",
      "{\"video_ocr\": \"儿子|妈终于找到你了|谁|谁是你儿子啊|车上是我老板|大哥对不起|三年前我哥走后|我妈就成了这样了|您老板和我哥长得|实在是太像了|我妈又犯糊涂了|实在是对不起|停车|我不走了|我背您回家|咱们到家了|￥5.64|我去买点面|姑娘等等|手机给我|刷刷这个|这是什么啊|还有红包啊|这是快手极速版|刷刷视频就能赚钱|新人下载|最高可领56元现金红包|妈你看|我没事儿的时候|就可以刷刷视频补贴家用了|这软件从哪儿下载的呀|点击屏幕下方|就能下载了|我的零钱|论颜值的重要性|¥98.64|齐值|理理|e彤视指指豪 额值约账要度0生活对款下手了作|奖励金额以活动规则为准|16:48|零钱朋搬|常见包题\", \"video_asr\": \"儿子胆子大，中医治好的谁谁是你儿子车，这是我老板。|大哥。|他说不想，三年前我哥走后，我妈就成了这样了，老板和我哥长得实在是太像了。|我妈又犯糊涂了，实在是对不起。|嗯。|停车不嘛，我不走了，我陪你回家。|来。|哇，咱们到家了。|妈，我去买点面。|姑娘等等手机狗。|刷这个哇，这是什么啊？还有红包啊，这是快手极速版，刷刷视频就能赚钱，新人下载最高可领五十六元现金红包呢，你看我没事就可以刷刷视频补贴家用啊！|点击屏幕下方就能下载了。\"}\n",
      "multi-modal tagging model forward cost time: 0.022065162658691406 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '多人情景剧', '静态', '室外', '平静', '全景', '动态', '路人', '惊奇', '喜悦', '极端特写', '特写', '悲伤', '手机电脑录屏', '愤怒', '亲子', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.77', '0.77', '0.76', '0.74', '0.16', '0.08', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08582dedf8fad885b8ce7c0d4fd2e1df.mp4\n",
      "{\"video_ocr\": \"先生您好|我在后台看到您用9.9抢到了100元的话费|感谢您对拼多多的支持|我们这个活动只需要下载拼多多|首页搜索立即领取|查看更多 下拉就可以找到了|点击|点击视频下方链接下载拼多多|快来享受这个优惠吧|04限量2件 ¥9.9|¥9.9-+0限量2件 移动联通电信话费充值100元[3月6日发完|新人专享 10|DEF|价格:|¥23990 v1179回|移动、联通、电信通用|2位好友在拼多多购买了7件商品|秒杀万人团|【3月6|拼小圈|立即领取>|周日23:00开抢 即将开抢|TOV|21:00正在疯抢|限量特惠|充值心|更多预告|话费快光|9.9元秒杀|v526|￥239，4团人路|电信通用|v1179Mxm|100元|MNO 分阳 WXYZ|拼多多|PQRS|元|发完|确认|拼音\", \"video_asr\": \"喂，您好，我在后台看到有九块九抢到了一百元的话费，感谢您对拼多多的支持，我们这个活动只需要下载拼多多首页搜索立即领取。|点击查看更多，下拉就可以找到了，点击视频下方链接下载拼多多，快来享受这个优惠吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016477108001708984 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '静态', '平静', '手机电脑录屏', '推广页', '特写', '动态', '办公室', '场景-其他', '极端特写', '多人情景剧', '室内', '配音', '喜悦', '工作职场', '填充', '家', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.60', '0.50', '0.37', '0.15', '0.08', '0.07', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/085edcfc5f7e6252314e2a0ae5ca5c71.mp4\n",
      "{\"video_ocr\": \"冷导孩子功黑|工作太忙 没时间辅导孩子功课|最快一秘二对错 支持90%M拍|计算题检查|最后我们还有多版本 教材A1听写|赶紧点击下方链接 下载注册吧|成绩却不太理想|孩子上小学|数学语文作业拍一拍 自动批改校对答案|错题本生成定制练习|举一三、无|举一反、定向提升|大力爱辅导app 帮您全搞定|免费批改|不仅如此|以下三种人一定要下载|错题还能自动加入|下班回家不用再 费心劳神的教导孩子|一键扫描|前复习半笞|考前复习事半功倍|并且现在所有功能 限时全免费|检查作业却太费神|更有点读功能 高效辅导孩子学习|1-6年级语文数学英语|键拍照智能批改|大力爱辅导|I0|2\", \"video_asr\": \"以下三种人一定要下载大力爱辅导APP一孩子上小学成绩却不太理想。|二，工作太忙，没时间辅导孩子功课。三，有时间辅导孩子功课，检查作业却太费神，大力爱辅导APP数学语文作业。|自动批改校对答案，一键扫描，最快一秒查对错，支持百分之九十以上的计算题检查，不仅如此。|做题还能自动加入错题本生成，定制练习，举一反三，印象提升，考前复习事半功倍。最后，我们还有多版本教材AI风险。|更有点读功能，高效辅导孩子学习，下班回家不用再费心劳神的教导孩子大力爱辅导APP帮你全搞定！|并且现在所有功能限时全免费，赶紧点击下方链接下载注册吧！|今天。\"}\n",
      "multi-modal tagging model forward cost time: 0.016629457473754883 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '平静', '配音', '中景', '混剪', '动态', '特写', '场景-其他', '远景', '静态', '幻灯片轮播', '室内', '情景演绎', '城市景观', '影棚幕布', '动画', '全景', '喜悦'], 'scores': ['1.00', '1.00', '0.97', '0.93', '0.91', '0.67', '0.62', '0.44', '0.02', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/086387db901077a833292b4041a21291.mp4\n",
      "{\"video_ocr\": \"皇上。越南的芒果到货了|把她拉出去，斩了!|为什么呀???|这么好吃的芒果|为什么不早点给我买|好大！好嫩!好多汁！|皇上，饶了我吧|我马上去加一单还不行吗？|贵不贵呀?|比水果店卖得还便宜呢!|快下单！不然饶不了你|现在点击视频下方|在拼多多下单|不但领券立减2元|还包邮呢!|下载拼多多 领券减2元包邮到家|领券下单 专供超市品质|NC 4秒前给出五星好评|领券 2元无门槛商品券|已拼10万+件 小编推荐 【越南青芒】进口当季新鲜芒果水果玉 芒大青芒果批发2/3/5/10斤装坏果包赔|¥11.13起￥39.8|吃吃喝喝|晶喝|1嗨|吃|急速发货|立减2元\", \"video_asr\": \"皇上为啥在芒果到货了。|把他拉出去斩了为什么呀。|这么好吃的芒果，为什么不早点给我买？好大好嫩好多汁！|往上涨了我吧，我马上去，下一个还不行吗？贵呀，比水果店卖的还便宜呢，快上班无聊无聊咩，现在点击视频下方在拼多多下单不带你去立减两元还包邮呢！\"}\n",
      "multi-modal tagging model forward cost time: 0.016070127487182617 sec\n",
      "{'result': [{'labels': ['推广页', '静态', '中景', '现代', '多人情景剧', '家', '喜悦', '特写', '极端特写', '手机电脑录屏', '惊奇', '宫格', '配音', '情景演绎', '古装/武侠', '转场', '场景-其他', '动态', '全景', '影棚幕布'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.82', '0.74', '0.12', '0.11', '0.04', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0877b1482d2040ac488d04afd302f1f3.mp4\n",
      "{\"video_ocr\": \"林凡|等下面试要加油哦|小雪|魏宇|哇|这么多年不见|还是这么美|这位是|这是我老公|就是那个|在家混吃混喝|等死的那个|没用鬼|请注意你的用词|我的老公怎么样用不着你管吧|可我已经很努力了|还面试啊|这么大的公司|你配吗|清洁工你都不配|3T2A6|少爷|家族对你的限制已经解除|这是您的至尊黑卡|从现在开始|您可以动用家族的一切资产|林凡你又演什么戏啊|闲杂人等|需要解决吗|别伤害她|五分钟之内|收购魏氏|.312.6|间历|点击下方链接阅读精彩后续|求职|《最佳女婿》|免费小说|华云|丝看|纱后|现金|MA5TP8|务长\", \"video_asr\": \"嗯嗯。|林凡等一下面试要加油哦。|小雪位于哇，这么多年不见还这么美，这是为这是我老公。|哦，那在家混吃混喝等死的那个用。|慧宇，请注意你的用词，我的老公怎么样，你找你玩的。|可我已经很努力了。|认识你这么多的东西太多。|手机。|姐姐。|少爷家族对您的限制已经解除，这是您的至尊。|从现在开始，您可以动用家族。|一切资产。|把你有联系啊。|家人等需要解决吗？你伤害他。|五分钟之内说话为什。|好。\"}\n",
      "multi-modal tagging model forward cost time: 0.01673603057861328 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '动态', '室外', '惊奇', '特写', '朋友&同事(平级)', '喜悦', '极端特写', '悲伤', '夫妻&恋人&相亲', '全景', '平静', '手机电脑录屏', '愤怒', '厌恶', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.90', '0.85', '0.75', '0.60', '0.31', '0.15', '0.12', '0.08', '0.06', '0.05', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/087a0f2e7eb9a3cc573f479389623cda.mp4\n",
      "{\"video_ocr\": \"哟|这不是白家的私生女吗|童轻颜你够了|姓白的|看我今天怎么弄死你|住手|段哥哥|你怎么来了|白神医|你没事吧|没事|现在|给你一个救你爷爷的机会|童轻颜|你知道你惹了谁吗|你不要相信她|她是骗子|断绝和童家的一切来往|封杀童家在海城的所有产业|三日之内|让童家滚出海城|还请白神医救救我爷爷|点击视频下方 下载QQ阅读 海量原著 想读就读|月著想就读|海 风暑想读就读|海量原道有 读|海量原著想读就读|最原美 读就读|海量著想请|海量厅陪想禊就镇|联龙烧|老祖宗她又美又飒|QQ阅读|O0门读|小香摧烤肉馆|猪烤肉馆|请勿倚靠|唤@P>|00000|请勿|G0\", \"video_asr\": \"哟，这不是白家的私生女吗？恭喜你，够了你。|你姓白呢，看我今天怎么弄死他，你怎么出手。|大哥哥你怎么来了。|暗沉一你没事吧，没事。|现在给你一个姓余的机会同情你，你知道你惹了谁吗？段哥哥，你不要相信他，他是骗子，断绝与洪家的一切来通杀通家在海城的所有产品。|三日之内让童佳关注海盛世才，请来神医救救。|点击视频下方下载QQ阅读海量原著，想读就！\"}\n",
      "multi-modal tagging model forward cost time: 0.016339778900146484 sec\n",
      "{'result': [{'labels': ['多人情景剧', '中景', '现代', '推广页', '特写', '静态', '动态', '愤怒', '惊奇', '夫妻&恋人&相亲', '拉近', '喜悦', '极端特写', '平静', '室外', '情景演绎', '朋友&同事(平级)', '配音', '悲伤', '填充'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.82', '0.58', '0.44', '0.16', '0.13', '0.10', '0.08', '0.06', '0.06', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/087b7df1daa461337f6ec3c77ea34e93.mp4\n",
      "{\"video_ocr\": \"One two|three four five|诶!青青妈妈|恭喜你家青青英语|又考全班第一啊|怎么教育的啊|还不是因为|我给孩子报了|作业帮直播课|那是什么?|是专业教育机构|国内外毕业名师|带队授课|课后还有辅导老师|1对1答疑呢|师资这么厉害|而且课程好记易懂|这不|孩子背单词的效率|高多了|语法知识点|也轻松掌握|赶紧告诉我|怎么报名吧|点击视频下方链接|就能报名|限时9元13课时|上课内容与收到礼盒以实际为准|8大单词记忆法/趣味语法口诀|英语名师课 抢|单词本 成长笔记|经典易错100题|单词宝典*|3元|郑重点|致直播课 的我|wow!|Mow\", \"video_asr\": \"为亲情妈妈恭喜人家身心英语又考了全班第一啊，怎么教育的呀，哎呀，还不是因为我给孩子报了作业帮直播课吗？那是什么作业帮直播课啊？是专业教育机构，国内外毕业名师带队授课，课后还有辅导老师啊，一对一答疑呢，狮子这么厉害啊，而且啊，课程好记易懂。|这不，孩子背单词的效率高多了，语法知识点也轻松掌握了，赶紧告诉我怎么报名吧！点击视频下方链接就能报名四十九元十三课时，赶快抢课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016787052154541016 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '喜悦', '填充', '亲子', '室外', '全景', '平静', '动态', '惊奇', '单人口播', '特写', '配音', '愤怒', '朋友&同事(平级)', '路人', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.94', '0.90', '0.87', '0.41', '0.16', '0.07', '0.04', '0.03', '0.03', '0.03', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0886de513cb0f7fecae61f8324db723c.mp4\n",
      "{\"video_ocr\": \"女|女生睡的床|你老公睡的床|你到底是怎么睡的|唉老婆你别生气啊|你看|这是我新买的四件套|全棉面料亲肤透气|手感温糯细腻|不褪色、不缩水|而且还是AB双版设计|你看这两面的花纹多好看|你又乱花钱|我没有|这个四件套是我在拼多多上买的|才9块9|下载并打开拼多多|搜索立即领取|点击查看更多|就能找到这款纯棉四件套啦|那还不多买几套|新人礼包 爆款男士中简袜|立即领取 立即 里 里脊 李痢疾|[款式丰富，随机发货]四件套[11月7 日发完]|商品详情|立即领取抢购99年机|[品牌款式丰富，腾机发货]凤凰山地|能榨汁机[11月7日发完]|限量2件 1月03日23.00开始 9.9|立即领取9.9 立即领取抢购|[品牌款式丰富，随机发货]|无定金不用等|确认|空格 搜索|床单式|款式|顶部|好物狂欢盼你买YA|Q月ilingq|立即领I|新一代印染技术让缩水率更低|¥天丹回固白|朵天#固固白|亲肤透气|气垫床龙|缩水率低|*限时活动抢购价9.9元|22:41|购买同款 拼多多|V410|下载拼多多\", \"video_asr\": \"女生对着玩呢，你老公说的吧，你到底是怎么说的？老婆你别生气啊，你看这是我新买的新的。|全面面料亲肤，手感温润细腻，服帖色不缩水，而且还是AB双板设计，你看这两边的花纹多好看，没有文化啊，我没有这四件套是我在拼多多上买的，才九块九哦。|下载并打开拼多多搜索立即领取，点击查看更多就能找到这款纯棉四件套了那么多粉丝！\"}\n",
      "multi-modal tagging model forward cost time: 0.016654253005981445 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '家', '手机电脑录屏', '喜悦', '夫妻&恋人&相亲', '特写', '惊奇', '动态', '平静', '极端特写', '愤怒', '悲伤', '配音', '朋友&同事(平级)', '单人口播', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.76', '0.65', '0.50', '0.36', '0.35', '0.17', '0.10', '0.09', '0.09', '0.06', '0.03', '0.03', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0890815076f896e1f620e36fef04a3cc.mp4\n",
      "{\"video_ocr\": \"妈你听我解释|起来起来|妈我真的没作弊|你没作弊|那你告诉我|这次考试|LA 阅读理解全对|IL|作文就扣2分|两周以前 你还没达到班级平均分呢|好了乔乔别哭了|哎呀乔乔妈|你这是干什么|你看把孩子吓的|我每天放学都和晨晨|一起听|作业帮直播课的|小初语文 阅读写作提分班|都讲得可好了|编 接着编|阿姨|乔乔没有骗您|都是清北毕业名师 带队教学|课后还有 班主任一对一答疑|这是真的吗|是啊|晨晨这次也被老师 表扬进步很大呢|这门课程专门针对|语文成绩不好|的孩子设计的|讲解了八大阅读技巧|和十大写作手法|而且课程三年内|都可以无限次回放呢|这个课这么好|是不是很贵啊|13节课才8元|现在报名还送 教辅大礼包呢|妈妈对不起你啊|那我们现在就去报名|好不好|视频为演绎精节|上课内容与收到礼盒请以实际为准|清华北大毕业名师带队教学 全国包邮送教辅大礼包|小初语文阅读写作提分班|快速提升阅读写作能力|中国国家女子排球队官方教育品牌|中国女排|13节重难点提分课|-ILA|F|OOto|anlto|立即报名|PIU|rW|特惠\", \"video_asr\": \"你怎么。|我真的买错的。|你没作弊，那你告诉我，这次考试阅读理解全对，做完就够了，这两个月前你还没有达到班级平均分了。|哎呀，小小妈，你这是干什么？快把孩子吓得我每天放学都会去再一起拼作业，帮直播课的小公司一个小笨蛋都假的可好了，别家那边阿姨一桥上没有骗你，都是清北毕业名师带队教学。|哥，还有班主任一对一答疑是真的吗？是啊，成名这一次也被老师表扬，进步很大呢。这个课程专门针对语文成绩不好，阅读写作拉分的孩子设计的，想写了八大阅读技巧和十大写作手法，而且课程三年内都可以无限次回放呢，这课这么好，很贵呀，十三节课才八元。|现在报名还送教辅大礼包呢，妈妈对不起你啊，那我们现在就去报名好不好？点击视频下方链接就可以报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01657390594482422 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '推广页', '中景', '亲子', '静态', '室外', '喜悦', '平静', '动态', '特写', '全景', '朋友&同事(平级)', '愤怒', '路人', '悲伤', '惊奇', '家庭伦理', '单人口播', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.63', '0.56', '0.41', '0.32', '0.10', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0891c58948f721a387cd519715940e91.mp4\n",
      "{\"video_ocr\": \"要不你在拼多多上看看|这里的包包很便宜|最后天|每个都才10块钱不到呢|不到呢|而且款式很多搭配衣服超级容易|最适合你这种预算不多又喜欢新鲜的人啦|ins超火上新帆布休闲斜挎小包 帆布小包包女2019新款可爱搞怪 买过的店 极速退款|拼手速|4人在拼单，可直接参与 查看全部|去拼 剩余23:58:14.8|卡通小包包女包新款2019可爱少|¥8.8|女链条单肩斜挎包网红个性小圆|谦谦71...宝藏|1チ了|帆布包女斜挎个性女包包新款20 购买过|立减5元 品牌特卖|已拼4.5万件|商品评价(34299)|商品评价(26053)|艾草、网友|插队拼单|Bermo|¥8.8起￥45|07:17:59后恢复￥9.9起|￥48|v10.8|2.9折|ins超火上新帆布休闲斜挎小包女2019新款韩版百搭网红 全场包邮7天退换·48小时发货假一赔十|帆布包女斜持个性女包包新款2019网红小包chic链条搞怪|甜美迷你小包包斜持包女学生韩版百搭可爱毛绒包包秋冬 >|50元券|剩余2358:59成 去拼|已拼7086件|还差1人拼成 剩余23:46:22.5|16:41 综合 销量|热门 男装 女装 运动 鞋包 电器 食品 水果|商品证价(40434)|下载拼多多APP购买同款|全场低至￥19.9|暴天 个人中心|sontbetruste tistotjok,|价格|拼多多好物推荐榜|9块9特卖|断码清仓|限时秒杀|拼多多\", \"video_asr\": \"要不你在拼多多上看看，这里的包包很便宜，每个都才十块钱不到呢。|而且款式很多，搭配衣服超级油腻，最适合你这种预算不多又喜欢新鲜的人了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01643848419189453 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '平静', '配音', '手机电脑录屏', '静态', '场景-其他', '动态', '单人口播', '喜悦', '填充', '情景演绎', '特写', '混剪', '室外', '全景', '朋友&同事(平级)', '惊奇', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.94', '0.49', '0.39', '0.27', '0.21', '0.15', '0.06', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/089c63c3e8015672b15f1e223fe2dbd9.mp4\n",
      "{\"video_ocr\": \"71+26-18等于几|79|8|57|32-17+56-9等于几|5岁孩子快速算出100以内的加减法|16|你的孩子能做到吗?|实只要掌握了数学思维|你的孩子一样可以做到|建议各位2~8岁的孩子家长|赶快给孩子报名一个|斑马A课思维体验课|课程专为2~8岁的孩子设计|通过动画儿歌游戏等孩子感兴趣的形式|将抽象的数理思维数学知识|以具象化的形式展现出来|每天只需15分钟轻轻松松培养孩子|包括推理运算想象记忆|在内的9大思维能力|为小初高的学习打下坚实的基础|现在点击视频下方报名|仅需49元10节课|还包邮赠送教具礼盒|赶快报名助力你的孩子成为学霸吧|i+2 ihaO+Se|的七 以utJ（a宝s|学思维 学英语2-8岁上斑马|猿辅导在线教肓 出品|ethamphetam|Methamghe|猿输导任线教育出品|PEPM|P季 PPh|P f PPn V=|2Sa|MK|(AHo toHcS|g - Cu|t“I|Lq-Cu 世heqpX XotKt|e2 -CU|FfQo ZReqci-Xh|2ED|m.-老 wxv I艺|320to|S- x ie vt|a+2阳赏+a一4 s|Ky BM|CV Is Iueine|HgC tOA|8岁上斑马|斑马A|课|斑马7 l课|斑马А 果|2.8|3。|ctiOn=(aWe2|tion -lAlee|NO Kv. g…cu|Hongcystei|Keactie ve毫 Enve Ve|H点|bCt|VaEea|v-tCso|tcu.wxetvt VC|V22|三元 去H|k2B.|Mctr|HCe|ActOn|1PACt Oh|muK|会L你H款|\\\"pheto|429|o0|‘Oo.O、|V=t xx2vt VCr 众手|以份-停|三x-Xoニ.V|Melatonn|-C|v xxevt VC|SHw:so  vli|V-Ct tD|tG/. he|cCu|C HG6VQ|-E4|f-E|rxEvt Ve|-cu|g·Cu|V一W|NH，|ON|Ho,o|9、|X，|F-|CHg|A2|:4|Ig\", \"video_asr\": \"七十一加二十六减十八等于几。|七十九四十九键，二十六减十八等于几五七，三十二减十七，家五十六减九等于几三十二。五岁孩子快速算出一百以内的加减法，你的孩子能做到吗？其实只要掌握了数学思维，你。|孩子一样可以做到，建议各位二到八岁的孩子家长赶快给孩子报名一个斑马AI科思维体验课课程，专为二到八岁的孩子设计。|通过动画，儿歌，游戏等孩子感兴趣的形式，将抽象的数理思维，数学知识以具像画的形式展现出来，每天只需十五分钟。|轻松松培养孩子包括推理，运算，想象，记忆在内的九大思维能力，为小初高的学习打下坚实的基础。|现在点击视频下方报名仅需四十九元，十节课还包邮赠送教具礼盒，赶快报名，助力你的孩子成为学霸吧！|好。\"}\n",
      "multi-modal tagging model forward cost time: 0.016070842742919922 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '静态', '推广页', '平静', '影棚幕布', '室内', '特写', '配音', '喜悦', '教师(教授)', '情景演绎', '拉近', '动态', '全景', '单人情景剧', '手写解题', '办公室', '远景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.04', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/089dd3cce10a427e40eb86b57c90a27e.mp4\n",
      "{\"video_ocr\": \"我竟然重生了 O-O|只要臣可以跟公主和离|QO|臣愿意一死|是因为苏凝雪?|我今天誓要跟你和离|那我今天就成全你们|公主!|顾将军求见|(本故事纯属虚构) 免费看书100年|七猫免费小说|《腹黑将军喜当爹》|七猫免费小说APP|0.o|休書\", \"video_asr\": \"我竟然重生，只要臣可以跟公主和离，臣愿意一死，因为说留学。|我今日誓要跟你和离，我今天成全你们公主顾将军，求见顾将军。\"}\n",
      "multi-modal tagging model forward cost time: 0.016147851943969727 sec\n",
      "{'result': [{'labels': ['推广页', '古装/武侠', '古代', '中景', '平静', '多人情景剧', '上下级', '工作职场', '特写', '愤怒', '宫格', '家庭伦理', '室外', '静态', '惊奇', '悲伤', '动态', '喜悦', '极端特写', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.04', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/089ff365ed28763eb8ac5ac9dc6491eb.mp4\n",
      "{\"video_ocr\": \"数学怎么这么难啊|猿辅导|小学重难点特训班|49元14天课|2. 一个等腰直角三角形，它其中的一条直角边长 7cm，这个三角形的面积是((24|极一最日很团一御转树，每隔3米种一棵，共种了5棵树，|A.|课了，效迎大家来到第一讲领程|课了，欢班大家果指第一满证月|华，那么|高的一半，那么 4.(导学号|A本4|А.甲大|6(导学事U9sNoo)yt门 分阱腿千行四边形的两备边上的 7.(导学梦：f9s4110|6(导学影L9swNo9) 有图中。人B分别是了行四边形的两条边上的 中点，阴影都分面机占平行阳边思面科的|B.川b C.Y|С.将| ) om2|填空。(8题7分，其亲母l！个 1.下面图形中、是轴对称图形的有 口a|填空。(8题7分，其余每题2分，共21分) 1.下面图形中，是轴对称图形的有(口山日ゆ)，请画出它们的对称轴。|原术三角用的面我屠()cm|四、面积计算 选择|1.选择|小学数学计算能力特训班|掌 握б类巧算法 计算又快又准|底的1.5倍，上底是く4)分米，下底是(【 )分米。|7(手评特：19salo) 一个三角心原束的底是 I4cm.如果将底增加 2cm，面积就增加|0)cm 5.一个知形租 个平行四边形同底，且面积相等，已知三角形的高是3.4dm，则平行四 边形的高是（6 dm|3.(导学号:11954108 一个梯形的面积是65平方分米，高是6.5分米，它的下底是上 4.面积相等的两个行四边形，一个底是 I5cm，高是80cm，另一个底是12cm，高是|立即报名|石|小学数学|D.两|cm.|做606|得分|等底|用同|70|三图\", \"video_asr\": \"数学怎么这么难呀。|猿辅导小学重难点特训班四十九元，十四天课。\"}\n",
      "multi-modal tagging model forward cost time: 0.01629924774169922 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '室内', '极端特写', '单人口播', '场景-其他', '特写', '平静', '教辅材料', '配音', '动态', '情景演绎', '手机电脑录屏', '商品展示', '才艺展示', '全景', '动画', '拉近'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.63', '0.40', '0.24', '0.21', '0.18', '0.15', '0.10', '0.06', '0.04', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08a0c4e1760c506a3dd3977f5bc4a9b5.mp4\n",
      "{\"video_ocr\": \"从现在开始|大家努力学习数学3个月|能提高多少分|30分?|很可能的结果是|一分不提|很多同学平常在数学上|下了很大的功夫|做了很多的题目|努力到感动自己|结果考试成绩一塌糊涂|道理很简单|你的头脑中|没有方法思维的输入|就不可能有成绩的输出|你的努力|只不过是多做了几道题而已|一道题|因为你根本就没有理解|题目的原理和本质|其实数学解题是有模型的|没有模型出题人是无法命题的|模型学会了|自然就会解题了|我是高中数学首席主讲老师|殷方展|擅长通过解题模型 来解决数学问题|现在加入我的数学提分特训营|解决你的数学困惑|15节高中全科直播课|只需3元|想要快速提分|就赶紧来抢它|3元领取 点击查看详情报名|跟准学高中数学抢分名师|融囍鞍粉盥|以上高中数学教学经验|只不过是多做了几道题而已 只不过是多做了几道题而已|只不过是多做了几道题而已 不会因为你多做了几遍就会了|s融验|\\\\键孽融辫粉验|指导价|一线名师解析数学提分干货|一|跟谁学|在线学习更高效|高中全科培优特训营 掌握高效学习法，快人一步成黑马!|数学重难点题型高分突破 英语 单词+语法高效记忆|现15节课仅需3元|金牌讲师|12090元|十跟\", \"video_asr\": \"从现在开始，大家努力学习数学，三个月能提高多少分？三十分，五十分不，很有可能的，结果是一分不提。|很多同学平常在数学上下了很大的功夫，做了很多的题目，努力到感动自己，只要到考试成绩一塌糊涂，道理很简单，你的头脑中没有方法思维的输入，就不可能有成绩的输出，你的努力只不过是多做了几道题而已，一道题不会因为你多做了几遍就会了，因为你根本就没有理解题目的原理和本质。其实数学解题是有模型的。|没有模型出题人是无法命题的，模型学会了自然就会解题了。我是跟谁学？高中数学首席主讲老师殷方展。|擅长通过解题模型来解决数学问题，现在加入我的数学提分特训营，解决你的数学困惑，十五节高中全科直播课，只需三元，想要快速提分？|就赶紧来抢它。\"}\n",
      "multi-modal tagging model forward cost time: 0.016590356826782227 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '单人口播', '静态', '室内', '教师(教授)', '平静', '配音', '特写', '场景-其他', '宫格', '极端特写', '动态', '课件展示', '教辅材料', '办公室', '情景演绎', '多人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.33', '0.10', '0.05', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08a0c813cec3ea2fc728e32ce984afe5.mp4\n",
      "{\"video_ocr\": \"王夫人|给孩子报什么19块钱的班|自己天天穿成这样|就是|有这钱|不如给孩子报个好点补习班|阿姨们好|乐乐也来了啊|这次全班就你一个人优秀|也不知道怎么做的|这次考试很多题|我都能直接写答案呢|小孩子可不能撒谎呦|52×11等于多少呀|572|那9999 ×8呢|89七十二放两边|er1a 3个9放中间|等于79992|这孩子怎么变这么厉害啊|清北网校数学秋季特训班|教的啊|老师专门针对孩子|做题没思路|计算慢|总结了解题大招和技巧|这些经常考到的|计算题鸡兔同笼|图形计数这些应用题|老师都总结的明明白白的|这师资力量怎么样啊|清华北大毕业名师|带队授课|平均教龄8年|课后还有辅导老师|1对1答疑呢|那在哪能报名呢|元|此视频为演绎情节|渍北网校 小初年级|86700Gt|867530c|8675|清北毕业名师带队授课|精装样板村间|秋季数学特训班|1凯开|1a凯|a1aEL|新月19|清北\", \"video_asr\": \"错了。|王夫人，哎，你给孩子报什么十九块钱的包，字母偏偏穿成这样，就是有这钱，不如给孩子报的好点的，不喜欢阿姨们好乐乐也来了，这次前面就你一个人优秀，也不知道怎么做的。|这个考试很多题我都能。|你爹写的呢，小孩子可不能撒谎哟，五十二乘十一等于多少呀？五百七十二，那九千九百九十九十八吗？|十二放两边，三个九放中间，等于PY九千九百九十二，这孩子怎么变这么厉害啊，清北网校数学秋季特训班教的呀，老师专门针对孩子做题没思路，计算慢，总结了解题大招和技巧，并且经常考到的计算题，鸡兔同笼，图形技术，这些应用题，老师都总结的明明白白的，这师资力量怎么样啊？清华北大毕业名师带队授课平均。|零八年课后还有辅导老师一对一答疑呢，那在哪能报名呢？点击视频下方就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016350269317626953 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '现代', '多人情景剧', '静态', '喜悦', '亲子', '动态', '惊奇', '平静', '特写', '室外', '全景', '拉近', '悲伤', '夫妻&恋人&相亲', '家庭伦理', '路人', '极端特写', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.92', '0.79', '0.67', '0.55', '0.25', '0.17', '0.12', '0.10', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08a1d0532df0468a64c9f7b7cc4d3b26.mp4\n",
      "{\"video_ocr\": \"财课，理财技巧实操，送完为|喂老公|我在这看到子言了|他在这捡瓶子|你看错了吧|你们这么多年兄弟了|我怎么能看错呢|你快过来看看吧|子言?|不是不是|你这是怎么了|出什么事了|我爸病了|我把公司给卖了|我..|不想麻烦兄弟|你把我当兄弟|你就这么躲着我|出这么大事|你都不告诉我|我这有张卡|你先拿着应个急|把你手机给我|我给你报名了一个|快财商学院|你在上面好好学习理财|每天打开手机|就会有额外收人|这不会是骗人的吧|兄弟我能骗你吗|我身边好多人|都在用它理财|收入不比工资低|放心|有什么困难|兄弟陪你一起|这个报名费贵不贵啊|点击视频下方链接|0元就可以免费报名了|快财|投资有风险，选择需谨慎，.风险责任由购买者自行承担|投资有风险，选择需谨慎， ，风险责任由购买者自行承担|改变从现在开始，免费6天理|止!|直播 学理财上快财|查看详情|京|AE\", \"video_asr\": \"没老公，我在这看到子言啦，他在这捡瓶子，你看错了吧，你们这么多年兄弟了，我怎么能看错呢？你快过来看看吧。|哼。|子言。|哎，不是，你这是怎么了？出什么事了？我我爸病啦，我把公司给卖了，我不想麻烦兄弟，你把我当兄弟，你就这么躲着我啊，出这么大事你都不告诉我，我这有张卡，你先拿着应急。|把手机给我。|我给你报名了一个快财商学院，你在上面好好学学理财，每天打开手机就会有额外收入，这不会是骗人的吧？兄弟，我能骗你。|我身边好多人都在用它理财，收入不比工资低，放心有什么困难啊？兄弟陪你一起，兄弟陪你一起，这个报名费贵不贵啊？点击视频下方链接，零元就可以免费报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01612710952758789 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '填充', '推广页', '全景', '悲伤', '静态', '朋友&同事(平级)', '夫妻&恋人&相亲', '喜悦', '动态', '远景', '手机电脑录屏', '平静', '(马路边的)人行道', '路人', '惊奇', '混剪', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.91', '0.90', '0.82', '0.80', '0.73', '0.29', '0.28', '0.20', '0.15', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08a6287eb7ecaaead384d28cf4305066.mp4\n",
      "{\"video_ocr\": \"张老师|不知道你对我们家阳阳 是不是有什么意见|阳阳妈妈|阳阳这些题目明明都是做对了|为什么给批了0分|现在啊都是电脑阅卷|阳阳这是吃了字体的亏|你看|这是阳阳同桌彤彤的试卷|那是因为她妈妈给她报名了河小象|什么|是河小象少儿硬笔书法练字课|上面都是名师教学一对一在线辅导|每天在家拿着手机电脑就能学习|从笔画入门到单字到成语 到古诗组成|课程编排科学有效|而且原来199块钱的 现在0元就能购买|我说彤彩的字怎么进步这么快|免费的|在哪购买呀|现在点击视频下方链接就能购买|快点击屏幕下方报名吧!|(0分钟 秘100分)|9.在括号里填上合适的数|期末真题训练|德高为师，身正为范！用心做好每一节课|ti)ws W.wy|(2)700,670，640,(60 8.用608这三个数缴成体|二、选择题。(每题只有一个正用售菜 1.只读一个零的数是|读作:(二千二百零一)|清在图上画一画， A.5360 B.840|个 10.用一张长12陛着复》爆着有用说 (8)厘米|10.用一张长12厘米、宽8厘米的长) （？)厘米。|免费领取名师写字课| )，它们相发(|提升孩子写字水平|GRAZY格子|河小象|7.按规律旗。|(1)385,390，395，|写作:(229|(602\", \"video_asr\": \"青岛学习张老师，不知道你对我们再杨洋是不是有什么意见啊，杨颖呢？|杨洋这题目明明都是做对的，为什么得被零封？|哦，阳阳妈妈现在呀，都是电脑阅卷，杨洋只是吃了自己的亏，你看这是洋洋童和彤彤的试卷分析。|妈妈给他报名了河小象。|什么啊？适合小像少儿硬笔书法练字课，上面都是名师教学，一对一在线辅导，每天在家拿着手机，电脑啊就能学习，从笔画入门，到单字，到成语，到古诗组成，课程编排科学有效，而且原来一百九十九块钱的，现在一元就能购买哦，我说你能不能四怎么进步这么快？|免费的唉，那这个河小象少儿硬笔书法练字课在哪购买呀？现在点击视频下方链接就能购买。\"}\n",
      "multi-modal tagging model forward cost time: 0.016299724578857422 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '推广页', '中景', '静态', '亲子', '特写', '悲伤', '平静', '喜悦', '惊奇', '愤怒', '手机电脑录屏', '家庭伦理', '夫妻&恋人&相亲', '朋友&同事(平级)', '家', '极端特写', '路人', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.96', '0.84', '0.44', '0.41', '0.32', '0.24', '0.22', '0.15', '0.15', '0.14', '0.06', '0.03', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08ab07de3a40398e47b98cb980269245.mp4\n",
      "{\"video_ocr\": \"不管基础怎样|刘颖妮中国人民大学硕士 不管之前成绩如何|只要按照我的要求|都有可能|那我怎么做呢|我不会教孩子常规 的解题方法|而会先用一些 讨巧的解题技巧|能够让他很快的做出|但是注意 这个时候|孩子的基础其实并不扎实|不过 他已经有了|而不是像以前一样|看到数学题 看到难题|就本能的去抵触|刘颖妮 之后 我会引导孩子|落实具体的书写过程|理解题目的答题逻辑|我是作业帮直播课|教师刘颖妮|具有一线教龄12年|听听我的方法吧|30元18节课|点击视频下方链接报名吧|立即报名＞|一些难题和压轴题|数学解决问题的兴趣|一步一步的掌握|初中数学差的孩子 一定要看|上课内容与收到礼盒请以实际为准|*赠送12件套教辅礼盒|国际注册教师资格|国际|小初高数学 名师提分班|目人民大学硕士|中|小学数学公式 30 DAYS|草稿本 成长笔记|致直福课|21天\", \"video_asr\": \"初中数学差的孩子一定要看，不管基础怎样，不管之前成绩如何，只要按照我的要求都有可能，那我怎么做呢？我不会教孩子常规的解题方法，而会先用一些讨巧的解题技巧，能够让他很快的做出一些难题和压轴题。|但是注意这个时候，孩子的基础其实并不扎实，不过他已经有了数学解决问题的兴趣，而不是像以前一样。|道数学题，看到难题就本能的去抵触，之后我会引导孩子一步一步的掌握，落实具体的书写过程，理解题目的答题逻辑。|我是作业帮直播课教师刘颖妮，具有一线教龄十二年，快来作业帮直播课听听我的方法吧，三十元，十八节课。|点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015928983688354492 sec\n",
      "{'result': [{'labels': ['中景', '现代', '单人口播', '推广页', '静态', '平静', '室内', '场景-其他', '动态', '喜悦', '配音', '多人情景剧', '教辅材料', '室外', '宫格', '过渡页', '全景', '路人', '转场', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.72', '0.22', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08b02ab8da211c71d59db7c7e08f5d43.mp4\n",
      "{\"video_ocr\": \"小朋友 你看|废墟|Fish （鱼)|你看|扫二|ShelI (贝壳)|大姐|你的英语 也太烂了吧|赶紧走人吧|阿姨|你小时候没有用过|腾讯开心鼠英语ABCmouse|纠正英语发音吗?|我知道啊|是专为你们|3-8岁孩子 设计的|英语启蒙课嘛|还能学发音吗?|当然可以|北美外教 纯正发音|线上有|8000多个 互动学习内容|线下还 免费赠送|配套教材礼盒| × 和点读笔|孩子可以跟读练习|轻轻一点|让孩子说一回|现在|才49.9元|赶紧给你的孩子 报名吧|省得你那蹩脚的英语|耽误了孩子|你说得对|在哪可以报名啊?|点击视频下方链接|就可以报名啦|体验课|S1|体|太脸课|超值教材大礼包 包邮 老师专业全程辅导|15节趣味英语课|英话|启蒙英语，就上腾讯开心鼠|腾讯开心鼠英语|mnoUse|SNDHL|SWNEHE|SEKH|SOETHE|NNETAHA|SNENHA|ABCmouse|X X|浴术|IC|成长纪念册 Want|398元-|体验课S1|BLACKsS|ik and|Hats tBig|Cam My|I Have\", \"video_asr\": \"小朋友啊，你看废墟。|你看扫二三二。|大姐，你这英语也太烂了吧，赶紧走人了啊，你小时候没有约过，你去看看数英就在。|他因吗？腾讯开心鼠英语我知道啊，是中药你们三到八岁孩子设计的英语启蒙课吗？还能认发音吗？当然可以，北美外教纯正发音。腾讯开心鼠英语线上有八千多个互动学习内容，线下还免费赠送配套教材礼盒。|和点读笔，孩子可以跟读练习，轻轻一点北美外教发音，让孩子说一口纯正的意，现在才四十九块九，赶紧给你的孩子报名吧，审核你的蹩脚的英语，翻绿的孩子，你说对吗？可以报名啊，点击视频下方。|你家就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01610708236694336 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '单人口播', '室内', '平静', '愤怒', '喜悦', '家', '动态', '朋友&同事(平级)', '全景', '亲子', '特写', '教师(教授)', '学校', '手机电脑录屏', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.28', '0.20', '0.13', '0.11', '0.11', '0.06', '0.03', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08b14fc982438878fe59f1fdc8ff3d41.mp4\n",
      "{\"video_ocr\": \"有请我们的上门女婿|为我们表演一个舞狮|叶飞你敢...|谁敢上前我就废了他|本故事纯属虚构\", \"video_asr\": \"哎哎哎。|尤其我们的上门女婿啊，我们讲一个武士。|谢谢那个谁一点上钱。|我就废了吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016289949417114258 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '动态', '静态', '推广页', '特写', '愤怒', '全景', '拉近', '平静', '惊奇', '悲伤', '朋友&同事(平级)', '喜悦', '家', '夫妻&恋人&相亲', '极端特写', '单人口播', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.91', '0.57', '0.37', '0.30', '0.19', '0.16', '0.14', '0.12', '0.11', '0.07', '0.07', '0.05', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08bdeb69a7b2903c687bb41ad8dcc34a.mp4\n",
      "{\"video_ocr\": \"你好|我叫对象|真名老婆|小名媳妇|外号...|宝贝~|想和我见面吗|来找我聊聊吧|王月天|视频相亲·恋爱交友.实名认证·真实靠谱|点击下方链接下载 介绍给单身朋友|找对橡象 上伊对|伊对\", \"video_asr\": \"三二一。|嗯。|我。|嗯。|你好，我叫对象，真名老婆，小名媳妇，外号宝贝，想和我见面吗？|来找我聊聊吧。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.01633000373840332 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '喜悦', '单人口播', '静态', '动态', '平静', '拉近', '全景', '特写', '室内', '手机电脑录屏', '惊奇', '场景-其他', '室外', '多人情景剧', '混剪', '拉远'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.86', '0.81', '0.79', '0.59', '0.18', '0.03', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08d21d6d76d29224f4e9a82be90f7bf1.mp4\n",
      "{\"video_ocr\": \"5年 诶哟 别拍了 别拍了|怎么了 李老师|星|这高途课堂|全科名师班|真的太火了|我现在帮着给同学|安排班主任|我都忙不过来了|为什么有那么多|家长报名呢|你想啊|咱们这个课程|只要9块钱|就能学到语数英物|4门学科的学习方法|还都是北大清华毕业的|名师授课|这多值啊|有数学|3年棋拟 503个必考知识点|5年高考|语文37个阅读理解|答题模板|高分作文万能句|英语单词5种记忆法|阅读完形大招|物理67个出题套路|+秒题技巧|让孩子做一道题|一年都|就能掌握一类题|原来有|这么多干货啊|那当然了|你可别小瞧我们|这9块钱的课|孩子跟着北大清华|毕业的名师|梳理一遍知识点|不光知道了|哪些是必考点|重难点易错点|还能学到秒杀解题技巧|和技巧背后的应用原理|轻松举一反三|哇这么好的课|在哪儿报名啊|点击视频下方|填写相应的年级|就可以报名了|01:00:084810|01:00:10.:200|0100:11:44|01:00:16:44 0，|01:00.23:201|浙江卫视|高途课堂|园浙江卫视|高途课堂|2|【最后一期】9元4科16节高 中课，免费赠电子资料|上/|3年焓|2年道|名师出高徒·网课选高途|新高考遗用|语文|高考|英语|物理|3.2.1..1.2.3|查看详情|REC\", \"video_asr\": \"哎呀，别拍了，别拍了，怎么了？你老师这高途课堂全科名师班真的太火了，我现在要帮着给同学安排班级，我都忙不过来了，为什么有那么多家长报名呢？不影响啊，咱们这个课程只要九块钱就能学到，语数，英物四门学科的学习方法，还都是北大清华毕业的名师授课，这多值啊。|有数学五百零三个必考知识点，语文三十七个阅读理解答题模板，高分作文万能句，英语单词五种记忆法，阅读完形大招，物理六十七个出题套路加秒题技巧，让孩子做一道题就能掌握一类题，有这么多干货呀，那当然了。|可别小瞧我们这九块钱的课，孩子跟着北大清华毕业的名师梳理遍知识点，不光知道了哪些是必考点，重难点，易错点，还能学到秒杀解题技巧和技巧背后的应用原理，轻松举一反三。哇，这么好的课在哪报名啊？点击视频下方填写相应的年级就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.02074718475341797 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '单人情景剧', '中景', '平静', '动态', '教师(教授)', '静态', '教辅材料', '办公室', '工作职场', '愤怒', '室内', '手机电脑录屏', '极端特写', '家庭伦理', '惊奇', '单人口播', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.81', '0.53', '0.34', '0.25', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08dab948281f33385b1f06ef2001a22e.mp4\n",
      "{\"video_ocr\": \"您好|缺钱可以用京东金条|这年头|谁还敢借网贷啊|放心|京东数科旗下大品牌|最高借款客京东数科旗下大品牌|最高借款额度(元)|安全靠谱|最高可借20万|到账快|开通金条 万元日息最低才两块五|而且还可以分12期|慢慢还|安全便捷 通过率高|日利率低至0.025%|借款灵活|1/3/6/12期 30分钟内到账|随借随还|率低至5折|就能在线申请借款额度了|京东金融APP|点击视频下方链接|贷款有风险， 借款需谨慎，请根据个人能力合理货款|金条|五余 现玉1款|京东金条 现金借款|还盐寻汪|7|200000\", \"video_asr\": \"你好，谢谢，欢迎京东京设计，同时还感谢王金，龙宿科技下大品牌，安全靠谱，最高二十万。|万元日息最低才两块五，还可以分十二期慢慢还呢！点击视频下方链接就能在线申请借款额度啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01638317108154297 sec\n",
      "{'result': [{'labels': ['推广页', '静态', '现代', '手机电脑录屏', '中景', '配音', '场景-其他', '多人情景剧', '平静', '喜悦', '特写', '惊奇', '悲伤', '拉近', '单人口播', '愤怒', '动态', '室内', '朋友&同事(平级)', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.90', '0.64', '0.51', '0.16', '0.11', '0.05', '0.04', '0.04', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08e19ce33594afdd6cc7cc402967edba.mp4\n",
      "{\"video_ocr\": \"如果你们家孩子还处在 一个什么阶段呢|语文一般般|数学容易崩溃|物理让人心碎|英语比较颓废|你千万千万 不要再逼着孩子|死记硬背 或者去刷题|因为那样对孩子的 成绩提升帮助不会大|一定要让孩子掌握 有效的学习技巧|大是至关重夏知|立即报名|新学员9元专享|9元16节暑期特训营|夏日炎炎 即刻出发系列精品|暑假班 秋季班|高中重难点提升班 名师有秘籍领跑新学期|最s16节提分课 公开体验课|名师出高徒.网课选高途|冲刺高考语文137分计划|省高考状元带队授课 老师平均教龄11年+|高二升高三语文+英语高分 训练营 抢占名额|高二升高宝|[语高分|分词 ABC|我娃|距离高考83天，时间完全来得及!|名师特训 高效提分 高途介绍|课咨询|9元4科|高二升高三数学+物理+语|高二升高三数学+ 文高分训练营|小理+语|JKL GHI|清空|PQRS|好好 学习|9 立推|名师特训班|清北名师为您定制|ckup|高三|搜索课程/老师|我要成为学霸|读书使人进步 Reading makes one make progress|Reading makes one make progress|李光耀|TUV|物理|语文|热门搜索|6天|成为 去公司 睡了|姐姐|君 笔记|woza|沈涛著名主持人|高途课堂？ 英语|化学|陈瑞春|文请文|WO|不要十一点|生物|英语|搜索|MNO|WXYZ|DEF|yo|更多|技巧 知识|魔教\", \"video_asr\": \"JOB。|问题啊。|好了。|如果你家孩子还处在一个什么阶段呢？语文一般般，数学容易崩溃啊，物理让人心碎，英语比较容易被你，千万千万不要再逼着孩子去死记硬背或者去刷题啊，因为那样对孩子的成绩提升帮助不会打你，一定要让孩子。|掌握有效的学习技巧才是至关重要的是。\"}\n",
      "multi-modal tagging model forward cost time: 0.016499042510986328 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '混剪', '中景', '手机电脑录屏', '场景-其他', '平静', '影棚幕布', '静态', '单人口播', '全景', '多人情景剧', '拉近', '城市道路', '课件展示', '动态', '公园', '配音', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08e7251b9f2c2e90ef2d360e34395aef.mp4\n",
      "{\"video_ocr\": \"65|65+32=97|38+53=85|不对|刚电税 应该是91|镰|哼我怎么会算错|幼儿园里的小朋友|都说我是数学小神童|那我问你|98-54+32等于多少|76|你怎么连加减混合运算|都算的这么快|因为这些速算技巧|斑马A课思维体验课 的老师|早就教过我了|这个啊就是报名|教具礼拿|送的教具礼盒|他啊可是由|十风年资深教研 团队研发|出几资深教研|快递 在动画和Al实景互动中|轻松锻炼数学思维|而且这个课啊|现在报名才49块钱|0节误|妈妈我也想看动画课|这个课在哪报名啊|点击视频下方链接|就可以报名啦|枪铺号|2-8岁上班马 学思维学英语 精训导|痕铺nww cn|痕铺聘a+|狼辅导+n|猿辅导aunn mn|狼铺导aan黄 m|瘪辅号|b5+32 :7)|天看10老课呢|40006|斑马AiR|斑马ΑI课|4000683333 www.icbog com|ichox com|83333|泱榄|思维|S3|91\", \"video_asr\": \"六十五加三十二等于九十。|三十八六五三。|一一一八十五九百应该九十一。|我怎么会告诉幼儿园里跟小朋友。|我是数学家枕头。|看完了你九十八点五十四加三十二等于多少钱。|你怎么连这点火锅就算能上的这么快，你说就先说。|上期小班课四节课，老师早就教过我了吗？可是没钱课的老师。|这个呀，这是报名斑马AI课思维体验课啊等等教具礼盒，他呀可是视频的私申请团队研发的，他做法和AA视频互动中轻松锻炼数学思维。|写这个课现在报名才四十九块钱，有十节课呢嘛，那我也下克帮忙克这句可现在报名啊，点击视频下方链接就可以报名啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016596078872680664 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '多人情景剧', '静态', '亲子', '平静', '特写', '手机电脑录屏', '动态', '路人', '极端特写', '(马路边的)人行道', '场景-其他', '家庭伦理', '教辅材料', '喜悦', '单人口播', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.82', '0.70', '0.54', '0.46', '0.25', '0.24', '0.14', '0.05', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08e7acbf42d7cdd20fce2d34c448b43e.mp4\n",
      "{\"video_ocr\": \"妈妈我没骗你|关门|妈|妈妈...妈|你还说你没撒谎|上次数学才刚及格|这次为啥考得这么高|不信你考我|8乘99|9写中间|八九七十二写两边|答案就是792|331乘11|两头一拉中间相加|等于3641|妈我是跟着|猿辅导的数学老师|学的这些速算技巧|那可是清华北大|毕业的名师在线授课|教我的方法都特别实用|就是你爸花29块钱|给你报的那个|猿辅导数学名师特训班|对啊|那妈妈也要|给你妹妹再报一个|点击视频下方链接|就可以报名啦|猿辅导\", \"video_asr\": \"妈妈我没骗你关门。|哇。|你还说你没撒谎，上次数学才刚及格，这次为啥考这么高，不信你考我八成九十九九写，中间八九七十二写两边答案就是七百九十二。|三百三十一乘十一，两头一拉，中间相加等于三千六百四十一吗？我是跟着猿辅导的数学老师学的这些速算技巧，那可是清华北大毕业的名师在线授课，教我的方法都特别实用，就是你把花二十九块钱给你报的那个猿辅导数学名师特训班，对呀，那妈妈也要给你妹妹再爆一个。|点击视频当旺链接就可以报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016416549682617188 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '多人情景剧', '室外', '静态', '平静', '动态', '愤怒', '汽车内', '喜悦', '亲子', '全景', '特写', '家庭伦理', '惊奇', '远景', '路人', '门口'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.95', '0.94', '0.94', '0.39', '0.34', '0.29', '0.11', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08e8dc46af8e4991ba6fb5b02610985e.mp4\n",
      "{\"video_ocr\": \"我们班长|就是报的这个提分班|你知不知道|那天考试的时候|她都检查三遍了|我连卷子都没写完|她后面跟我说的|那些解题大招|我连见都没见过|你帮着你爸是吧|你有关心过我吗|这个课程是由|北大清华毕业的老师|带队教学的|里面好多老师|都带出了高考状元|他们教的那些|都是他们花了|11年教学经验|才总结出来的|你为什么宁愿|跟爸爸怄气|也不让我试试|我当时不是没敢信吗|这怎么报名啊|现在点击屏幕 就能报名|让孩子试试吧|我们都已经离婚了|你还来干嘛|这孩子高考落榜|这么重要的事情|为什么不跟我说|跟你说有用吗|你是能给她报班|还是能教她呀|我给孩子报的|高中提分课|你为什么不让她听|妈妈说没有用|闭嘴|给你女儿报的|9块钱的课|你可真行啊|上这种课有什么用|别吵了|才考上重点大学的|极尽一热R|极热R|1飞热R|水会热战|水层二热R|2020全科名师班|高途课堂|浙江卫视|网课选高途|《极限挑战》第六季 官方推荐 中小学生在线教育平台|北大清华 毕业名师授课|9元16节课|辅导老师1对1答疑|名师在线 挑战极限|即刻出发逆袭学霸|初高中 全科辅导|10\", \"video_asr\": \"你都已经。|你还来干吗？这孩子高考落榜这么重要的事情，为什么不跟我说？跟你说有用吗？你是能给他报班还是能教他呀？给孩子报的高途课堂，高中提问，可你为什么不让他听？|有用闭嘴。|给你女儿报的九块钱的课你可真行啊，上这种课有什么用啊，别吵了。|我们班长就是报的这个提问班，在考上重点大学的。|你知不知道，那天考试的时候他都检查三遍了，我连卷子都没写完啊，他后来跟我说的那些解题大招，我连见都没见过你吧，这里巴士吧，你有关心过我吗？|这个课程是由北大清华毕业的老师带队教学的，好多老师都办成。|他们交换信息给大众不超过二十。|经验他总结出来的。|你为什么你宁愿跟爸爸呕气也不让我试试啊？我当时不是没敢信吗？|这怎么报名啊。|现在点击屏幕就能报名，让孩子试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01667046546936035 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '填充', '静态', '室外', '动态', '全景', '亲子', '平静', '悲伤', '愤怒', '家庭伦理', '特写', '喜悦', '路人', '单人口播', '惊奇', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.98', '0.93', '0.89', '0.78', '0.68', '0.45', '0.11', '0.05', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08ee5edf4aa6382c4ccac7b049e73bfb.mp4\n",
      "{\"video_ocr\": \"!|上次就是我去的家长会|这次你去|我去|我可不去|他这次这成绩|肯定又不及格|我可丢不起那人|我可都听到了啊|你们再看看我的成绩单|你是不是搞什么小动作了|我没有|我是用我的零花钱|报的高途课堂|9块钱就有16节名师课|包括语数英物四科|我们班同学成绩好的|都报了这个了|9块钱 就能有这么大的进步|就有北大清华毕业 的名师授课|教的都是有用的 学习方法|而且|还从20万热搜难题里面|精炼出了|503个必考知识点|170个就|176个易错点|144个重难点|我现在才发现|我之前成绩差|都是方法没用对|那你跟着学|一遍能记得住吗|课程支持 3年内无限次回放|课后还有辅导老师|1对1答疑|不怕学的慢|那这么好的课|你在哪儿报名的呀|点击视频下方|就可以报名啦|浙江卫视|高途课堂|园浙江卫视|高二三班期中考试成绩排名|titve|暑期初高|高途课堂|园浙江卫视|w06g南|霸 |全科名师班 只需9元|再不抢就晚了！高中16课时|北大清华师资平均教龄11年+|名师出高徒·网课选高途|文紫琪 罗璇 孙佳楠|提分课，原499元，现9元抢|litUle|iule|ite|OH1e'|OHe|Cne|823|周志明|19023|103 217|222|127|查看详情|112|全科特惠|胡小华|数学 英语 理综|姓名|语文|于题\", \"video_asr\": \"有上次就是我去的家长会，这次去我去我可不去，她这次得成绩肯定要不及格，我可就被别人。|爸妈，我们都听到了啊，你们再看看我的成绩单。|都说你是了，工作了我没有，我是用我的零花钱报了高途课堂，九块钱就有十六节名师了，包括语数，英物四科，我们班同学成绩好的都报这个了。|九块钱就到了这么大的金额，九块钱就有北大清华毕业的名师，授课，教授，都是有用的学习方法，而且还有狂二十万的多难听，里面经历除了五百零三个必考知识点。|一百七十六个易错点，一百四十四个重难点，我现在才发现，我之前成绩差都是方法没用，对啊，你跟着全一百多几个。|课程支持三年内无限次回放，课后还有辅导老师一对一答疑，一不怕怕学的慢，那这么好的课在哪报名啊？点击视频下方就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016507625579833984 sec\n",
      "{'result': [{'labels': ['多人情景剧', '填充', '现代', '推广页', '中景', '家庭伦理', '家', '夫妻&恋人&相亲', '静态', '亲子', '喜悦', '愤怒', '惊奇', '宫格', '平静', '动态', '室内', '特写', '极端特写', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.91', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08efe8da8a7c2b83a12d69305931c7e4.mp4\n",
      "{\"video_ocr\": \"阅卷老师30秒钟|课|就改一篇作文|如何快速抓住他的眼球|拿到高分呢|你需要有一个|完美的作文开头|董小姐教你一招|简单对比出哲理|帮你轻松打败|全国绝大多数的选手|写母爱|你可以这样开头|用自然界的声音和|母亲的声音作对比|突出“母爱”主题|你学会了吗|我是作业帮直播课|董俣老师|北京大学中文系毕业|在我的课堂|这样的作文|写作高分技巧|还有36个|招招助你冲击|满分作文|29元20节课|给孩子一个|提升成绩的机会|别犹豫了|赶快点击视频下方链接|报名吧|新学期语文高分特训营 29元=11节语文直播课+9节多科提升课|重1|北京大学毕业 董俣|不如母亲的声音动听。|松林的絮语，山泉|的叮|的叮咚，禽鸟的啁嗽，|山泉|是自然界最美妙的声音。|可是在我心中，它们都|松林的絮语，|市区高考状元|是自然界最美|上课内容与礼盒详情以实际为准|中国女排|双师伴学 辅导老师随时答疑，课程3年内无限次回放|2.5亿题库，更懂学生 一线名师直播授课，随时随地放心学|阅读写作双提升\", \"video_asr\": \"右键老师三十秒钟就改一篇作文，如何快速抓住他的眼球，拿到高分呢？你需要有一个完美的作文开头，懂小姐教你一招，简单对比出哲理，帮你轻松打败全国绝大多数的选手。写母爱，你可以这样开头，松林的絮语。|山泉的叮咚，群鸟的啁啾，是自然界最美妙的声音，可是在我心中，他们都不如母亲的声音动听。|用自然界的声音和母亲的声音做对比，突出母爱主题，你学会了吗？我是作业帮直播课总于老师，北京大学中文系毕业。|在我的课堂，这样的作文写作高分技巧还有三十六个着着着你冲击，满分作文二十九元，二十节课，给孩子一个提升成绩的机会，别犹豫了，赶快点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.017144441604614258 sec\n",
      "{'result': [{'labels': ['现代', '填充', '单人口播', '中景', '静态', '推广页', '平静', '场景-其他', '配音', '极端特写', '室内', '教师(教授)', '教辅材料', '手写解题', '特写', '拉近', '幻灯片轮播', '影棚幕布', '学校', '重点圈画'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.83', '0.25', '0.25', '0.15', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08f24c04b0640e54b8432a06f139c18b.mp4\n",
      "{\"video_ocr\": \"大少爷继承集团|这里已经没有你的容身之处了|(想不到我堂堂柳家二少）|(竟被赶出家门)|(沦为范家上门女婿)|唷|这年头|真是什么人都想攀高枝啊|(卑躬屈膝)|(饱受嘲讽)|喂？|你女儿情况不好|手术费需要五十万|救救我女儿|这里有五十万|离开妍妍|哎呀！江文|怎么好意思让你来这里呢|我就过来看看阿姨|妍妍和孩子|如果你真的想救你女儿的话|那就听江文的建议|少爷|柳家只能您主持大局了|当初不是你们赶我走的吗|大少离世了|只要你肯回来|柳家所有资产任你调动|可以|另外|好好”招呼他们两个|谷eqU|qU|合s qu|BLICK UNIQUE|1884 39436|台婚U|Up量|谷播|2019|CRNNLEl|P0炫g|BE|Ev\", \"video_asr\": \"大少爷继承了集团，这里已经没有你的容身之处了。|想不到我堂堂柳家二少。|竟被赶出家门，沦为范家的上门女。|哟，这年头真是什么人都想攀高枝啊！|卑躬屈膝饱受嘲讽。|喂你女的情况。|手术费需要五十万。|妈，救救我女儿这里有五十万离开妍妍。|红，哎呀，江文怎么好意思麻烦你来这里呢？我就过来看看阿姨妍，如果你真的是想救你的女儿的话，那就听听我的建议，少爷，柳家只能您主持大局了，当初不是你们组也只能可怕吗？|向所有资产你要动可以先救我女儿。|另外好好照顾她们两。\"}\n",
      "multi-modal tagging model forward cost time: 0.016393184661865234 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '动态', '极端特写', '特写', '静态', '悲伤', '朋友&同事(平级)', '家', '愤怒', '喜悦', '惊奇', '夫妻&恋人&相亲', '室内', '手机电脑录屏', '配音', '情景演绎', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.94', '0.84', '0.81', '0.58', '0.40', '0.37', '0.22', '0.20', '0.15', '0.06', '0.05', '0.03', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/08ff6cd35045c7fc34bea49024108b77.mp4\n",
      "{\"video_ocr\": \"yang|yue|ノ了|00|shan|教具礼金全国包邮 (潜澳合地区除外)|白然.天气|阳|针对2-8岁读书识字敏感期|星云|趣味动画语文课|¥49 10节A动画课|山水|太 阳|月亮|白 云|下丽|雪 花|大 山|拼音为系统课s3课程|手机/平板在家学|专业老师全程辅导|2800元/年|火K|中 水|斑马Al课|雨雪\", \"video_asr\": \"自然天气阳太阳，月，月亮星星星。|云白云与下雨雪，雪花山，大山。|水水花。\"}\n",
      "multi-modal tagging model forward cost time: 0.016458511352539062 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '极端特写', '推广页', '静态', '教辅材料', '配音', '课件展示', '手写解题', '动画', '宫格', '才艺展示', '填充', '转场', '商品展示', '喜悦', '影棚幕布', '重点圈画', '中景', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.47', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0900a77a06ac9276bc97c8653c7fe981.mp4\n",
      "{\"video_ocr\": \"之前这款游戏|要找我代言的时候|我说|我得考虑一下|它的广告词说的天花乱坠|什么千套华服|自由剧情|古风体验|但是|当我打开游戏的那一刻|我就决定接下这个代言|这个游戏画面是3口的|画风非常精致|剧情跌宕起伏|能否晋升 沉默|全靠你的步步选择|选择对了|帅气的小哥哥会送你华服|哇 大旗|过套真好看|这套也好好看啊|娇生惯养 知书达理|但是注意了|一不小心|以泪洗面 果断重选|快来试试你能玩到什么身份吧|妹妹假装摔倒诬陷你，你该|妹妹1表搔诬陷你 你该|选择萌娃培养模式|卿哥|开局就身处冷宫|洋为卿歌|半上生为卿歌|请以游戏为准|供参考，请以游戏为准|画面仅供参君，馈以游戏为准|面仪供参考。|画供参考|画面|画面 又供参者 以游戏为准|面仅|为准|鹊桥·芳华 与纂、香备！绿 处子|一篡，香蚕|氨 处子|鹊桥芳华1d|鹊桥芳华来道看|北球知态府千金 罗男相风之路 经天敬月，远口|岁步期凰之路|北贵加府千金 少步朝风之|辩解|你拥有一次重新选择的机会|经云敬月，远风回营|钰云敬月，园风0|洋动歌|滟歌|淫迦歌|深动沙歌|洋生球为歌|22e|系统提示|妹妹邀请游园|初始万案 还原|景煨|衣服 背饰|强化|图鉴 捏脸|妆容|一山|制衣 染色|保存\", \"video_asr\": \"之前这款游戏要找我代言的时候，我说我得考虑一下。|它的广告词说的天花乱坠。|什么千套华服，自由剧情，古风体验，但是当我打开游戏的那一刻，我就决定接下这个代言，这个游戏画面是三D的。|风非常精致，剧情跌宕起伏，能否晋升全靠你的不，不选择，选择对了，谁记得小哥哥会送你华夫？|哇，这套真好看，这套也好好看啊，还有萌萌可以养成，太可爱了吧！但是注意了，一不小心有可能会被打入冷宫哦，快来试试你能玩到什么身份。\"}\n",
      "multi-modal tagging model forward cost time: 0.016071319580078125 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '配音', '平静', '动态', '特写', '多人情景剧', '喜悦', '室外', '愤怒', '惊奇', '手机电脑录屏', '场景-其他', '朋友&同事(平级)', '家', '混剪', '悲伤', '全景'], 'scores': ['1.00', '1.00', '1.00', '0.92', '0.78', '0.77', '0.74', '0.71', '0.49', '0.16', '0.12', '0.09', '0.08', '0.06', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/090119010d636523c8426b8dcb5dd40a.mp4\n",
      "{\"video_ocr\": \"柯南老师|快帮我家孩子选选英文辅导课吧|最近我在网上|看到太多外教试听课的广告了|什么9元的29元的49元的|到底哪一种才适合我家孩子呀|真是太难选了|这些课都是录制好的录播课|没有什么区别|金鹰卡通推荐的51Talk|不是录播课|是真人外教1对1在线教学|一学年只要5799元|一节课低至40元起|少儿英语外教体验课 立即抢购|51Talk:|《天天向上》|首席学习官|天天回上|19元抢10节\", \"video_asr\": \"柯南老师会帮我家孩子学习英文辅导课吧，最近啊，我在网上看到太多外教试听课的广告了，什么九元的二十九元的，四十九元的到底哪一种才适合我们家孩子呀？真是太难选了，这些课都是录制好的的博客啊，没有什么区别。今天卡通推荐的五一套不是的，波特不是小班课，是真人外教一对一在线教学，一学年只要五千七百九十九元，一节课低至四十元起。\"}\n",
      "multi-modal tagging model forward cost time: 0.016084671020507812 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '平静', '朋友&同事(平级)', '手机电脑录屏', '喜悦', '极端特写', '惊奇', '特写', '单人口播', '动态', '夫妻&恋人&相亲', '悲伤', '餐厅', '路人', '家庭伦理', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.83', '0.79', '0.74', '0.72', '0.35', '0.20', '0.04', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0901e2a55ffd7a9945bfd9728730465d.mp4\n",
      "{\"video_ocr\": \"冬天很冷。|这是一个装满 的杯子。|的杯|约子。|她在哭。|他关门了。|乌龟跑得慢。|10节A1动画课 手机/平板在家学|识字互动游戏书 反义词二|反|这是一个空杯子。|大象比 老战|大象比老鼠重。 老鼠比大象轻。|兔子跆快|兔子跑得快。|趣味动画语文课 针对3-6岁读书识字敏感期|教具礼盒 全国包邮(港澳台地区除外)|慢空满|曼空 满|专业老师全程辅导|笑 哭轻|哭轻|笑哭|热冷 快|热冷忡|热快|*非赠品|重开关|重升|轻重|马A课|¥49\", \"video_asr\": \"这。|ZZZZ。|这。|ZZZZ。|你。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.0159909725189209 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '推广页', '极端特写', '教辅材料', '静态', '配音', '绘画展示', '课件展示', '动画', '宫格', '商品展示', '图文快闪', '手写解题', '幻灯片轮播', '重点圈画', '才艺展示', '动态', '室内', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09026ab814468c49e240c6e838e7a53f.mp4\n",
      "{\"video_ocr\": \"9块钱|真能提升那么多嘛|现在报名还来得及嘛|儿子儿子|妈妈给你抢到了|抢到什么了|你看|高途课堂的 高中全科名师班|9块钱就有 16节名师直播课呢|真的吗|我们同年级的同学 都在家偷偷学这个呢|你这天天刷题也太辛苦了|高二这一年 一定要好好学啊|这是专门教语数英物|四门课程的解题大招 和解题技巧|谢谢妈|我一定会好好上课的|课后有不会的问题|我就找老师|随时1对1给我辅导答疑|开学逆袭高分|仅需9元，冲剩领跑新学期|清北毕业名师教学，快速全面提升|17天|我是高途课堂周帅老师|省级高考状元|高考数学满分|在15年教学生涯中|我总结了|不只数学|英语、语文、物理|都有清华北大毕业师在线授课|无论你基础如何|我们都能帮到你!|【查看详情】就能报名!|华少|语款物4科全面提升 16节精讲直播互动课 弹无限次回放复习|手年无限澳回蕨复习|新生特惠 16节名师直播课+1对1答疑+3年无限回成|并初三 初升高|周|请选择孩子9月升入年级|升高二|439个知识点|全国百佳教师带队教学 平均教龄11年|2% 3轮严格筛选师资|开材三 小升初|发飞|5我严格踪选师内|备退17天|浙江卫视指定在线教育品牌|井高三|90%|主讲老师90%毕业于重|主讲老师每堂课精心|桑取率不足2|新用户专享 立即体验|高途课堂全科训练营|点高校及师范院校|10年|平均教龄10年以上|主讲老师教学经验丰富|￥9|167个考点|57个难点失分点|北京大学学士省级高考状元|浙红卫视|名师特训班|中国电信 15:42 4G|80个易错点|69%|谢欣然 马小军 王冰|主讲|平均|499|名师\", \"video_asr\": \"九九块钱真能提升那么多吗？现在报名还来得及吗？儿子儿子。|爸爸给你抢到了？抢到什么了？你看高途课堂的高中全科名师班，九块钱就有十六节名师直播课呢，真的吗？我们同年级的同学都在家偷偷学这个呢，你这天天刷题也太幸苦了，高二这一年你一定要好好学啊，这是专门教语数英物四门课程的解题大招和解题技巧，谢谢吧。|我一定会好好上课的，课后有不会的问题我就找老师随时一对一给我辅导答疑，肯学逆袭高分。我是高途课堂周帅老师，省级高考状元，高考数学满分。|在十五年教学生涯中，我总结了四百三十九个知识点，一百六十七个考点，八十个易错点，以及五十七个难点，失分点不止数学，英语，语文，物理都有清北毕业名师在线授课，无论你基础如何，我们都能帮到你，查看详情就能报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.016680240631103516 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '家', '单人口播', '亲子', '平静', '教师(教授)', '手机电脑录屏', '极端特写', '学校', '愤怒', '悲伤', '室内', '全景', '家庭伦理', '教辅材料', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.93', '0.66', '0.57', '0.44', '0.37', '0.19', '0.14', '0.11', '0.11', '0.04', '0.03', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0907a6be952e3df9119c7c998fb08a5d.mp4\n",
      "{\"video_ocr\": \"怎么让孩子开心的 学|学好英语呢?|当然是方法要用对|不要逼迫孩子学英语|让孩子自己发现 学习英语当中的乐趣|做到主动学|家长也省心|家长不妨试试|伴鱼自然拼读课|它旨在让孩子 掌握26个英文字母|及字母组合的|发音规则|让孩子学英语|像学拼音一样自然|力求让孩子做到|见词能读|听音能写|课程还可以无限次回放|用游戏和动画的方式教学|点击下方链接就能报名|现在特惠活动|14节课仅需29|还送配套学 成长地图哦|为什么很多孩子英语 就是学|就是学不好？|孩子开心学 启蒙更有效|Level3|AB|PHONCS|7KDS|\\\"87KD|Phonics|伴鱼自|elk|HON|[T\", \"video_asr\": \"为什么很多孩子英语就是学不好，怎么让孩子开心的学好英语呢？当然是方法要用对，不要逼迫孩子学英语。|让孩子自己发现学习英语当中的乐趣，做到主动学，家长也省心，家长不妨试试吧和自然拼读课，它旨在让孩子掌握二十六个英文字母。|即字母组合的发音规则，让孩子学英语像学拼音一样自然，力求让孩子做到了这词能读，听音，能写。|课程还可以无限次回放，用游戏和动画的方式教学，让孩子轻松学英语，点击下方链接就能报名，现在特惠活动，十四节课仅需二十九元，还送配套学习成长地图哦CAF PHONICS！\"}\n",
      "multi-modal tagging model forward cost time: 0.016620397567749023 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '平静', '配音', '手机电脑录屏', '场景-其他', '单人口播', '办公室', '特写', '多人情景剧', '极端特写', '喜悦', '家', '动态', '课件展示', '室内', '情景演绎', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.84', '0.67', '0.34', '0.30', '0.28', '0.05', '0.04', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/091421d75198448d7059cdd4c5690eea.mp4\n",
      "{\"video_ocr\": \"你们谁能拿到秦家的合同|唐家家主的位置|就给谁|等我当上唐家的家主|你就跟你这个废物老公杨潇|一起滚出唐家吧|没到最后我是不会放弃的|多可笑|你看一下你那个废物老公|有多识相|你能不能别再给我丢人了|怎么啦|我可以继承家族|但是|我有一个条件|秦总您看这个合同咱们今天可以签了吗|唐先生|您稍等|杨潇|你跑这来干嘛|这可是我们龙门少主啊|开玩笑吧秦总|唐先生不可无理|没想到吧|欺负我老婆|我不但要让你滚出唐家|还要|当年阴差阳错进入唐家|看都市战神 高调护娇妻|本故事纯属虚构|热门小说 立即查看全文|低调的他决定摊牌|真实身份震惊众人|为争夺家主之位|妻子被人陷害|《企划\", \"video_asr\": \"你们谁能拿到秦家的合同，唐家家主的位置就给谁懂，我当上唐家的家主，你就跟着这废物老公杨潇。|一起滚出唐家堡，没到最后我是不会放弃的，多可笑，你看一下你那个废物老公有多食性。|你能不能别再给我丢人了。|怎么了。|我可以继承家族。|但是。|我有一个条件，辛总，您看这个合同中。|最强程先生，您稍等。|杨逍你跑这来了，这可是我们龙门下祖国的，你开玩笑的。|唐先生不靠谱。|没想到吧，我老婆带一份抽砖家。|还要。\"}\n",
      "multi-modal tagging model forward cost time: 0.01612067222595215 sec\n",
      "{'result': [{'labels': ['填充', '中景', '现代', '推广页', '静态', '多人情景剧', '夫妻&恋人&相亲', '室内', '平静', '特写', '悲伤', '惊奇', '动态', '喜悦', '家', '拉近', '愤怒', '极端特写', '朋友&同事(平级)', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.81', '0.79', '0.63', '0.12', '0.10', '0.08', '0.06', '0.04', '0.04', '0.04', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/091570db31d4d5592c6180ca001d52a5.mp4\n",
      "{\"video_ocr\": \"成语大侠一定是|朋友多次安利我不听|新活动当前|抱着试试的心态|下载了成语大侠|结果|iPhone11、P40手机|赚翻啦!|真后悔没有早些参加!|现在|用户不用集碎片|登录就能赢iPhone11|通过此视频下方链接|同|闯过5关|还有机会|直接获得一部|价值4000元的|鼓作气+3|P40手机!|活动力度那么大！|你要不试试真是亏大了|快点击下载吧!|HUAN|领取|00:40内完成关卡|动荡|你获得 奖品碎片|HUAWEIP40|在线奖励|抽手机|第11关 过关条件:|投桃 报李|土人|下一关|花股骨三|结兰蹊|情 意伤 风然|意伤|1012王|可2二三2三|10|玩成语游戏|赢iPhone11、网4|成语大侠|达成2连击|EIP40|疯了！|浩浩荡|x1|悚 然|恭喜过关!|成群|08:08|MaslEr|pMastr|+80|恶意中|开始答题|连击|选择关卡|新春换肤|魅眼识珠|初出茅庐|P40|获得神宠 宠物|首页|已满|股 悬梁|玩游戏|免费|提示|看视频领取|10已满|广告|5C\", \"video_asr\": \"疯了疯了，超越大侠一定是疯了，朋友多次安利我不贴心，活动当前放着试试的心态下载了成语大侠，结果广凤十一。|第四个手机赚翻了，真后悔没有早些参加，现在用户不用集碎片登录就能赢IPHONE十一，通过此视频下方链接下载成语大侠闯过五关还有机会直接获得一部价值四千元的利润，比手机活动力度那么大，只要不是真是亏大了，还挺下载吧！|哎。\"}\n",
      "multi-modal tagging model forward cost time: 0.016311168670654297 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '配音', '手机电脑录屏', '平静', '填充', '室内', '商品展示', '静态', '极端特写', '特写', '中景', '宫格', '动态', '办公室', '喜悦', '拉近', '单人口播', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.93', '0.92', '0.37', '0.18', '0.16', '0.09', '0.06', '0.05', '0.05', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/091669322581429badaa951c5f11124a.mp4\n",
      "{\"video_ocr\": \"你在看什么呀|我在看我的男神|哇长得还挺帅的|那当然了|喜欢就去追啊|这不自己的说话声 音不好听方言又重|我都不好意思给他 发微信|呀这有什么呢，我 我都怕他嘲笑我哎|我前段时间报名了|潭州播音学院的播|学完这个之后呢不 仅可以让声音变得|这样你就可以大胆|而且呢还可以利用|跟你说|音主持配音课程|更好听|的去表白了呀|业余时间做副业|FOI'|O1'|FOLLCWAA|FOLLOW ME|MOVE|MO'\", \"video_asr\": \"你在看什么呀？你在看我的男神啊，长得还挺帅的，那当然了，喜欢就去追呀。|这不自己的说话声音不好听，荒原中我都不好意思给他发微信，我都怕他嘲笑我，我有什么跟你说？我前段时间报名了潭州播音学院的播音主持配音课程，学完这个之后呢，不仅可以让声音变得更好听，这样你就可以大胆的去表白了呀，而且呢还可以利用业余时间做副业。\"}\n",
      "multi-modal tagging model forward cost time: 0.0163421630859375 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '静态', '多人情景剧', '推广页', '喜悦', '惊奇', '特写', '平静', '朋友&同事(平级)', '极端特写', '家', '单人口播', '动态', '配音', '家庭伦理', '场景-其他', '路人', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.78', '0.48', '0.36', '0.34', '0.10', '0.04', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0928d969151e3b9e0147b7277c239d96.mp4\n",
      "{\"video_ocr\": \"你就是新来的|是|那可就得懂规矩|我跟你们拼了|拼多多天天领现金|开始啦|点击|天天领现金|邀请好友助力|就可以获得|最高|最高100元的|现金红包|哈哈哈哈|还真100元|就这样到手啦|帮好友助力也可以领到现金哦|面对面扫码|人猛评情 拥多多天天锁现金活动打款已到 账，快去喊好友一起来领现金吧|人验订值 获多多天天领魂金活动打款已剑|现金拿去花 男装|三水哥哥|天天领福利，好运永不停|我的账单 三更多|三账户安全|已获得96元|95.00|2359.544后现金等失效|23:59:59.5后现金将失效|查看详情|商家付款入际通知|EXIT|今日已送出现金￥37074800|三我的账单 =账户安全|女神|今日已送出现金￥22216600|微信支付|累计到100元就能提现|食品|记录|规则|提现>|何款森户 拼多多|人际户 等钱|最高100元，具体金额以实际领取为准|￥100.00|拼多多|推荐|鞋包\", \"video_asr\": \"你就是新来的是，既然是新来的那刻就得懂规矩上我给你们吹了。|不多，天天领现金开始了，点击天天领现金，邀请好友助力，就可以获得出售一百元的现金红包啦，还这一百元就这样到手了！|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016343355178833008 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '手机电脑录屏', '多人情景剧', '特写', '喜悦', '极端特写', '配音', '室外', '愤怒', '单人口播', '惊奇', '动态', '平静', '朋友&同事(平级)', '家', '室内', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.94', '0.90', '0.21', '0.20', '0.17', '0.15', '0.09', '0.09', '0.03', '0.03', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0929ea258fb63003a44c3ba222d4aad0.mp4\n",
      "{\"video_ocr\": \"老周帮帮忙吧|你这不是前两天|刚提了新车|还差钱?|快别说了|家里出了事急用钱|把车卖了还差十几万呢|今天就要凑齐|十多万的话|我可以帮你|患难见真情|好兄弟|谁说我要借你钱了|是平安银行新一贷|额度高达50万|零抵押零担保|资料齐全快至1小时放款|最长可分36期慢慢还|快 教我怎么申请|点击屏幕下方|输入手机号|就可以申请啦|国平安 PING 亥业让生活更雨g|专业 让生活更简单|中国平安 PING AN|请根据个人能力合理贷款|网相支付|网阳支|贷款有风险、借款需谨慎|贷款额度、放款时间等以实际审批为准|金融·科技|江湖救急|￥500000\", \"video_asr\": \"嗯。|好。|ZZZZ。|ZZZZ。|我。|老头帮帮忙吗？你这不是前两天刚提的新车，还差钱快别说了，家里出了事儿，急用钱把车卖了还差十几万呢，今天就要凑齐十多万的话我可以帮你。|患难见真情，好兄弟，谁说我要借你钱啊，是平安银行新一贷，额度高达五十万，零抵押，零担保，资料齐全，快至一小时，放款最长还可以分。|就只会教我怎么申请，点击屏幕下方输入手机号就可以申请了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016453266143798828 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '多人情景剧', '静态', '平静', '亲子', '喜悦', '家庭伦理', '特写', '极端特写', '家', '愤怒', '夫妻&恋人&相亲', '汽车内', '惊奇', '单人口播', '办公室', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.81', '0.76', '0.47', '0.43', '0.40', '0.11', '0.05', '0.03', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/093bdb2473cd2414d6c4209a02bb27ea.mp4\n",
      "{\"video_ocr\": \"我不求资产过亿|也不求一夜暴富|我只求|知道我们|疯读极速版|的朋友|都能免费领取到|我们的P30手机|是少有的|全程不花一分钱|就能得到|P30手机的软件|通过此视频链接|碎片×4 参与新人阅读|领取福利得9个碎片|签到再得1个|还有啊|听说|《绝品神医》这本小说 天|碎片掉落凡率里|会更大哦|阅读几章。“林|就能集满10个碎片|肉10个碎片就能|直接领取P30手机|而212 直接|免费包邮送到家|活动链接就在视频下方|下载早的朋友|都已经领取到自己的|P30手机了|你也抓紧来试试吧|免费赢手机|阅读60分钟 去阅读|日 40积分|病因呢？病因查出来没有?|叹喜当然。陆逸点头笑道：\\\" 至春 不是没有查出周夫人的病因|陆逸嘴一撇，说： 我来说，简直就是小菜-┐|瞬间，李梦寒脸色大变，惊恐|中医治好了周夫人的病，从今以后|&人根本请不动他。 八贷来践王 。只是胡青牛|\\\" s町寒奇怪的问道：院长|VEI|不及时救治，恐怕随时会有生命危|问陆逸：莫非你有办法?|那你能治吗？|陡然听到周夫人发出咳嗽的声音|院长急匆匆地朝病房走去。|有本事你来试试？你要是用|立穴む用金针治病，所以|心文会不院长站在护工值班室的|'EIP|去签到|当前累计获得1枚，集满10枚|魔剑|长沉声问主治医生:周夫人情况 望着病床上昏迷的美妇，林院|生气，这都什么时候，这家 护工怎么呢？告诉你，可别|手续呢。李梦寒嘴角有着苦笑， 没想到自己堂堂的西医博士，居然|J童咱你是说国医圣手胡 ]解释着。|站在门外，脸色都气青 八蛋，翻脸居然比翻书|(1/10)|积分商城|主积 08:08|小农民会医术，村里姑娘们个个爱心挡不住|下午三点。江州医院。|讨论病情？陆逸噗嗤一笑|都能|笑，手|周夫人已近痊愈了。李梦|的医生|常齢不！\\\"一个老中医突然 金目】|我开的啊!|HUAVErP|免广告|重症监护室的第三号床前，围|自：虽然你医术不错，但是 了指床上昏迷的美妇，对李梦|么?|痊愈了？这么快就好了？\\\"|]问道。刚才看陆逸 什么是金针渡穴？|啊，聘书现在也没有….|即可兑换手机。 立即兑换|很不乐观。主治医生一脸 凝重地说道：周夫人昏迷不醒，|战护工，有时候比你这个大博 朋多了。陆逸撇嘴道。|连一个护工都不如。 ”周夫人身体刚好，怎么就急|这 金娄一ト院长倒吸一口冷气 人夫周卦，|个时候，李梦寒走了过|着一个四十多岁的美妇，双眼紧闭 ，面色苍白。|你一个护工懂什么？ 李梦|这小子还真是人才啊。 我刚给周夫人做了体检，她|咱人夫周了，金针王的绝技啊| 林 赶紧出去，别打扰我看 说完，就把林院长赶了|+2|满了一大群医生和护士。病床上躺|靠定，就你，治不了她的病。|林院长吃了一惊，没看出来，陆逸|帯」好像真的很厉害似|，苛剑，连评聘书也没有？那|其他人掩嘴偷乐，惹诈|老毛病了，没事儿。对了， 周夫人怎么样了？林院长问。|治病，那为什么现在全世界都流行|普非了！天啊，他用的竟 穴小ヂ|头小子，连医师资格证|逸的|的望着陆逸。|李梦寒深呼吸了一下，推开了 值班室的门。|，我李梦寒认你当老师，跟着你学 中医。李梦寒气的满脸娇红，胸|呆会不会搞错了，我|这里？咦，您脸色怎么 是不是哪里不舒服？|福利中心|福利|也一边看还一边咽口水，太恶|、就不 能治|失人就|想当中医科的主任，你|日常福利 每日签到|身体确实好了。她现在正在办出院|P30手机(7/10)|看视频领积分|药鼎仙途|林院长急问。|I个一ベ|具体活动以实际情况为准|具体沽动以头际情况|具体活动以实防 青兄为准|体活动以情况为什|HUAWEI P40|10积分|妖鬼异闻录 绝世剑魔|业V|2天 3天4天5天6天|2天  3天 4天 5天  6天|最后一个鬼师|“日30积分|灭道夺天|查看更多>)|还债，踏上了赌石这条不归路..|了。 偏惹这|睛问道。 会醒|寒说。|西医?|EI P30|读享快乐 每日阅读轻松赚积分|天啊!我变成|新人阅读福利|最后 万古杀帝|古亲帝|赢手机|HUNHE|西三1王|看小说|去抽奖|绝品神医|杀帝 狐颜乱语|著)|P30手机碎片*1 恭喜获得P30手机碎片*|，郸一没注意李梦寒的表|免广告特权|扫地机器人|怎么样了？|不乱。|盲宴t 目△十年|iPad|4月11日星期四|连续签到赚更多积分|我安7|推荐|去完成|今日已读/分钟|阅读15钟|第1章 中邪|3:20|红伞+作品|08.41|半小|着出院？|疯读\", \"video_asr\": \"我不求资产过亿，也不求一夜暴富，我只求知道我们疯读极速版的朋友都能免费领取到我们的P三零手机。疯读极速版是少有的全程不花一分钱就能得到被三零手机的软件，通过此视频链接下载疯读极速版参与新人阅读领取福利的九个碎片。|千岛赛的一个是吧？听说绝品神医这本小说碎片掉落几率会更大哦，阅读几章就能集满十个碎片，十个碎片就能直接领取P三零手机，直接免费包邮送到家，活动链接就在视频下方，下载扫的朋友都已经领取到自己的P三零手机了，你也抓紧来试试吧！|贝贝更加讲。\"}\n",
      "multi-modal tagging model forward cost time: 0.023336172103881836 sec\n",
      "{'result': [{'labels': ['场景-其他', '手机电脑录屏', '现代', '配音', '推广页', '静态', '中景', '单人口播', '喜悦', '平静', '多人情景剧', '办公室', '惊奇', '拉近', '工作职场', '特写', '动态', '游戏画面', '红包', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.84', '0.72', '0.71', '0.67', '0.48', '0.28', '0.21', '0.10', '0.06', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09449d469b0a896583aa1071503ce474.mp4\n",
      "{\"video_ocr\": \"哎|你们欠我的房租|准备什么时候给我|我老公查出恶性肿瘤|我正着急筹钱|给他准备医药费呢|医药费|怎么不找保险公司呢|我们都是基层打工人员|哪有什么钱买保险啊|我之前不是 让你们免费领了|轻松保严选重疾险吗|你老公得的这种病|就在保障范围内|可以申请理赔的|我想起来了|之前我们填写过手机号 和身份信息|兔费领了|他们真的能够理赔吗|当然了|为了让你和你的家人|有应对疾病的能力|首次800元保额|后续通过参与相关活动|保单保障期间内|累计最高65000元的保额|能保障100种重大疾病|确诊即赔|确诊 即赔|太好了|这下我们有救了|不要等疾病来临时 才后悔|赶紧点击视频下方链接|为你和家人|免费领取一份保障吧|点击下方链接|立即领 保障权益|姓名 信息保密，仅 身份证 信息保密，仅用于投保|脏嫌膜疾病、心包疾病、川度房室传导阳阻滞、心 两、严重痴呆、运动神经元病变、深度昏迷、短暂|定、肺气肿、间质性肺病、肺纤维化、肺淋巴管肌|2投保人:投保人为年满18周岁及以上，具有完全|之间的身体健康、能正常工作或生活的中国境内公 被保险人职业:被保险人的职业属于 众惠财产|投保人:投保人为年满18周岁及以。|泪互保险社职业分类表》中1-4类职业。|保障时间长|1:260946000000800粤1CP备11088817号-3|《保险条|留病、肺泡蛋白质沉积症、慢性呼吸功能衰竭、慢|三肝炎、肝硬化、肝豆状核变性(或称Wilson 两)、慢性肝功能衰竭、胃/十二指肠溃疡、克罗|大、骨简纤维化、骨髓增生异常综合征、急性弥漫 生血管内凝血、血小板减少性紫癫、血友病;|5保险人:众惠财产相互保险社。|保险期间:1年。保险期间内，不因保额增加而变 3保单生效日期:投保次日即可生效。|更保险期间。|9领取份数:本产品在同一保险期间内每人限领取|｝.开颅手术、多发性硬化、帕金森氏病、癜痫、阿 尔茨海默病、脊髓疾病、微痪、重症肌无力、精神|生脑缺血、脑梗死、脑出血；|区) 经纪有限公司，通过互联网在全国区域销售，保障|民事行为能力人。 民。限投保人本人。|8被保险人:出生满18周岁(含)至60|确诊即赔 100种重疾全面保障 输人手机号立即领取|最后一步，填写被保人信息 为谁|住院或被要求进一步检查、手术或住院治疗|全患有以下疾病或存在下列情况者:|开颅手术、多发性硬化、|本投保申请涵盖的被保险人中，如有2周岁以下|几梗死、肺动脉高压、肺源性心脏病、主动脉疾|儿童(含2周岁)，请同时告知以下事项: 是否有早产，且出生体重低于2500克者?是否在出|投保须知|承保公司及销售区城:本保险产品”轻松保·重疾险|胆石症、胆囊息肉；|.坏死性筋膜炎、肌营养不良症、不明原因的肌肉 签缩、斯蒂尔病、强直性脊柱炎、椎间盘突出症、|65000元。 份。|动，可随机领取保额，保单保障期间内保额累计最|本人|配偶|受益人:被保险人本人。|0保额:初始保额800元，后续通过参与相关活|为谁投保|生脑|下简称众惠相互”)，销售公司为广东轻松保保险|免费领65000元重疾保隆|轻秘保严选|确认被保人是否符合投保条件|食先活关|食夫亏头|好保呤念 要严选|激活保障|保严选|为保证被保人的保险权益在理赔时不受影响请|(贵任痕国内|(责任范围肉内，|体重减轻5公斤以上)。|本产品由众惠财产相互保险社承保 具体费率及保费金额以实际情况为准|免费领|1赔付比例:100%给付。|了解详情\", \"video_asr\": \"哎，你们欠我的房租准备什么时候给我姐？我老公查出恶性肿瘤，我正找着镜头前给他准备医药费呢，药费。|怎么不找保险公司呢？我们都是基层打工人员。|哪有什么钱买保险啊，我之前不是让你免费领了轻松保研选重疾险吗？你老公的这种病就在保障范围内，可以申请理赔的。我想起了之前我们填写过手机号和身份信息，免费领了，他们真的能够理赔吗？当然了，为了让你和你的家人有。|应对疾病的能力轻松保研选重疾险首次八百元，后续通过参与相关活动，保单保障期间内累计最高六万五千元的保额能保障一百种重大疾病确诊即赔，太好了，这下我们有救了，不要等疾病来临时才后悔！|赶紧点击视频下方链接，为您和家人免费领取一份保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01706981658935547 sec\n",
      "{'result': [{'labels': ['中景', '现代', '填充', '多人情景剧', '推广页', '静态', '拉近', '愤怒', '悲伤', '公园', '夫妻&恋人&相亲', '惊奇', '手机电脑录屏', '喜悦', '室外', '朋友&同事(平级)', '平静', '路人', '动态', '远景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.91', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0946254c2e67800b56e0ba6e7d7a615f.mp4\n",
      "{\"video_ocr\": \"脑子转得快|怎么了|算数不用张指|逻辑思维好|这就是你的秘密武器|斑马思维课|现在报名响9元|就可以体验十节课|教具礼包|免费包邮到家|赶紧点击|视领题方链接|视频下广方链接|报名吧|斑马A1说|学思维学英语2-8岁上斑马 猿h有出品|猿辅品品|猿寻而城出品|猿辅导在线教出品|在线教出品|出品|祆在线教育 出品|3多每在教育出品|骗导在线歌而出品|8岁上斑书|第1日|里|须射导在钱教育出品|好多好玩的|跟教学材料|49元 体验10节课|思维|S1|逻辑训练 数独游戏|NOMOSL HINUMOSG|TOY|Toy|薄术小急E|更三重|数的比般|s2|现马AI课思维买统送|的马A银思姐买统|1TOy|JCHIND,|Iil|体验课 思维练习册|10|斑马Al课果|马A课|斑马Al课|马AI1果|进马A湮|数学玩具|Illl|红情谭|MOSGHI|更多\", \"video_asr\": \"脑子转的快怎么了？算数不用掰手指怎么了？逻辑思维好怎么了？这就是你的秘密武器。|AS V课。|哎。|现在报名四十九元就可以减十节课，我要去礼包免费包邮到家，赶紧点击视频下方链接报名吧！|吃吃吃。\"}\n",
      "multi-modal tagging model forward cost time: 0.01631021499633789 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '平静', '教辅材料', '场景-其他', '多人情景剧', '填充', '极端特写', '静态', '亲子', '室内', '单人口播', '手机电脑录屏', '配音', '家', '喜悦', '动态', '朋友&同事(平级)', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.95', '0.93', '0.89', '0.79', '0.67', '0.55', '0.43', '0.41', '0.40', '0.31', '0.27', '0.06', '0.04', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09557d0b01ce7b3a7bff7736d434849e.mp4\n",
      "{\"video_ocr\": \"昨日你在君上面前如此扎眼|尚服局已经没有留你的必要了|不是的|我当时是...|拿好你的东西|别让我再看见你|司制大人|我方才收拾行李的时候|还看到一块玉|那玉瞧着价格不菲|颇为可疑|她定是手脚不干净|还请司制大人给她定罪|你把玉还给我|这玉|娘娘饶命|来人|把这以下犯上的丫头|给我拉出去掌嘴|娘娘|这是个女官成长游戏|开局就身处冷宫|侠 重新选身份一路逆袭|看到秀女在欺负小孩 帮助公主 接近宠妃|与帅气的君上谈一场甜甜的恋爱|从小小女官到权倾天下的王后|一路斗智斗勇|晋升后还有君上赏赐的漂亮衣服|大旗|把自己打扮得美美的|再养个可爱的萌娃|母凭子贵 稳坐后位|雍荣华贵|画面仅供参考请以游戏为准|玩耍|前刖制止 视M不|化内不|浪鼓|START|步步莲花|随机人生|常在 秀女|公主|洗澡 呼乳\", \"video_asr\": \"昨日你在君上面前如此眨眼，上服已经没有留你的必要了，不是的，我当时是拿好你的东西，别让我再看见你四肢大人。我方才收拾行李的时候，还看到一块玉，那玉翘着，价格不菲，颇为可疑。|你是手脚不干净还行素质大人给他定罪。|哎你你把玉还给我致郁。|娘娘，饶命，来人，把这以下犯上的丫头给我拉出去，掌嘴娘娘。|这是个女官成长游戏开局，生在冷宫重新选身份，一路逆袭，帮助公主接近宠妃，与帅气的君上谈一场甜甜的恋爱，从小小女官到权倾天下的王后，一路斗智斗勇，进身后还有君上赏赐的漂亮衣服，把自己打扮的美美的，再养个可爱的萌娃，母凭子贵，稳坐后位。\"}\n",
      "multi-modal tagging model forward cost time: 0.01632857322692871 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '全景', '特写', '平静', '动态', '惊奇', '喜悦', '室外', '朋友&同事(平级)', '极端特写', '愤怒', '亲子', '家庭伦理', '拉近', '路人', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.87', '0.74', '0.60', '0.55', '0.27', '0.13', '0.09', '0.04', '0.04', '0.03', '0.03', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0957c4f42b93b9f5065e1a7ee8560f80.mp4\n",
      "{\"video_ocr\": \"华少|形象新颖，别具一格。|能写出这样的句子靠的是|孩子的灵气和天分吗|并不是|靠的是科学的思维训练|掌握了科学的学习方法|你的孩子也能写出这样|有创意的文字|我是高巍巍|现在是高途课堂|小学语文体系的创始人|成为更加优秀的自己|查看详情 就能报名|快!跟我一起来|全国百佳教师带队教学 平均教龄11年|我的妈妈是有颜色的，|就像一个“采|就像一个“彩虹人|当她很开的时候|当她很开心的时候是绿色的，|因为|因为她开心的时候京|她|她生气的时候头上就像燃烧|她生至封凯明美上 就像燃烧|候是到色的，|语文科学思维训练|浙江卫视|望无j|望无边的田野；|着的|着的火把!|点评:|高分作文|妈妈|妈妈生气的时候是乡|妈妈生气的时候是红色的，|妈妈生气的时|好老师+个性+技法|新用户专享 立即体验|年教龄 累计学员超们万|名师特训班|好老师的启发孩子的个性 作文技法|浙江卫视指定在线教育品牌|新用户专享立即体验|高巍巍|高途课堂|1学y|仅需|￥9|加r|11\", \"video_asr\": \"我的妈妈是有颜色的，就像一个彩虹人，当她很开心的时候是绿色的，因为他开心的时候就像那一望无边的田野，妈妈生气的时候是红色的，而生气的时候头上就像燃烧着的火把。|Y。|能写出这样的句子，靠的是孩子的灵气和天分吗？并不是靠的是科学的思维训练，掌握了科学的学习方法，你的孩子也能写出这样有创意的文字。好老师的启发加孩子个性，加必备的作文技法，一定会诞生出一篇高分作文。我是高薇薇，现在是高途课堂小学语文体系的创始人，成为更加优秀的自己，查看详情就能报名，快跟我一起来！\"}\n",
      "multi-modal tagging model forward cost time: 0.01662611961364746 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '静态', '填充', '影棚幕布', '配音', '教师(教授)', '平静', '场景-其他', '幻灯片轮播', '特写', '室内', '转场', '图文快闪', '学校', '知识讲解', '教辅材料', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.10', '0.06', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0964b0db45d3740e0c6273419bdc80c8.mp4\n",
      "{\"video_ocr\": \"只要产品不过关|我是绝对不会买的|你放心|电信星卡绝对会给你惊喜|杨秘书|你翻译翻译|什么|才叫惊喜|惊喜就是|电信星卡每月有200G高速流量|+5G全国通用流量|首月免费用|而且话费充50得100|还送您3个月视频会员|那它月租是多少|每月19元|而且第一月还是兔费|好|而且我们点击视频下方|就可以免费办理|还可以包邮送到家|100分钟免费通话|免费领取|200G 0元领取 包邮到家|中国电信 CHINA TELECOM|看视频免流量|中国电信星卡升级版 超百款热门APP免流量|升级版星卡|大流量卡|5G通用流量|限量1000张|不限速\", \"video_asr\": \"只要产品不过关，我是绝对不会买的，您放心，电信新卡绝对会给您惊喜杨幂哦！|你翻译翻译。|什么才叫惊喜？惊喜就是电信新卡每月会有二百G的高速流量加五G全国通用流量，首月免费用，而且话费充五十得一百，还送三个月视频会员免费月租是多少？每月十九元，而且第一个月还是免费好，这才叫洗衣，而且我们点击视频下方就可以免费版，还可以包邮送到家。\"}\n",
      "multi-modal tagging model forward cost time: 0.016452550888061523 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '填充', '推广页', '平静', '单人口播', '手机电脑录屏', '喜悦', '全景', '多人情景剧', '路人', '办公室', '场景-其他', '朋友&同事(平级)', '惊奇', '特写', '动态', '采访', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.75', '0.53', '0.08', '0.07', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09724ab52ffe90feba0aa199e01fa9c3.mp4\n",
      "{\"video_ocr\": \"儿媳妇|我跟大伟|已经离婚了|你这是干什么呀|以前都是妈不对|妈拆散了 你们|现在大伟得了肝病|需要几十万|好歹你们夫妻一场|你可不能不管呐|那你赶紧找|保险理赔呀|保险|大伟买保险吗|我俩早在健康的时候|就都买了|微医保百万医疗险|保额高达600万呢|就是那个|首月保费1元|次月14元起的|那个保险吗|保费那么低能报吗|怎么不能|保险责任范围内|大小病意外住院|产生的住院费医药费|社保未覆盖的进口药|自费药都可以申请报销|赶紧向保险公司申请啊|哎哎|那我现在就申请|是妈亏久你了|你也赶快点击下方|给自己和家人一份保障吧|首月1元次月14元 起|(次月起保费随不同年龄、有无社保等情况变化)|(一般住院医疗1万免赔额/年、100种重疾住院0免赔)|患病看得起|泰康在线 申请成功等待核实|TK.CN|*内容仅为广告创意 纯属虚构|泰康在线财产保险股份有限公司承保|申请成助寺侍假头|阅读保险条款重要内容和投保须知，确认被保人符合健康告知和投保条件|73DLI\", \"video_asr\": \"媳妇，我跟大卫已经离婚了，你这是干什么呀？你说什么话做什么？|打打看去尽是我，好歹夫妻一场，你给我滚，那你赶紧找保险理赔啊，保险大卫买保险啊，我俩早在健康的时候就都买了微医保百万医疗险，保额高达六百万呢，就是那个首月保费一元。|十四元起的那个保险吗？保费这么低，能宝马怎么不能保险责任范围内大小病，意外住院产生的住院医疗费，社保未覆盖的进口药，自费药都可以申请报销，赶紧向保险公司申请啊，哎哎。|我现在就申请。|申请成功等待何时？|快，媳妇是妈亏欠你的，你也赶快点击下方，给自己和家人一份保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01658320426940918 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '中景', '静态', '喜悦', '路人', '室外', '全景', '平静', '悲伤', '惊奇', '动态', '愤怒', '手机电脑录屏', '夫妻&恋人&相亲', '朋友&同事(平级)', '亲子', '极端特写', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.85', '0.77', '0.76', '0.72', '0.70', '0.44', '0.36', '0.21', '0.18', '0.12', '0.03', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0972d12723091413920be5a2fdcfaec1.mp4\n",
      "{\"video_ocr\": \"再加送|再扣暖送|终于到啦|斑马A课思维体验课|免费赠送的教具礼盒|哇|这里面的东西还挺丰富|正方休积本|数独游戏棋|推理点游|百变思维盒|思维教具礼盒|练习翻|思维练习册|思维动动卡|配套贴纸|专为3-6岁孩子设计|通过动画|儿歌|和游戏互动的形式教学|激发孩子的学习主动性|培养孩子运算能力|逻辑推理能力|和记忆能力|49元10节课|让孩子爱上学习|现在报名|还包邮赠送|赶紧点击视频下方|查看详情报名吧|多耀马赛克正方体积术|逻辑训练|守护小镇大作战|百变木棒 思维盒|体验课|S1|猿辅导在线教育出品|斑马AI课 猿辅导在线我得 山品|狼辅导在线教算 出场|3节国庆AI互云课!|30节国庆 I云课!|教具礼盒免费包邮送到家|出品|爱辅导 unm|狼辅导在线教期 出品|狼辅导在线视育 l|现在购买归元/10课时 的思维体验课|2-8岁上斑马 学思维 学英语|TXT|国庆特惠啦!|全套教具大礼盒\", \"video_asr\": \"斑马AI课国庆特惠了，四十九元，十节课加全套教具大礼盒，再加送三十节国庆AI互动课。|终于到了大吗AI课思维体验课免费赠送的教具礼盒。|哇，这里面的东西还挺丰富啊，就像单身职业了，医生说要在操场上，真的是门的可以。|那你是水草莓兵，如果你有一点是因为你的笑貌元无耻听到这首轻松的旋律。|斑马AI艾克思维体验课专为三到六岁孩子设计，通过动画，儿歌和游戏互动的形式教学，激发孩子的学习主动性，培养孩子运算能力，逻辑推理能力和记忆能力。四十九元十节课，让孩子爱上学习，现在报名还赠送思维启蒙教具礼盒，赶紧点击视频下方查看详情报名吧！|说不清。\"}\n",
      "multi-modal tagging model forward cost time: 0.021448373794555664 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '配音', '平静', '中景', '静态', '室内', '单人口播', '教辅材料', '场景-其他', '课件展示', '极端特写', '特写', '手机电脑录屏', '动态', '过渡页', '教师(教授)', '室外', '影棚幕布'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.96', '0.89', '0.61', '0.09', '0.03', '0.03', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09928212f257a98dbf95d6dc5af3b323.mp4\n",
      "{\"video_ocr\": \"我叫欧阳铁柱|最近刚拿到女神的联系方式|相信用不了多久|我就能追到女神|想想还有点小激动呢|怎么跟她搭讪好呢|有了|怎么还没回复我|等再过|四|九十九天|女神肯定会回复我的|她回了|这不是拼多多推出的|天天领现金活动吗|打开拼多多|点击天天领现金|邀请好友助力|就可以获得|最高100元的现金红包啦|助力成功后|女神肯定会因此感谢我吧|哇真的领到啦|BznuqzhkKoDoPt.|¥91.20|96.00|¥100.00|拼多多APP专享|女神|好物疯抢|拼多多\", \"video_asr\": \"我叫欧阳铁柱，最近刚拿到女生的联系方式，相信用不了多久我就能追到女孩，立即激动等等，怎么跟她搭讪好呢，有了。|怎么还没回复我？算了，再等过一二三四五九十九天，女神肯定会回复我的回了他回了，这不是拼多多推出了天天领现金活动吗？打开拼多多，点击天天领现金，邀请好友助力，就可以获得最高一百元的现金红包了，成功后。|是吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.01645946502685547 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '手机电脑录屏', '推广页', '单人口播', '喜悦', '平静', '多人情景剧', '家', '极端特写', '室内', '夫妻&恋人&相亲', '家庭伦理', '配音', '动态', '全景', '朋友&同事(平级)', '混剪', '拉近'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.71', '0.50', '0.41', '0.36', '0.09', '0.09', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0996762b68a28915bb9245312969274f.mp4\n",
      "{\"video_ocr\": \"我的小祖宗啊|你还想不想干了|你快点啊|这个统计表|客户一直在催你知道吗|我来|表格汇总做好了|赶快发给客户吧|这么快|我会PYTHON啊|/PYTHON可以帮你批量处理表格|数据分析可视化 实现自动化办公|全面提升工作效率|我也想学|可是我没有基础 能学么|能学在扇贝编程报名|只要6块9|0基础带你入门Python|这么好在哪报名学习啊|现在点击视频就能报名啦|涨薪季 6.9元代元|6.9|PythonM础课|Python基础课+认知课|学什么|视频为演绎情节|扇贝编程|立即购买\", \"video_asr\": \"我的小祖宗啊，你还想不想干了快点，丫头两字不一直在森林大吗？我来。|九，哦，不懂不好了，赶快发给客户吧，怕我会拍什么呢？拍照可以帮你提亮嘴角和数据分析可视化，实现自动化办公全面屏。|我教育我也想学，可是我没有基础能学吗？能学在上面就能报名，只要六块九，零基础的你就能开门，这么好在哪能报名学习啊，现在就下方链接就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01640915870666504 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '办公室', '推广页', '手机电脑录屏', '静态', '平静', '工作职场', '外卖', '极端特写', '特写', '朋友&同事(平级)', '企业家', '愤怒', '上下级', '喜悦', '路人', '家', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0999d91b86011132a3a122a993e43649.mp4\n",
      "{\"video_ocr\": \"存款标准了|你的存款达到|如果没有别气馁|给我12天时间|我让你学会如何 超过平均工资标准|很多人拼命工作|付出大量的时间体力|却没有任何存款|拼命赚来的钱|抵不过物价上涨|通货膨胀|这是因为|只靠死工资是远远不够的|你需要学会投资理财|即使不工作|靠额外收入|也能过自己想要的生活|推荐你报名微淼商学院 小白理财训练营|科学系统的学习理财|0基础怕风险 这些都不是问题|仅需12元12天|专业理财老师 会手把手的教你|如何使用基金国债 股票等多种理财工具|让你学会如何钱生钱 增加被动收入|即便没有工资 也不怕生活水平下降|现在点击视频下方按钮|报名提高你的存款吧|女敉年龄荐款一览表|微淼商学院|限淼|商学院|微|学理财上微淼|1n加|学理财上微淼|学见上微|学财微|>IうZ山>-U|>エうz>-0|广I1z4山>-0|，仅12元!|存款(万元)|存款|年龄 孩子年龄 存款(万元)|30岁应该有 多少存款?|汕即yゃ|LOUIs VUITTON|后悔才知道!月薪3000也可|1974年|出生年份|45|198卡元|32|DIOR|呼玉|以理财!12天学会理财思维|年龄 孩子年龄|视频为演绎情节|120|60|17|KeauaTilu|OKTaN|不8|查看详情|ale|HANEL|21\", \"video_asr\": \"你的存款达到平均标准了吗？如果没有联系嘞，给我上二天时间，我让你学会如何超过平均工资标准，很多人拼命工作。|说大量的时间体力，却没有任何存款。|拼命赚来的钱，只做物价上涨，通货膨胀，这是因为只靠死工资是远远不够的，你需要学会投资理财历史不工作，靠额外收入也能过上自己想要的生活。推荐你报名微秒商学院小白理财训练营，科学系统学习，理财零基础。|怕风险，这些都不是问题，七十二元十二天，专业理财老师会手把手教你如何使用基金，国债，股票等多种理财工具。|让你学会如何钱生钱，增加被动收入，即使没有工资，也不怕生活水平下降，现在点击视频下方按钮，报名提高你的存款吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016730308532714844 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '单人口播', '平静', '静态', '室内', '推广页', '配音', '手机电脑录屏', '教师(教授)', '场景-其他', '特写', '混剪', '宫格', '喜悦', '影棚幕布', '惊奇', '动态', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.68', '0.57', '0.30', '0.27', '0.03', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09a6a1b9e1307651f1c5f0842700fe15.mp4\n",
      "{\"video_ocr\": \"很多家长不知道该|怎么培养孩子的|语文思维|我给你推荐|斑马A课语文体验课|课程是将汉字的由来|清楚的传达给孩子|利用相关的物体|表现汉字的意义|根据不同年龄段的孩子|每个阶段|都会采取不同的|教学方案|针对性的培养|3-6岁的宝宝|每天15分钟|动画儿歌教学|短时高频|让孩子在快乐中|自主学习语文|爱上语文|现在报名|49元10节体验课|还赠送定制教具礼盒|赶紧点击视频|报名吧|元10节课|斑马A|课 猿辅导在线教育 出品|猿辅导在线影|猿辅导在线权制的|猿辅导在线秒 ea|悯农|锄禾日当午|汗滴禾下上|谁知盘中餐|粒粒皆辛苦|TENTION|北大中文系硕博精心打磨课程|妆与A果|极马A1果|在线教青出品|猿辅导|语文|孩子|适合3-6岁|趣味启蒙课|49\", \"video_asr\": \"锄禾日当午。|汗滴禾下土。|谁知盘中餐，粒粒皆辛苦苦，很多家长不知道该怎么培养孩子的语文思维，我给你推荐斑马AI课语文体验课课程是将汉字的由来清楚的传达的，孩子利用相关物体表现汉字的意义，根据不同年龄段的孩子，每个阶段都会采取不同的教学方案。|针对性的培养三到六岁的宝宝每天十五分钟动画，儿歌教学，短时高频，让孩子在快乐中自主学习语文，爱上语文。|现在报名四十九元十节体验课，还赠送定制教具礼盒，赶紧点击视频报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016286134719848633 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '静态', '单人口播', '家', '平静', '场景-其他', '亲子', '特写', '配音', '宫格', '室内', '喜悦', '多人情景剧', '拉近', '情景演绎', '动态', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.98', '0.44', '0.38', '0.24', '0.16', '0.09', '0.06', '0.05', '0.03', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09b153bd298b8c6d4fe7ca3a9dccd153.mp4\n",
      "{\"video_ocr\": \"妈妈，我考完回来了|这么早，说!|你是不是又交白卷了|怎么可能|这次考试内容那么简单|我做完就出来了|行了行了|你什么成绩我还不知道啊|喂|欣欣妈妈，欣欣这次|考试做题正确率又高|速度又快|以后要持续下去啊|好好好|哎!你怎么回事啊|你忘了上次花30元|给我报的作业帮直播课|我这次写题这么快|还不是因为|清北毕业老师带队教学|教我的解题大招|之前刷那么多题|根本没用|技巧才最重要|我现在随便一道题|都能很快写出答案|就是已经熟练的多了啊|上一题正确率37%|此题目前正确率80%|有了一个很大的飞跃啊|看来我好像讲明白了啊|作业帮直播课|帮助大家提升解题能力|举一反三|课后1对1辅导答疑|不懂的问题随时问|点击视频下方|查看详情报名吧|上课内容以实际为准|报名就送超值大礼包 上课内容与收到礼盒请以实际为准|名师提分班 ￥30=18节课|小初高数学|教辅 礼包 包邮\", \"video_asr\": \"妈妈就考完回来了。|这么早说你是不是交白卷了，怎么可能这次考试那么那么简单，我做完就出来了，行了行了，你什么成绩还不知道啊，没，谢谢妈妈，估计这次考试做题正确率又高，速度又快。|要持续下去啊，呵呵呵呵哎，怎么回事啊，你忘了上次花三十元给我报了作业，帮直播课，我这次解题这么快，还不是一清北毕业老师带队教学。|教我解题大招，之前刷那么多题呀，根本没用，技巧特别重要，我现在随便一道题都能很快写出答案，就已经熟练了多了一百。|妻子屏幕前正确率百分之八十，用了一个很大的一个飞跃啊，看来看来我好像讲明白了啊。|随意帮直播课帮助大家提升数学解题能力，举一反三，课后一对一辅导答疑，不懂的问题随时问，点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016263961791992188 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '多人情景剧', '静态', '家', '家庭伦理', '亲子', '平静', '极端特写', '喜悦', '愤怒', '动态', '惊奇', '全景', '教辅材料', '夫妻&恋人&相亲', '手写解题', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.62', '0.56', '0.40', '0.24', '0.11', '0.03', '0.03', '0.03', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09b2fd5a585ccd070f614eaa7451d002.mp4\n",
      "{\"video_ocr\": \"点击下方链接|继续阅读全文\", \"video_asr\": \"来来来。|谢谢。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.0164792537689209 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '特写', '中景', '平静', '极端特写', '多人情景剧', '愤怒', '情景演绎', '动态', '喜悦', '室内', '办公室', '单人口播', '手机电脑录屏', '拉近', '家', '惊奇', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.87', '0.48', '0.17', '0.13', '0.07', '0.06', '0.04', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09b35f892d181046abc598107c5dc26f.mp4\n",
      "{\"video_ocr\": \"老板 这是什么|这是年终奖 给你的福利|这是年终奖 给你的福利|就一张电话卡|这个电信星卡|可以让你月享40G流量|和300分钟免费通话|你这平时玩游戏看剧什么的|不都全够了|那不得交话费啊|首月免费给你用 不要就算了|哎呦 老板 我错啦|哎呦 老板 我错啦|您就当做什么都没发生|我给你揉揉肩|想领取的朋友也赶紧点击视频下方|0元免费领取吧|七O中国电信 CHINA TELECOM|中国毛信|它中电编|中平骗|好中信|中国|$出国电情|家中国电馆|NATELECOM|Hl|ITEECOMA|NTGUCOM 宝|HINASFLEGOM|字牛三一|三＄三|060|0000\", \"video_asr\": \"老板，这是什么？这是年终奖啊，给你的就一张电话卡。|这个电信新卡可以让你乐享四十G流量和三百分钟语音通话，你平时玩游戏，看剧什么的。|钱够了，那不得交话费了，首月免费给你了，不要就算了，哎呀，老板，我错了，就当什么事都没发生了。|下个趣的朋友，赶紧点击视频下方领取免费领取啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016152381896972656 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '平静', '单人口播', '室内', '办公室', '动态', '特写', '喜悦', '教师(教授)', '愤怒', '极端特写', '惊奇', '工作职场', '企业家', '情景演绎', '手机电脑录屏', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.78', '0.66', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09b3d4117a6b60fa7633c30c9b0bbf8f.mp4\n",
      "{\"video_ocr\": \"你这好好的叹什么气啊|我这网店开了一年多了|到现在还没做起来|这访客不是挺多的吗|那是我花钱买推广了|这访客是上去了|可是销量还是没上去|是不是产品卖得太贵啊|价格超级低|几平没有什么利润|可就是卖不出去|你说|我是不是不适合开网店啊|哎呀你肯定是没找对运营方法|要不你试试潭州教育|淘宝运营课程免费在线直播试学|讲师免费对店铺进行诊断|还有将推广引流转换为销量的实战方法等等|让你开网店少走弯路|这也太好了吧|你赶紧告诉我怎么报名|想要知道更多增加销量的方法|那就直接点击视频下方链接|获取免费试听名额吧|潭州教育|的店铺有新的访客|10/2-1781968|/ 19G8|点击视频下方链接|淘宝运营课程|访客+1|短信\", \"video_asr\": \"这。|这红包太什么气啊，我这网店开了一年多了，到现在还没做起来，这访客不是挺多的吗？那是我花钱买推广了，访客是上去了，可是销量还是没上去，是不是产品卖的太贵啊，价格超级低，几乎没有什么利润，可就是卖不出去，你说我是不是不适合开网店？|呀，你肯定是没找运营方法，要不你试探教育淘宝运营课程，免费在线直播，是学讲师免费对店铺进行诊断，还有将推广，引流转换为销量的世界方法等等，让你开网店少走弯路，这也太好了吧！你赶紧告诉我怎么报名啊！想要知道更多增加销量的方法，那就赶紧点击视频下方链接获取免费试听名额吧！|亚足联。\"}\n",
      "multi-modal tagging model forward cost time: 0.016069889068603516 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '填充', '静态', '推广页', '平静', '惊奇', '喜悦', '单人口播', '特写', '家庭伦理', '夫妻&恋人&相亲', '朋友&同事(平级)', '亲子', '室内', '教师(教授)', '家', '拉近', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.82', '0.76', '0.60', '0.24', '0.18', '0.08', '0.06', '0.05', '0.05', '0.03', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09b4e0caee3db42d59e484635eb09b56.mp4\n",
      "{\"video_ocr\": \"几子|上车吧|下这么大雨|咱们赶紧走吧|好不好 儿子|我就不回去|哎呀 儿子|爸爸妈妈知道错了|现在就给你报|不就9块钱嘛|晚了|早就没名额了|你们就知道|忙忙忙忙忙|我都上高二了|别人家的孩子|作业帮直播课|我也不知道|9块钱就能报名|国内外毕业名师|带队直播教学的课程啊|考试别的同学|英语作文已经写完了|我阅读理解都没写完|人家试卷已经|检查两遍交卷了|可我最后连作文 都没写完呢|嗨|有了!有了!|英语单词 语法名师课|又开始授课了|而且啊还是9块钱|就能上13节名师课呢|教研团队整合的|解题技巧和学习方法|都直击考点|而且啊支持|三年内无限次回放|那还等什么呀|赶紧给咱孩子报名|好!|屏幕前的家长|赶紧点击下方链接|给你的孩子报名吧|上课内容与收到礼盒请以实际为准|名师有大招 解题更高效|上课内容请以实际为准|报名就送超值大礼包|英语单词语法名师课|9元/13节课|致直播课 的我们|单词本 成长笔记|经典易错100题|高分学习地钱|单词宝典\", \"video_asr\": \"儿子上车吧，咋这么大，你能抬你走吧好不好，儿子我就不回去。|爸爸把妈妈知道错了，现在就给你报，不就是九块钱吗？晚了早就没名额了。你们就知道，忙忙忙忙忙，我都上高二。|别人家的孩子早就报名了作业帮直播课，我也不知道九块钱就能报名作业帮直播课，国内外毕业名师带队直播教学的课程啊，考试的同学英语作文。|完了我阅读理解都没写完，人家十月元检查两遍交卷了，我最后连作文都没写完了，有了有了。|帮直播课，英语单词语法名师课又开始授课了，而且啊，还是九块钱就能上十三节名师课呢！居然团队整合的解题技巧和学习方法都直击考点，而且要支持三年内无限次回放，还等什么呀？|孩子报名平平的家长，赶紧点击下方链接给你的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016538619995117188 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '中景', '推广页', '亲子', '静态', '全景', '愤怒', '悲伤', '喜悦', '室外', '动态', '平静', '朋友&同事(平级)', '路人', '家庭伦理', '单人口播', '惊奇', '亲戚(亲情)', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.77', '0.54', '0.54', '0.16', '0.15', '0.08', '0.06', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09bda0d2d49be145983790e3b00f9c21.mp4\n",
      "{\"video_ocr\": \"白送你iPhone|你都不要|您别烧了|闭嘴|我们飞机工坊|免费送iPhone11手机|你还想要什么|我最后一次强调|是免费抽|iPhone11|不用你花1分钱|上线就能|直接抽手机|不搞套路|不玩虚的|这些手机|3天内送不完|点击视频下方链接|下载最新版|就能直接抽|你也赶紧去试试吧|华为mate30 5G手机碎片 填写收货信息 (0.0/10)|每天全网随机限量发放，机 完善资料 抽奖机会+1|看视频|去完成|惊喜红包! 你获得|抽奖机会x1|连续签到7天有惊喜 我的奖品|获得更多抽奖机会|送手机 图鉴 开始战斗|(0/1)|解锁15-20级飞机 抽奖机会+1|每看一次抽奖机会+1|今天 明天再来|解锁6级飞机 抽奖机会+1|1-1 成就 任务|限时抽奖|合成升级|95%|100%|签到领惊喜|具体奖励以实际完成任务为准|没有足够的停机位了|解锁15-20级飞机|解锁25级飞机|会有限，先到先得)|如何兑换手机|完善资料|免费抽5G手机|開|时机工坊|08.08\", \"video_asr\": \"白送你分你都不要。|白送你一分你都不要。|告诉你，别说了，闭嘴，我们飞机工坊免费送IPHONE十一，手机都不要你要什么？|我最后一次强调，我们飞机工坊是免费抽IPHONE十一，不用你花一分钱上线就能直接抽手机，不搞套路，不玩虚的，这些手机三天内做不完，我全收了。|点击视频下方链接下载最新版飞机工坊，就能直接抽IPHONE十一手机，你也赶紧去试试吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016425132751464844 sec\n",
      "{'result': [{'labels': ['现代', '静态', '推广页', '中景', '单人口播', '手机电脑录屏', '多人情景剧', '喜悦', '平静', '动态', '特写', '场景-其他', '拉近', '愤怒', '惊奇', '室内', '朋友&同事(平级)', '配音', '路人', '单人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.73', '0.56', '0.12', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09c8ab23bd75b05f04082bbeaa2f6049.mp4\n",
      "{\"video_ocr\": \"都怪你|整天就知道忙工作|我们班同学家长|都给他们报名了|作业帮直播课 英语单词语法名师课|谁也没想到|9元13节课|效果能这么好啊|我现在给你报名还不行吗|晚了|现在早没有名额了|如果我能早能学会|超级拼读法和|高效记单词这些方法|我也能篇篇得高分|真的是国内外名校毕业 的名师带队教学|人家一次掌握了|八大单词记忆法|高效解决我们单词记不住|全靠死记硬背的笨方法|其他同学都做完检查了3遍|可我呢|我连作文都没写完|李总 桃桃|又有名额了|你看|而且还包邮赠送 全套辅导礼盒|我给我儿子也报名了|之前做题速度慢|答题没技巧|错的多|现在学习英语的积极性|特别的高|而且课后还有辅导老师 1对1答疑|相当于花了1份钱|请了2个老师|那这课在哪报名啊|点击视频下方|就可以报名了|报名就送超值大礼包 上课内容与收到礼盒请以实际为准|名师有大招解题更高效|B|单调本 单词宝典\", \"video_asr\": \"ZZZZ。|都怪你整天就知道忙工作，我们班同学家长都给他们报名了作业帮直播课，英语单词，语法名师课啊，谁也没想到九元十三节课效果能这么好，现在给你报名晚了，现在早没有名额了，如果早能是不会超级拼读法和高效记单词的方法，我也能拼分的高分。|我也没想到啊，真的是国内外名校的名师带队教学，人家一次掌握了八大。|单词记忆法高效解决我们单词记不住全靠硬背的笨方法，其他同学都做完检查了三遍，可我呢，我连作文都没写完。|李总，陶陶又有名额了，你看还是九元十三节课，而且啊，还包邮赠送全套辅导礼盒，我给我儿子也报名了，之前做题速度慢，答题没技巧，错的多，现在学习英语的积极性啊，特别高，而且课后还有辅导老师一对一答疑，相当于花了一份钱请了两个老师不在了，你点击视频下方就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016171932220458984 sec\n",
      "{'result': [{'labels': ['填充', '多人情景剧', '现代', '推广页', '中景', '汽车内', '惊奇', '喜悦', '愤怒', '家庭伦理', '静态', '亲子', '平静', '夫妻&恋人&相亲', '特写', '家', '悲伤', '手机电脑录屏', '全景', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.89', '0.84', '0.74', '0.33', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09ca475272886f09befb919c53cd5aaa.mp4\n",
      "{\"video_ocr\": \"¥170，950.00|¥40，00.00|print(salesDF.head())|要点1.应用rename函数，首先要用字典定义原列名和新列名的关系。|因此对Dataframe的列赋值菱将值转化为5eries|从上述可以看到，等伤要求和工作经验的值比较少且没有放失值与异常值，可以直接进行分标:仙聚资的分布比较宝，总计有75种，为了更好地进行分析，我们|代码 要点1.ExcelFi1e函数不可以直接输入文件的地址，否则会被判断为字符串无法解码；|100.00%|148，20000¥130，700.00|工作经验|erros=\\\"coerce’表明该格式为强制执行，如果原始数据不符合日期格式，转挽后的值为a|￥9.000.00|要点2.inplace-FaLse时指新创建一个DF并在新的DF上修改，|satesDF[销售时间*]-pd.Series(datelist)|无法使用.astypeQ)，需要使用pandas包的to_datatime()函数 要点2.将字符申数据类型转换为日期数据类型，|28.98%|6.41%|图片设计元素|22jpg|要点：datelist本身是个列表，Dataframe的绿列都是一个Series|5年度相关数据及所有渠道或者代理商名称(要求与基础数据表一致、且渠道数量完整)即可，其他数据及图表为自动生成，请勿随意录入数据。|里商名称(要求与基础数据表一致、且渠道蚁量完整)即可，其他数据及图表为自动生成，请勿随意录入数据。|Column Selection Mode|xl-pd.ExcelFile(salesmed)|datelist.append(dateStr)|学大数据沂 就去开课0|2017上半年各渠道商总体销售数据对比分析(全自动，第1名醍目提示)|渠道商总体销售数据对比分析(全自动，第1名醒目提示|年渠道每月度及总体销售数据情况，包括各渠道销售额占比，同比增长，环比增长，月均增长率、销售额排序等关键数据分析，使用方法:本年度各月数据来源|import pandas as pd|dateStr-datesplit[o]|salLesDF.shape|for value in salesSer1:|s.mkdir(dir)|营销实战工具—销售业务管理|2016上半年合|2015年下半年|70.95%|79.11%|12.70%|Baikeba 开课吧|19.86%|九填充|print(salesDF.loc[O：3,:D|长率|10:10%|排名\", \"video_asr\": \"\"}\n",
      "multi-modal tagging model forward cost time: 0.017253398895263672 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '手机电脑录屏', '推广页', '才艺展示', '配音', '特写', '图文快闪', '宫格', '填充', '重点圈画', '商品展示', '平静', '幻灯片轮播', '动态', '绘画展示', '拉近', '手写解题', '课件展示', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09cd302bfee2935fb6286317fe22953b.mp4\n",
      "{\"video_ocr\": \"T|可节，|宝迂套教材礼盒|包邮 送配套教材礼盒|速马A课英语体验课|49元/10节|学思维/学英语/2-8岁上斑马|GranEpa,grandpe,gradpa. Grandpa isworlkinge ，节爷，节爷。节爷在劳动。|Granepa,girandpa,grandpa,Grandpaisworking. 节爷，爷爷，节节爷杀在劳动。|贸，爷爷，带爷爷爷在劳动|Grandpa,grandpa,grandpaishappy D爷很高六|带市，奇节爷很高六。|Grandma,grandma，grandma. Grandmais working. 奶奶，奶奶，奶奶奶奶在劳动。|奶奶，奶奶，奶奶很高兴|OD\", \"video_asr\": \"滚滚滚滚滚滚滚滚。|闺女闺女闺女闺女，闺女，闺女闺女。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.016675233840942383 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '情景演绎', '才艺展示', '影棚幕布', '中景', '全景', '喜悦', '场景-其他', '手机电脑录屏', '室内', '极端特写', '特写', '配音', '朋友&同事(平级)', '绘画展示', '课件展示', '多人情景剧', '填充'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.95', '0.42', '0.38', '0.13', '0.08', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09d3b037a2052173f9fd6440d6851886.mp4\n",
      "{\"video_ocr\": \"各位年轻人|老头子我|以过来人的身份|劝告你们|一定要在自已|健康的时候|购买一份保险|如今你们的机会来了!|现在满28天|以及60周岁以下的人|都可以申请购买一份|轻松保百万医疗险|不管有没有医保|有没有退休金|皆可投保|首月只需1元|次月13.33元起|就能获得|最高600万的 医疗保障|医疗费花多少赔多少|不限药品|这个啊|也是对自己|对家人的一份保障|当意外来临|咱们老百姓|不用因为病而致穷|不要老觉得无所谓|只会瓦解你|和家人的心情和财力|所以“保险”就是责任|就是对 所有爱你的人的责任|点击屏幕下方链接|为自己和家人|添一份保障吧|最高报销|查看详情|具体以保险合同为主 保费随年龄、有无社保等情况变化|轻构保|轻凇松保|600万\", \"video_asr\": \"各位年轻人，老头子，我以过来人的身份劝告你们一定要在自己健康的时候购买一份保险，如今，你们的机会来了。|现在满二十八天一及六十周岁以下的人都可以申请购买一份轻松保百万医疗险，不管有没有医保。|有没有退休金皆可投保，首月只需一元，次月十三点三三元起，就能获得最高六百万的医疗保障。|医疗费花多少赔多少？无限药品，无限疾病，这个也是对自己，对家人的一份保障，当意外来临。|咱们老百姓不用因为病而治穷，不要因为钱而苦脑，不要老觉得无所谓，意外来临，只会发觉你和家人的心情和财力。|所以，保险就是责任，就是对所有爱你的人的责任，点击屏幕下方链接，为自己和家人添一份保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016437530517578125 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '平静', '动态', '场景-其他', '室内', '手机电脑录屏', '办公室', '全景', '喜悦', '室外', '配音', '特写', '混剪', '拉近', '极端特写', '红包'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.91', '0.52', '0.42', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09e9bda4d87622a90ac66cdf12bcdbc1.mp4\n",
      "{\"video_ocr\": \"快快快|咱们追的剧今天大结局了|我们现在看吧|哦 对对对 好|我 我流量不够用了 看不了了|没关系 那就用我的看吧|你就不怕流量不够用吗|不怕啊|我用的是电信星卡|每月40G的高速流量|每个月还有300分钟 的免费通话时长|看剧打电话 完全不用担心的|而且月租才29元|首月还免费用|要不要给你也领一张呀|在哪里可以领啊|点击视频下方链接|就可以领取啦|包邮免费送到家|RADIO\", \"video_asr\": \"快快快，咱们这就去今天大结局了，我们现在看呗，对对号。|我，我流量不够用了，你看不了了。|没关系，那就拿我的看吧。|你就不怕流量不够用吗？不怕呀，我用的是电信新卡，每月流量，每个月还有三百分钟的免费通话时长，看剧打电话完全不用担心的，而且月租才二十九元，首月还免费。|给你先领一张，在哪里可以领啊？点击视频下方链接就可以领取了，免费包邮送到家！\"}\n",
      "multi-modal tagging model forward cost time: 0.01628279685974121 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '静态', '推广页', '喜悦', '单人口播', '朋友&同事(平级)', '平静', '惊奇', '家', '夫妻&恋人&相亲', '极端特写', '手机电脑录屏', '动态', '特写', '悲伤', '室内', '亲子', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.47', '0.43', '0.34', '0.28', '0.23', '0.07', '0.05', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/09f1892e4e1531c6a625882e10ab4df4.mp4\n",
      "{\"video_ocr\": \"少爷病危，老爷失踪，杨家群龙无首|少爷，您可不能见死不救啊|起开 NIKE|别耽误我给我老婆做饭|少爷|老婆，饭做好了，可以吃饭了|除了做饭你还会干嘛|我是嫁了个厨子吗|就算杨潇是个窝囊废|我也不可能跟他离婚|你们别再做这种卖女求荣的梦了|喂，我愿意回来继承家业|只要你答应李总的小小要求|这五百万的合同就是咱家的了|不可能|J|签个字|五千万就是你的|杨潇你|《虎婿》 本故事纯属虚构|小说 阅读吧 小说阅读吧\", \"video_asr\": \"嗯。|老爷失踪，杨家群龙无首，少爷你可不能骗。|就别耽误跟老婆说话。|少爷。|好吗？饭做好了，先吃饭了。|除了做饭，你还干吗？|是这样的，我这就像有销售没有。|我也不可能在他结婚。|你昨天诅咒让你说过。|为我院也不能继承家业。|要不带你怎么小小的就这五百万的，呵呵，就是咱家的了。|可能签个字。|我再说。\"}\n",
      "multi-modal tagging model forward cost time: 0.01618051528930664 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '动态', '特写', '家', '悲伤', '夫妻&恋人&相亲', '平静', '极端特写', '惊奇', '喜悦', '愤怒', '亲戚(亲情)', '转场', '厌恶', '单人口播', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.87', '0.70', '0.64', '0.60', '0.49', '0.48', '0.41', '0.38', '0.29', '0.17', '0.03', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a15985cd7d41e72506b9da90fc1c02f.mp4\n",
      "{\"video_ocr\": \"让”-0|会孩子独立思老|及子独立思考|锻炼孩子逻辑思维|飞上思老|编和|更有全套 编程大礼盒赠送|学生配套书|数学思维秘籍|编程启蒙书 SCRA|学生日记|好习惯养成自律表|趣味贴纸|你愿意花9块9元 让孩子0基学编程吗？|0基Z学编程叼？|其学份程吗？|培养自主 5的能力吗？|一老的胎カ吗？|9块9元5节课|一u业し 之辑思维|举一反三 孩子进步看得见|赶紧报名吧|爆升狼子理铺思排能力精备超爆学习后劲 高年级|提升孩子逻辑思维能力 储备超强学习后劲|激发孩子数感天赋用数学牵动思维|释放孩子的创追力让孩子高效思考|孩子一看就懂的|WALNUT EDUCATION|WEDNESDAY FRIDAY|核秘编程|我些养成好习惯|5节编程启蒙课·世界机器人冠军主讲|Scratch编程启蒙书|每个出生在科技时代的孩子都不可不学的知识|知识宝箱|孩子的第一堂幼程课|送精美编程大礼盒 立即抢报|启发中国孩子的学习力|RATL|习需贴蜓|请贴上|核桃|THURSDAY|SUND|WEDNESDAY THURSDAY|星期|MONDAY|里期四|SATURDAY|(a+6|班级|姓名|习惯\", \"video_asr\": \"你愿意花九块九让孩子零基础学编程，培养自主思考的能力吗？九块九元五节课锻炼孩子逻辑思维，交会孩子独立思考，举一反三，孩子进步看得见，更有全套编程大礼盒赠送，学生配套书，数学思维秘笈，编程思维密集。|编程启蒙书，学生日记好习惯养成自律表聚为贴纸。|要用电脑上课哦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016001224517822266 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '场景-其他', '手机电脑录屏', '喜悦', '动态', '室内', '平静', '极端特写', '混剪', '配音', '单人口播', '特写', '教辅材料', '亲子', '多人情景剧', '愤怒', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '0.94', '0.80', '0.67', '0.66', '0.55', '0.30', '0.05', '0.04', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a19c6d6a79226cb0ae144d7d64b9dea.mp4\n",
      "{\"video_ocr\": \"高考考入理想的|211大学|有什么经验|可以分享给|学弟学妹们吗|努力很重要|但是选择大于努力|怎么说呢|一到六年级的时候|把学习也没有当回事|当时老天眷顾|每次考试|成绩都是班级前三|到了初中开始|学习就显得很吃力|考试排名靠后|到了初二的时候吧|就开始各种补习班补习|整个初中|换了七八个补习班|后面上了高中|高一上半学期是|班级中等水平|后面天天跟着|学霸一起吃饭|问他们学习方法|问多了发现|发现他们当时都学着|高途课堂|语数英物课程|跟着同学们|学习了几节课后啊|决定报名试一下|周帅老师|史心语老师|谢欣然老师|马小军老师|学习着|每科的解题技巧|和重难点直攻|直到到了|高二上半学期的时候|考试成绩|就仅次于我们班学霸啦|那这个课程|现在还有吗|有|现在特惠|9元就能学到|16节课|如果|高中的学弟学妹们|也需要这个课程|就可以点击视频下方|了解详情|报名购买啦|中考难点)|现在完成时一一影响性|全国百佳教师带队教学 平均教龄11年|中高一检测]已知底面半径为1em.高为 内接正方体，求这个内接正方体的校长。|新用户专享 立即体验 浙江卫视指定在线教育品牌|完了，是不是很简单!|用这么费劲吗?|华少|一个口流 九九归~。|九九归-一个Q2|九归-，一个国减|语文资深主讲 高中物理负责人 北京大学毕业|12年教龄|谢欣然|英语专业八级 TESOL资格认证|浙江卫视|省高考状元|名师特训班|马小军|人大附中优秀教师|周帅 史心语|B.haslost|A.lose|C. lost D. had lost ZYX can't go to the ZXY's concert becau|秒杀|￥9\", \"video_asr\": \"的，考入理想的二一一大学，有什么经验可以分享给学弟学妹们吗？努力很重要，但是选择大于努力，怎么说呢？低到六年级的时候，把学习更当回事。|当时老天炫舞每次考试成绩都是班级前三，到了初中开始学习就显得很吃力，我身边不厚道了，初二的时候吧。|就开始各种补习补习。|整个中国换了七八个了媳妇，后面出了高中高一上在这。|按其功能。|男朋友发现他们当时同学上高中课程的语数英物课程非常，同学们学习了，姐姐高速啊，学习方式一下，后面跟着周帅老师，史心语老师，谢欣然老师吗？小学老师学习长辈技巧和重难点施工，知道到了高二上半学期的时候，考试成绩就仅次于我们学霸了，那这个课程现在还有吗？有？|这些都为九元就能学到语数，英物四科十六节课，如果高中的学弟学妹们也需要这个课程，就可以点击视频下方了解详情报名购买了。\"}\n",
      "multi-modal tagging model forward cost time: 0.0164487361907959 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '平静', '多人情景剧', '单人口播', '亲子', '家', '特写', '动态', '室外', '极端特写', '喜悦', '家庭伦理', '室内', '手机电脑录屏', '全景', '转场', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.87', '0.48', '0.17', '0.15', '0.13', '0.03', '0.03', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a2e9f321eb34a22364f2f285c7e6341.mp4\n",
      "{\"video_ocr\": \"本故事纯属虚构|奶奶住院了|赶紧收拾收拾跟我出门|啊|都什么时候了能不能把你的破玩意放下|医生我奶奶怎么样了|颜儿|你把他带来干啥|林阳|他就是个只会绣花的上门女婿|敢问阁下 是否灵首传人|你倒有点见识|还求出手相救|我相信你 求求你救救奶奶|医生准备一套银针|你们两把床挪为南北走向|你去给我买一份黄焖鸡米饭|哎 买那玩意儿干啥|我饿了|他到底行不行啊|闪开闪开|你救了我|你提条件吧|我要跟她离婚|《女神的超级赘婿》|点众阅读|全本小说免费阅读|pOmMYo3|POMYos|工心|:5\", \"video_asr\": \"奶奶住院了，赶紧收拾收拾，跟我出门都什么时候，能不能办得快放下。|你找我吧怎么样。|雅儿你把他带来干了林洋，他就是个只会绣花的上门。|嗯。|刚阁下是否灵手传人，你到有点认识，还请人出手相救。|医生准备脱银针，你们的宝贝，他没走下去后悔。|哎，买那玩意干啥。|不行不行咱班班长。|为什么？|你就拥有。|我要跟他离婚。\"}\n",
      "multi-modal tagging model forward cost time: 0.01592707633972168 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '特写', '静态', '家', '推广页', '愤怒', '动态', '家庭伦理', '夫妻&恋人&相亲', '喜悦', '极端特写', '悲伤', '朋友&同事(平级)', '平静', '惊奇', '亲子', '亲戚(亲情)', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.93', '0.71', '0.67', '0.57', '0.39', '0.37', '0.20', '0.17', '0.10', '0.06', '0.02', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a2fa560f31f57106c97981dfc7e9be4.mp4\n",
      "{\"video_ocr\": \"你那死工资|怎么买得起|这最新款的包包啊|因为我有做副业啊|工作之余赚赚外快|妈妈妈妈|我想买动感超人|不行的小新|你这么专业|应该学了很久吧|我之前也是声优小白的|在潭州教育|学习了一两个月吧|平时就用手机平板|在线学习|现在点击|视频下方链接报名|从0开始|练就百变声线|你也可以哦|视频为演绎情节\", \"video_asr\": \"你那死工资怎么买得起车最新款的包包啊？没有做副业工作之余赚赚外快，某某某某。|我想买动感超人，不行得小心哇，你这么专业应该学了很久吧，我之前也是生有小白的，在谈州教育学习了一两个月吧。|时就用手机，平板在线学习，现在点击视频下方链接报名，从零开始练就百变声线，你也可以哦！|哇。\"}\n",
      "multi-modal tagging model forward cost time: 0.016037464141845703 sec\n",
      "{'result': [{'labels': ['现代', '静态', '推广页', '中景', '家', '单人口播', '喜悦', '手机电脑录屏', '情景演绎', '室内', '配音', '悲伤', '平静', '动态', '特写', '多人情景剧', '室外', '混剪', '办公室', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.75', '0.59', '0.43', '0.15', '0.14', '0.12', '0.11', '0.10', '0.05', '0.04', '0.03', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a307b0a35dc55804ef518932d4544ca.mp4\n",
      "{\"video_ocr\": \"哎...|Myteeth 我的牙齿|one,two,three,four..|Twentyteeth 二十颗牙齿|Wigglewiggle 晃啊 晃啊|Wiggle it loose 把它晃松|Grow grow 长吧 长吧|hopeitgrows 希望牙齿赶快长大|报名就送超值教材礼包|89元20节 Al儿童英语系统课|斑马英语|立即报名|Z银EBRA\", \"video_asr\": \"我。|MY TEETH ONE TWO THREE FOUR TWENTY TO GO GO GO GO GO HOPE ITS GROWTH。\"}\n",
      "multi-modal tagging model forward cost time: 0.01662755012512207 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '亲子', '平静', '家', '喜悦', '特写', '单人口播', '极端特写', '手机电脑录屏', '动态', '家庭伦理', '惊奇', '教辅材料', '夫妻&恋人&相亲', '场景-其他', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.95', '0.72', '0.62', '0.59', '0.31', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a315b24542edfa5bfa73936aa6d8d19.mp4\n",
      "{\"video_ocr\": \"老板|你今天|必须把工钱给我!|我闺女等着这钱救命呢|工钱已经给你们|包工头了|找我也没用啊|小罗|干什么呢|姜总|大叔|我们欠您多少钱|15820|给我15800就行|该多少钱|一分钱都不会少|你现在就让财务|把钱打到大叔卡上|谢谢姜总|当初让公司所有员工|工人给自己和家人|免费领平安i动保|您没给女儿领吗|领是领了|只是我女儿|得的是恶性肿瘤|这免费的保险管吗|你放心|这平安动保|有最高|100万的医疗保障|责任范围内不限疾病|不限社保用药|都可以申请报销|而且|每天走走路|就能换保额|最高还能领取|10万元重疾险呢|看病|基本不用花|咱自己的钱|太好了|我闺女有救了|千万不要|等到生病再后悔|现在新用户|点击视频下方链接|输入手机号|免费领取一份保障吧|平安健事保险|平等团宾|享安健康 保险|平安健康乐忙|平宾健康保睑|平安建康保陆|平安康保险：|本产品由平安健康险公司承保|保障内容以保险合同为准|投保需如实健康告知|买保险 就上平安健康APP|生过四|住过|PINGAN|平安 健康\", \"video_asr\": \"老板，你今天必须把工钱给我，我闺女等着这钱救命，红钱已经给你们发互投了，周五也没用啊，干什么呢？张总说，我们欠你多少钱？一万五千八百二十五，给我一万五千八百九十三多少钱就是。|一三小龙，你现在就让财务把钱打到大叔卡上市，谢谢。姜总在说，当初让公司所有员工，工人给自己和家人免费领平安动保，你没给女儿领了，领是领了，这是我女儿得的是恶性肿瘤，这免费的保险管吗？大叔你放心，这平安I动保有最高一百万的医疗保障责任范围内，不限疾病，不限社保用药都可以。|而且每天走走路就能换保额，最高还能领取十万元重疾险呢！看病啊，基本不用花咱自己的钱了，我闺女。|了，千万不要等到生病再后悔！现在新用户点击视频下方链接，输入手机号，免费领取一份保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016563892364501953 sec\n",
      "{'result': [{'labels': ['静态', '中景', '现代', '多人情景剧', '推广页', '愤怒', '喜悦', '全景', '平静', '路人', '惊奇', '特写', '室外', '悲伤', '动态', '夫妻&恋人&相亲', '极端特写', '家庭伦理', '厌恶', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.91', '0.87', '0.84', '0.72', '0.53', '0.50', '0.47', '0.34', '0.07', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a38bd21af8017214f226561e69d90d6.mp4\n",
      "{\"video_ocr\": \"多少现金吗？|想知道走的路能换|点击视频下方链接下载 全民健走|走走路赚金币|走的越多赚的越多!|上不封顶|赶快点击下载吧!|重要通知!|的一铁|的老铁们|下载就得5.88元红包|收益满0.5元即可提现!|0.5元|5.88元|33元|58元|为回馈用户|再放狠招人|红包频率提升30倍|领|转发给|用户的|即日起|通过本视频下载|3种方式直接获得|20|邀请好友还能再得|576|2917 提现 抽手机|￥0.10 9s|+0.03|I1llll!)|现金红包|还剩04:39|18|包金额偕!|红包金额翻倍!|翻倍! 红包金额|随时提现|自时到账|走路赚现金|x88|恭喜获得18金币!|恭喜您获得坝金红包|离线收益 领取初始步数奖励|免费领超大流量|超大流量|福袋每5分钟+1金币|金币翻倍 AD|+100 去打卡|最高可得58元!|好的|去走路|洛o|随机奖励 步数同步|填写邀请码|请码|具体金额以实际活动为准|排行榜|女+18 去喝水|喝8杯水(0/8)|44|65|3005 提现|邀请好友|全海走|18金币|每日唱|排行榜抢金币|￥￥|恭喜获得|去领取|活动赚 转盘赚大礼|天天赢现金 点手|矾奖励|去填写|去邀请|去同步 +58|抢金币|包频率提升|每日运动|+2|限时 抢现|04分|30I\", \"video_asr\": \"转发给全明星走用户的重要通知，全民健走为回馈用户在放狠招，即日起通过本视频下载全民。|总的，老铁们，下载就得五点八八元红包，邀请好友还能再得三十三元红包，频率提升三十倍，红包金额翻倍三种方式直接获得近。|高，最高可得五十八元收益，满零点五元即可提现，随时提现，随时到账。想知道走的路能换多少现金吗？点击视频下方链接下载全民健走，走走路赚金币，走的越多赚的越多，上不封顶，赶快点击下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016601085662841797 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '配音', '场景-其他', '平静', '中景', '动态', '静态', '全景', '混剪', '室内', '手机电脑录屏', '填充', '极端特写', '幻灯片轮播', '喜悦', '城市景观', '情景演绎', '单人口播', '办公室'], 'scores': ['1.00', '1.00', '0.99', '0.94', '0.62', '0.55', '0.34', '0.13', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a38e2ae43dcf901a0a87d7fed50ca7a.mp4\n",
      "{\"video_ocr\": \"太好了|终于抢到了你看|我看看 看看|喂儿子 我给你抢的|作业帮直播课|的礼盒送到了|你快去取一下|她们都在抢什么呢|现在我们去了 解一下吧|语文阅读写作提分班|29块钱啊|就有20节名师课|还是清华北大毕业老师|带队教学呢|老师还帮我们总结了|10大写作方法|8大阅读技巧|24个阅读理解万能公式|32张高分写作模板|快速掌握写作技巧|自己写出高分作文|课后还有班主任老师|一对一辅导答疑|轻松解决孩子作文不会写|阅读理解没思路的问题|再也不用担心|孩子的语文学习了|而且呀|还包邮赠送|配套学习礼盒|这么好的课程|家长们千万不要错过|赶紧点击视频下方|给你家的孩子报名吧|提分班请下载并登录作业帮直播|优秀作文集|短信/彩信 今天16:04|您好!购买了作业帮直播课语文阅读|课APP。|致直播课 的我|上课内容与收到礼盒请以实际为准|*赠送12件套教辅礼盒|成长) 基础知识手册|语文 29元 20课时|16:09|课|立即报名＞|金长|4G|作业|HR\", \"video_asr\": \"太好了，终于抢到你看看！|我也儿子，我给你抢了作业帮直播课的礼盒送到了，你快去取一下，他们都在抢什么呢？现在我们去了解一下吧，是作业帮直播课语文阅读写作提分班二十九块钱就有二十节名师课，还是清华北大毕业老师带队教学呢？|帮我们总结了。|这个方法八大阅读技巧，二十四个阅读理解万能公式，三十二张高分写作模版。|掌握写字技巧，自己写出高分作文，课后还有班主任老师一对一辅导答疑，轻松解决孩子作文不会写，阅读理解没思路的问题，再也不用担心孩子的语文学习了，而且要二十九元二十节名师课，还包邮赠送配套学习礼盒，这么好的课程，家长们千万不要错过，赶紧点击视频下方给你家的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016606569290161133 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '室外', '平静', '多人情景剧', '喜悦', '动态', '特写', '极端特写', '全景', '惊奇', '路人', '(马路边的)人行道', '家庭伦理', '手机电脑录屏', '朋友&同事(平级)', '亲子', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.94', '0.93', '0.69', '0.49', '0.32', '0.15', '0.12', '0.02', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a38f803c00f0d54114264dab3927458.mp4\n",
      "{\"video_ocr\": \"好难选哦|欧式风格的地板颜色好优雅|我好喜欢|但是地中海的整体设计更潮|小姐姐你是做餐厅管理的吗|哎呀我是在游戏里开中餐厅啦|你看|这是我在象岛开的分店|可以白由选择餐厅的装扮风格|除了欧式和地中海之外|还有好多不同类型的装饰物|最重要的是 jan|尔还可以选择明星水伙伴加盟你的餐厅|我步仁行|为你增加人气|开局一块地白手起家做生意|中华八大菜系风靡世界的任务就交给你啦|签约 特卖 144.56|8|古典柱灯 宫廷柜子|绿色方格地板 粉色波纹地板|墨灰隔断栏杆 墨灰栏杆柱|10/10|复古优难格|主题加成:10%|放置 限量|简约吊灯|制作|小国叶盆载|鲜美虾汤|装修|限时出售 象岛餐厅食材用完啦，赶紧去|今日不可购买|花瓶挂饰 萌猫立灯|已获得|收集中|32%|装饰物:6/150|装修商店|餐厅装修|独一无二的性装婚an|专天你始是|ba|24小时好营|菜谱交换开启 人才交换开启|菜谱交换开启|紫色菱形地板|灰白栏杆花池|激萌猫咪风格 魅力|魅力泰式风情 梦幻星空主|浓情|节日限定! 浓情七夕主题|红烧鳕鱼|湖西外婆菜|光芒正能 501.93万 12.59万|主题总加成:|菜单 探索 商会|比拼|30元|萌猫小爪墙壁|餐厅售价:57.7|v:0U:13 览出神人化一成;|黄金烛台|粉玫瑰盆栽|785.15万|35096|175480|7天乐 今日限|全部|292|驯鹿挂件|售价-24|轻从温的人|轻从号温的球杵|好州验土识的球性|餐桌区域 基础|丝凉面|牛肉|Di L|Wa g un ka|白手起家 做生|价格加成5%|请期待|科尔马|Kun|生意|SWEE#|传统\", \"video_asr\": \"好难受啊，欧式风格的地板颜色好优雅，我好喜欢，但是最终的整体设计更潮，小姐姐。|我是在游戏里开的中餐厅了，你看，这是我的向导开的分店，可以自由选择餐厅的装扮风格，除了欧式和地中海之外，还有好多不同类型的装饰物，最重要的是还可以选择明星小伙伴加盟你的餐厅，增加你的人气，开局一块的白手起家做生意，中华八大菜系风靡世界的任务就交给你。\"}\n",
      "multi-modal tagging model forward cost time: 0.016479969024658203 sec\n",
      "{'result': [{'labels': ['现代', '中景', '动态', '推广页', '静态', '配音', '场景-其他', '手机电脑录屏', '喜悦', '极端特写', '多人情景剧', '平静', '惊奇', '拉近', '特写', '愤怒', '室内', '情景演绎', '游戏画面', '室外'], 'scores': ['1.00', '0.99', '0.98', '0.98', '0.95', '0.80', '0.37', '0.35', '0.33', '0.22', '0.17', '0.09', '0.07', '0.04', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a419be900d1c54c6e20f4763b269310.mp4\n",
      "{\"video_ocr\": \"都是你惹的事|害你爸爸住了医院|公司破了产|你呀|就是个灾星|你爸爸住院的钱|破产欠的钱|都由你来负责|爸|我该怎么办|阮南希|是你|一个小时前|你们家的债|我已经帮你还清了|还有你父亲的医药费|我也帮你交齐了|你为什么帮我|你妈答应把你嫁给我|这些只是彩礼|入了我陆家门|这辈子你就别想离开了|更多精彩剧情，继续阅读|《闪婚蜜情：娇妻太难宠》|《闪婚蚤肯:娇太难龙)|《闪婚蚤情 娇妻太难宠》|蜜情：娇妻太难龙》|(本故事纯属虚构)|欲知后事，请下载 连尚读书 搜索|本故事属虚构|许雅惠|陆漠北|陆氏集团总裁|艺江南|连尚 读书\", \"video_asr\": \"都是你惹的事，爱你爸爸住了医院，公司破了产。|你呀，就是灾星爸爸，住院的钱破产千的钱都由你来负责。|放。|我该怎么办。|走吧。|我。|阮楠溪。|是你。|一个小时前。|你们家的债我已经把它还清了，还有你父亲的医药费。|不管你交不起了，你为什么帮我。|你妈等你把你嫁给我。|这些只是残忍。|入了我六厦门，这辈子你就别想离开了，破产你闪婚嫁入豪门，即使不受待见，也有总裁把她宠上天，与，之后实行下载连上，露出搜索闪婚秘籍，教妻太难宠。\"}\n",
      "multi-modal tagging model forward cost time: 0.016089200973510742 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '家', '多人情景剧', '动态', '特写', '悲伤', '情景演绎', '愤怒', '平静', '配音', '手机电脑录屏', '极端特写', '工作职场', '单人口播', '上下级', '室外', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.93', '0.88', '0.87', '0.73', '0.48', '0.27', '0.10', '0.09', '0.08', '0.06', '0.06', '0.06', '0.04']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a550ed1bd06760e0d5ef105dd9c6921.mp4\n",
      "{\"video_ocr\": \"韩老师 不是我说呢|孩子学习再不好 也不能这样呀|小芳 先别哭了|跟妈妈说说怎么回事吧|妈我作文写不出来|今天语文考试|孩子作文到最后 一个字都没写出来|孩子有点着急 情绪就有点绷不住了|老师不瞒你说|这孩子的作文 我也挺着急的|可就我这水平 我也帮不上什么忙呀|之前背句子上补习班|这些我都给她试过|可就是不见长进|妈你就听我的|今天必须给我报|猿辅导 秋季语文特训班|小强就是跟着里面|清北毕业的名师 一块学出来的|怪不得小强 最近作文这么长进|我是听说过这|可是它9课时才29块钱|这效果能好吗|这个跟其它的不一样|不但会教我们|7类常考作文|还有23类语文写作技巧呢|从人文素养到 阅读写作技巧|都教的特别细致|我就说 五感写作法和素描法|这些都是 作文写作的高阶技巧|小强现在就会运用了|那这在哪报名呀|妈妈现在 给你报名还来得及吗|当然来得及|点击屏幕下方 现在报名语文|只要再加1元|还能获得 同课时的数学特训班呢|—点击按钮选择年级|办精彩干闪运，做文明|猿辅导阅读写作特训班|清北毕业名师教 能写会说得分高|1元加购 可再得同价值数学思维提升课 猿辅导秋季语文名师特训班|23种作文技巧 cCTV《中秋诗会》 官方推荐|秦博 北京大学 张燕楠|答 “这里的老师很注重培养孩子…”|幼儿园大班 一年级|19类阅读题型答题技巧 7大常考作文类型精讲|猿辅导在线教育|月售 21万+|在线客服|限购一次|初一|苏日娅|仅需|29\", \"video_asr\": \"啊。|我华老师不是我说的，你还得学习，在不好也不能这样啊，我现在有空了，你跟妈妈说说怎么回事吧。|妈，我作文写不出来，我就可以语文考试，孩子作文都最后一个字都没写出来，孩子有点着急，情绪就有点崩溃。|老师不瞒你说，这孩子的作用我也挺着急的，就我这水平我也帮不。|什么忙呀，之前背句子呀，上补习班这些我都给他师傅不就是不见长进吗？你就听我的一天必须给我报猿辅导秋季语文特训班，小强就是跟着里面清北毕业的名师一块学出来的，怪不得小强最近作文这么长进，我是听说过这原文辅导秋季语文特训班，可是他九十三二十九块钱的效果那么好吗？|这个跟其他的不一样，他们会教我们七类常考作文，还有二十三类语文写作技巧呢，从人文素养，道，阅读，写作技巧，都教的特别细致，我就是无聊写做法和作文法，这些都是作文写作的高级技巧，强，现在就会运用了。|在哪报名呀？妈妈现在给你报名还来得及吗？当然来得及，点击屏幕下方现在报名，语文只要再加一元，还能获得同课时的数学特训班呢！\"}\n",
      "multi-modal tagging model forward cost time: 0.016140460968017578 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '多人情景剧', '中景', '静态', '亲子', '平静', '愤怒', '单人口播', '动态', '路人', '悲伤', '拉近', '(马路边的)人行道', '家庭伦理', '填充', '特写', '喜悦', '全景', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.97', '0.88', '0.11', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a7072f2105a082c78d53bf5a51e33ff.mp4\n",
      "{\"video_ocr\": \"NIKTE I'm在做题|l'm搞学习|Im不会做|I'm干着急|直到遇上宮英语so easy|一天熟记500个单词|真的可以做到吗|来作业帮直播课|英语单词语法名师课|由国内外毕业名师带队教学|八大单词记忆法|高效解决单词记忆问题|掌握速记技巧|单词过目不忘 脱口而出|打破单词与单词间的壁垒|记单词的效率翻三倍|现在报名|13节直播课仅需9元|还包邮赠送全套教辅礼盒|赶快点击视频下方链接报名吧|16|pa li 有礼桃的:客气的|f：在确的愉当的 *入…p郎票:印章|HON折|HONTE.|H美ONVTE|HONMIE|(Um3)|知识梳理与复对|名师有大招|解题更高效|中国女排为作业帮直播课代言，|2.beioe 在为么、|2.he，id在务动:|conve on方向方位|00|¥入mp邮票;印章|*g＿四建议:提议 pumte gYd明信片|em 建议:提议|l0.d|me什么，请再说一次|me对不起:打扰一下 foor二/三楼|potgYd明信片|yl有礼桃的:客气的|中国女排|*上课内容与收到礼盒请以实际为准|¥正确的，愉当的|00方向:方位|rs课程学科|12.c|Tee|se课程:学科 6.comve ＿|课|中国国家女子排球队官方教育品牌|心要来语界|8.re——口要来请来 ree——o方向方校|9元 13节课|聚能闯关\", \"video_asr\": \"他们在做题M搞学习M不会做M干着急，直到遇上他应SO EASY今天手机五百个单词真的可以做到吗？来作业帮直播课英语单词语法名师课，有国内外毕业名师带队教学。|八大单词记忆法，高效解决单词记忆问题，掌握速记技巧，单词过目不忘，脱口而出，打破单词与单词间的壁垒，记单词的效率。|三倍现在报名十三节直播课仅需九元，还包邮，赠送全套教辅礼盒，赶快点击视频下方链接报名吧！|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.015645265579223633 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '家', '喜悦', '室内', '平静', '特写', '配音', '多人情景剧', '手机电脑录屏', '惊奇', '亲子', '场景-其他', '悲伤', '教辅材料', '家庭伦理', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.81', '0.54', '0.31', '0.12', '0.11', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a73f702b38fd33a02f2fba2a0941ea9.mp4\n",
      "{\"video_ocr\": \"是你让我来拿的快递嘛|对 您签收下|哟还学习呢|现在自己出来|就想攒点钱考个心仪的大学|其实现在的社会|誊洁|只会攒钱的话|你的生活可能并不会有太大起色|你想啊|存钱存银行并不能给你|带来多高的收益|甚至还会亏损|不如用钱生钱|学会科学的理财方法|以后哪怕专心考试不上班也有收入啊|理财|我从来没有接触过啊|那你可以上这个快财商学院的|小白理财训练营看看|这是专为理财小白设计的|现在只需0元|就能获得⑥节理财爆款课|18个财富增值技巧|22种理财避坑指南|都是专业理财名师在线直播讲解|课后还有24小时班主任|一对一在线答疑|基金股票等低风险投资|干货全面讲解|有时候找到科学的理财方法赚钱|可比卖体力赚钱更轻松|那这个快财商学院怎么报名啊|点击屏幕下方|查看详情就可以报名啦|免费名额只有前200名哦|学理财上快财|学理 财上 快 财|6621826|获取验证码|请瑜入的级码|1|穷与富的秘密:如何打适富人思推? 甩开同龄人，适合普通人的投资工具?|0元抢购|老师介绍|理财名师直播授课 手把手带你轻松学理财|实用的快速入门课程 基金、股票、低风险投资理财干货|作业实时批改反馈， 不让问题过夜|专业的理财学习平台|听课方式|报名须知|中通快递|告别死工资，小白理财如何少走弯路？|小白理财如何少走弯路? 告别死工资，|免费报名|理财小白 月光一族|踩坑老手|什么是运会小白的“低风险“投资跟略?|牧之老师|课程特色|借的平台 专业的老师|2016年上线至今，拥有|工资少?没存款?|基础为0，希望通过 工资不够花，想通过|严谨的教学 一流的服务|6天理财名师爆款直播课|没有投资技巧的人 家庭主妇|全国知名理财专家|个财富增值实用技巧 22 种投资必备退坑指南|小白如何选指数基金？|无保附不投资:投资如何进行风险管理?|720年投资分析:家重应该如何配置财产?|社群学习，方便学习随时回看|6天全过程社群餐学|1007199|应该如何配置财产?|班主任督学|作业实时批改反馕，不让问题过夜|D0|适合人群|丰巢|200名|2127263|ZO|爆款理财小自|视频为演绎情节|HITACH|潭程个绍|训练营|直播|完善的财商教育体系|快财商学院|HIFTAChi|难迪\", \"video_asr\": \"是你让我来拿的快递吗？对，您签收一下有，还学习呢，现在自己出来就能早点起来多个新的大学，其实现在这个社会只会攒钱的话，那生活可能并不会有太大起伏，存钱存银行并不能给你带来多大的收益，甚至还会亏损，不如用钱生钱学会。|的理财方法，以后哪怕专心考试，不上班也有收入啊，才，我从来没有接触过，那你可以上这个快财商学院的小白理财训练营看看，这是专为理财小白设计的，现在只需零元就能获得六节理财爆款课，十八个财富增值技巧，二十二种理财避坑指南，都是专业理财名师在线直播讲解。|课后还有二十四小时班主任一对一在线答疑，基金，股票等低风险投资干货全面讲解，有时找到科学的理财方法赚钱可比卖体力赚钱更轻松。|在这个快财商学院怎么报名啊？点击屏幕下方查看详情就可以报名啦，免费名额只有前二百名哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.015897750854492188 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '推广页', '静态', '多人情景剧', '喜悦', '平静', '极端特写', '惊奇', '单人口播', '配音', '(马路边的)人行道', '动态', '夫妻&恋人&相亲', '场景-其他', '朋友&同事(平级)', '悲伤', '路人', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.95', '0.87', '0.81', '0.50', '0.13', '0.12', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a74a0cb2de51226b75eaddc7e0158dc.mp4\n",
      "{\"video_ocr\": \"从诗文写作|得语文者得天下|人比|得小学语文者|得全局|语文学习的关键|从来不是天赋|而是好方法|国学|国学素养|三オ面全面抓起|让孩子不仅|能读懂古诗文|还能从古诗文中|拆解出写作的奥秘|不能答好阅读题|还能从大师笔下|学习写作的框架|我是高途课堂 张莹菁|武汉大学学士|荷兰莱顿大学 文学硕士|专泣一线小学语文|累计线上学员超过|6万人|我擅长用古诗文|打开孩子写作与 表达的大门|引导孩子|找角度 巧思考 新学员9元专享|喜变化|写别具一格的 优秀作文|点击视频下方|让孩子来我的魔法 课堂体验一下吧|立即报名|勃春回大地。|手法描写了孩子们眼中的香天敲颂了春天的无 限美好，表达了对春紧对大然的喜爱之情|5|椅兰莱顿大学文学硕士|高途课堂名师特训营|1春天来了!春天来了!|符解触杜鹃|T着IhQx 因肌怯.怕生或做错了事怕人厦(chD笑面心中不安:怕难为情，伤|害差i0Qx因暴恢、怕生成做错了事怕人暖(ChD)笑而心中不安:怕难为情，例|乌语花香、萱歌|乌诺花眷、誊柳枝上荡秋干， 蒸舞、花红柳绿，囊国也差找券天|4小草从地下探出头来，那是春天的眉毛吧？|想.满尖，尾长，身体大每为属色、肩和度蒂自包，制何:44统人们称|7解冻的小溪叮叮咚咚，那是春天的琴声吧？|[眼睛看一视觉|探伸触碰寻找一寻鬼 仔相 认真 解冻一融化|们走出家门的速度很快，表|“走出”好不好？为什么？|阅读理解|我的语文课程|雨绵绵、生机勃|、万紫千红、春.肉一章..|选做:你找到的春天是什么样的?仿照第4~7自然段或 第8自然段说一说庄题归纳:作者运用比喻拟人的|我们仔细地找哇，找哇。|账躲藏额idu6 dui cdng cong把身体隐蔽起来.不让人看见，例句:大家一起玩|躲藏能idid dui congcong 把身体隐戴起来.不让人看见，例句:大家一起靴|2我们几个孩子脱掉棉袄，冲出家门，奔向田野，|脱获掌羞姑越掩探搬|找春天 行细|在风筝尾巴上摇哇摇；她在喜鹊、杜鹃嘴里叫，在桃|6树木吐出点点趣芽，那是春天的音符吧?|世外桃滑|春天来了！我们看到了她，我们听到了她，我们闻到 了她，我们触到了她。|O春天来了！春天来了![朗读指导:两个~泰关|Q春天来了|春夫率了I[朋速指导:尚个”本|认真|语文 下册|教材解读|名师出高徒.网课选高途|8春天来了！我们看到|L7|巧记:|ben体1|ben字A跄|花、杏花枝头笑....|5早开的野花一朵两朵，那是春天的眼睛吧?|T桃|3春天像个害羞的小站娘，遮鹰遮掩掩，躲躲藏藏。|去寻|北大清华毕业师资\\\\平均教龄11年以上|找人|然娘囊徐娘|柳忘外柳红柳绢|en钱投布|只需9元|名师特训班|4气毛花|生占气走门花|cang1妆|寻娘|冲姑吐杏|人方 仔细|去找春天。|中荡|zarg(宝S\", \"video_asr\": \"得语文者，得天下的小学，语文者的全局，语文学习的关键从来不是天赋，而是好方法。|我的语文课程从诗文写作，国学素养，阅读理解三方面全面抓起，让孩子不仅能读懂古诗文，还能从古诗文中。|拆解出写作的奥秘，不仅能答好阅读题，还能从大师笔下学习写作的框架。我是高途课堂张芸京，武汉大学学士，荷兰莱顿大学文学硕士，专注一线小学语文，累计线上学员超过六万人，我擅长用古诗文打开孩子写作与表达的大门，引导孩子。|角度小思考，写变化，写出别具一格的优秀作文，只需九元，点击视频下方，让孩子来我的魔法课堂体验一下吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016196250915527344 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '单人口播', '推广页', '静态', '平静', '配音', '室内', '影棚幕布', '动态', '教师(教授)', '场景-其他', '情景演绎', '家', '教辅材料', '喜悦', '学校', '手机电脑录屏', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.41', '0.39', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0a90b15558c09ebfc210641f1ee46e36.mp4\n",
      "{\"video_ocr\": \"小学语文很简单|可是你真的会教吗?|不是摘抄背诵|这些你都能给到孩子吗?|6-12岁是培养 学科思维的黄金时期|千万不能再耽误了|我是高途课堂王勒老师|北京师范大学毕业|专注语文教学12年|5条素材积累原则|带孩子轻松搞定语文!|9块钱|2周见证孩子的变化|还在等什么|点击视频下方查看详情〗|报名吧!|！招劳背酒温暖 买具甘甜关劳 纹像云恋合求|产礼|牌音租C思.得在查字儿，看商猜得对 上ょ很柔软|急的年天复|可是，大象的耳朵眼儿里、经常有小虫子飞进 每天，大象站着睡觉的时侯，就用两竹竿把|读课文。说说雷雨前、雷雨中租雷雨后景色的变化|接天莲叶无穷碧， 映日荷花别样红。|去呢？如果妈妈在身边、问 河水哗哗地流着。小马为难|学：“兰己经委了，还圃千什么！”|华少|都泛技级葱软毛异恋舍求挑|广k师朵，像商子仅的，年拉行 金一姐无边的大h4小河杰力的裤|读读，说说你见过什么样的雨，当时是怎样的情景。|6大阅老点|令途课墓提分像|引注满味息|位桶篮英玻璃这圾|怎么才能让耳朵坚起来呢?|新新新新迎迎迎 迎动梓|毕竟西湖六月中， 风光不与四时同|汉去吧” 决地往磨坊跑去。跑着跑看，|君教绿丝方 娘吐柳挑香 员源战局堆礼|脱袄寻羡姑速抢探微得解触杜|大象也不安起来，他自言自语地说：“他们都这 .，是不是我的耳朵真的有毛病啦？我得让我的|黑黑压压圧屉（压力屋|[宋]场万里|说：“你已经长大了，能 马连蹦带跳地说：“怎么不|很多家长觉得|自己就能教|语文要培养的|培养思维 创造力|4大习作类型|永杨|小屁、小马，还有小老成，见到了大象，都要|雷雷|雷雷鸟乌邬身(与西9|晓出净慈寺送林子方|一匹小马|亡羊补宇|还在里面跳舞，吵得他又头痛，又心烦。|天子，注意加点的词，再把句子纱写下米。|，看见一头老牛在河边吃草，|Δ大习伦米刑|浙江卫视|大阳甩七|你的年朵怎么是耷拉着的呢？”|高途课堂|行啊！可是他离家已经很远了|最二天￥上，他去硬早，专现早少了一只|快美甲维|小桶因龙像战士一样笔直地站那里|#给大|暴币|新用户专享 立即体验 浙江卫视指定在线教育品牌|19|角坚起来。”|老马高兴地说：“那好啊，|全国百佳教师带队教学 平均教龄11年|糕特低买份编龙汁|一只如辣从 一条移虹挂在天空|羊 小羊也说：“大象|寺二首|过河|厅|久现墨而没有限食了|可道“车伯伯|义务教门教科|定是出毛病了|小学语文资深主讲老师|名师特训班|聚挥起来|广诵课文|的耳朵|下册|二年级|扎 扑都扑|满天的乌云|青的审定|2017|91|75|查看详情|毛毛币|星文|黑王\", \"video_asr\": \"很多家长觉得小学语文很简单，自己就能教，可是你真的会教吗？|语文要培养的不是摘抄，背诵，是思维，是创造，这些你都能给到孩子吗？六到十二岁是培养学科思维的黄金时期，千万不能！|再耽误了，我是高途课堂王勒老师，北京师范大学毕业，专注语文教学十二年。|总结出四大题，多类型，五条素材积累原则，六大阅读考点，带孩子轻松搞定语文，只要九块钱，两周见证孩子的变化，还在等什么？点击视频下方！|查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016158342361450195 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '单人口播', '静态', '配音', '教师(教授)', '场景-其他', '影棚幕布', '特写', '平静', '室内', '转场', '情景演绎', '动态', '学校', '教辅材料', '图文快闪', '极端特写', '填充'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.84', '0.41', '0.27', '0.18', '0.11', '0.06', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ab9a2b902d55366d33d13fce0ce9e50.mp4\n",
      "{\"video_ocr\": \"丽丽听说你中|对啊|我女儿刚刚参加了|英语演讲比赛|那是怎么|我在斑马英语|给女儿报名|只要89元就有20节课|海外卜老师动画视频互动|那|那还等什么|名额有限|赶紧抢呀\", \"video_asr\": \"丽丽，听说你对呀，我女儿刚刚参加了英语演讲比赛，那是怎么？我在斑马英语给女儿报名只要八十九元，就有二十节课海外老师动画视频互动，那大汉等什么？名额有限，赶紧抢呀。\"}\n",
      "multi-modal tagging model forward cost time: 0.01577615737915039 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '喜悦', '平静', '单人口播', '填充', '混剪', '动态', '特写', '室外', '全景', '场景-其他', '配音', '惊奇', '远景', '情景演绎', '(马路边的)人行道', '拉近'], 'scores': ['1.00', '1.00', '0.99', '0.98', '0.81', '0.70', '0.15', '0.15', '0.11', '0.09', '0.05', '0.04', '0.03', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ad6a78ae29db5a2042f34e55027a0ce.mp4\n",
      "{\"video_ocr\": \"妈妈你别生气了|别生气了|怎么了 萱萱妈|气死我了|这个孩子 她现在数学学不好|倒学会撒谎了|她跟我说 这么多东西|报名作业帮直播课|正好您这也报名了|今天你马爷爷在这呢|看你还怎么狡辩|这么大个礼包|的确是买不来|但是现在 不是买|是送的|啊|这么厚一摞大大 小小几十本呢|清华北大毕业的 名师带队教学|集中提升孩子的 逻辑思维能力|这个小学阶段|不打好基础|以后上那个 初高中那是更费劲了|家长朋友们|如果您的孩子也有计算慢|做题没有思路的 学习问题|赶紧点击视频下方链接|给您的孩子报名吧|立即报名＞|中国哪家女子排球队官方教育品牌|成长笔记|小初高数学 名师提分班 30元18节课|上课内容与收到礼盒请以实际为准|*赠送12件套教辅礼盒|MINI|小学数学公式|21天\", \"video_asr\": \"妈，你别生气了，别生气了吧，气死我了，这个孩子他现在数学学不好，倒学会撒谎了，她跟我说，这么多东西报名作业帮直播课送的，正好您这也报名了，今天你马爷爷在这呢，看起来怎么小便哎，这么大个礼包的确是满的，但是现在不是买是送的后羿咯，大大小小。|这个呀，是作业帮直播课，清华北大毕业的名师带队教学，集中提升孩子的逻辑思维能力，这个小学阶段不打好基础啊。|以后上面的初高中那是更费劲了，家长朋友们，如果您的孩子也有计算慢做题，没有思路的学习问题，别犹豫了，赶紧点击视频下方链接给您的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016068220138549805 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '中景', '室外', '全景', '动态', '平静', '亲子', '家庭伦理', '愤怒', '静态', '喜悦', '教辅材料', '配音', '亲戚(亲情)', '朋友&同事(平级)', '极端特写', '悲伤', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.93', '0.81', '0.29', '0.16', '0.06', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0af70c10e02a767a0ebf8c11973c2e1a.mp4\n",
      "{\"video_ocr\": \"你这车就8万|你卖不卖|大哥|我真是家里有急事|我才卖这车|您多少给加点|就8万|不卖拉倒|我卖|这车我们不卖了|小妹啊|你就别捣乱了行不行|你嫂子前些天 查出来乳腺癌|光手术费就几十万|我把车卖了|都还差不少钱呢|之前我不是让你们买了|微保百万医疗险了吗|买了啊|可是这个首月1元|后续每月14元起|的保险能靠谱吗|哥你放心吧|腾讯旗下保险代理平台|保险责任范围内 的100种重大疾病|均可申请理赔|保额高达600万呢|太好了平E|你嫂子的病有救了|嗯|哎美女|在哪里可以买啊|点击视频下方链接|阅读保险条款和投保须知|确认被保人符合 健康告知和投保条件|就可以给自己和 家人投一份保障了|首月1块钱|WORIDWIDE|DWIDE|(保费随不同年龄有无社保等情况变化)|(以保险合同约定为准)|WOE|0S|泰康在线|保险业务由泰康保险机构提供 具体费率及保费金额以实际情况为准|TK.CN|GUC|“0.C|最高保额600万|UCCl|JCC|GO|LC\", \"video_asr\": \"你这车啊就八万你卖不卖？大哥，我真是家里有急事我才卖这车，你你多少给加点就八万不卖拉倒卖卖卖卖，我卖这车我们不卖啦，小妹啊，你就别捣乱了，行不行啊，你嫂子前些天查出来乳腺啊。|我手术费就几十万，我把车卖了都还差不少钱呢，之前我不是让你们买了微医保百万医疗险了吗？买了呀，可是这个十月一元，后续每月十四元起的保险能靠谱吗？哥你放心吧，这微保百万医疗险是腾讯旗下保险代理平台，保险责任范围内的一百种重大疾病均可申请理赔，保额高。|六百万呢，太好了，你嫂子病有救了，嗯哎，美女只为保百万医疗险在哪里可以买啊？点击视频下方链接，阅读保险条款和投保须知，确认被保人符合健康告知和投保条件，就可以给自己和家人投一份保障了。\"}\n",
      "multi-modal tagging model forward cost time: 0.015873432159423828 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '全景', '亲子', '室外', '喜悦', '极端特写', '动态', '悲伤', '单人口播', '平静', '家庭伦理', '惊奇', '朋友&同事(平级)', '夫妻&恋人&相亲', '路人', '教师(教授)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.88', '0.42', '0.33', '0.28', '0.17', '0.14', '0.08', '0.04', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0af79efa8e99a5748ccdd772c0cb42af.mp4\n",
      "{\"video_ocr\": \"姐夫|哎呀|让你姐知道|我·我死定啦|出大事啦|快借我1万块钱啊|你少来这一套|我前男友结婚|还给我发请束|你说这不是欺负人嘛|我必须得走面|姐夫必须帮你争这口气|把你手机拿过来|哇|100000元|你太好了|你居然借我10万块钱|好啊你|居然背着我藏钱|还10万|合哪来的?|不不不|媳妇|这不是我的钱|这是她在|360借条上的额度|每个人在360借条上|都有属于自己的额度|不信你打开手机看看嘛|我在360借条|居然有15万额度|老公|这360借条是啥?|是借款大平台|最高可以借20万|1分钟申请|最快5分钟放款呢|那这利息得多高啊|借1万元日息|最低只需要2块7毛钱起|还有借4万|最长免息30天的福利呢|姐夫姐夫|你快告诉我|在哪里申请啊|点击视频下方链接|就可以申请你的额度啦|¥ヨ60借条|¥ ◎借条|￥亘 借条|￥借条|￥36口借条|¥ ヨ6C惜条|斗 ヨ60借杀|斗 ヨ6Q条|B6回借祭|B6可 借条|BE口借条|速择360借条的四个理由|恭喜您成功领取免息券|30天息费优惠券|20方级|立即领取|活施舰即|BGQ级|最高20万服度|6T|点击视频下方立即申请|贷款有风险借款需谨慎请根据个人能力合理贷款|贷款额度 放款时间等以实际审批为准|最长|最长30天免息|全民兔息狂欢|借一年，慢慢还\", \"video_asr\": \"哎呀，你放手放手，让你姐知道我知道啥了，快借我一万块钱啊，少来这一套，姐夫我前男友结婚还可以放心点，你说这不是欺负人吗？必须得走面啊，姐夫必须帮你争口气，把手机拿过来。|姐夫你太，你居然有十万块钱，好啊，你居然背着我藏钱还十万，哪来的媳妇，这不是我的钱，这是他在三六零借条的额度，每个人在三六零借条。|有属于自己的额度，你打开手机试试看吧，哇，我在三六零借条上居然有十五万的额度，老公这三六零。|铁是啥？这三六零借条是借款大平台，最高可以借二十万，一分钟申请，最快五分钟放款呢，那只有一七六多高啊！这是三六零借条，借万元日息最低只需要两块，七毛钱起，还有借四万最长免息三十天的福利呢！姐夫姐夫，你快告诉我在哪里申请啊，点击视频下方链接就可以申请你的额度了。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.015985965728759766 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '多人情景剧', '手机电脑录屏', '汽车内', '喜悦', '特写', '极端特写', '夫妻&恋人&相亲', '单人口播', '路人', '惊奇', '动态', '平静', '悲伤', '亲戚(亲情)', '室外', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.95', '0.61', '0.41', '0.14', '0.12', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0afa1c05cc877c2b489376dcae43906b.mp4\n",
      "{\"video_ocr\": \"你层N|你家孩子是不是 英语不敢开口|语感特别差|英语成绩跟不上|我给你推荐 伴鱼自然拼读课|现在面向全国|推出14节自然拼读课|让孩子掌握26个字母 及字母组合的发音规则|教孩子学英语 就像学拼音一样自然|力求让孩子做到见词能读|听音能写|而且课程 还支持无限次回放|现在报名 还包邮赠送学习成长地图|2698元的全套课程|现在仅需体验价 29元14节课|按要求完课之后 还可以领取奖学金|名额有限|快点击下方链接报名吧|aei ou|元音字母|清晨 wake|蛇 snake|自然拼读 26个字母中aeiou|C AB\", \"video_asr\": \"A E I O U为五个元音字母，剩余的叫二十一个辅音字母。|A的拼读清晨WAKE来到LAKE。|掉到SNAKE。|爸爸BIG，你家孩子是不是英语不敢开口语感特别差，英语成绩跟不上，我给你推荐伴鱼自然拼读课。伴鱼自然拼读课现在面向全国推出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样自然地球，让孩子做到见词能读。|听音能写，而且课程还支持无限次回放，现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价二十九元，十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧！|NEXT。\"}\n",
      "multi-modal tagging model forward cost time: 0.016159772872924805 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '单人口播', '场景-其他', '配音', '静态', '课件展示', '办公室', '室内', '平静', '手机电脑录屏', '动画', '家', '知识讲解', '动态', '宫格', '喜悦', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.83', '0.81', '0.79', '0.23', '0.16', '0.05', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b04ccdcb722cd74f0193a587805a409.mp4\n",
      "{\"video_ocr\": \"坐坎向离 明堂受制|药神|我观阁下|青气尽散唇青舌墨|怕是|要死|装神弄鬼|滚|什么西医圣手|我沈家宁死不受|灌金汁之辱|沈老太君药石不进|恐怕|只有我能治了|优京|我与沈大小姐|同为鸳鸯蝶命的命格|救丈母娘理所应当|哪里来的混账东西|给我打出去|沈先生|想必每晚皆是|梦魇缠身神如炼狱吧|我若不出手|必步老太君后尘|你一个保洁也配谈治病|不信啊|小腹下三寸自己摁|仙师救我|救我啊|太乙神针|你就是我未婚夫叶飞|叶天师|你若能出手治病|小女以及整个沈家|愿归您所有|什么|1i|一直不被人看在眼里的山村小神医 一到都市却受到世家公主|疯读小说|减小说|疯读|集团女总裁的青睐 真实原因居然是|本故事纯属虚构 具体奖励以实际活动为准|海量小说永久免费|《桃运狂医》|7号 NO.7\", \"video_asr\": \"坐坎向离，明堂受制，我想要神都治不了的病，就等着死吧，药神我观阁下，心系竞赛怕是。|要死双生弄鬼的滚。|什么牺牲小我身价宁死不受管制，只蜘蛛神庙大君要是不进，恐怕恐怕只有我能治了。我与沈大小姐同事鸳鸯牒命的命格就丈母娘理所应当，哪里来的混账东西？|打出去，沈先生想必每晚皆是梦魇，我若不出手，必不老太君后尘，你一个保洁一杯谈治病不信啊。|小服下三寸自己嗯。|一。|先生教我教我太乙神针，你这是我老婆。|太医传人，一开始你若能出手，治病，效率及整个身家就归你所有。|什么。\"}\n",
      "multi-modal tagging model forward cost time: 0.02308487892150879 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '愤怒', '静态', '动态', '特写', '全景', '室外', '惊奇', '平静', '夫妻&恋人&相亲', '喜悦', '拉近', '单人口播', '家庭伦理', '路人', '朋友&同事(平级)', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.96', '0.96', '0.96', '0.95', '0.67', '0.42', '0.13', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b194a18a83d07efc703ac49aeb07e15.mp4\n",
      "{\"video_ocr\": \"Ihed|Theo!|heONnE|推荐孩子们使用的|钢琴智能陪练APP|慢练分手、分乐句|慢练这些基本功能全都有|而且Al娃娃还可以|把孩子们练得不熟练|和有错地方|标注圈框出来|只练不会的地方|进步特别快|小叶子智能陪练|郎朗看诺诺 用小叶子练琴|学钢琴的孩子们|这是郎朗亲自测试后|AI高科技|让你练琴|更快捷|更有效|市场价2600元|无限体验2周|快来和我练琴吧|i|小汤大汤 考级|巴斯蒂安|车尔尼 钢琴初步教程|(1770-1827)|[德]Lvm.贝多芬|错音|节奏问题|不熟练东这段|查看更多>|分钟18秒|考级专区|汤昔森系列|A纠错 消灭错音 献给爱丽丝|好，分析中……|曲谐库|调节速度|郎朗推荐 考级必备AI智能陪练|孩子们的|当出|椿金了|只要49元|Km|点击钱|点击链接|49./周|客方价2600元/年 提升孩子识谱能力 A旧自动标出重点练习|哈农 拜厄|24小时无限次练琴|开始|帮安|国际钢琴大师|Piano|最近练习|活动优惠|不熟练\", \"video_asr\": \"好费用，太棒了，这个录音，我们究竟我来了？好学钢琴的孩子们，这是朗朗亲自测试后推荐孩子们使用的钢琴智能陪练APP。|曼联分手分越聚慢练，这些基本的功能全都有，而且AIR Y还可以把孩子们练得不熟练和有错的地方标注圈框出来，只练不会的地方进步特别快。小叶子智能陪练AI高科技让你练琴更快捷，更有效。|原价两千六百元，现价只要四十九元，无限体验两周，点击链接，快来和我练琴吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016124963760375977 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '静态', '平静', '家', '手机电脑录屏', '配音', '多人情景剧', '室内', '单人口播', '情景演绎', '场景-其他', '夫妻&恋人&相亲', '亲子', '混剪', '教师(教授)', '家庭伦理', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.94', '0.56', '0.46', '0.26', '0.22', '0.15', '0.07', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b374abed8dee42558f5291c0c4686ba.mp4\n",
      "{\"video_ocr\": \"许多家长朋友啊|经常说 自己工作忙|没时间照顾孩子|更别提 辅导孩子功课了|那么如何 正确的把握|或者引导小孩子的 数学成绩|在这里我推荐|作业帮直播课|新学期数学名师 提分班|清北毕业名师 带队教学|帮助孩子 学重点练难点|重点培养孩子|学习数学的 兴趣和思维|课后还有班主任老师 1对1辅导答疑|教援各类 速算技巧和解题大招|让孩子 秒算小题|现在报名|30元 18节直播课|还包邮赠送 全套教辅资料|赶紧点击 视频下方链接|给你的孩子 一个蜕变的机会吧|9元 13节课 市场价:499元|上课内容与收到的礼盒请以实物为准|小初高数学名师提分班 快速掌握重难点 轻松领跑新学期|课|立即抢购\", \"video_asr\": \"取得家长朋友啊，经常说自己工作忙，没有时间照顾孩子，更别提辅导孩子功课，那么如何正确地把握或是引导小孩子的数学成绩呢？在这里我推荐作业帮直播课，新学期数学名师提分班，清北毕业名师带队教学，帮助孩子学重点。|难点重点培养孩子学习数学的兴趣和思维，课后还有班主任老师一对一辅导答疑，教授各类速算技巧和解题大招，让孩子秒算小题，秒杀大题，现在报名三十元十八节直播课，还包邮赠送全套教辅资料，赶紧！|点击视频下方链接，给你的孩子一个蜕变的机会吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015891551971435547 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '亲子', '多人情景剧', '静态', '平静', '特写', '家', '家庭伦理', '单人口播', '教辅材料', '极端特写', '喜悦', '拉近', '动态', '全景', '愤怒', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.86', '0.70', '0.50', '0.24', '0.17', '0.07', '0.06', '0.03', '0.03', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b386772b6ece5a38dbf48326d0e3869.mp4\n",
      "{\"video_ocr\": \"来了|快进来|这么好的房子 都让你找到了|你真行|\\\"S3IX3I0|现在选房 可是个技术活|户型结构是否合理|配套设施是否齐全|交通是否便利|环境如何 价格怎样|好在我有安居客|安居客|对呀|真的超实用|海量房源|还可以vr实景看房|地图找房|随时查看房价|点击下载安居客|解决选房烦恼|一起安居乐业|挑好房 上安居客|28万|34万|以上均为建筑面积|Anjuke|SIIXJ|IIXI0|SIIXOIU|JICU|JI0|CHAGBNEL|S3JY|SIXnrr|X3I0|3IX3|S.3|IHOIO|SIYHIa\", \"video_asr\": \"来了快进来。|哇，这么好的房子都没找到，或许现在选房可是个技术活，户型结构是否合理，配套设施是否齐全，交通是否便利，环境如何？价格怎样？好在我有安居客安居OR，对啊，安居客真的超实用，海量房源，价格公道，还可以VR实景看房地图找房，随时查看房价，点击下载安居客，解决选房烦恼，一起安居乐业！\"}\n",
      "multi-modal tagging model forward cost time: 0.015961408615112305 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '单人口播', '家', '平静', '喜悦', '手机电脑录屏', '亲子', '家庭伦理', '室内', '夫妻&恋人&相亲', '动态', '愤怒', '转场', '全景', '悲伤', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.85', '0.69', '0.18', '0.05', '0.03', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b3a9312798deaa0f05b7bf4da3499a7.mp4\n",
      "{\"video_ocr\": \"哎闺女|放学啦|我今天自己回去|这又闹什么脾气呢|气咱俩没有给她报|高途课堂 全科名师班呗|给你报|不就9块钱吗|晚了 早就没名额了|天天就知道 忙忙忙|我都高二了|别人家爸爸妈妈 都知道给孩子|那谁也没想到 9块钱就能上|北大清华毕业名师|直播教学的课程啊|考试的时候|别的同学 3分钟一道小题|人家检查三遍|都交卷了|有了有了|又开始报名了|还是9块钱就能学|4科16节课|教研团队整合出的|解题技巧和学习方法|都可以直击考点|而且课程支持 3年内无限次回放|那还等什么呀 赶紧给孩子报名啊|名额有限 屏幕前的家长们|赶快点击下方|给你的孩子报名吧|浙江卫视指定在线教育品牌|新用户专享 立即体验|视频为演绎情节|枫频为演|浙江卫视|全国百佳教师带队教学平均教龄11年|初高全科名师班|名师特训班|华少|儒|仅需|¥9|p，\", \"video_asr\": \"哎，闺女放学了，我今天自己回去哎，闺女着闹什么脾气呢，气咱俩没有给他报了高途课堂全科名师班呗。|给你吧，我就九块钱吗？晚了早就没名额了，天天就知道忙忙忙，我都高二了，别人家爸爸妈妈都知道给孩子报名高途课堂全科名师班，那谁也没有想到九块钱就能上北大清华毕业名师直播教学的课程啊，考试的时候别的同学三分钟一道小题，五分钟一道大题，人检查三遍都交卷了，九十九了，好多和他全科名师班又开始报名。|还是九块钱就能学四科十六节课，教练团队整合书的解题技巧和学习方法都可以知其考点，而且课程啊支持三年内无限次的回放，那还等什么呀？赶紧给孩子报名啊！名额有限，屏幕前的家长们，赶快点击视频下方给您的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015811681747436523 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '多人情景剧', '静态', '平静', '全景', '室外', '动态', '喜悦', '愤怒', '朋友&同事(平级)', '惊奇', '亲子', '悲伤', '路人', '汽车内', '单人口播', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.25', '0.23', '0.20', '0.15', '0.04', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b4281c2b8e8e3ef10cd2cdf80a562f6.mp4\n",
      "{\"video_ocr\": \"你要悄悄努力|然后惊艳所有人|就在上个星期|工作三年的我|终于拥有了人 生第一辆车|虽然不是什么 豪车|但是也算有了|可以遮风挡雨 的代步工具|这种感觉很幸福|一年前的我还 是月光族|每月工资|勉强维持着信 用卡的开销|后来我报名了|后州播音学了|才明白|还可以利用业 余时间做副业|声音进阶之路，让你的声音变观|百变市线|PS:后期深入诚领城学习后才能达到以上效果育 你将收获|普通话发自人门 精音主得进阶|发音针对性指导|了解自身优劣势，|见上才能享受优惠课程哦，未成年购买无效 让你的声音变现 还能配出灵魂|配音各神人物|镇领城字习后才糖达刺以效策考|清端表达|0基础入门播音配音|18岁以上才能享受优惠球程哦，来咸年购/无发|字正龄值|0000000000000000000000e|不止配的信|枪自咨种人物|掌握发声技巧，改尊音色 语音面貌检涮|收获|选撷|提音主掩进阶|入门|一自套优劣券，|师解答|02专业老师解答|人生多一种机|每价99元|覃州教育 COM|[ANZHOUEDM.COM|ANZHDuEDU.cOM|型育|魔长 州到育|潭教育|潭州教育|舒通话发音入门|18岁攻上才能寒受优惠课视哦，未成车的买无装|声音进阶之路，|不止起时|很音名种人物|¥|潭州配音精品课|现仅需9.元 辅音主持配春卜朗通|播音|主持|配音\\\\朗诵|本课程未成年学员领取无效|还靴配出|纯指导|:56|11:56|时性指导|CI3H00|鲜量铺|ut4G|l4G|除工资|03|人年彩|HOSHSh\", \"video_asr\": \"你要悄悄努力。|有人，就在上个星期，工作三年的我，终于拥有了人生第一辆车。|虽然不是什么豪车，但是也算有了可以遮风挡雨的代步工具，这种感觉很幸福。|一年前的我还是月光族。|每月工资勉强维持着信用卡的开销。|后来我报名了潭州婚姻学院，才明白除了工资，还可以利用业余时间做副业。\"}\n",
      "multi-modal tagging model forward cost time: 0.01626420021057129 sec\n",
      "{'result': [{'labels': ['中景', '现代', '静态', '推广页', '手机电脑录屏', '全景', '配音', '单人口播', '喜悦', '情景演绎', '动态', '极端特写', '平静', '特写', '混剪', '室内', '多人情景剧', '办公室', '室外', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.90', '0.88', '0.81', '0.68', '0.26', '0.08', '0.08', '0.06', '0.04', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b45b8d3e745653abbb49821538dbbb7.mp4\n",
      "{\"video_ocr\": \"19|做题还在列竖式|那就太慢了|看看我在猿辅导|学习的速算方法|我现在做题不仅快|正确率100%|小学数学是|学习数学的基础|而算数尤为重要|如果掌握不了正确的方法|孩子以后学习数学|将会举步维艰|猿辅导小学数学|清北毕业老师|讲解速算巧算方法|在家就能帮助孩子|夯实数学基础|巩固数学知识|87-16=7!|十4|+千|猿辅导х最强大脑一|23+52=7|高考状元 清华大学本科|88-32=|18个核心知识点 活动特惠|36=口3|猿辅导在线教育|九年教龄|@ 13课时直播互动课|邓诚老师|暑期数学特训班|拥导快速速算训纯|35-12=|3|34+57|59+14=|23+|2、|69-27=|96-45|52|52-14-|13+55=68|6天课后辅导答疑|57+23=|88|54+36=|76-32=44|36+17=t3|8=3r|28=|8=入|57-38=|猿辅导|9=67|9= 67|so\", \"video_asr\": \"做题还在列竖式，那就太慢了，二克我在研辅导学习的速算方法，我现在做题不仅会正确率百分之百，小学数学是学习数学的基础，而算数尤为重要。|如果掌握不了正确的方法，孩子以后学习数学将会举步维艰。猿辅导小学数学清北毕业老师讲解速算巧算方法，在家就能帮助孩子夯实数学基础，巩固数学。|真不懂。\"}\n",
      "multi-modal tagging model forward cost time: 0.016320466995239258 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '静态', '推广页', '平静', '极端特写', '填充', '手写解题', '特写', '教师(教授)', '中景', '场景-其他', '家', '室内', '动态', '多人情景剧', '教辅材料', '情景演绎', '家庭伦理', '学校'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.88', '0.54', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b5f71e1cfdac7e1baa43d5a53901057.mp4\n",
      "{\"video_ocr\": \"家长们注意了|如果你的孩子学英语|注意力不集中|学习兴趣低|成绩提升难|不要责怪她|这是因为|她没获得成就感|只要解决了孩子|单词记不住|发音不标准|语法易混淆的问题|她就会越学越有劲|成绩提升看得见|小初英语 单词语法名师课|国内外毕业名师|带队教学|9元5天13节课|教给孩子|8大 单词记2忆法|9个 音标发音方法|趣味口诀提分大招|高效解决单词 记忆难题|结合台13个语法 思维故事|让孩子轻松掌握 知识点|语法词汇都搞定|课后还有 专属班主任伴学|精美学习礼盒 包邮到家|赶快点击视频 下方链接|报名吧|特惠:9元/13节课|作业帮直播说|加级音最(<)否(×)相同。|课|Canada|掌握重难点 做题快又准|上课内容与收到礼盒以实际为准|中国女排|小初英语单词语法名师课|项。 teach|单词宝典|she 7ift|mother fruit|单调本|vit sixteen|0n|全国 包邮\", \"video_asr\": \"家长们注意了，如果你的孩子学英语注意力不集中，学习戏剧系成绩提升难，同样责怪他，这是因为他没有获得成就感，只要解决了孩子单词记不住，发音不标准，语法易混淆的问题，他就会越学越有劲，成绩提升看得见作业帮直播课小初。|单词语法名师课，国内外毕业名师带队教学，九元五A十三节课，教给孩子八大单词记忆法，九个音标发音方法，五大区位口诀提分大招，高效解决单词记忆难题，结合十三个语法思维故事，让孩子轻松掌握知识点，语法词汇都搞定。|还有专属班主任办学精美学习礼盒包邮到家，赶紧点视频下方链接报名吧！|不可能。\"}\n",
      "multi-modal tagging model forward cost time: 0.016021728515625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '单人口播', '推广页', '静态', '平静', '室内', '配音', '拉近', '喜悦', '家', '教辅材料', '极端特写', '过渡页', '手写解题', '场景-其他', '情景演绎', '办公室', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.44', '0.06', '0.03', '0.03', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b65618376a030d48d42af76d7faadd8.mp4\n",
      "{\"video_ocr\": \"姐夫姐夫|哎呀在公司叫经理|行经理|借我3万块钱|手机拿来|诺 你看|哇|8万|哪来那么多钱啊|这个叫玖富万卡|通过玖富万卡|最高可以申请|20万的借款额度|点这里|来申请你的额度|最高可借20万元|实际额度以审批结果为准 请珍惜信用，暗示还款，勿过度举债|清珍惜石用 实际额宜以取批|勿过厦举债|请秘惜信用，暗示还款，勿过度举债|请珍惜目暗示还款|请珍情信|，暗示还款，勿过度举债 适会惜信用|实际额度以审批结果为准|勿过度|l小还款，|OME CARO|以万卡|饮富万卡|不款|￥80005.22|MUSCLE|心贷\", \"video_asr\": \"哎呀，这公司找经理，经理就三五块钱，小心了。|那你看我。|哪那么多钱，这个叫酒放开，通过九放款，最高可申请二十万的借款额度，点击来申请你的额度。\"}\n",
      "multi-modal tagging model forward cost time: 0.01610255241394043 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '手机电脑录屏', '静态', '多人情景剧', '特写', '单人口播', '喜悦', '家', '朋友&同事(平级)', '平静', '办公室', '动态', '惊奇', '场景-其他', '极端特写', '工作职场', '配音', '填充'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.87', '0.86', '0.60', '0.54', '0.41', '0.38', '0.25', '0.08', '0.07', '0.06', '0.05', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b78980bc67f2a08f77ea7364c9f26eb.mp4\n",
      "{\"video_ocr\": \"第一单词记了又忘|第二英语学习没有思路|第三找不到学习的重点|那么这些同学呀请你一定要注意|其实一个技巧就能让你颠覆|整个高中英语的认知|统计学规律发现|在我们完形填空|和阅读理解的题型中啊|核心的单词|只有800个单词|就是我们高考|常见的高频考点|学会10大阅读理解|掌握12大完形填空解题技巧|跟谁学呢|当然跟我学啦|我是徐磊老师|加入我的高中英语强化课程|帮你夯实基础|不要把问题留在高二|积极备战冲刺高三|点击下方详情|还在犹豫什么|快来听课吧|知能 提升|定语太长，孟有生命，伤用o所有格|知识点|所有格|名词|清北名师授课|数学 重难点题型高分突破|使用条件:无生命的名镯|温等提5|o1所有格|记忆秘诀|90分以下的同学|面临这几个问题|课程仅需:9元|表示动作的执行者或者承受者:myfathers praise|'s(s)|础知识|口识 第一节 名词|?|跟谁学|在线学习更高效|物理对于我来讲很难 3.Two pairs ot trousers are on the bed.两条裤子在床上|三年过去了 ait of trousers is on the bed，一条祷子在床上|就达|意义一致|语法一敢|耍原则|便用条件:表部分概念或者某种感情|易考|我姐姐的一个朋友|2the poem ot my teachers我老师的这首饰|学习 误区|秒杀难题，秒出答案|详解|挑战高考英语135+|高中数英培优特训营|trousers are on the bed.两条销子在床上|sare on the bed.两条精子在床上|The teacher and wnter is going to gveusa lak next Sunds|许多老师已到达了车站 这位老师燕作家要在下周日给我们作报告|oneof my sister's tnends|my sister s tnends|概述|谁学|14|流理|的分类|升华|英语 单词+语法高效记忆|谁学金牌讲师 母年高中英语教学经验 ャ徐磊|9元领取|款近一败|词和|不只一个人已经提出了这个理议|这位老师和这位作家都在下周日|The teacher and the wtiter are going to give us a talk next Sunday. 人已经提出了这个建议|高壁英语|基本上|先到先得 左滑视频即可报名|第一节 名词|简介|于我来讲报难|四个重|去功能|Thebookison the dask这本书在课桌上 句法|3人后用|按词汇意义|school loday|today|总结|students are able to attond|sare able to attend|to atend|三组|调的|指导\", \"video_asr\": \"高二英语九十分以下的同学啊，基本上面临着这样几个问题，第一，单词记了又忘。|第二，英语学习没有思路，第三，找不到学习的重点，那么这些同学啊，你一定要注意，其实一个技巧就能让你颠覆整个高中英语的认知。|统计学规律发现在我们完形填空和阅读理解的题型中啊，核心的单词只有八百个单词，这八百个单词就是我们高考。|见的高频考点，学会十大阅读理解，掌握十二大完形填空解题技巧，跟谁学呢？当然跟我学了，我是徐磊老师。|加入我的高中英语强化课程，帮你夯实基础，不要把问题留在高二，积极备战冲刺高三。|点击下方详情，还在犹豫什么，快来听课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01610708236694336 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '单人口播', '中景', '教师(教授)', '静态', '平静', '场景-其他', '配音', '室内', '特写', '影棚幕布', '教辅材料', '极端特写', '手写解题', '幻灯片轮播', '喜悦', '手机电脑录屏', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.61', '0.61', '0.11', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b78a9b98afec58bca76236dd07a0447.mp4\n",
      "{\"video_ocr\": \"你看看你这个计算题|每次都不得分|回家以后|再给我刷两套速算题|16分)|多列列竖式|知道了吗|208X5|姐|教孩子呢要用对的|学习方法|否则刷再多题都没有用|来叔叔帮你看看这道题|107+8|115|聪明|115写前面|7×8=|56|56写后面|所以这道题答案是|11556|哎 小哥|你这么好的方法|是从哪学的呀|给孩子报一个|作业帮直播课吧|18节名师课呢|才30块钱|这套学习礼盒呢|也是免费送的|我天天啊往你们小区|送快递|所以呢|就给儿子报了一个|他天天说|有清华北大毕业老师啊|带队授课|学了很多解题方法|我听多了也就会了|哎等一下|点击视频下方链接|就可以报名了|精x8人 107X108-|用整式计算:(16分)|下面计算正确的是|包算:|小初高数学名师提分班|l.800+(125X8)|复.用整式计算:|计算(共37分)|满整式计算:|(1+50~|1、口算， 周暨式计算; 633:88~|107X108=|同￥50~|中国女排 上课内容与收到礼盒请以实际为准|600*125，下面计算正确的是( 茶长100米的直跑道上，画出的跑道线是|B、(600:8)+(125X8|中国女排为作业帮直播课代言|+(125X8)|中国国家女子排球队官方教育品牌|30元 18节课|OTIs|OTs eecrc|B、线段|C、直推|510:3|识口了|D口|射线 B、9|T0|报名 即送\", \"video_asr\": \"你看看你这个计算题，每次都不得分，回家以后再给我上两个速算题，多练练出事知道了吗？姐教孩子们要用贵的学习方法复杂，刷再多题都没有用来，叔叔帮你看这道题啊，一百零七加八等于多少啊？一百一十五聪明，一百一十五写前面七乘八等于五十六。|六写后面，所以这道题的答案是一万一千五百五十六，哎，小哥，你这么好的方法送哪学的呀，给孩子报个作业，帮直播课八十八节，名师可能才三十块钱，知道学习礼盒呢也是免费送，我天天玩，你们小区送快递，所以呢就给儿子报了一个，他今天说。|有清华北大毕业老师带队，首歌有了很多解题方法，我听多了也就会了。|哎，等一下，你这么好的课从哪报名的呀？点击视频下方链接就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01621222496032715 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '多人情景剧', '中景', '静态', '路人', '平静', '喜悦', '亲子', '惊奇', '动态', '特写', '愤怒', '悲伤', '家庭伦理', '夫妻&恋人&相亲', '全景', '室外', '手机电脑录屏', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.85', '0.40', '0.37', '0.19', '0.13', '0.11', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b8b7db5e7eab8693be2932632b4d4f3.mp4\n",
      "{\"video_ocr\": \"教会孩子22计算题|现在花30块钱|就可以改变孩子 的未来|不要怀疑|作业帮直播课|现推出|小初高数学名师提分班|30元18节的名师课|解题大招|25个应用题经典解法|清北毕业名师带队教学|专攻孩子 解题思路和做题技巧|让孩子做一道题|会一类题|一次学不会也不要紧|因为课程支持3年内 无限次回放|您放心买|孩子安心学|现在报名|还免费包邮赠送12件套 教辅礼盒|点击视频下方|报名吧|记得选好对应的年级哦|班主任老师1对1答疑|凑十法|高分冲刺解题技巧|重难点提分妙招|速算技巧|数学思维|思维导图|小学学公式|(13)|1|速算|学什么|掌提解题技巧|作业帮累计用户超8亿|位置关系|免费|作业带且描妹|T业带且播保|TF业带且味|[上课内容与收到礼盒以实际为准】]|展示礼包为小学礼包|5L14）|小学数学|数学|图形计算|赠送|名师提分班|国回|周长|相关问题\", \"video_asr\": \"现在花三十块钱就可以改变孩子的未来，不要怀疑作业帮直播课现推出小初高数学名师提分班三十元十八节的名师课，教会孩子二十二个计算题，解题大招，二十五个应用题经典解法，清北毕业名师带队教学，专攻孩子解题思路和做题技巧。|让孩子做一道题会一类题，一次学不会也不要紧，因为课程支持三年内无限次回放，你放心买孩子安心学，现在报名还包邮赠送十二件套教辅礼盒，点击视频下方报名吧，记得选好对应的年级哦！|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.01583719253540039 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '单人口播', '平静', '静态', '极端特写', '教辅材料', '手写解题', '室内', '动态', '配音', '室外', '特写', '场景-其他', '转场', '家', '学校', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.86', '0.50', '0.34', '0.06', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0b9bb7f0db9bed99fc132ca0a3f38b48.mp4\n",
      "{\"video_ocr\": \"天教给大家几种创造被动收入的方法|联盟行销|自媒体带货|三写文章做图贩卖知识产权|还有一种最高效也最容易实现|训籍|就是跟着快财商学院的|小白理财训练营学习|这里有金牌理财团队在线授课|0基础的小咱也能|轻松掌握理财入门技巧|正确规避基金股票风险|正确分配财产比重|让你用钱赚钱|现在报名0元即可摆脱穷人思想|、戸1|ト万|掌握富人思维|超快点紧视频报名吧|直播|※快财商学院小日五|※4快财商学院小理财洲东|只“快财商学慌院|日理对训s|院小自理财训|完小日理财洲菊|中时学院小理财洲训|电财商学院小理财洲|商学院小理财训燕|做图 知识|肖院小押|隍小押|只 小戸珥|?文章 做图 知|?交章做图知|っ交章 做图|2号文章 做图上识变切|2号交章 做图矢|?文章 做囚|2号文章|贝识产权|姻图 知识产权|文章 做囚 知识产权|联|行销|盟分销|2自媒体蒂|体带货|煤体带贷|快财\", \"video_asr\": \"今天教给大家几种创造被动收入的方法，一，联盟行销，二，四媒体带货，三，写文章作图，外卖，知识产权，还有一种最高效，也最容易实现，就跟着快财商学院的小白理财训练营学习，这里有金牌理财团队在线授课，零基础的小白也能轻松掌握理财入门技巧，正确规避基金。|股票风险正确分配，财产品中，让你用钱赚钱，现在报名零元即可摆脱穷人思想，掌握富人思维，赶紧点击视频报名吧！|一。\"}\n",
      "multi-modal tagging model forward cost time: 0.016127586364746094 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '室内', '手机电脑录屏', '平静', '喜悦', '情景演绎', '动态', '配音', '极端特写', '多人情景剧', '全景', '学校', '家', '拉近', '手写解题', '填充'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.94', '0.80', '0.36', '0.30', '0.24', '0.16', '0.13', '0.05', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0baa022c321bea09179f948267cab15e.mp4\n",
      "{\"video_ocr\": \"今天|我会得罪 所有赚钱软件的商家|我们玩赚来电 这样承诺|要是今天下载|最高100元红包的|新人没有领到|我赔|没有获得 *5元红包奖励的|吃饭喝水 睡觉玩游戏|没有领到红包的|我保证|都是可以直接 提现到微信|直接当钱花|唯一的下载链接|就放在视频下方|赶快动动手指|下载吧|微信登录|看创意视频免费领 去观看|立即提现|打开签到提醒+200金币|点这里 就可以把当前视频设成来电秀哦!|活动倒计时:23:59:33|新人限时闯关领5|恭喜你！获得新人奖励红包|天天签到领金 已经连续签:|已经连续签到1天|魔性小黄鸭 心情好就得跳个舞|以金币形式发放，可提现|看视频赚钱 0/1|80 100|50元|玩今来电|嫌来电|完成新手任务电|技女出没有获得电|电|都是可嗷囊|去观，百元红包|+1元|明天签到最高领取6.6元红包|领奖|百元红包|快点击链接 领取你的100元吧|明日领取 最高6.6元|提现金额 价人专享|**号|今日已签|6.6|RR无笑到最高可领6.6元|兑换3000金币|00:27.96 限时领取|送红工自|VR 透明主题 重力感应|随机福利|新 人奖励|签到成功!+20金币|8元140 高8.|设置来电秀|100最高8.8元140|最高|25元 0.3元|2377.2W 1552.1W|14天|福利中心|收藏 热门|小姐姐|+20金币|红包雨|金币赚不停 立即赚|来电秀 铃声 定制|现金收益|设桌面|具体金额根据实际规则为准 解冻红包活动，满100元即可提现|兑换1000000金币|去设置|30天 21天|看图猜成语|睡觉赚钱|更多|搜索|成语填字|规则|提现|萌系 音乐 搞笑|金币122220>|吃饭|情侣|8.8|+2|12.22|我的金币|赚现金\", \"video_asr\": \"今天我会得罪所有赚钱软件的商家，我们玩转来电这样承诺，要是今天下载玩转来电新人没有领到最高一百元红包的我赔。|完成新手任务，没有获得五元红包奖励的我赔吃饭喝水睡觉玩游戏，没有领到红包的我赔，我保证所有领到的红包都是可以直接提现到微信，直接当钱花，唯一的下载链接就放在视频下方，赶快动动手指下载吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016216039657592773 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '手机电脑录屏', '单人口播', '静态', '平静', '配音', '场景-其他', '喜悦', '家', '办公室', '室内', '特写', '多人情景剧', '动态', '宫格', '朋友&同事(平级)', '商品展示', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.58', '0.46', '0.13', '0.11', '0.08', '0.04', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0bb07842820fec8bd56c0ddb47af4c7c.mp4\n",
      "{\"video_ocr\": \"妹妹我真的帮不了你|季氏集团 不是你想进就能进的|就你还想进我公司|关你什么事|放开我|这难道是那天救我的女人|这孩子简直和您一个模子刻出来的一样呢|5年了|你终究还是逃不过我的手掌心|你得入上是就不过我的手口心\", \"video_asr\": \"我真的帮不了你，你真是集团，不是你想进就能进的，就你还想进我公司，你什么事放开我，难道是那天救我的女人？|这孩子简直和你一个不氪出来的有五年了。|你终究还是逃不过我的手掌心。\"}\n",
      "multi-modal tagging model forward cost time: 0.01607489585876465 sec\n",
      "{'result': [{'labels': ['中景', '现代', '静态', '多人情景剧', '平静', '推广页', '喜悦', '夫妻&恋人&相亲', '愤怒', '单人口播', '室外', '惊奇', '动态', '特写', '朋友&同事(平级)', '全景', '家', '悲伤', '室内', '亲子'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.98', '0.94', '0.77', '0.53', '0.29', '0.13', '0.13', '0.07', '0.02', '0.02', '0.01', '0.01', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0bbb82e1a10dd36b0b6bf5c2d87e76ce.mp4\n",
      "{\"video_ocr\": \"中年危机的源头是什么|是不努力吗|错年轻时不注意理财|靠时间和体能赚钱|收入单一|早就埋下了隐患|你学不会用钱赚钱|就只能工作到老|一旦你的时间|和体能下降|中年危机就此出现|就是现在|加入 尚德小白理财训练营吧|十年理财资深行家|手把手教你玩转|股票 债券基金 保险|等多种理财产品|现在报名一元 就能获得五天的直播课|赶快点击视频|下方链接报名|让自己收获|学会理财投资技巧 5节直播课|资领取表|员工工资领取表|为02|B8S|802天|B美|B7神E4|西 盼T4|HE云.国|带你挑战财富翻倍|手把手教学|告别死工资!如何入门理财？|查看理财课程详情>>|备注|实发工资 员工签字|视频为演绎剧情|Tv4|加班补贴。|苣|总裁办|三国三|C00|0080\", \"video_asr\": \"中年危机的源头是什么？是不努力吗？错，年轻时不注意理财，靠时间和体能赚钱，收入单一早就埋下了隐患，你学不会用钱赚钱，就只能工作，到了一淡定的时间和体能下降，中年危机就此出现，就是现在加入上午的理财小白训练营，八十年理财资深行家手把手教你玩转。|债券，基金，保险等多种理财产品，现在报名一元就能获得五天的直播课，赶快点击视频下方链接报名，让自己收获机遇风险的能力吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.02215266227722168 sec\n",
      "{'result': [{'labels': ['中景', '现代', '推广页', '静态', '办公室', '单人口播', '平静', '工作职场', '多人情景剧', '动态', '室内', '喜悦', '惊奇', '极端特写', '愤怒', '拉近', '上下级', '朋友&同事(平级)', '全景', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.89', '0.79', '0.60', '0.45', '0.15', '0.13', '0.04', '0.04', '0.04', '0.02', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0bc09c284e92352434216f6ee03bd4da.mp4\n",
      "{\"video_ocr\": \"你连30块钱都不 舍得给孩子花吗?|现在有一个机会 让你花30块钱|就可以改变孩子 的末来|不要怀疑|作业帮直播课 现推出|小初高 数学名师提分班|30元18节 名师课|教会孩子数学|21个重难考点|18个解题大招|还免费包邮赠送 教辅大礼包|清华北大毕业的名师 带队教学|专攻孩子解题思路 和做题技巧|让孩子做一道题 掌握一类题|一次学不会也不要紧|因为课程支持 3年内无限次回放|你放心买 孩子安心学|各位家长们|给你得孩子一个 改变的机会吧|点击视频下方 报名吧|小初高数学 30元18节名师课抢|作业帮累计激活用户超8亿 清北毕业名师带队授课 作业帮旗下产品累计用户量|班主任老师1对1答疑|展示礼包为小学礼包 【上课内容与收到礼盒以实际为准】|课|小学学公式|免费 赠送\", \"video_asr\": \"你连三十块钱都不舍得给孩子花吗？现在有一个机会，让你花三十块钱就可以改变孩子的未来，不要怀疑，作业帮直播课现推出小初高数学名师提分班，三十元十八节名师课，教会孩子数学二十一个重难考点，十八个解题大招，还免费包邮，赠送教辅大礼包，清华北大！|名师带队教学，专攻孩子解题思路和做题技巧，让孩子做一道题，掌握一类题，一次学不会也不要紧，因为课程支持三年内无限次回。|你放心买，孩子安心学，各位家长们给你的孩子一个改变的机会吧！点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01611781120300293 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '中景', '单人口播', '室外', '平静', '静态', '转场', '特写', '动态', '配音', '室内', '场景-其他', '混剪', '教辅材料', '极端特写', '公园', '全景', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c0c86d432e717a20b9e2ba2a77c78ff.mp4\n",
      "{\"video_ocr\": \"去十法|数学 其实很简单|跟着正确的老师|学习正确的方法|我是猿辅导老师邓诚|市高考理科状元|清华数学专业毕业|猿辅导老师联盟|均为北京一线好老师|教学教研经验丰富|累积好评率达98%以上|我是老师邓诚|数学辅导就在|猿辅导|元|710|根尔4裁价头众气+ x长|根气与数的天么.久+— x长|501|49-45=(） 59-54-(5)69-63=()|59-54-(5)69-63-（|a啡|清华大学毕业 教出63位清北学生|不4式|等山|o一元西潭方糕术极公虎|手看优|6天专学重难点 冲刺领跑新学期|12-9=(3）|猿辅导在线教育|11 -4=()11-5=()11 -6-( )|11 -4=(7) 11 -5=()11-6=()|11 -4=(7)11 -5=(6) 11-6=|7)|清华大学数学专业 猿辅导老师教龄9年|15-6=(|15|9+3=(12)|6 10|9+6=|9+|6=(1)|14 -7=|-5=(）17-4=(）|+3=(|7-4=()|机气|机有4|)0|特惠 猿辅导名师 邓诚|等盂教利|2=(|) 9+4=(|9+5=()  9+4=(|9+5=()|12 -9=()13-8=()14-7=()9+6=()9+5=(|12 -9=(）’ -8=()14-7=(|12 -9=(3）13-8=(5）14 -7=() 9+6=(1)|12-9=(3)13-8-(5)|13 -8=(5)|第点领刊|29-27=( ） 39-36=(|27=(） 39-36=|19-18=() 39-36=(|29-27=() 39-30|19-18=()29-27=()39-36=1| 9 108|暑期数学名师特训班|69-63=( )|59-54=(5) 69-63=( )|69 603|509 54|49-45=(l|(5)|11-7 8=(）11-9=()|11-7=() 11-8=()11 -9=(|1 7|- 4-(|邓诚|20 7|49\", \"video_asr\": \"哎。|数学其实很简单，跟着正确老师学习正确的方法，我是猿辅导老师，邓城是高考理科状元，清华数学专业毕业，猿辅导老师联盟均为北京一线好老师，教育教育经验丰富，累计好评率达百分之九十八以上，我是老师邓城数学辅导就在猿辅导员通知。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.015923500061035156 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '手写解题', '推广页', '静态', '平静', '极端特写', '中景', '填充', '教师(教授)', '学校', '场景-其他', '特写', '配音', '动态', '家', '宫格', '混剪', '教辅材料', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.03', '0.03', '0.03', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c12c5485395015609f73e4b9f55c726.mp4\n",
      "{\"video_ocr\": \"我们 白送手机|为什么不要啊|这么多手机 送不出去|我们都要被 老板开除了|你俩别哭了|这白给 4000多的P30手机|天上 掉馅饼的事|大家 当然不信了|人格保证|我们疯读小说|集碎片兑换P30 是真实有效的|连续签到7天|就能获得 每认真阅读五章9个碎片|看小说做任务 还得碎片|集齐10个 P30手机碎片|集齐其他 相对应的碎片|还能兑换iPad|扫地机器人等|直接包邮 送到家|你们都 帮帮我吧|我们真的 不想被开除|你们赶快点击 视频下方链接|来把这些手机 都兑换走吧|点击下方链接下载|具体金额以活动规则为准|具体金割活对准|具优如准|休金 则为准|月金额以活动规则隹|活动规则为准|り活动规准|状现以活动规准|其妙怀孕。她生下三个天… 现代言情 总裁爹地|他刚伸手，美女就突兀地坐|一胎三宝，总裁爹…. 嫁给植物人之后，沈琉璃莫名|×|每认真阅读五章得1枚，最高可得20+碎片|距结束仅剩 6天23时59分|尔已签到3天，别中断吗|新人福利|新人|2天|本FIコ士 书架|言情小说·总裁文|超给力|极品桃运医圣|我的奖品 碎片明细>|福利中心 规则|阅读领取|23:59:59|已兑换|4月11日 星期四|明日签到继续领碎片 签到提醒|明日签至|领取|00|3/3|了起来，双手抱住赵逸杰...|恭喜您获得!|08:08|剩余200份/共200份 已兑换10次|jP30手机|X日.|X...|查看详情>|x2|疯读|西三|分类\", \"video_asr\": \"你。|爱上手机，你为什么不要这么多，手机送不出去，我们两大老板开除了，哎呀，你俩别哭了，就白给是下落的P三零手机天上掉馅饼的事大家当然不信了，我也仁哥抱歉，我们疯读下去。|碎片兑换P三零是真实有效的，连续签到七天就能获得九个碎片，看小说做任务还多碎片。|继续十个P三零手机碎片就能兑换P三。|手机急急，其它相对应的碎片还能兑换IPAD。|到底机器人等直接包邮送到家，你们都帮帮我吧，我们真那么早被开除，你们敢碰？|视频下方链接，大白，这些手机都摔坏走吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01618671417236328 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '场景-其他', '推广页', '配音', '静态', '多人情景剧', '单人口播', '办公室', '动态', '喜悦', '愤怒', '平静', '惊奇', '工作职场', '特写', '游戏画面', '拉近', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.95', '0.94', '0.92', '0.81', '0.74', '0.16', '0.07', '0.04', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c19501c24c5cdca1149487fd98fe6f1.mp4\n",
      "{\"video_ocr\": \"让你细心做题 别写那么快|小题验算大题写清步骤|你看你错的|你都初二啦 成绩还是垫底|考都考完啦 说那么多有什么用啊|你有错我还不能说啦|那我同桌佳佳|人家3分钟一道小题 5分钟一道大题|卷子还检查了3遍 那么快|我题都没做完呢|也没见她考砸啊|那人家怎么做到的啊|人家有个好妈妈|给她报了作业帮直播课|9|高中多科名师训练营 三新学期压力大?|清华北大毕业名师带队教学|秒杀考试中遇到的难题|那.妈妈也不知道|这9块钱的课 能有这么好呀|那你怎么不给我报名试试呐|佳佳已经掌握了 多科的核心知识点|课后还有班主任老师 1对1辅导答疑|学习质量有保证 效率高|等人家考上重点高中 我还在垫底|你就后悔去吧|报妈妈现在就给你报|等等|点击视频下方就可以报名|一刘鑫|秒薪七 策十章|5科 数理化语美全面提升|1对1|新用户特惠|作业帮直播课|学会一个知识点 掌握一类解题技巧|你是不是也遇到了这样的间题?|科目多压力大|免费 赠送|课程详情|上课内容与收到礼盒请以实际为准|高一|初中|请选择9月开学所在年级|送超值教辅大礼包|9元 4科12节|一刘颖妮|一谭梦云|名师训练营|高中多科\", \"video_asr\": \"让你细心做题，别写那么快，小题验算，大题，写清步骤，你看你错的，你都初二的成绩。|试电笔考都考完了，什么都能用啊，有错我还不能说了，那我从我家接人家三分钟用小缇你就能做到，只赚他检查了三遍。|么会我题都没做完呢，她也能快，你们家那口子啊，那人家怎么做到的？人家也有好妈妈给他报了作业，帮直播课，高中全科名师训练营，清华北大毕业名师带队教学，老师考虑中遇到的难题，那妈妈也不知道这九块钱的课能有这么好呀，那你怎么不给我都能顺顺呢？在家已经掌握了租客的核心知识点。|课堂氛围，老师一对一辅导答疑，学习质量有保证，效率高，等人家考上重点高中，我还在垫底，你就后悔去吧。|报妈妈现在就给你报等等，点击视频下方就可以报名。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016301631927490234 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '平静', '中景', '多人情景剧', '全景', '室外', '动态', '静态', '亲子', '喜悦', '极端特写', '惊奇', '特写', '路人', '家庭伦理', '朋友&同事(平级)', '配音', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.44', '0.34', '0.24', '0.16', '0.13', '0.12', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c2783714511ca553b66d05c8f42829e.mp4\n",
      "{\"video_ocr\": \"台下人走过|不见旧颜色|颜色|唱着|不人唱着|爱别歌|心碎爱别歌|情字难落墨|她唱以来和|她唱须次血莱和|她唱频血获和|唱须以血来和|戏幕起戏幕落谁是容|想轻松学音乐 关注我\", \"video_asr\": \"爱心。|不起球颜色。|开始。|我心随你。|妻子。|超兮兮兮木兮兮木。|这是一颗。|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016563892364501953 sec\n",
      "{'result': [{'labels': ['才艺展示', '填充', '现代', '情景演绎', '静态', '平静', '室内', '特写', '中景', '单人口播', '喜悦', '拉近', '演播室', '宫格', '古装/武侠', '单人情景剧', '影棚幕布', '动态', '手机电脑录屏', '励志逆袭'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c2a722d18a3934f7159741924e8ca69.mp4\n",
      "{\"video_ocr\": \"人们总渴望能坚持做一件事 People arealways keen to be persistent.|我们总会雄心壮志地设下目标 Wesetambitious goals|像是每天早上做50个俯卧撑 like doing 50 push-ups everymorning|或者每天学习英语三小时 orlearning English for three hourseveryday.|但没坚持几天我们就会放弃 But we tend to quit within ashort period of time.|我们往往把目标设得过高 That's because weusuallyset our goals too high,|使得自己难以坚持 making them hard to reach.|但你知道吗 Butyou know|有意识地培养小习惯 cultivating small habits|能让我们更好地坚持做一件事 would actually make things much easier to stick with.|所以如果每天做50个俯卧撑很难的话 Soifit's hard to start with 50 push-ups every day，|尝试做5个吧 try to start with5.|如果你觉得每天学习英语三小时很难 If you find it difficult to learn English for3hours,|那就试着学习15分钟 try tostart with15minutes.|而对于英语学习我推荐开言简单学 And for your English learning,\\\"'d like to recommend this app to you.|开言简单学为用户提供各种不同主题的精品课程 It provides quality courses in different topics,|每个话题15分钟 each for15minutes.|来吧让这个好习惯为你在职场中助力 Come on, let it help you stand out in your workplace.|如何坚持做好一件事？|地道口语轻松学|告别哑巴英语|点国61B-41u咸|面161B-141615|日台各652|A:061-24|mi664165.|盘i64|[416010-641n016n4|S0S茶1下|80J|Y880J|、g9)|开言简单学|你怎么穿着这么多衣服?|many clothes?|It's cold outside!|50|3 hrs|2230A DAT23|73A T23|oUDNUNO|OUJX|ouNLX|J|15 mins|免立即下载|0千|Hey! Why are you wearing so|n INNKE\", \"video_asr\": \"PEOPLE ALWAYS CAME TO BE PERSISTENT WITH THAT AMBITIOUS GOALS LIKE DOING FIFTY PUSH UP EVERY MORNING OR LEARNING ENGLISH FOR THREE HOURS EVERYDAY。|WHAT WE TEND TO QUIT BUT THINGS ARE SURE A PERIOD OF TELLING THATS BECAUSE WE USUALLY THAT I WILL GOES TO HIGH MAKING THEM HARD TO RICH。|BUT YOU KNOW COLD EVEN THINGS MORE HAVE IT WOULD ACTUALLY MAKE THINGS MUCH EASIER TO STICK WITH SO IF ITS HARD TO SERVICE FIFTY PERCENT EVERY DAY HI TO SERVICE LIFE IF YOU FIND IT DIFFICULT TO LEARN ENGLISH FOR THREE HOURS TRYING TO STRIVE AND FIFTY MINUTES AND FOR ENGLISH LEARNING I LIKE TO RECOMMEND THIS APP TO YOU PROVIDE SECURITY FORCES IN DIFFERENT HABITS EACH。|FIFTY MINUTES COME ON LET IT HELP YOU STAND OUT INNER WORK PLACE下载开言简单学，告别哑巴英语。\"}\n",
      "multi-modal tagging model forward cost time: 0.015859365463256836 sec\n",
      "{'result': [{'labels': ['中景', '现代', '填充', '推广页', '静态', '单人口播', '室外', '喜悦', '动态', '室内', '特写', '多人情景剧', '情景演绎', '平静', '夫妻&恋人&相亲', '愤怒', '惊奇', '家', '全景', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.95', '0.81', '0.55', '0.53', '0.36', '0.21', '0.21', '0.12', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c2ceb0b87c86bbe8b04e62a446555a9.mp4\n",
      "{\"video_ocr\": \"F.4aF 肥d d/B 高考物理满分110(100)分|pdv|而很多同学连40分都考不到|在高考的拉分程度是仅次于数学的|物理一科就可以让|你超过或者落后别人大几十分|我是高途课堂张展博老师|清华本科及硕士|高中物理常年满分|千题一法归一|千题一解 方法归一|12大通解体系会帮你用很少的时间|学很少的知识考很高的分数|让你彻底告别过去|低效甚至无效的学习方式|给自己一个黑马逆袭的机会|查看详情 抓紧报名吧|飞小d-制/Bd|zd-dl/Bd|Hdl-/Jd+ /光d|阳小d制l/Bds|Hd-/d+/提小|小山--dBd|d一制/Bd|王dd/Bd|d-/Bed|ddlB山|a-/1ds+/米山|d贵B以s|d款/B|Ha小/d+/果4|只需9元|Hd-/1d+是d|Hud-/]d+ /半山|=v + ar|1ar =vot+|静摩擦力:0≤f静<f静mac|wt+多gr|频率f与周期T的关系:f=7T|F|F=n|m1a|mC|110|CA|zdl|E-dl|vo2|¥02|六、力的合成法则:平行四边形等则|2、竖直上抛运动 -gt|2、向心力公式: -mro2=mr|12个通解|四、胡克定律: F=kx|物体加速度向下，为失重现象。|(2)竖直方向|力提供，即F合=F向|v=v十at即a=|动摩擦因数 其中，y|其中，M—— 动摩擦因数|=v十 gt|+ gt|vo + gt|t=T 线速度v与角速度o的关系：v=ro|位移:a—— 加速度；t——时间|七、物体平衡条件:合外力等于零，即F合=0 八、牛顿第二定律:F会=m0|Agr|2gt2|华少|七、物体 八、牛顿|3、平抛运动|向心加速度公式:|Id+/e|￥d-/1d+/光4|张展博 （1d/e|+a-/]d+/器|Hdl-/1ds+|二、匀变速直线运动|滑动摩擦力:fuFN — 弹簧的形变量|抛体运动|线速度ν==|FN —— 正压力|角速度0=|全国百佳教师带队教学 平均教龄11年|匀速直线运动的位移公式:s=v|F— 弹簧的弹力|1、匀速圆周运动的几个基础公式: 2Tr|弹簧的劲度系数 k ——|新用户专享 立即体验 浙江卫视指定在线教育品牌|三、自由落体运动（(即w=0，a=g 的匀变速直线运动)|现象。|九、高|匀速直线运动|3、在匀速圆周运动中，物体做匀速圆周运动所需要的向心力由合外 4、(1)当F合=F向，物体做匀速圆周运动；|必修一|所有的抛体运动的F合=mg=ma ，知加速度为ag，为匀变速运|弹簧的弹力|自由落体运动|(2)当F合>F间，物体做向心运动;|=mc|A4纸学习法|ds|PB·ds o|V+V|y+y|其中，f静max 最大静摩擦力|28|m72|高中物理必备公式|高途课堂 浙江卫视|v—— 初速度；1—— 末速度；|即F合=0|U向=下=ro2=r （T|md 他山 o|Z=m bBd o|z=md|pB. ds-o|Z-mc|eB.d-o|B-ds-o|d -o|B·山so|中B|pB·.ds|-m|五、摩擦力|动。|F.“帮|清华大学学士|F.巢|x- yot|— 10t|高途课堂名师特训班|名师特训班|九、高中阶段，一般情况下，物体加速度竖直向上，为超重现象;|高中物理主排春师|新用户专享 立即体验|F-C|F三m|=7|Ima|物体加|t2|.t|=v2|Vot|v0|Vx|LaIS|F——|洲啡|．…|曲イ|仅需|¥9\", \"video_asr\": \"想考二幺幺，物理八十分是基础，想上九八五起码得上九十分，但很多孩子觉得物理太难了，小题容易错，大题不考，物理满分一百一十分，而很多同学连四十分都考不到，在高考的拉分程度是仅次于数学的，物理一科就可以让你超过或者。|落后别人大几十分，我是高途课堂张展博老师，清华本科及硕士，高中物理常年满分，先题意解，万法归一。|十二大通解体系会帮你用很少的时间学很少的知识，考很高的分数，我的A四纸学习法，让你彻底告别过去低效甚至无效的学习方式，只需九元，给自己一个黑马逆袭的机会，查看详情抓紧报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016090869903564453 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '教师(教授)', '单人口播', '静态', '场景-其他', '平静', '学校', '影棚幕布', '手机电脑录屏', '拉近', '室内', '喜悦', '家', '配音', '特写', '幻灯片轮播', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.64', '0.26', '0.26', '0.07', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c3c56099ffe62c18c7985a8110baac5.mp4\n",
      "{\"video_ocr\": \"我听过|是钱是挣来的|不是攒来的|有些朋友听了并不理解|当他们卡里有超过|五千五万的时候|他们开始花几千块钱|送自己一个包包|花几瑟身送自奋包也手表|花几万块送自己一块手表|理财这件事|对他们来说可有可无|但是人无千日好|一旦你遇到危机|工资被公司拖欠|交不上房租|这个时候才发现|买来的包包手表|一点忙都帮不上|只能打骨折甩卖|特别是家人急症|要做手术的时候|更是体会到钱|一分钟就必须到账的感觉|这个时候|良好的理财习愦和方法|良好理无频方法|存下的不是财富|而是|所以理财的重要性|不是在于 你能获得多大的财富|当动荡来临的时候|你的生活是停车 还是彻底翻车|现在花一块钱报名|未来用一身的理财本领|给自己重头再来的勇气|和大胆选择的底气|最毒鸡汤|第二次生命|理财小白特训营|尚德机构 SUNLANOS|元小白理财课，1元就可抢!|1元!1元!1元! 市场价399|查看详情\", \"video_asr\": \"我听过最毒的鸡汤是钱是挣来的，不是攒来的，有一些朋友听了并不理解，当他们卡里有超过五千五万的时候，他们开始花几千块钱送自己一个包包，花几万块送自己一块手表，理财，这件事对他们来说可有可无，但是人无千日好，一旦你遇到危。|一公司被公司拖欠交不上房租，这个时候才发现买了个包包手表，一点忙都帮不上，只能打五折甩卖。|特别是家人急症要做手术的时候，更是体会到钱一分钟就必须到账的感觉，这个时候良好的理财习惯和方法存在的不是财富，而是第二次生命。所以理财的重要性不是在于你能获得多大的财富，而是在于桑送到碗里的时候，你的生活是停车还是彻底三折。|现在花一块钱报名理财小白特训营，未来木一身的理财本领，给自己从头再来的勇气和大胆选择的笔记。|点击下方链接！\"}\n",
      "multi-modal tagging model forward cost time: 0.01588606834411621 sec\n",
      "{'result': [{'labels': ['中景', '现代', '填充', '平静', '拉近', '室内', '知识讲解', '拉远', '单人口播', '动态', '惊奇', '静态', '宫格', '教师(教授)', '多人口播', '单人情景剧', '推广页', '喜悦', '励志逆袭', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.58', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c3e1318eaa22df12ceaebe743bcf9c2.mp4\n",
      "{\"video_ocr\": \"开学孩子的英语课本 都拿到手了吧|注意看统编英语|每个单元都有自己的主题|教孩子利用思维导图|速记主题和单词的顺序|再通过音标和 发音规律 拼读单词|让背单词比玩游戏 更吸引孩子|学英语我只教一个方法|作业帮直播课英语单词 语法名师班|由中外名校毕业老师 带队教学|平均教龄10年以上|重点培养孩子的 学习习惯和方法|动画互动式课堂|让孩子不仅能轻松掌握|上干词汇|还有8大单词速记法|13个语法思维故事|帮孩子吃透|英语学习的核心规律|挑战英语高分|现在报名9元13节课|还送全套教辅礼包|孩子学的好家长更省心|赶紧点击视频 下方链接报名吧|单词语法名师课|最常见发|[人]最常见4|目拼域|音|英语|nicel|]当c|好的，亲切 八时|方法|13节|国内外名校毕业老师带队教学|dhmtOne|Unit One|B. watched TV|They|2. My mum watched TV last night.|教龄4年 王佳宝|丁宇|国专级|四年级|chool|My school|AFRICA|Yes. it is. I here|Presentation|宾夕法尼亚大学|名校海归|NORTH|EUR|S0nowman? Can I make a|Whot are those?|OCQ|名师带队|9元|at time is|Weather|ee|UnitTh|t the farr|Four|动将|catLkaet] 当c+e,i 卜物|I5]当c+e，|catLkaet|猫，|英语教学认证|教学诙谐幽默|自然拼读|拼读|教学经历|课 作业帮直播课|北京外国语大学|9元13f|尚小云|watch TV|Isit mniing?|Ie/You|刀的|猫科动物|清明快|But it isn't cold|Practice|罗迪迪\", \"video_asr\": \"开学孩子的英语课本都拿到手了吧，注意看统编英语每个单元都有自己的主题，教孩子利用思维导图速记主题和单词的顺序。|那通过音标和发音规律拼读单词，让背单词比玩游戏更吸引孩子学英语，我只教一个方法，作业帮直播课，英语单词语法名师班。|有中外名校毕业老师带队教学，平均教龄十年以上，重点培养孩子的学习习惯和方法，动画互动式课堂，让孩子不仅能轻松掌握上千词汇。|有八大单词速记法，十三个语法思维故事，帮孩子吃透英语学习的核心规律，挑战英语高分，现在报名九元十三节课还送全套教辅礼包！|孩子学的好，家长更省心，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.019762277603149414 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '填充', '单人口播', '静态', '平静', '教师(教授)', '拉近', '教辅材料', '喜悦', '室外', '动态', '配音', '室内', '全景', '多人情景剧', '拉远', '惊奇', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.94', '0.70', '0.59', '0.57', '0.35', '0.33', '0.25', '0.17', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c3fe11dff809fd91bd923c70d37eba1.mp4\n",
      "{\"video_ocr\": \"做数学计算题|你家孩子是不是|还在这样列竖式|这样计算不仅|正确率低还浪费时间|猿辅导|数学名师特训班|北大清华毕业名师|带队教学|总结了20个必备计算技巧|25个常考应用题巧解法|和50+经典习题精练|9课时只要29元|加1元还能再得|9课时语文阅读写作课|让孩子快乐学数学|妈妈更省心|现在报名还有 两套教辅材料|免费包邮送到家|赶紧点击视频下方详情|报名吧|27件原创教辅 礼盒全国包邮|小学款学|学习贴婚|猿辅导在线教膏 加1元可换购9课时语文课|豹学 特训班|语文|语文特训班|导|Sml|学特|数学特训现|UDAO|狼辅导amuw|UANFUDAO|全国 包邮|便签本|(3|t4\", \"video_asr\": \"做数学计算题，你家孩子是不是还在这样列竖式，这样计算不仅正确率低，还浪费时间？猿辅导数学名师特训班，北大清华毕业名师带队教学，总结了二十个必备计算技巧，二十五个常考应用题，调解法和五十家经典习题经练。|九课时只要二十九元，加一元还能再得九课时语文阅读写作课，让孩子快乐学数学，妈妈更省心！现在报名还有两套教辅材料，免费包邮送到家，赶紧点击视频下方详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.020874500274658203 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '平静', '静态', '单人口播', '室内', '极端特写', '配音', '动态', '手机电脑录屏', '教辅材料', '场景-其他', '拉近', '情景演绎', '多人情景剧', '手写解题', '家', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.82', '0.75', '0.43', '0.37', '0.33', '0.22', '0.08', '0.05', '0.03', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c4810b5e3f8b5c418255bd12b799b3d.mp4\n",
      "{\"video_ocr\": \"我姐妹又和她男朋友又出去玩了|而且他们住的好漂亮呀|哎呦|我带你出去玩|而且咱们住更好的|这次我们用小猪APP|好温馨呀|比外面酒店好太多了|小猪APP里有很多你喜欢的民宿|各种风格都有|那咱这套|得花多少钱呀|你放心|在小猪人均一晚89元起|而且咱这套还是|会员特价房|更优惠|V|带女朋友出来玩|就住小猪|赶快点击视频下方|下载小猪APP吧|O1の1011今110101K|601今1110KM|整套出租1室0厅、1张床宜住2人 1点评 会员尊享￥143|ONLINE AND SO ON|6279J|79、|醉.奢-静!江汉路步行街/万无超|◆1010101010|原价￥178|UIMHEAND800|酒店之外就住小猪|酒店之外就住 小猪|酒店赵外就住|云深处浪漫近地铁螃蟹岬江汉路复|中南路284双地铁，|整套出租2室1厅2张床宜住4人：2点评 速订长租优恋 会员M享￥231|小猪 XIAOZH U|XIA ◎ Z HU|XI-A-OZH6|XIaeZ HU|果票会员管价房源|O110K|900夺单专奇专0000|DECISION|¥ill 4G|O]011 1 I II1 11 I 11 I(IOIK|33|jOIOIOI I I IIOIOIOIO|4含|lhew I hold yon|l hua z hoia|I holedyoz hent.|I ho|Llhe i|006.今|AAINSCACTUS|TAIIIIAN|OU /101/1//01010111II I IIAIIIT|《KKKK《e》|见P0|Scandinavian|10|KKKKK\", \"video_asr\": \"我姐妹又和他男朋友出去玩了，而且他们住得好漂亮呀，哎呦，我带你出去玩，咱们住更好的，这次我们有小猪APP。|好温馨呀，你问你酒店啊，好，太多了，小猪APP里有很多你喜欢的民宿，各种风格都有，那咱这套得花多少钱呀？|你放心在，小猪人均一晚八十九元起，而且咱们这套还是会员特价房更优惠。|带女朋友出来玩就猪小猪，快点击视频下方下载小猪APP啊。|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.016465187072753906 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '喜悦', '惊奇', '家', '平静', '夫妻&恋人&相亲', '单人口播', '特写', '亲子', '动态', '朋友&同事(平级)', '配音', '拉近', '场景-其他', '全景', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.86', '0.81', '0.30', '0.21', '0.16', '0.14', '0.06', '0.05', '0.04', '0.04', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c4a8d907b13c653cae9affeeb396e2c.mp4\n",
      "{\"video_ocr\": \"模批考试怎么样|语数英物|各科成绩稳步提升|预备冲刺重点大学|你呢|我觉得二本都难啊|你怎么不报名|高途课堂全科名师班|我们家条件不好|9块钱你还扰豫|我从高一开始就跟着|清华北大毕业名师|学习高效的秒题技巧|我每次检查3遍都交卷了|你还没写完|不着急吗|那我现在报|还来得及么|来得及|9元16节直播课|好老师教好方法|能让你马上看到效果|点击视频下方|报名吧|新用户专享 立即体验 浙江卫视指定在线教育品牌|高途课堂 名师特训班|全国百佳教师带队教学 平均教龄11年|浙江卫视|整起|华少|￥9\", \"video_asr\": \"甚至模拟考试怎么样？|语数，英物各科成绩稳步提升，预备冲刺重点大学，你呢？|我觉得二本都难啊，你怎么不报名高途课堂全科名师班啊，我们家条件不好，九块钱你还犹豫？我从高一开始就跟着清华北大毕业名师学习高效的秒题技巧，我每次检查三遍都交卷了，你还没写完不着急吗？那我现在报名还来得及吗？来得及九元十六节直播课，好老师教好方法，能让你马上看到效果，点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016321897506713867 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '中景', '静态', '全景', '亲子', '室外', '喜悦', '特写', '朋友&同事(平级)', '悲伤', '动态', '平静', '学校', '家庭伦理', '极端特写', '配音', '单人口播', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.77', '0.61', '0.60', '0.39', '0.26', '0.22', '0.12', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c4ef8180155e1f4e5171a8f99351a0d.mp4\n",
      "{\"video_ocr\": \"为什么你|总比别人落后一步|十年前 让你去干互联网|你不去|结果别人发财了|让你去一线城市买房|面2×刁王|现在疯读极速版发福利|可22DT|告诉大家|看小说就能|免费领P40手机|结果现在别人都拿到了|免费的新手机|机会只给能抓住的人|如果你不想 再比别人落后|就抓住机会去尝试吧|就是这个疯读极速版|马文ォ，下载之后不除根，|不载免后山，接下|多需萝着50章小说|只需要看5@章小说|马就能领到十介个碎片|十个P40碎片|就能免费党换P40手机|不花一分钱包邮寄到家|不花一|货真价实的P40手机|很多人已经收到了|免费看小说|拿P40手机|赢手机 疯读极速版|陆逸打残就行了吧？” “必须除掉。”马大志口吻|就得斩草除根。” 马文才咽了咽口水，问道:|“你还好吧？疼不疼?” 看着李梦寒满脸关切的看着|“一定栗这么做吗？”|有些不安起来。|08:08|填写收货信息|疯读|着马大志，说道：“爸，要不把 马文才一惊，不可置信的望|这种人要么不动他，一旦动他， 道他不会成为大人物？所以说，|“陆逸你醒了？”|“王大雷。”李梦寒帮腔道|“对，就是他，王大雷。”|听到这话，马文才明白了。 就在此时，唐家，唐海燕也|恭喜您 获得P40手机一台|韦49卓 请个安宰贝母 “除掉陆逸！”|丈夫，你怎么就听不进去呢？别 看陆逸现在还是个小卒子，要是|江州医院，特护病房。|脸上有着憨厚的笑容。|“你什么也不需要做。”马|并 我的奖品|18:30|在给他几十年的时间成长，谁知|陆逸终于从昏迷中醒了过来|陆逸觉得青年有些眼熟，惊 咦道：“你不是那个叫什么来着|大志笑道：“这年头，有钱能使 鬼推磨，有钱什么都好使。”|¥ 3友|春风吹又生。如果只是打残陆逸 ，谁保证以后他不报复我们？” “他都残了，怎么报复我我|与其坐以待毙，还不如放手 一搏。何况，只是陆逸这么一个|我哥去查了，相信很快就会有结 果的。”李梦寒趴在陆逸的床边|听到李梦寒的话，陆逸问道 ： “他之前来过？”|海燕就觉得自己想多了，他很清 楚军刀的身手，在他看来，军刀|6天23|提交|来，他就会被林春秋赶出医院。|么狠。陆逸你别担心，我已经让|眉，问道：“你怎么又来了?|脑子里刚出现这个念头，唐|“可是——” “没有可是。”马大志打断|大志很清楚，如果不除掉陆逸， 我们父子俩很快就会完蛋。”马|“还说没事，你手流了那么|治好的。|我们文子呐很就会完蛋。|没有，电ロ也打不通，这让唐海 燕感觉有些不妙。|们？”马文才不以为意，在他看|小人物而已，他的生死又有谁会 关心？|安慰的说道。 陆逸笑了笑，没有说话。|“中午来过。”李梦寒说: “也不知道他从哪里知道了你住|来，只要陆逸残了，一切问题都|对付陆逸是绰绰有余。|“必须这么做。否则的话，|自己，陆逸笑了笑，说：“我没 事。”|陆逸想了起来，王大雷前不久送 他受伤的爷爷来急救，就是自己|坚决|5这公|4：天刀去除掉逸邦好 几个|自己|道：“我从小就告诉你，无毒不|提着手提袋从外面走了进来，他|怎么做？”|具体活动以实际活动为准|惝当不 县体活动以寒活动为准|可是，怎动際准|当不|ThuMar26|他就会失|北京市朝阴区**小区**单元**室|明日签到继续领碎片 签到提醒|明日签 04月08日星期三|收货人姓名 李晓*|看个视频可再领取1枚碎片哦!|看个视频可再领取1极|看个视频|手机号 135****568|x10|远行内存 80GB|2640x1200 Androl0程序安全补丁级制|内核版本 ToeMay122323:13CST2020|... ...”|10/10|剩余67份|iPhone11碎片|Phone|银3日星影E|青兔|三国球|青国游珠|青兔志芳|甘国雄我|440|每认真阅读五章得1枚，最高可得20+碎片 距结束|跟答束0|陆逸，一个身怀医术的超级高|继续阅读|都市|开宝箱得碎片 3枚碎片|碎片明细>|t解注了|对的自|绝品神医|一颗善良的心|87份|手，为寻找未婚妻，来到繁...|4”4|MEID|看小说|新人福利|0/3|法律信息|面>女工|一键领取|1天|关于手机|x2|领取|详细地址\", \"video_asr\": \"为什么你总比别人落后？十年前让你去干互联网，你出去，结果别人发财。|十年前让你去一线城市买淘米购买，结果别人成土豪了，现在疯读极速版，发福利告诉大家，看小说就能免费领P四零手机，你不去试了，结果现在别人都拿到了免费的。|手机机会只给能抓住的人，如果你不想再给别人落后，就抓住机会，不去尝试吧，就是这个疯读极速版，下载之后只需要看了五十章小说，就能领到十个碎片，十个P四零碎片就能免费兑换P四零手机，一分钱包邮寄到家，货真价实的P四零手机很多人已经收到了。|点击下方链接下载疯读极速版，免费领包！\"}\n",
      "multi-modal tagging model forward cost time: 0.016343116760253906 sec\n",
      "{'result': [{'labels': ['推广页', '手机电脑录屏', '填充', '现代', '场景-其他', '极端特写', '商品展示', '中景', '配音', '室内', '平静', '静态', '全景', '愤怒', '情景演绎', '动态', '喜悦', '路人', '特写', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.18', '0.15', '0.15', '0.09', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c5102b1f3d204f4bc92b37aef6b3b82.mp4\n",
      "{\"video_ocr\": \"看我今天又领了200元哦|领钱了领钱了|自从开始在拼多多上领现金|我就有忡天上掉金币的感觉|击下方链接|了开拼多多|天天领现金|京 可以领取现金红包了|过年也不打烊哦|打开红包领现金|分享给好友，再领一次现金|恭喜你获得一个现金红包|¥96.77|天天领现金打款秒到账|拼多多\", \"video_asr\": \"这是前提下，自从开始的地方方正的现金，我就有种非常的感觉。|多多首页点击天天领现金就可以领取现金红包，天天领扫年也不大赢哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.019593477249145508 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '家', '中景', '静态', '配音', '手机电脑录屏', '多人情景剧', '平静', '朋友&同事(平级)', '喜悦', '特写', '场景-其他', '情景演绎', '悲伤', '动态', '单人口播', '惊奇', '极端特写', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c579747b6c51944daef3e23dc7e5425.mp4\n",
      "{\"video_ocr\": \"怎么两个人还|这么费纸啊|不费|买纸才一块钱|ROMANTIc 一块钱|呐|12包的原木抽纸|在超市买得二三十呢|在拼多多上|一块钱就买了|真的|新用户打开拼多多|新人专享 这么多好物|下载拼多多|统统一块钱|快去抢购吧|手撕面包|谷物主义早餐手撕面 铁艺阳台客厅花盆架|桌面手机支架可折叠|茶叶绿茶2019新茶|抽纸盒纸巾盒收纳盒 皮带男自动扣休闲|内衣收纳袋布艺挂袋|1元抢购|现竹系列本色抽纸 6包璞竹本色竹浆抽|收藏店铺优先发货 8包植护原木抽纸实|男士皮带商务自动扣|看电影追剧|迎丰焦糖核桃2F|龟牌汽车硬壳玻璃|新用户一元抢购 海量好物带回家|5.5元抢购 LED带镜子护眼台灯|加厚款5卷装量贩|全自动小清新三折4|拼多多新人专享 一元火爆抢购|MANTIP|原六 迹品|拼多多|梦！|千0\", \"video_asr\": \"怎么两个人还这么废纸。|不费我买给他一块钱。|那十二包的原木抽纸在超市买的二三十呢，在拼多多上一块钱就买了。新用户打开拼多多这么多号通。|给你一块钱，快去抢购吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.021698951721191406 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '手机电脑录屏', '场景-其他', '平静', '多人情景剧', '单人口播', '特写', '室内', '配音', '朋友&同事(平级)', '填充', '惊奇', '拉近', '喜悦', '动态', '愤怒', '单人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.93', '0.57', '0.53', '0.21', '0.20', '0.14', '0.06', '0.05', '0.03', '0.02', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c588576c9d9674b1aff441f41de5d9f.mp4\n",
      "{\"video_ocr\": \"哥我不想上学了|上学不得花很多钱吗|我已经用360借条解决了|最高可以借20万|最快5分钟就放款了|如果你也急用钱|点击这里|就可以申请你的额度了|借4万最长免息30天|中国·美适|全民免具狂欢 最长30天免息|N|贷款额度放款时间等以实际审批为准|￥ ヨ60借条|360借条|最高20万额度|首期免息优惠券|立即申请|免息\", \"video_asr\": \"哥，我不想上学了，上学不得花很多钱吗？我已经用三六零借条解决了，最高可以借二十万，最快五分钟就放款了。如果你也急用钱，点击这里就可以申请你的额度了。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.015990257263183594 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '路人', '静态', '喜悦', '惊奇', '手机电脑录屏', '特写', '极端特写', '动态', '平静', '室外', '全景', '愤怒', '朋友&同事(平级)', '单人口播', '(马路边的)人行道', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.87', '0.74', '0.74', '0.37', '0.27', '0.06', '0.05', '0.04', '0.03', '0.03', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c6646e23c7799d604a1315801f1ebff.mp4\n",
      "{\"video_ocr\": \"范建明 你什么意思|我早就受够你了|你看人家|温柔懂事又贤惠|表嫂|叛徒|不就差10万块钱吗|我是在恒易贷上借的|最高可借20万|最快2小时放款|点击视频下方链接|1|200.000|恒易贷\", \"video_asr\": \"范建明，你什么意思？我早就受够你了，你看人家温柔懂事又贤惠，娘子我去不就差十万块钱吗？总是在恒易贷上借的，最高可借二十万，最快两小时放款，点击视频下方链接可以申请你的额度啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.015983104705810547 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '动态', '室外', '特写', '喜悦', '静态', '惊奇', '路人', '平静', '极端特写', '朋友&同事(平级)', '全景', '单人口播', '愤怒', '悲伤', '夫妻&恋人&相亲', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.94', '0.93', '0.88', '0.87', '0.67', '0.50', '0.12', '0.05', '0.03', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c71334b1c30883e26a9e121c1acc4d8.mp4\n",
      "{\"video_ocr\": \"这样的英语课你的孩子上过吗|课程通过趣味动画|和互动游戏的形式展开|培养孩子听说读写的能力|还有A智能发音|让孩子在家就能上到|北美外教动画课堂|斑马A课英语体验课|49元10节课|斑马课后还有985老师1对1辅导答疑|从而培养孩子的英语思维|现在报名|还包邮赠送全套教辅礼盒|快点击视频报名吧!|猿辅导在线教育 出品|猿辅|猿辅导在维有出|猿辅导在线出品6|ARP 8RTHDRY|JCHORY|R UHOAy|RARPY|BIiRTHDRy|BiRtHDAY|英语|2-8岁上斑马 学思维 学英语|蛋糕 cake|面包 bread|牛奶 milk|汉堡|斑马Al课|Han burq|Hamb urg|RUH0|Ar H0|JHA|MILk|>>|APP1|CAMEL|iIk|S3|S3(微2)|(原L2)|UOW\", \"video_asr\": \"蛋糕蛋糕CAKE。|KKK面包面包。|BABY。|牛奶，牛奶，牛奶。|MILK MILK MILK汉堡汉堡HAMBURGER。|HI BURGER。|这样的英语课你的孩子上过吗？课程通过趣味动画和互动游戏的形式展开，培养孩子听说读写的能力，还有AI智能发音，让孩子在家就能上到北美外教动画课堂，斑马AI课，英语体验课四十九元十节课，课后还有九八五老师一对一辅导答疑。|从而培养孩子的英语思维，现在报名还包邮赠送全套教辅礼盒，快点击视频报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016431093215942383 sec\n",
      "{'result': [{'labels': ['推广页', '配音', '现代', '室内', '动画', '平静', '影棚幕布', '中景', '情景演绎', '全景', '场景-其他', '课件展示', '静态', '特写', '动态', '极端特写', '喜悦', '商品展示', '教辅材料', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c724ffd199f9fe5b3a6d00bd5f6578f.mp4\n",
      "{\"video_ocr\": \"你这次考的怎么样|你|怎么成绩还是这个 样子啊|我不是让你跟我 一起报了|高途课堂 全科名师班了吗|那9块钱的课|能有效吗|你怎么做到的|跟着北大清华毕业 的老师|梳理了一遍知识点|不光知道了哪些是|必考点|重难点|易错点|还学到了|秒杀解题的技巧|和技巧背后的应用原理|现在做题|举一反三|又快又准|这么厉害|具体都教什么呀|有数学|503个必考知识点|语文37个阅读理解 答题模板|高分作文万能句|英语单词5种记忆法|阅读完形大招|物理67个出题套路|加秒题技巧|都是干货呀|我也要学|快告诉我怎么报名|点击视频下方链接|就能报名|仅需|那，共70分.解答应写出必要的文字说明、证明过程或演算步骤)|，将其困象身右平移以(p>0）个单位长度后得封通数g(+)的图象，着 道共6小照，共7分.料等应写出必要的文字说明、证明|在数列la.1中， (…*年)的值为|11.已知2in2 12.在数列l，1中，|，-3)，若如-2b与d垂直.则实数人二 的最小值为＿ ，将其园象鸟右平移p(p>0)个单位长度后得到 数g(n)的图象.若面数|1向量as(YD.b…(2.-33，若ha-2b与a 垂直，则实数k… 10分)|C.-3或3 B.20|的值为|more lke a society. where enables students lo leam to get along wel wilth each other.thus|全国百佳教师带队教学 平均教龄11年|新用户专享立即体验 浙江卫视指定在线教育品牌|P.a.b，c分别为角A.8.C的动边.若6…m，|华少|名师特训班|品活 第]卷(非选择题 共9分) 大银头4小题，每小题5分，共20分，把答案填在题中的横线上)|有空题1本大是共+小睡，每小题5分，共20分，把答案填在题中的横线上)|浙江卫视|文明守|Y-C)|变命|¥9\", \"video_asr\": \"你这次考得怎么样？你怎么成绩还是这个样子啊？我不知道你跟我一起报了高途课堂全科名师班了吗？那九块钱的课能有效吗？你怎么做到的？高途课堂全科名师班啊？跟着北大清华毕业的老师梳理了一遍知识点，不光知道了哪些是必考点重。|点和易错点还学到了秒杀解题的技巧和技巧背后的应用眼里，现在做题举一反三，又快又准。|这么厉害，具体都教什么呀？有数学五百零三个必考知识点，语文三十七个阅读理解答题模板，高分作文万能句，英语单词五种记忆法。|阅读完形大招，物理六十七个解题套路甲，秒题技巧都是干货呀，我也要学，快告诉我怎么报名啊，点击视频下方链接就能报名！\"}\n",
      "multi-modal tagging model forward cost time: 0.016604900360107422 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '多人情景剧', '推广页', '静态', '平静', '极端特写', '室内', '手写解题', '教辅材料', '亲子', '特写', '家庭伦理', '家', '朋友&同事(平级)', '单人口播', '惊奇', '喜悦', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.90', '0.29', '0.14', '0.13', '0.12', '0.10', '0.10', '0.07', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c7298ff457b65779b0f78b343cfb1e8.mp4\n",
      "{\"video_ocr\": \"你看你作业这写的|口算题都你能算错|怎么了 姐夫|他学习不好|那是因为没有用对方法|那你有什么好的方法|用猿辅导呀|在线就能学|那有效果吗|当然啦|都是清华北大毕业的老师|在线辅导|爸爸 你赶紧给我报名吧|36元/节 高质量直播课 小学数学秋季系统班|现在报名赠送 配套教辅书精美文具套装 立即报名|猿辅导\", \"video_asr\": \"你看你的作业写的口算题你都能算错，怎么了？姐夫他学习不好，那是没有用对方法，那你有什么好的方法用猿辅导呀，在线就能学，那有效果吗？当然了，都是清华北大毕业的老师在线辅导，你赶紧给我报名吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016340970993041992 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '家', '推广页', '亲子', '静态', '平静', '家庭伦理', '喜悦', '单人口播', '特写', '惊奇', '动态', '拉近', '手机电脑录屏', '夫妻&恋人&相亲', '愤怒', '全景', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.85', '0.67', '0.60', '0.43', '0.30', '0.06', '0.05', '0.05', '0.04', '0.03', '0.03', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c740a82259e22bd3dbd6236e7da154b.mp4\n",
      "{\"video_ocr\": \"这次家长会啊|我想点名表扬一下晓晓|晓晓上课发言|积极踊跃|课下不懂就问|学习态度特别好|珍珍妈妈|晚上带着珍珍|来我们家一起吃饭啊|啊好啊|我倒是要看看|你们家晓晓|有什么学习秘方|诶|晓晓|猿辅导要开课了|赶紧去上课呀|太好啦|又可以上猿辅导啦|猿辅导|珍珍|跟晓晓一起|赶紧去看看|看来这个猿辅导|真不错|孩子还真学进去了|还盯着孩子学习呢|不盯着|孩子自己也不|好好学呀|给你们家珍珍也报名|一个猿辅导啊|正好它现在推出了|猿辅导小学数学|秋季系统班|36元一节课|让孩子养成|8大基础能力|掌握多种解题思路|孩子自主学习|更积极|更主动|而且现在报名啊|还送120元|优惠券呢|这么好|我得赶紧回家给|孩子报名这个|别急呀|点击下方按钮|就可以报名啦|将计就计:O展开，化简日对应项系数相等 则a+b的值是多少？ 杨字E|负型剑链1利用整式集法确定字母取值|种桃树，|净社学法生告习领(书夹卷）小等玉年级(下)B|2へ十0~20|20 2么十0X力|27x+0ぺ+，|龙值量:1|记字E~1|36元/节 高质量直播课 小学数学秋季系统班|现在报名赠送: 配套教辅书、精美文具套装|计算100题 配套练习册 小学数|二下+(20)ベ|9m1|9@:1|立即报名\", \"video_asr\": \"这次家长会啊，我想点名表扬一下小小小小上课发言积极踊跃，课下不能做为学习态度特别好。|今天妈妈说今天来我家吃饭啊啊好啊，我倒是要看看你们家小姐有什么学习秘方哎，小小部分就开课了。|赶紧去上课呀，太好了，又给你上原辅导了，猿辅导哎，真真跟小小一起赶紧去看看好。|看来这个猿辅导真不错，孩子还真学进去了吗？还盯着孩子学习呢，不盯着孩子自己也不好好学呀，哎，给你家珍珍也报名一个猿辅导啊！正好的现在推出了猿辅导小学数学秋季系统班，三十六元一节课，让孩子养成八大基础能力，掌握多种解题思路，孩子自主学习更积极，更主动。|而且现在报名啊，还送一百二十元优惠券呢，这么好，我得赶紧回家给孩子报名这个猿辅导小学数学秋季系统班，别急呀，点击下方按钮就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01647806167602539 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '推广页', '中景', '静态', '亲子', '平静', '家', '特写', '家庭伦理', '单人口播', '极端特写', '室内', '喜悦', '夫妻&恋人&相亲', '悲伤', '教辅材料', '朋友&同事(平级)', '愤怒', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.94', '0.75', '0.60', '0.34', '0.30', '0.21', '0.18', '0.05', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c7a24c79be375f78ad275613a8eccd5.mp4\n",
      "{\"video_ocr\": \"妈妈给我报名的|猿辅导数学名师特训班|和英语特训班的大礼包|终于到啦|我们大家一起来看看|Yom 里面有什么吧|1|20|签本|悄悄告诉你|这么多东西|统统都是妈妈给我报名|级荸|猿辅导免费送的哦|29元9课时的|+1元还能再获得|9课时的英语特训班|相当于30块钱 学特|上了18课时|好划算|而且有清华北大 猿辅导|毕业的老师带队教学|老师上课2分钟|跟我互动一次|课后还有班主任老师|1对1辅导|你一定很喜欢|现在点击屏幕下方|查看详情|就可以报名啦|千万不要错过哦|孩子的每一分进步 都值得鼓励与记录|上铆指南|100天进步计划表|学习笔记|错题本|Xvd|Ri|X9|NOTEBOOK. 狼辅导 最强l|回狼辅导11强|猿辅导在线救育|第7讲 实践应用综合复习(三) 暂无内容|YUANFUDA|小学秋学特训班|基于500万小学生搜睡数据|map|STUDY NOTES|Stickors|5分钟 算题卡|英语|影小学数学5IA+班通用|影小学数学|影小|小学|常用公式定理持|理拄|第3讲 图形认知终|心综|长方形的降铁:（长一宽）2 圆维的体积|100-DAY PLANNER|jam|创新意识|学句贴纸|贴纸|旦木|英语特训班|心德辅导|ship|第7讲圈(二)|覆盖90%重难点|浪季 热搜题topl|5R|Q 9课时名师直播课堂|最强大|号 1|Unit 2 jump|几传问盾|g!|第2讲 平行四边形的 第3讲 梯形的面积|狼城导|长方体的 长货|+1元可换购9课时英语课|讲 比的应用(二)|讲 利润和折扣|讲 浓魔问题|第6讲四边形里的|图环的面积|边长+4|最辅导1同强|回猿铺导|口痕输导|第4讲 正与反 第3讲 放大与练小|119N3d|Ce4a 正方形的周长 正方形的套积：边长·边长|猿辅导|最的大精|SSU8|第5讲 一样高的三|YUANE|第1讲 神奇的铅笔|讲 单位 “1”的转化|wn|捕导|最强*|图形|02|痕铺导|第6讲 实践应用综合复|3讲 分敢应用题（|FUDAO|DAO|活动特惠|一 猿辅导х最强大脑 一|讲 百分数|第8讲 圆的综合|年级|单词财|单 河见|长方形的套积|calia \\\"by|同强|INTROOUCTION|小学秋季|实践应用|错颗ホ|德辅|29元|讲比|Er\", \"video_asr\": \"妈妈给我报名的猿辅导，数学名师。|荷叶也是你们的大礼包终于到了！|大家一起来看看你们有什么吧，一二三四。|五六七八九十十一十二十三十四十五十六十七十九，二十二十一十二。|悄悄告诉你，这么多东西可都是妈妈给我报名猿辅导免费送的呦，二十九元九课时的数学名师特训班。|加一元还能再获得九课时的英语特训们，相当于三十块钱上了十八课时，好划算，而且有清华北大毕业的老师带队教学，老师上课两分钟。|跟我互动一次，课后还有班主任老师一对一辅导，你一定很喜欢，现在点击屏幕下方查看详情就可以报名了，千万不要错过！|珍珠吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.015996456146240234 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '教辅材料', '单人口播', '家', '特写', '亲子', '平静', '配音', '场景-其他', '静态', '家庭伦理', '室内', '喜悦', '极端特写', '多人情景剧', '动态', '拉近', '商品展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.81', '0.80', '0.75', '0.73', '0.65', '0.62', '0.61', '0.56', '0.32', '0.15', '0.13', '0.10', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c7b6ac07279c8061813dbcec207b0b7.mp4\n",
      "{\"video_ocr\": \"高考改革之后|热门专业普遍要求必修物理|美文明|物理学与人类文明|高途课堂|物理\", \"video_asr\": \"高考改革之后，热门专业普遍要求必修物理。\"}\n",
      "multi-modal tagging model forward cost time: 0.01626420021057129 sec\n",
      "{'result': [{'labels': ['现代', '填充', '场景-其他', '推广页', '手机电脑录屏', '配音', '静态', '平静', '中景', '室内', '单人口播', '特写', '喜悦', '商品展示', '手写解题', '教师(教授)', '幻灯片轮播', '图文快闪', '动画', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.17', '0.15', '0.10', '0.02', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c8b198c8b120a8f2e089c84ca69eef9.mp4\n",
      "{\"video_ocr\": \"孩子这么写下去|哎 那也没办法呀|他不学|差距越来越大|能不能考上大学|成绩又上不去|这么死记硬背|咱那会上高中的时候|都是死记硬背啊|可我听说她同学|都在高途课堂上学习|这进步呀可快了|高途课堂?|那都是平均教龄11年|的名师线上授课|教给学生高中数学|167个常考点|语文三大作文高分 黄金法则|英语阅读理解 十字口诀|物理十二通解模型|哪有这么好的课啊|就算有那也一定很贵|不贵|9元就能买|语数英物16节课呢|这么便宜啊|从哪报名啊|点击视频下方链接|就能报名|快给孩子|儒|也不是个事啊|别的孩子都在学|都难说|也不行啊|不也一样么|一个逆袭的机会吧|新用户专享立即体验 浙江卫视指定在线教育品牌|名师特训班|全国百佳教师带队教学 平均教龄11年|浙江卫视|￥9\", \"video_asr\": \"孩子这么学下去也不是个事啊。|那也没办法呀，他不学别的孩子都在学，差距越来越大，能不能考上大学都难说，成绩又上不去，这么死记硬背也不行啊，咱们微博上高中的时候不也一样吗？都是死记硬背啊，可我听说她同学都在高途课堂上学习着，进步可快了，高途课堂那都是平均教龄十一年的名师线上授课。|教给学生高中数学一百六十七个常考点，语文三大作文高分黄金法则，英语阅读理解十字口诀，物理十二图解模型。|哪有这么好的课，就算有卖也一定很贵，不贵，九元就能买，语数英物十六节课呢，这么便宜啊，从哪报名啊？|点击视频下方链接就能报名，快给孩子一个逆袭的机会吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01607823371887207 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '填充', '推广页', '家', '家庭伦理', '静态', '平静', '亲子', '夫妻&恋人&相亲', '特写', '喜悦', '动态', '愤怒', '惊奇', '悲伤', '极端特写', '全景', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.91', '0.65', '0.09', '0.05', '0.03', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c8b8db9a1b03a259652595c58b6b578.mp4\n",
      "{\"video_ocr\": \"家长们注意啦|3-8岁是孩子大脑发育的高峰期|这个时期学习绘画|可以极大程度的开发孩子的智力|千万不能错过|小熊美术AI课|是专为3-8岁孩子开设的|系统性美术专业课程|全新AI互动模式|加真人实景互动教学|加专属老师1对1辅导|为孩子的美术学习保驾护航|现在报名10节课仅需49元|就能给孩子一个全新的未来|还包邮赠送绘画礼盒|包含每节课的配套绘画工具|真的是太值了!!!|赶紧点击下方链接给孩子报名吧|小熊美术|熊美米|小能ィ|小熊美术|美术宝 49元10节|在家学习 适合3-8岁 1对1辅导|立即报名 随材礼盒为课程配套物品 不同级别的礼盒略有差异|儿童绘画启蒙课|BEARARTTE|出品|包邮\", \"video_asr\": \"家长们注意了，三到八岁是孩子大脑发育的高峰期，这个时期学习绘画可以极大程度地开发孩子的智力。|千万不能错过，小熊美术AI课是专为三到八岁孩子开设的系统性美术专业课程，全新AI互动模式下，真人实景互动教学家专属老师一对一辅导，为孩子的美术学习保驾护航，现在报名十节课仅需四十九元，就能给孩子一个全新的未来。|包邮赠送绘画礼盒，包含每节课的配套绘画工具，真的是太值了，赶紧点击下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01627206802368164 sec\n",
      "{'result': [{'labels': ['推广页', '配音', '现代', '填充', '平静', '场景-其他', '极端特写', '静态', '课件展示', '绘画展示', '室内', '教辅材料', '知识讲解', '手机电脑录屏', '转场', '宫格', '全景', '动画', '室外', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.49', '0.37', '0.20', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0c99ffbfcf1bfff13ac5a8538de46b8e.mp4\n",
      "{\"video_ocr\": \"这是全新一代洗车神器|它|只要9.9|9.9|最快3秒出水|告别漫长等待|3种模式|全方位洗车|现在下载拼多多|进入首页|立即领取 输入|点击 查看更多|就能领到这台9.9的洗车神器|喜欢爱车的你还在等什么??? 购买同款|幸福小镇 拼多多|dfghik|100元|03水龙头/自吸两用|完全不怕老 全民 幸福日|20亿红包|1212份大奖许你吃穿不愁 12.12|无门槛券|时AMZMA|*限时活动抢购价9.9元|搜索|限时秒杀 更多预告|办秒杀万人团|即将开抢|2月21|智能|TAURUS|CNC|超压|高力\", \"video_asr\": \"嗯。|现在。|你。|你。|这是全新一代洗车神器，他只要九块九，九块九啊，最快三秒补水告别们！|它三种模式全方位汽车，现在下载拼多多，进入首页，输入立即领取，点击查看更多就能领到一台九块九的洗车神器，喜欢爱车，对吗？在吗？\"}\n",
      "multi-modal tagging model forward cost time: 0.016408443450927734 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '静态', '多人情景剧', '全景', '动态', '室外', '特写', '惊奇', '混剪', '手机电脑录屏', '喜悦', '远景', '配音', '拉近', '平静', '拉远', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.91', '0.66', '0.54', '0.53', '0.22', '0.19', '0.17', '0.12', '0.11', '0.09', '0.05', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ca02b41f44db7f65c8102d04ad4f281.mp4\n",
      "{\"video_ocr\": \"一朵黄色的圆盘状野花|诚 在微风中摇曳|在柔和的微风中|乐滋滋地摇曳|夏天的午后|散发出一种淡淡的清香|其实写作文有很多的套路和方法|由北大中文系毕业团队倾力打造的|学而思网校秋季语文特训班|小初10课时直播课|专门针对孩子阅读和写作等难题|教会孩子有效的阅读写作方法|现在报名 只需9元|超值教辅礼盒免费包邮赠送|现在赶快点击视频下方链接报名吧|LOFSiMAICITY|L0 S/MICIT|UIC|HIESIGNSHOW|NUSIN:SNOW|WHEA MU|全国包邮 官方价399元|珍贵教辅|冲刺专项练 学而思网校 乍突破班|专攻重难点，阅读写作高效提分|秋季语文提分特训班 达吾力江|立即报名 元/10课时|一每天进步一点点|野花|数量|一朵野花|颜色 形状|方位|形容词|时间 嗅觉|北京大学中文系|DACI|HISK|ule|uICH|小初高\", \"video_asr\": \"野花加数量，一朵野花加颜色和形状，一朵黄色的圆盘状野花加方位。|一朵黄色的圆盘状的野花在微风中摇曳加形容词，一朵黄色的圆盘状野花在柔和的微风中乐滋滋地摇曳。|加时间和臭脚，夏天的午后，一朵黄色的圆盘状的野花在柔和的微风中乐滋滋的。|叶散发出一种淡淡的清香，提升写作文有很多的套路和方法，和北大中文系毕业团队倾力打造的学而思网校经济的特训班。|小初十课是什么课？专门针对孩子阅读和写作的难题，教会孩子有效的阅读写作方法。现在报名仅需九元，超值教辅礼盒免费包邮赠送！|现在赶快点击视频下方链接报名！\"}\n",
      "multi-modal tagging model forward cost time: 0.016386032104492188 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '平静', '推广页', '静态', '填充', '宫格', '室内', '教师(教授)', '多人口播', '混剪', '场景-其他', '配音', '教辅材料', '特写', '极端特写', '拉远', '办公室', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.45', '0.09', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cb243b2809941baa482d0856496ca34.mp4\n",
      "{\"video_ocr\": \"正在工作的你|是否因为工资低|而发愁|放下手机|利用空余时间|来学习播音主持吧|播音主持|包含有声读物|影视配音等|那么像这些行业|社会需求量巨 大|含金量超高|我们把兴趣|変成职业|现在加入我们|O元试学|点击下方链接|赶紧报名吧\", \"video_asr\": \"正在工作的你，是否因为工资低而发愁？放下手机，利用空余时间来学习播音主持吧！|播音，主持包含有声读物，影视，配音等，那么像这些行业呢，社会需求量巨大，含金量超高，我们把兴趣变成职业。|现在加入我们零元试学，点击下方链接，赶紧报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01618504524230957 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '中景', '静态', '平静', '推广页', '室内', '拉远', '特写', '影棚幕布', '填充', '拉近', '动态', '办公室', '知识讲解', '场景-其他', '教师(教授)', '混剪', '家', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.10', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cb87907cf0c21e7d2cf69645195984f.mp4\n",
      "{\"video_ocr\": \"鼠牛虎|牛 虎|虎|斑马A课语文体验课|兔龙蛇|49元/10节课|北大中文系硕博精心打磨课程|鸡狗 猪|老 鼠|鸡狗|兔子|狗猪|立即抢报|原创动画教学 包邮赠送教辅大礼包|斑马A课|nid|京剧脸谱|马羊 猴|羊猴|马羊|二生肖的|猿辅导|非赠品|语文\", \"video_asr\": \"我。|老师。|加油。|老虎虎虎。|我。\"}\n",
      "multi-modal tagging model forward cost time: 0.016172409057617188 sec\n",
      "{'result': [{'labels': ['场景-其他', '现代', '静态', '推广页', '极端特写', '教辅材料', '配音', '课件展示', '填充', '宫格', '中景', '转场', '手写解题', '动画', '动态', '拉近', '办公室', '商品展示', '平静', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cbba9f999e8665b4a035ba0016dd350.mp4\n",
      "{\"video_ocr\": \"姐夫|我妈给乐乐买的图书和算术练习册|多少钱|359|把嘴给我闭上|我说个数|49元10节体验课|再送个超值教具礼盒|没错|斑马A课思维动画课|现在报名49元10节体验课|免费赠送超值教具礼盒|包邮到家|还额外赠送30节国庆TV课|语文思维英语各10节|三科一起学|课程通过儿歌、动画、游戏等|孩子感趣的教学方式|教孩子数量、结构、 空间|这些基本概念|掌握基本的思维逻辑|每节课15分钟|短时高频教学|让孩子边玩边学|提升孩子的学习兴趣|你还在等什么|赶紧点击视频下方查看详情|给你家孩子报名吧|猿辅导在线教育出品|在线教育 出品|猿号|袁辅导看出品|袁输导|袁辅寻d出a|袁辅享|出品|在线教育|斑马A课|恩维 S|2-8岁上斑马学思维 学英语|疯狂报名|K-思维|10|4一|10-4=6|A2|H态 2|CONYSRS\", \"video_asr\": \"姐夫，我妈给乐乐买的图书和算数另一侧啊，多少钱三百五十九，多少三百五十九八字给我闭上。|我说个数四十九元十节体验课。|再送个超值教具礼盒！|没错，斑马AI课思维动画课现在报名四十九元十节体验课，免费赠送超值教具礼盒，包邮到家，还额外赠送三十节国庆TV课，语文思维，英语个十节，三科一起学课程通过儿歌，动画，游戏等孩子感兴趣的教学方式，教孩子数量，结构，空间这些基本概念掌握基本的罗。|既思惟每节课十五分钟短时高频教学，让孩子边玩边学，提升孩子的学习兴趣，你还在等什么，赶紧点击视频下方查看详情给你家孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016307830810546875 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '静态', '平静', '单人口播', '室内', '家庭伦理', '家', '多人情景剧', '特写', '教辅材料', '场景-其他', '课件展示', '极端特写', '喜悦', '配音', '亲子', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.47', '0.25', '0.19', '0.13', '0.11', '0.10', '0.08', '0.02', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cc8598b506c59a4fb25234cfdbbad6f.mp4\n",
      "{\"video_ocr\": \"新用户专享|努力的人那么多|但为什么满分的人却那么少!|差的就是方法!|我是高途课堂周帅老师|省级高考状元|高考数学满分|在15年教学生涯中|我总结了|439个知识点|167个考点|80个易错点|57个难点失分点|查看详情]就能报名!|妈不知道从哪|搞了套|数学卷子让我做|一会儿~ 还会检查|所以你就真的 解决了它|早知道我就不是 学习的料|做多少遍|付出多少努力|周帅|12年高考教学经验|高中数学资深主诽老师|高中数学黄深き|北京大学学士|¥9|夕r|浙江卫视指定在线教育品牌|新用户专享 立即体验|高途课堂名师特训班|浙江卫视|高途课堂｜浙江卫视|高途课堂|嘘|立即体验|全国百佳教师带队教学 平均教龄11年|仅需|别让妈听见了|名师特训班|9块钱|华少\", \"video_asr\": \"快手上厕所吗？|你在？|别让你妈听见了吗？不知道从哪搞套数学卷子让我坐一会还要检查。|所以你就真的解决了，它，早知道我就不是学习的料，做多少蝙蝠是多少，努力都是没用的，努力的人那么多，但为什么满分的人却那么少？|差的就是方法，我是高途课堂周帅老师，省级高考状元，高考数学满分，在十五年教学生涯中，我总结了四百三十九个知识点，一百六十七个考点。|十个易错点以及五十七个难点失分点，查看详情就能报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.01622939109802246 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '单人口播', '静态', '平静', '室内', '教师(教授)', '特写', '多人情景剧', '场景-其他', '极端特写', '亲子', '家', '全景', '愤怒', '惊奇', '家庭伦理', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.88', '0.49', '0.18', '0.09', '0.06', '0.04', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cc9e4d907165b5fc0605204399b0229.mp4\n",
      "{\"video_ocr\": \"gand geni 爱们绝尘|女生们先生们好戏登场|女生们先生们|1t'd|樣下来|大麟照|dlies and genilem.|uiliesand gentem|nnies andgetiemn|好戏登场|Show time|变声技巧|上音大师|跟随声控大咖学习配音|潭州教|TANZHOUEDU.COM|二#9IV音戳 每晚19:30 播音VIP体验课|Show time Shou time|果|潭州菜育|声变\", \"video_asr\": \"美丽是认真的。|我站。|别打别打。\"}\n",
      "multi-modal tagging model forward cost time: 0.01695561408996582 sec\n",
      "{'result': [{'labels': ['推广页', '场景-其他', '现代', '填充', '配音', '平静', '动画', '幻灯片轮播', '宫格', '过渡页', '手写解题', '静态', '特写', '课件展示', '影棚幕布', '中景', '知识讲解', '重点圈画', '图文快闪', '转场'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.99', '0.22', '0.10', '0.06', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ce2699107708240beaedfac5bf00c73.mp4\n",
      "{\"video_ocr\": \"0、0|Q、O|Q-0|今天你还是照例擦拭霍少爷的身体|闭嘴|你是谁|你昏迷了三年|为了给你冲喜让你醒来|我是你名媒正娶的妻子|孩子的病已经入了苏城医院的档案|O0|就等医院找到合适的骨髓后移植|谢谢医生|0-O|你胆子可真大|夏心妍|真是没想到|你这样的女人|也敢嫁来我们霍家|我妈妈是好人|别走|别走好吗|妍妍|我好想你|三年前的那一夜|这一辈子|我都会爱你的|会不会就是你|(本故事纯属虚构)|七猫免费小说APP|原来就是你|抢走了我的妈妈|《报告爹地妈咪要逃婚》|七猫免费小说|免费看书100年|1又|0.o|O.O|民医\", \"video_asr\": \"原来就是你尝尝我的妈妈，今天你还是照例擦拭和少爷的身体。|比赛。|你是谁你？你昏迷了三年，为了给你冲喜让你醒来，我是你明媒正娶的妻子。|孩子的病已经入境了苏城医院的档案，就等医院找到合适的骨髓后移植，谢谢医生。|你胆子可真大，下心眼真是没想到你这样的女人，你敢下来我获奖。|妈妈是好人。|别走别走好吗？爷爷，我好想你啊，三年前的那一。|爷爷，这一辈子我都会爱你的，三年前的那个男人。|会就是你。|你说有。\"}\n",
      "multi-modal tagging model forward cost time: 0.01612401008605957 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '现代', '静态', '多人情景剧', '特写', '愤怒', '悲伤', '情景演绎', '家', '室内', '平静', '夫妻&恋人&相亲', '动态', '配音', '喜悦', '极端特写', '家庭伦理', '惊奇', '汽车内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.93', '0.90', '0.73', '0.46', '0.41', '0.37', '0.31', '0.27', '0.12', '0.09', '0.04', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ce445b73f5d8b7b20197a512c52236b.mp4\n",
      "{\"video_ocr\": \"睡前擦香香|会变更漂亮|好哇|我就说我的神仙水怎么用的这么快|上豌豆公主给你买一瓶新的不就行了|豌豆公主有正品吗|豌豆公主是一款专注日淘的APP|100%日本原装进口正品|里面美妆、护肤用品一应俱全|新人注册市场价1540元的sk-l神仙水|新人仅需899元|点击视频下方链接就可以啦|全品类每满100减20|5率100%20|1088元可用|圣诞快乐 推荐 1%研究所|雅荐 1%研究所 美牧馆|豌豆公主 日本优品一站购|sk-1神仙水8，|sk-神仙水899元|美妆馆|1088元可用|SK-II|E想|圣您 任想田|花 30|5-E\", \"video_asr\": \"嗯。|睡前擦香香会变更漂亮，好好啊，我就说我的神仙水怎么用的这么快？刷豌豆公主给你买一个新的文学吗？豌豆公主有正品吗？豌豆公主是一款专注日淘的APP百分百日本原装进口正品里面的美妆护肤用品。|一应俱全，新人注册市场价一千五百四十元的SK TWO神仙水新人仅需八百九十九元，点击视频下方链接就可以了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01592874526977539 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '静态', '推广页', '中景', '多人情景剧', '喜悦', '家', '极端特写', '平静', '单人口播', '特写', '惊奇', '夫妻&恋人&相亲', '亲子', '配音', '动态', '室内', '室外', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.87', '0.65', '0.27', '0.26', '0.10', '0.04', '0.02', '0.02', '0.02', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cec6fb198954d807ad93aca5eef6171.mp4\n",
      "{\"video_ocr\": \"孩子为什么抵触数学|要知道 =87/|虽然数学题型多样=89/|但核心都离不开计算89l|有可能你家孩子刚做完题|懂方法的孩子|4|都已经检查三遍了=89/|都已经检查三遍了|学维|核桃数学思维特训营|清华北大毕业名师带队研发|针对新一年级和新三年级的学生|4-6节数学思维提升课|课程采用动画互动课堂教学|加专属老师服务的模式|让你的孩子爱上数学|现在报名只需9块9|家长们|赶紧点击屏幕下方链接报名吧|核桃数学核桃数学思维特训营|（× 99|6x99=1|7×99=7|乘法速算|2x99=198 7×99=613|乘法|字思维特训宫|桃数学思丝生训|あ 桃数学思维生训管|乖法速|7S|Wc|3x99三24]8×99-1|先x99三张9×99=1|5*y9-49×99=-1|3x99三z178×9-7|9x99三378×99-712|x99-6%3|5×99ニ|5x99ニ42|5x99二499x99-9|89/|3x99|5×|(× 99=1|6 x99=574|99=574|秋季数学|×99|x99 ニ1|2×99 =/9|2 ×99=198|x99 =-91|2x99 ニ198|2 ×99 二198 7×99=7|1=|3 ×99 =|ξx 99=|3 x99 =297|Q9=712|2 ×99|9=1|99= 1|6× 99-514|99=-6973|99:693|4 x99=|4×99=396|4 x99=3点9|3x99=2197 8x99=9|3 x99=z97 8 x99|29=9|0-712|9l|391|イc4|孩子更喜欢学 提升逻辑思维|核桃编程出品|AI动画互动课\", \"video_asr\": \"酒酒酒酒酒酒酒。|谢谢。|一九八九二九。|十七四九三十。|五九四十五，六九五十四，七九六十三，八九七十二，九九八十一，孩子为什么抵触数学？要知道，虽然数学题型多样，但核心都离不开计算，有可能你家孩子刚做完题，懂方法的孩子都已经检查完三遍了，核桃数学思维特训营清。|华北大毕业名师带队研发针对新一年级和新三年级的学生四到六节数学思维提升课，课程采用动画，互动，课堂教学，三专属老师服务的模式。|让你的孩子爱上数学，现在报名只需要九块九，家长们赶紧点击屏幕下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016005277633666992 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '学校', '静态', '室内', '教师(教授)', '喜悦', '配音', '情景演绎', '平静', '知识讲解', '特写', '手写解题', '拉近', '重点圈画', '手机电脑录屏', '场景-其他', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0cf8296433c9a93fce2a89658c47c4b6.mp4\n",
      "{\"video_ocr\": \"这些求救信号|第一|莫名其妙出汗|是心脏的求救信号|脸色发黑|突发性头晕目眩|脸部脚步水肿|虽然得了重病|非常可怕|但更可怕的是没钱治病|如果你的存款支撑不起|那么我建议|你在身体健康的时候|买一份首月1元|次月14元起的|微医保百万医疗险|这样|你就拥有|保额600万的保障|来看病|而且在保险责任范围内|大小病意外住院产生的|住院费医药费|社保未覆盖的进口药|自费药都可以申请报销|不要等到重病来临时|才开始后悔没买保险|现在赶紧点击|视频下方链接|为自己和家人|投一份保障吧|点击立即投保|第二步 填写信息|和健康告知内容 仔细阅读条款、投保须知|确认被保险人符合承保条件|支付首月保费1元|投保成功 关注井领取电子保单|首月1元次月14元起|社徽外费任内医疗费全保排|科份证 信非保京。饮期于校保|休憩保塞，仅期于换保|条款创|条款目菜|针对以上第之条健搬验查开常，始拥足以下情完，剩为州外|响， 请确认被保人健康状况是西持会视解条件|开通账号 微国保-百万医行险 微基保-百方医疗险月雪自匙模费|为诊换保接保人)|输保人|我已确认并同意以下造延及条款|武费方K|24量大疾润任住民幸飞始付 只险|24搬大疾痹住玩津飞培行|我卓影还销心emouLi|装保险人过击?年汽复有过住境成额要求进一秒检童，手术|暗入验证|验证药|12会何成立及 1.3告网约之事|以下M次可作 没保最件:|科神病。先 含甲类及乙类)|父母 姓名 请输入您的姓客|保险条款|以下情况可作为附外事项，仍符合贸保最件:|为了保证被保人的保她权益在崾想时不受影|豆联网保险专家|豆 保脸专家|巨 保险专家|互阀保险凌家|两保 险专家|手机号码|开通下一年自动级保 [G剑|只层世心服病，心功领不全二级战上，清功额不全。料费|大于900mmhgb .后心病。心额模死，熟楼死，超出白|你购买的保单，在等月保黄约支行自鸭通过 优兔从等钱迎除，哪铁不起时将从算帅支特方|微保|(次月起保费随不同年龄、有无社保等情况变化）|(保险责任范围内)|（一般住院医疗1万免赔额/年、 100种重疾住院0免赔)|欢月14元起保费随年龄、有无社保情况等变化|点击下方|为谁授微被保人)|填写捡保信退|有无保|食费|支付并升逼续费|(保费团不同年触，有无社保等信况空化)|月600万医疗保障|￥1.00|报销社保外医疗费|子女|百月¥1|重大|之属梦以下被保险人出生封时体重不誓于2.5公厅。等字在早产|充成|康告知|微信支付|当疾病来临前|&.婚何远保|国、气急，紫道，持银反复发热，拉瞧、不帆原活友下出 血点，感会、反重愿址。运余授难修成春增还难、短盘、浮|血点，感台、反复草址。远余授难修成春福塞难，晒血、浮|【视频纯属广告创意]|本保险由泰康在线财产保险有限公司承保 具体费率及保费金额以实际情况为准|包康售知|泰康在线TK.CN|25保险利网|身体会发出|为密套餐|泰人|有儿版保\", \"video_asr\": \"第一，莫名其妙出汗是心脏的求救信号，第二，脸色发黑是肝脏的求救信号。第三。|突发性头晕目眩是大脑的求救信号，第四，脸部，脚部水肿是肾脏的求救信号，虽然得了重病非常可怕，但更可怕的是没钱治病。如果你的存款支撑不起，那么我建议你在身体健康的时候，买一份首月一元，次月十四元起的微医保百万医疗险。|这样你就拥有保额六百万的保障来看病，而且在保险责任范围内，大小病，意外，住院产生的住院费，医药费，社保未覆盖的进口药，自费药都可以申请报销，不要等到重病来临时才开始后悔没买保险，现在赶紧点击视频下方链接，为自己和家人投一份保障吧！|第一步，点击立即投保，第二步填写信息，第三步，仔细阅读条款，投保须知和健康告知内容，确认被保险人符合承保条件。第四步，支付首月保费一元，投保成功。\"}\n",
      "multi-modal tagging model forward cost time: 0.016196489334106445 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '静态', '平静', '配音', '中景', '场景-其他', '室内', '推广页', '单人口播', '特写', '极端特写', '动态', '拉近', '多人情景剧', '喜悦', '办公室', '情景演绎', '厌恶', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d015bd7260808fb1881f58040e0c71d.mp4\n",
      "{\"video_ocr\": \"你们知道吗?|面对失败|最难过的不是我不行|而是我本可以|这句话会回绕在|每个失败者的脑海|每天晚上反反复复的想|如果当时怎么样|那我现在就怎么样了|从现在起|把握好每一天|可以学习的机会|因为那将是你人生中|最年轻的一天|但行好事，莫问前程|成人自考名校本科|每天三十分钟|改变你的一生\", \"video_asr\": \"你们知道吗，面对失败，最难过的不是我不行，而是我本可以。这句话会回绕在每个失败者吗？|每天晚上反反复复的想，如果当时怎么样了，我现在就怎么样呢？从现在起，把握好每一天可以学习的机会。|因为那是你人生中最年轻的一天，但行好事莫问前。\"}\n",
      "multi-modal tagging model forward cost time: 0.01656484603881836 sec\n",
      "{'result': [{'labels': ['填充', '推广页', '中景', '现代', '家', '静态', '动态', '特写', '室外', '情景演绎', '混剪', '全景', '室内', '多人情景剧', '极端特写', '配音', '办公室', '夫妻&恋人&相亲', '朋友&同事(平级)', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.94', '0.85', '0.82', '0.79', '0.65', '0.51', '0.39', '0.22', '0.22', '0.18', '0.14', '0.06', '0.05']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d0688385ef0c9eb321e86d3bdfe152e.mp4\n",
      "{\"video_ocr\": \"2020非全日制研究生培训通知|MBA MPA MEM等在职硕士|毕业获得:院校统一颁发的毕业证学位证，全网可查|报名目的:提升学历、突破职业发展瓶颈、升值加薪|查看详情|咨询非全日制研|学|学习方式:不用辞职，手机学习，每天30分钟|考试|考试难度:简单，满分300分只需175分即可通过|上班族自考985、211名校硕士|报名条件:专科毕业5年、本科毕业3年|报考院校:985、211及国际名校|报考条件 报考时间 考试科目 职业前景 薪资待遇|报考及培训信息\", \"video_asr\": \"二零二零非全日制研究生报考及培训信息报名条件，专科毕业五年，本科毕业三年，报考院校九八五二一一及国际名校。学习方式不用辞职，手机学习，每天三十分钟，考试难度简单，满分三百分，只需一百七十五分即可通过。|毕业获得院校统一颁发的毕业证，学位证全网可查报名目的，提升学历，突破职业发展瓶颈，升职加薪。|快点击查看详情，咨询相关院校和学费吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016478300094604492 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '填充', '配音', '手机电脑录屏', '平静', '幻灯片轮播', '中景', '单人口播', '重点圈画', '静态', '喜悦', '室内', '教师(教授)', '拉近', '办公室', '宫格', '知识讲解', '绘画展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.85', '0.60', '0.15', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d0848a53d0d98b0b3f5d8cbb3291176.mp4\n",
      "{\"video_ocr\": \"写出三个相邻的数字，让他们相加等千15|腾讯学习训练营公众号 1年教龄名师授课|1234567?|12345679910|612旧24|456|课后还有|对1辅导答疑 5天免费互动直播课，让孩子爱上数学.|腾讯企鹅辅导|915227\", \"video_asr\": \"今天我们来讲一下，写出三个相邻的数字，让他们相加等于十五。|ZZZZ。|腾讯学习训练营公众号五天免费互动直播课，让孩子爱上数学。\"}\n",
      "multi-modal tagging model forward cost time: 0.01707601547241211 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '配音', '动画', '平静', '推广页', '填充', '课件展示', '图文快闪', '幻灯片轮播', '混剪', '宫格', '室内', '办公室', '知识讲解', '过渡页', '中景', '特写', '餐厅', '城市景观'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d091c55afd7668d84f26897a96e6a0b.mp4\n",
      "{\"video_ocr\": \"带回家!|唉老婆|你给咱闺女|报补习班了吗|报了|我给了你两千|你给我拿回来多少|1991|9块钱|报9块钱的补习班|你是不是她亲爹啊你啊|9块钱能有 什么好老师教啊|你听我说|这个啊是|高途课堂 全科名师班|北大清华毕业的名师 授课|平均教龄 在11年以上的|那你这是一科的|我要的是全科的|唉呀|语数英物四科|教的都是有用的|王都是有开会|学习方法和解题大招|像什么数学几何模型|函数秒解法|语文阅读理解|万能模板|英语的单词记忆法|物理出题套路|和解题妙招|就她们班那个学霸|凌霄|就是在这上面上课|可咱们尖尖|哪儿能跟人家比啊|万一跟不上怎么办啊|不用担心|课程支持3年内|无限次回放|课上互动问答|课后还有辅导老师|一对一指导答疑|我都问好了|这个课|偏科|基础差|学得慢的孩子|都能学|那这么好的课|在哪儿报名啊|点击视频下方链接|查看详情|就能报名啦|名师有秘账额跑新学期 9元16节演|请选择孩子9月升入年级 身手价|课程指导价499-优惠券490|全国百佳教师带队教学 平均教龄11年|三该要福将价499-娱康务490 抓住暑假关腿时期|清北毕业名师教学，快速全面提升 温故知新，冲刺领跑新学期|O|通掉预子9月升入年|10年 17天|高途课堂特训营|14:60|新用户专享 立即体验 浙江卫视指定在线教育品牌|要9元!16节高途全科名师课|华少|名师特训班|名师出高徒.网课选高途|不要499! 不要299! 现在只|ESPAUL|PAUL.|浙江卫视|90%|上互|¥9|仅需\", \"video_asr\": \"老奶奶，闺女报补习班了吗？报了我给了你两千，你给我拿回来了多少？一千九百九十一九块钱，你就给他闺女报九块钱的补习班，你是不是在亲爹啊？你九块钱能有什么好办的？你听我说，这个是高途课堂全科名师班，北大清华毕业名师授课，平均教龄在十一年。|那你这是一颗呢，我要的是全科呢，哎呀，这这个就是全科的语数，英物四科，教的都是有用的学习方法和几个大招，像什么数学几何模型，函数描写法，语文阅读理解，万能模板，英语的单词记忆法，物理出题套路和解题妙招，就他们班那个学霸凌霄就是在这上面上课，可咱们今天哪能跟人家比呀，万一跟不上怎么办啊？|不用担心，课程支持三年内无限次回放，课上互动回答，课后还辅导老师一对一指导答疑，我都问好了，这个课偏科基础差，学的慢的孩子都能学，那这么好的课在哪报名啊？点击视频下方链接查看详情就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.015961408615112305 sec\n",
      "{'result': [{'labels': ['填充', '推广页', '现代', '中景', '多人情景剧', '夫妻&恋人&相亲', '静态', '家', '亲子', '惊奇', '家庭伦理', '手机电脑录屏', '喜悦', '单人口播', '平静', '极端特写', '宫格', '动态', '愤怒', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.87', '0.48', '0.45', '0.25', '0.24', '0.16', '0.10', '0.06', '0.03', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d14aa27162b28b5571df0e773606c7d.mp4\n",
      "{\"video_ocr\": \"你这个小电扇挺好使呀|哪买的|我也想整一个|拼多多咯|下载打开拼多|首页搜索USB迷你小风扇|上滑就能找到这款|7.9元拼团的小风扇了|酷夏出街|让你等车再也不会大汗淋漓|我你这在不好|钢量|月己的扩扇子你|29.9|配花|武属晨+充名战+亮皂思龙|花早能|PQRS TUV|上课不怕吵了|拼多多凡PP|拼多多APP专享|拼多:RP享|拼:N专享|WXYZ|清是过天|+充电线|起静音|拼\", \"video_asr\": \"闺女，你这个小电扇挺好使呀，哪买的？我也想找一个拼多多下载打开拼多多首页搜索USB迷你小风扇。|上滑就能找到这款七块九元拼团的小风扇了，酷夏出击，让你等车再也不会大汗淋漓！\"}\n",
      "multi-modal tagging model forward cost time: 0.01594829559326172 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '推广页', '静态', '平静', '多人情景剧', '动态', '喜悦', '配音', '特写', '场景-其他', '单人口播', '极端特写', '路人', '惊奇', '朋友&同事(平级)', '室外', '填充', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.82', '0.81', '0.49', '0.42', '0.41', '0.39', '0.34', '0.34', '0.03', '0.03', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d16754c1404e4363dd7c6bcce27315b.mp4\n",
      "{\"video_ocr\": \"最近好多家长问我|有什么适合3-6岁孩子|不出门在家就能|轻松学数学的课程|我呀今天就告诉大家|而且告诉你在哪里报名|看|这个是我家孩子正在学的|斑马A1课思维体验课|49元10节课|咱们首先点击 视频下方详情|看到首页详情有|不同年龄段对应的|难度不同的课程|根据自己家孩子的年龄|去选择对应级别的课程|点击付款|只要49元|成功啦!|然后返回填写一个|正确的收货地址就好了|就可以在家等待|接收数学启蒙礼盒啦|来让我们看看|这个沉甸甸的礼盒里面|都有什么吧|数独棋|推理桌游|思维动动卡|思维练习册|多维马赛克 正方体积木|这还不是重点呢|关键是由名校毕业的|资深教研团队研发|一节课15分钟|全程趣味动画教学|培养孩子的 逻辑思维和数学思维|赶紧点击下方详情|抢课吧!|斑马A|课 猿辅导在核教育 出团|猿辅导在线教背 场|猿辅导|猿辅是 an|斑AiE|斑AF|1ALEE|猿辅导在板教育H店|猿辅导在械教到出|并说一说几比4小|解决生活中的款学问题|对标国内外课程和考试标准 符合《3-6岁儿童学习与发展指南|养成习惯 学完系统课孩子学到|选择级别|赠价值108元随材礼盒|2人拼团 拼回价 立省￥151|培养数学兴趣，学习习惯养成快 C算口算又快又准，轻松举一反三|建立教理思性，幼小有接不技默|熟巷常见规律|3-4岁 53|次训练，助力大脑开发 3000+|2016年9月~2017年8月底出生的孩子，|¥200|￥49|o图形万值接巧认知|蚁学知识|上课简单 家长更省心 10000+次交互闯关，巩固成果|适合5~6周岁孩子|10+30|微尘 思维训练好，3-6岁孩子有多椰|轻松学6 大知识模块 16种思维方法|满足合购幼小带接全阶撒圈求|S1|限时赠30节国庆特辑课|仅需 49元|课程丰富 孩子眼着学|数学思维启蒙课|逻辑训练 数独游戏棋|小镇大作战|体验课|维马赛克正方体积木|200+个知说点柄航，原属金命|已减¥151 立即支付|B马AIR|18:20|*该手机号码仅用于上课|高频练习|礼盒收货信息将在付款后填写|课程丰孩子情都手|思维|马A课|动，|记亿豆间选铺，速视思雄初有成|10节A互动调|订单信息|【马上售罄】 49元=10节思维|个知识点拆解，覆盖全面|体验课+108元益智礼盒+30节|H团价|国庆特辑课|查看详情|AI课果|???\", \"video_asr\": \"最近啊，多多家长问我，你有什么适合三到六岁孩子不出门在家就能轻松学数学的课程？|我呀，今天就告诉大家，而且告诉你在哪里报名。|这个是我家孩子正在学的斑马AI课，思维体验课，四十九块钱十节课呢。咱们首先点击视频下方详情，看到首页，详情有不同年龄段对应的难度，不同的课程，根据自家孩子年龄去选择对应级别的课程。|给您付款只要四十九元，成功了，然后返回提醒一个正确的收货地址就好了，就可以在家等待接收数学启蒙礼盒了。来让我看看这个。|礼盒里面都有什么吧？做顿只欢迎桌游作为工作卡作为练习。|因为马赛克正方体积木，这还不是重点的，关键是有名校毕业的。|教研团队研发节课就十五分钟，全程趣味动画教学，培养孩子逻辑思维和数学思维解。|下方详情抢课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01614522933959961 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '静态', '平静', '场景-其他', '手机电脑录屏', '配音', '单人口播', '多人情景剧', '室内', '极端特写', '亲子', '教辅材料', '动态', '喜悦', '课件展示', '家', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.93', '0.77', '0.71', '0.63', '0.23', '0.16', '0.14', '0.09', '0.05']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d3aaef41a879697a26e7226800642c8.mp4\n",
      "{\"video_ocr\": \"进城务工被女总裁看上 女总裁提出年薪千万|秦秦经理|你别这样|谁|徐方|你不在家帮我洗衣服|跟这个女人|拉拉扯扯在干嘛|跟我回村|这位大姐|徐方跟着我|能年薪千万|你这又是何必呢|你喊谁大姐呢|我可是跟他一起住的村长|他是我们村唯一的医生|不光是我|村里所有的姑娘|都在等他回去看病呢|你怎么选呢|诶诶|继续阅读精彩小说|《第一村医》|点击本视频\", \"video_asr\": \"亲亲亲亲你，你别这样，谁举报你不在家帮我洗衣服，我跟这个女人拉拉扯扯在干嘛给我回村。|这位大姐，徐帆跟着我能连新千万，真是你喊谁大姐呢？我可是跟大家一起住了村长，他说我们村唯一的一声，不光是我。|村里所有姑娘知道现在回去看病。|徐方能怎么选择？|购买消息。|吃饭。\"}\n",
      "multi-modal tagging model forward cost time: 0.015838146209716797 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '特写', '愤怒', '亲子', '动态', '平静', '喜悦', '夫妻&恋人&相亲', '单人口播', '拉近', '悲伤', '全景', '朋友&同事(平级)', '家', '惊奇', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.84', '0.81', '0.50', '0.44', '0.42', '0.11', '0.10', '0.08', '0.05', '0.02', '0.02', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d3bd931f48b0c39e014050f15e1856a.mp4\n",
      "{\"video_ocr\": \"你当老板了不起啊?|开公司了不起啊?|上次我找你借钱|你借几万都不肯|昨天老郭找你借钱|你眼睛都不眨一下|10来万出去了|你还当我是不是兄弟啊!|放手!|放开他|这是我兄弟|我是真没钱|而且这次郭子的钱|不是我借给他的|是360借条|360借条最高可借20万|最长可分12期慢慢还|不使用不收费|兄弟啊 你快告诉我|在哪申请|我真的急用钱|点击视频下方链接|最快5分钟放款 点击视频下方立即申请|￥|960借茶|360奇|ョ6口借条|贷款额度 放款时间等以实际审批为准|最高20万额度 最长30天免息|最高可借20万|30天息费优惠券\", \"video_asr\": \"你当老板了不起啊，开公司了不起啊，上次我找你借钱，你借几万都不肯啊，昨天老公找你借钱，你姐姐都不傻呀，十来万出去了，你还当我是不是兄弟啊啊，快快快给他，这是我兄弟，上次你找我借钱，我是真没钱，而且这是公司的钱，不是我借给他的，是三六零借条，三六零借条。|三六零借条最高可借二十万，最长可分十二期，慢慢还不使用，不收费，兄弟你快告诉我在哪申请，我真的急用钱，点击视频下方链接就可以查看你的额度了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01617908477783203 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '多人情景剧', '平静', '单人口播', '室外', '路人', '工作职场', '愤怒', '惊奇', '上下级', '动态', '喜悦', '全景', '停车场', '手机电脑录屏', '特写', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.68', '0.33', '0.22', '0.14', '0.08', '0.06', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d439b8741c113fe3e534450ffecbc5a.mp4\n",
      "{\"video_ocr\": \"你的指尖 能到多远 讯视频纪录片频道|也可以到海洋暮色的深海区 《七个世界 一个星球》|近临街边巷口 讲出市井故事|讲出市井故事 近临街边巷|近临街 边 巷口|可以带你走进风云历史的中心|《风云战国之列国》|没有秘密的你 重磅心狐妖送你VP点击领取>|2019腾讯视频V视界|[纪录片]|中华美食群英榜|奶奶最懂得·独家更新|7|不奂|BBC七个世界·豆题9.9|BBC七个世界 豆道9.9|这才是真正的中国味道|精选|(中国机长)|斗罗大陆|美食 全网|川味第2季|真香温州人的早餐首选|热播大放送|可以到光年外的星云宇宙|《行星》|腾讯视频|纪录片|《机器人时代》|不负好时光|重新对世界好i|时空来电 山O|w儿|BBC七个世界·迷你版|远及大洋彼岸 感受新鲜文化|《我们的浪潮》|感受/新鲜文化|《早餐中国.2》|远 及大洋彼岸 感受新鲜文化|. 可以俯瞰 科幻去知的世界|也 可 以 俯 瞰 科幻 未知的世界|11:53 电视剧 电影|的眼潮|我们的眼蓖|大会|早餐由国.2|小年化\", \"video_asr\": \"你的指尖。|能到多远？|可以到光年外的星云宇宙，也可以到海洋慕色的。|深海禁区。|愿极大洋彼岸。|感受新鲜文化，进麻街边巷口。|讲出事情故事。|可以带你走进疯。|历史的中心。|靠看黄历。|知的世界。|腾讯视频纪录片频道。|重新对应。|世界好奇。\"}\n",
      "multi-modal tagging model forward cost time: 0.01620793342590332 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '配音', '喜悦', '静态', '平静', '场景-其他', '手机电脑录屏', '情景演绎', '动态', '室内', '特写', '悲伤', '填充', '极端特写', '全景', '远景', '混剪', '惊奇'], 'scores': ['1.00', '1.00', '0.99', '0.88', '0.74', '0.73', '0.57', '0.52', '0.51', '0.16', '0.12', '0.10', '0.10', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d4b255585578a2f4c4cfe84787c4c51.mp4\n",
      "{\"video_ocr\": \"果果，你想要那个礼盒吗|不用了|我们回家吧|这个一定很贵|妈妈平时太辛苦了|我不愿意让妈妈|花那么多钱|哇塞!|斑马A课礼盒|思维动动卡|正方形积木|数独棋|喜欢吗?|喜欢|可是这个应该很贵吧|一点也不贵|诶?您好|请问这个是什么呀|这个啊是报名|斑马A1课 思维体验课|送的礼盒|10年以上资深研究孩子|数学启蒙的教研团队打造|帮助孩子|建立对数的认知|形成数感|把数学啊|融入到动画故事|数字互动小游戏|儿歌里|让孩子在|熟悉的生活环境中|就能玩着学数学|怪不得我家果果那么喜欢|这2-8岁的孩子|不都爱看动画片吗?|边玩边学 思维-|孩子能不喜欢吗|这课多少钱啊?|现在活动价|还送这个超值教具礼盒|点击视频详情|就能马上报名啦|特辅导在线教育 出品|猿铺吴在线教育出品|信辅导在会|摄出品|猿辅异正义盲出品|猿辅导在线飘，出品|势喃导在线教盲出品|猿辅导在线教可|我红我育出品|一，出品|装在的出品|顺辅 出品|经是一出品|微任线教育出品|啸辅兽在教育出品|瘦电鲁在线敬育出品|k楠舞在幼教育出品|S1|5|守护小镇大作战|请戏取上面发包的排列耀律，收下需害卡片，投短律在概里撑一程， AB式推场|第1周|请双麻上面篱首的排列经律，取下笑西卡片，预理律在程于|斑马A|学思维学英语2-8岁上斑马|学思维学英语2-8岁上斑马|系统课课程指南|80|多维马赛克正方体积木|斑马 AΑ|课|O出马AI嫩|多维马裹克正方|22|49元10节课|班马AI课果|斑马思维|思结|000000|oooooo|训练 游戏\", \"video_asr\": \"火锅想要那个礼盒吗？那我们回家吧，这个一定很贵，妈妈平时太辛苦了，我不愿意让妈妈花那么多钱，哇塞，斑马AI这礼盒。|噔噔咯，这发型积木。|说不读题喜欢吗？喜欢。|十六个班。|点也不贵，你好，请问这个是什么呀？这个啊，是报名斑马雷艾克思维体验课送的礼盒，十年以上资深研究孩子数学启蒙的教研团队来的，帮助孩子建立对书的认知情感，把数学啊融入到动画故事。|数字互动小游戏，儿歌里，让孩子在熟悉的生活环境中就能玩着学数学，怪不得我家狗狗那么喜欢这两到八岁的孩子，我都爱看动画片吗？|玩边学，孩子能不喜欢吗？这个课多少钱啊？现在活动价九元一节课还送这个超值教具礼盒，点击视频详情就能马上报名了！|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016122817993164062 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '教辅材料', '中景', '静态', '室外', '多人情景剧', '全景', '特写', '场景-其他', '配音', '动态', '平静', '喜悦', '单人口播', '极端特写', '路人', '手机电脑录屏', '亲子', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.81', '0.81', '0.69', '0.62', '0.46', '0.31', '0.29', '0.06', '0.05', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d8a9df0d273749660bb4531fe1dd928.mp4\n",
      "{\"video_ocr\": \"1+4|小学数学是|学习数学的基础|而算数尤为重要|猿辅导 如果掌握不了正确的方法|孩子以后学习数学|将会举步维艰|猿辅导小学数学|清北毕业老师|讲解速算巧算方法|在家就能帮助孩子|夯实数学基础|巩固数学知识|凑十法|9 +5=|9+5=14|邓诚老师|暑期数学名师特训班|九年教龄 高考状元 清华大学本科|特惠 猿辅导名师 邓诚|6天专学重难点 冲刺领跑新学期|教出63位清北学生|猿辅导在线教育|家假|特训班|49|元\", \"video_asr\": \"嗯。|嗯。|小学数学是学习数学的基础，而算数尤为重要，如果掌握不了正确的方法，孩子以后学习数学将会举步维艰。猿辅导小学数学，清北毕业老师讲解速算巧算方法，在家就能帮助孩子夯实数学基础巩固。|首先。\"}\n",
      "multi-modal tagging model forward cost time: 0.01602768898010254 sec\n",
      "{'result': [{'labels': ['现代', '静态', '单人口播', '推广页', '中景', '极端特写', '家', '教师(教授)', '特写', '平静', '手写解题', '室内', '填充', '多人情景剧', '手机电脑录屏', '场景-其他', '教辅材料', '宫格', '家庭伦理', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.52', '0.08', '0.04', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d8c9201084c5547a9fc6d50b715abff.mp4\n",
      "{\"video_ocr\": \"不要死记硬背太多的语法规则 littlelsome|litlelsom不要死记硬背太多的语法规则|因为光用规则去反推语言|规则本身|你就非常难理解|lilesome笔如说我们小时候上语法课|比如说我们小时候上语法课|定语从句什么时候|只能用that不能用which|然后老师给我们列出很多条款|所有的语法书|都是这么干|我来问大家|你如果真的能够花这么多时间|把这么多规则背会|你英语早学会了|而且真的你能记住之后|就能活学活用吗|其实从本质上理解|这个语法点特别简单|仅仅是因为定语从句|修饰的先行词|指示特别明确|不具备选择性|你就只能用that|因为that指的就是那一个|而which指的是哪一个啊|他指示不明确|当定语从句的先行词|指示明确的时候|不能用which|你这个时候|回头再看看|这种规则|有必要背吗|想跟我一起了解更多英语提升的方法吗|9元就可以领取|7天满满的英语干货|如果你有兴趣|就赶紧点击屏幕下方立即报名吧|有道精品课|传统解释: (1)定语从句修饰的词(先行间〕前面有形容词最高细响|(1)定语从句修饰的词行词)前面有形容词量|本质原因:当先行词指示明确、不具备选择性时，只用that不用w.|本质原因:当先行词指示明确、不具备选择性时，|at不用which.|This is the best film tha ave seen.|which.|杨亮讲英文 定语从句什么时候只能用that不能用which?|为容词最高级时; ‘修饰时;|D|泽性时，只用that坏不用which。|(6)先行词既有人也有物时。|(5)先行词前面有who, which等景|(2)先行词被the onlylthe sa (3)先行词被序数词修饰时; last等修饰时|“he last等修饰”|\\\"so last你~|Ioct等你的|(2)先行词被the onlylthe samelthe last等修u，|级时:|ave seen.|#英文|赢换英文|流亮|nelittle/some等词|ellittlelsu|Great Wall.|ellt.|anythingleverything/nothing/fewlall/none/little/some等词|anythingleverything/nothing/fewlall/none/litt!e/some等词|The first place that they visited was the Great Wall.|The first place that they visited was the Great Wall.|INvCI!|NwClI?|InCI?|INCll?|rmCn?|(4)先行词正好是|He is the only one that I can trust.|This is the best film that lI have seen.|o/lifI.le|A行|ne等词|seewo|筝词|9元|7天\", \"video_asr\": \"不要死记硬背太多的语法规则，因为光用规则去反推语言规则本身你就非常难理解了，比如说我们小时候上。|这个语法课定语从句什么时候只能用THAT，不能用WHICH？然后老师给我们列出很多条款啊，所有语法书都是这么干，我来问大家，你如果真的能够花这么多时间把这么多规则背会，你英语早学会了，而且真的你能记住之后就能活学活用吗？其实从本质上理解这个语法点特别简单，仅仅是因为定语从句修饰的先行词。|指示特别明确，不具备选择性，你就只能用THAT。为什么？因为THAT指的就是那一个，而WHICH指的是哪一个呀？|他指示不明确啊，当定语从句的先行词指示明确的时候，你就肯定只能用THAT，不能用WHICH，你这个时候回头再看看这种规则。|必要背吗？想跟我一起了解更多英语提升的方法呢？九元就可以领取七天满满的英语干货，如果你有兴趣，就赶紧点击屏幕下方立即报名吧！|我。\"}\n",
      "multi-modal tagging model forward cost time: 0.016187191009521484 sec\n",
      "{'result': [{'labels': ['现代', '中景', '教师(教授)', '填充', '静态', '单人口播', '推广页', '知识讲解', '室内', '平静', '情景演绎', '配音', '影棚幕布', '手机电脑录屏', '惊奇', '学校', '特写', '场景-其他', '手写解题', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.95', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0d95f947529dd6bcf3fef55d735354b1.mp4\n",
      "{\"video_ocr\": \"我670670|天啊我680|能上北大了|人家都考600多分|你才考423分|你这三年怎么学的|你还说我|人家都报名了|高途课堂全科名师班|你报了吗|人家都开始挑战 985、211了|我现在连考个好的 本科都难|你满意了|那9块钱的课|能提升什么|人家都是北大清华毕业 的老师带队授课|每节课讲的都是 高频考题|人家还有半个小时 就检查完了|我却连题都没写完|那我也没有想到|那9块钱16节课|能有这么好的效果啊|阿姨|班里好多同学 之前都在学|都是平均教龄 11年以上的名师教学|着重解读高考 必考知识点|语数英物四科 重难点、得分点|他们都会总结的 明明白白|掌握了解题方法|1分钟一道小题|正确率也特别高|各位家长们|如果您的孩子 即将升入高三|那就快 点击下方查看详情|赶紧给孩子报名|抓住这个 冲刺名校的机会|新学员9元专享 立即报名|王皓轩|王嘉懿|省高考状元带队授课 老师平均教龄11年+|宋靖琪 杨瑾 王欣然|张梦梵|582|实验高中2020年高考分数|高途课堂|2020016 赵晟睿|539|498|名师出高徒·网课选高途|视频为演维情节|刘雅静|许天佑 612|527|海琼|李薇|590|陈浩南|665|姓名|名师特训班|分数|学号\", \"video_asr\": \"我六百七六百七天有六百万可上北大了。|人家能考六百多分，你才考四百二十三分啊，你这三年怎么学的？你还说我人家都报名了高途课堂全科名师班，你报了吗？人家都可以挑战九八五二幺幺了。|现连考个好的本科都难，你满意了，有危险的课能提升什么？人家都是北大清华毕业老师带队授课，每节课讲的都是高频考题，人家还有半个小时的检查完。|我连题都没做完，那我也没有想到的九块钱十六节课能有这么好的效果呀呀，阿姨这高途课堂全科名师班，班里好多同学之前都在学。|平均教龄十一年以上的名师教学，着重解读高考必考知识点，语数，英物四科重难点，得分点，他们昨晚总结的明明白白，掌握了解题方法，一分钟一道小题，三分钟一道大题，正确率也特别高。各位家长们，如果您的孩子即将升入高三，那就快点击下方查看详情，给你的孩子报名，抓住这个冲刺名校的机会！\"}\n",
      "multi-modal tagging model forward cost time: 0.016126632690429688 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '多人情景剧', '推广页', '亲子', '平静', '静态', '家庭伦理', '动态', '愤怒', '全景', '朋友&同事(平级)', '特写', '室外', '极端特写', '喜悦', '惊奇', '悲伤', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.94', '0.88', '0.51', '0.20', '0.09', '0.06', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0dabd1744dd7495a2510be896307cde8.mp4\n",
      "{\"video_ocr\": \"姐|你看，你借我点钱呗|我家孩子英语报补习班 又要缴费了|可是我这几个月|一直在领基本工资|看来养个孩子 确实得花不少钱呢!|那可不|但是别人家的咳子|都报英语补习班了|我可不想让我家孩子 输在起跑线上!|要不你给孩子报一个|伴鱼自然拼读|自然拼读课|是呀|and theyfight eachother forflamingos|What?|Heyits here!|那这课得很多钱吧?|一点都不贵|14节课只要29元|下单就能得到一张|免费包邮的学习成长地图|而且孩子只要按规定完课|就可以领取等额奖学金啦|那这么好的课在哪报名啊?|点击视频下方链接|就可以报名啦!|赶快行动起来吧!|掌握英语拼读规律|学习26个字母和字母组合|在单词中如何发音|力求掌握70%~80%|符合发音规律的单词|让孩子可以通过|watches|ches|Paul|Perfect|ABG|Retry|NB|1and|AE\", \"video_asr\": \"姐，你看你借我点钱呗，我家孩子英语补习班又要交费了，可我这几个月一直在领基本工资呀。|看来呀，养个孩子确实得花不少钱呢，那可不但是别人家的孩子都报英语补习班了，我可不想让我家孩子输在起跑线上。|要不你给孩子报一个伴鱼自然拼读课，自然拼读课，是呀，伴鱼自然拼读课让孩子可以通过学习二十六个字母和字母组合在单词中如何发音。|掌握英语拼读规律，地球掌握百分之七十到百分之八十符合发音规律的单词，那这可就苦多钱吧，一点都不贵，十四节课只要二十九块钱下单就能得到一张免费包邮的学习成长地图，而且孩子只要按规定完课就可以领取本科奖学金了，这么好的课！|在哪报名啊？点击视频下方链接就可以报名了，赶快行动起来吧WHICH PHONICS！\"}\n",
      "multi-modal tagging model forward cost time: 0.016100168228149414 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '静态', '单人口播', '场景-其他', '平静', '配音', '家', '室内', '课件展示', '多人情景剧', '亲子', '动画', '家庭伦理', '情景演绎', '手机电脑录屏', '特写', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.64', '0.63', '0.45', '0.04', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0db6b7f9a40e4f8c7618949dabdbbace.mp4\n",
      "{\"video_ocr\": \"价值299元绘画特训课|在线学画画 就找美术宝1对1|在线少儿美术|美术宝1对1|1对1)|JERY|TJERESY|J|JEAY|TM|TO.人A|T人A|现仅9元/9节|gU@|CAC\", \"video_asr\": \"AS AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016121387481689453 sec\n",
      "{'result': [{'labels': ['现代', '绘画展示', '场景-其他', '推广页', '静态', '极端特写', '才艺展示', '教辅材料', '中景', '手机电脑录屏', '配音', '喜悦', '填充', '商品展示', '情景演绎', '宫格', '转场', '全景', '室内', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0deec90238ba123a0419af96c82074a6.mp4\n",
      "{\"video_ocr\": \"嫂子|我哥我哥找到了|他在哪呢|快快跟我走|在那边快|你干嘛去了|你急死我了|我被害惨了|听他们说|走路就能赚钱|我走了好几天|结果才赚了这么点|赚钱你怎么不用|现在还有|每邀请一个好友啊|了6元 最高有36元的现金呢|这些钱就是白给你的|它上不封顶啊|手机给我|刷两下|微信到账40元|哇|一下多了这么多钱呀|对啊|它就是白给你钱呀|这个快手极速版|在哪下载啊|点击视频下方|就能下载|我呀保证你能赚钱|焦息不安|要哭了|￥0.2|撒钱 活动|7上T|震惊|(78.98|看视频 现金赚不停|星拜们排上最温下|我的￥|我的零钱|手MCN伯乐计划# 口小朵M的作品原声 m|小朵M的作品原障|&丢小柔M 他真的是里们班|@云小朵M|玩个填空游：附()独尊无关派|死值|东听 #快手MCN伯乐计划|刷视频轻松赚钱|详情见活动规则，具体金额视任务完成情况而定|视频为演绎情节|为准|容钱通给自己加|云八 405.4w|3天前|9221\", \"video_asr\": \"嫂子，我哥我哥找了，他在哪呢？我快给我走在这边，快走了，绳子你干吗去了，你急死我了。|我爱穿多少双路就能赚钱，我怎么能好几天。|传来传转转点赚钱你怎么不用快手极速版啊，现在还有撒钱活动呢，每邀请一个好友啊，最高有三十六块钱的现金，那这些钱呀都是白给你的，这上不封顶啊，快手极速版手机着。|刷两下。|微信到账四十元，微信到账三十元，我要多了这么多钱呀，对啊，这就是白给你钱啊，这快手极速版在哪下载啊？现在点击视频下方就能下载完呀，保证你能赚钱。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.0160825252532959 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '静态', '填充', '多人情景剧', '配音', '平静', '手机电脑录屏', '场景-其他', '室外', '极端特写', '室内', '愤怒', '动态', '朋友&同事(平级)', '单人口播', '惊奇', '家', '亲子'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.97', '0.94', '0.91', '0.54', '0.24', '0.17', '0.07', '0.07', '0.06', '0.04', '0.02', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e039bf30fbf59380244b863bad8f38c.mp4\n",
      "{\"video_ocr\": \"爷爷这是我送您的寿礼|南海夜明珠|孙儿 有心了|你不会连寿礼都备不起吧|爷爷 这是我准备的|醒消 你也太不尊重爷爷了|就送这么把破扇子啊|竟然拿这种破烂来糊弄我|滚出去|赶紧把你这破扇子收起来 UC|拿着你这扇子赶紧滚|魏老|魂老你总算来了|老爷子 这是给您的|哎呀 果然是鉴宝专家|一看就是上品啊|先不说那个|这么好的一把扇子|是谁给扔出去了啊|这不是...|这可是古代皇帝御用的扇子啊|价值5000万|龙王殿》|堂|赶紧点击下方链接|热门小说等你来看|小说 阅读吧|J0|C，|UCCI|内容纯属虚构|养肝护肝|养肝|1U|てCUC|50堂|il|古年\", \"video_asr\": \"爷爷，这是我送您的寿礼，南海夜明珠。|三个有心了。|你不会连受理都背不起吧，爷爷这是我准备的。|你也太不尊重别人了，就从这把破扇子居然拿出顿破烂来糊弄我，滚出去，我单刷。|来看啊孩子们。|对啊，你总算来了，来了，这是给您的。|哎呀，果然是贱宝，真香，一看就是上品啊。|这么好的一把单。|是谁给扔出去。|这不是，这不是古代皇帝御用。|价值五千万。|千万千万。\"}\n",
      "multi-modal tagging model forward cost time: 0.01595759391784668 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '动态', '愤怒', '家', '平静', '极端特写', '朋友&同事(平级)', '特写', '家庭伦理', '办公室', '拉近', '悲伤', '夫妻&恋人&相亲', '单人口播', '工作职场', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.92', '0.88', '0.67', '0.42', '0.34', '0.31', '0.28', '0.18', '0.12', '0.12', '0.11', '0.06', '0.06', '0.05']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e145015aa92042b8dc0531fed1b76a8.mp4\n",
      "{\"video_ocr\": \"你看那船好不好看|妈妈|我也想坐船|你把这景色啊|描述两句|给你小姨听听|说得好呀|妈妈就带你坐船去|湖水倒映着蓝蓝的天|白白的云|小小的游船|BrpOAu|美丽极了|不对!|这叫湖水共长天一色|B00!|轻轲荡漾镜面开|波光潋滟|水晶帘动微风起|你平时摘抄背诵了|那么多写景作文|你看看你妹妹|作文不是靠摘抄背诵|就能写得好|最重要的是学会方法|来学而思网校 秋季语文特训班|北大中文系毕业的老师 带队授课|从基础的字词句积累|到阅读的高频考题|5大成文妙招|6大阅读体系|让孩子阅读写作高效提分|现在报名 9元10课时|还包邮赠送|超值教辅礼盒|各位家长们|赶紧点击视频下方 报名吧|￥ 390 好课+名师+教辅=轻松学|每天进步一点点一|小初高|专攻重难点，阅读写作高效提分 17年北大毕业教研打磨 10课时名师互动直播|立即报名|(具体教辅礼盒以收到实物为准)|/TT|在T|红TETE|BreaZ|在TNT|AT礼|4天班主任跟踪答疑|北京大学中文系|RON/|BIGol|全国 包邮|特惠 立省\", \"video_asr\": \"看看传好不好看，妈妈我也想坐船，你把这景色啊描述两句给你小姨听听，说得好呀，妈妈就带你坐船去。|湖水倒映着蓝蓝的天，白白的云，小小的游船，美丽极了。和对象湖水共长天一色，请客荡漾，镜面看。|佛光连夜，水晶帘动，微风起，你平时摘抄背诵了那么多写景作文，你看看你妹妹作为不是靠摘抄背诵就能写的好，最重要的是学会方法来学欢送网校秋季语文特训班，北大中文系毕业的老师带队授课，从基础的字词句积累到阅读的高频考题。|五大成文妙招，六大阅读体系，让孩子阅读写作高效提分，现在报名九元十课时还包邮赠送超值教辅礼盒！|各位家长们，赶紧点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016219615936279297 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '多人情景剧', '静态', '推广页', '平静', '亲子', '家庭伦理', '喜悦', '惊奇', '特写', '动态', '路人', '极端特写', '全景', '室内', '室外', '单人口播', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.41', '0.37', '0.08', '0.05', '0.04', '0.03', '0.03', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e28c88562280c3c816aacd2ccaf63c0.mp4\n",
      "{\"video_ocr\": \"发福利啦|刷到这个视频的朋友|恭喜你赚到了|现在疯读小说|限时发放免费手机|不用花一分钱|这些手机全是你们的|从今天起|凡是通过本条视频下载|疯读小说的用户|看50章小说|就送10个碎片|10个碎片已到账 你没有听错|你最想要的|免费P40手机|10个P40手机碎片|就能换|活动仅限本视频下方链接|下载的用户|活动初期|福利力度大|现在兑换手机|还包邮送到家|全程不花用户一分钱|不相信就下载试试|试试也不花钱|说不定你随便一试|就得到了这部|价值6000多元的手机|手机目前正在|疯狂派发中|数量有限|先到先得|别再犹豫了|赶紧点击下方链接下载|领走你的P40手机吧|免费下载|我家后山通仙界 主角在大城市被公司太子爷无|捡破烂成全球首富 捡玩具车，能得到全球仅有一|明日签到继续领碎片|读满10分钟必得稀有碎片，每日可得2枚 换一批|立即领取 李三响霉运缠身，触底反弹，|碎片明细>|五家读书福利 规则 旷世小神农|获得神农传承，带领山村致…|我的奖品|理由炒了鱿鱼后，回到老家…. 都市|辆的超级跑车你信吗？捡张…·|确认|领取|听满10分钟必行 G⌒批 稀有碎片|你已签到5天，别中断哟|仙山医|剩余65份 1/10|阅读15分钟|通仙界|P40碎片|三5|失足掉下山崖，却意外获得一 块玉牌，嗯，玉牌功能开发…|全道图|认真阅读文章|福利中心 规则|独家听书福利|具体奖励以活动规则为准，限时限量权枞喵|限时限量|明日|时世|开宝箱得碎片|读享快|签到提醒|FSC|04月08日星期三|小神衣|恭喜您|疯读 点击下方|08:08|涵凌104金 +|““家|去阅读|1碎片|6天|x2\", \"video_asr\": \"发布了发布力，刷到这个视频的朋友，恭喜你赚到了现在疯读小说，现实太方便了，最手机不用花一分钱。|新手机全是你们大同今天起，凡是通过本条视频下载疯读小说的用户，看五章小说就送十个碎片，你没有听错，你最想要的免费P字！|手机十七岁虞，手机碎片就能换，活动仅限本视频下方链接下载用户活动初期无意义多大，现在电话手机还包邮送到家，全程不花用户一分钱，不相信的。|再试试试试也不花钱，说不定比随便一试就得到了这部价值六千多元的手机。手机目前正在疯狂派发中，数量有限，先到先得，别再犹豫了！|赶紧点击下方链接，下载你说你的妻子的手机吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015942096710205078 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '推广页', '静态', '中景', '配音', '场景-其他', '多人情景剧', '喜悦', '单人口播', '平静', '动态', '室内', '惊奇', '办公室', '愤怒', '极端特写', '拉近', '工作职场', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.86', '0.73', '0.59', '0.41', '0.27', '0.25', '0.05', '0.04', '0.02', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e30da177efaff7736bea5ae7ff035c7.mp4\n",
      "{\"video_ocr\": \"…他们选择了永恒，纵然谄媚诬蔑视听|依然微笑着说:我很幸福。因为我还有一颗|我拥一缕最暖的;；灼灼红叶，我拾一片最|如果你失去了健康，你只失去了一小半;如|2020新高考改革后|ǖ题目越来越灵活|想考高分难上加难|有时候|eEm光语文这一科|就能和别人拉开|得语文者得天下|可千万不能再忽视了!|我是高途课堂张宁老师|非常擅长用表格来|总结归纳知识点和考点|因此同学们都亲切地称之我为表姐|在我的课上我会带你刷新|高中语文学习的认知|无论你是什么基础|G只要上完这节课|元定可以有所收获|如果你也想体验一下|欢迎点击视频下方|报名吧~|指，壮心不已;陶渊明悠然南山，饮酒采菊|不再享有健康的时候，那些最勇敢的人可以|盈盈月光，我掬一杯最清的;落落余辉，|高途课堂|华少|躬耕陇亩，这是高雅的选择。 大家都觉得高中语文|提分慢|拉不开分|全凭感觉和运气|但真的是这样吗?|新用户专享 立即体验|也不随其流扬其波，这是执着的选择;；纵|健康的心。甚至当我们连心也不再存在的时|热的；萋萋芳草，我摘一束最灿的;；漫漫人|果你失去了诚信，那你就几乎一贫如洗了|CENTURY|然马革裹尸，魂归狼烟，只是豪壮的选择;|候，那些人类最优秀的分子仍旧可以对宇宙 大声说:我很幸福。因为我曾经生活过。|生，我要采撷世间最重的--—毅力。|纵然一身清苦，终日难饱，也愿怡然自乐|新用户专享 立即体验 浙江卫视指定在线教育品牌|楚大夫沉吟泽畔，九死不悔;魏武帝扬鞭东|很幸福。因为我们还有健康的身体。当我们|实用干货|全国百佳教师带队教学 平均教龄11年|站在历史的海岸漫溯那一道道历史沟渠:|当我们一无所有的时候，我们也能够说:我|让老师惊艳的开头|高途课堂名师特训班|名师特训班|适用主题：选择|学员最高提分|0-40 六|浙江卫视|张宁|30-40分|仅需 ￥9|尝\", \"video_asr\": \"大家都觉得高中语文提分慢，拉不开分全凭感觉和运气，但真的是这样吗？二零二零新高考改革后，题目越来越灵活，想考高分难上加难，有时候光语文这一科就能和别人拉开三到四十分的差距，得语文者得天下，可千万不能再忽视了！|我是高途课堂的张宁老师，非常擅长用表格来总结，归纳知识点和考点，因此同学们都亲切的称之我为表姐。在我的课上，我会带你刷新高中语文学习的认知，无论你是什么基础，只要上完这一节课，一定可以有所收获，如果你也想体验一下，欢迎点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01604747772216797 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '单人口播', '中景', '教师(教授)', '静态', '场景-其他', '家', '室内', '平静', '手机电脑录屏', '配音', '幻灯片轮播', '手写解题', '图文快闪', '重点圈画', '学校', '特写', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.20', '0.11', '0.09', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e41edcaff235e029abaf447e465f141.mp4\n",
      "{\"video_ocr\": \"city 城市|城市|city 城市|你家孩子是不是|语感特别差|英语成绩跟不上|我给你推荐|伴鱼自然拼读课|现在面向|推出14节自然拼读课|让孩子掌握26个字母|教孩子学英语|力求让孩子做到见词能读|听音能写|而且课程|2698元的全套课程|现在仅需体验价|按要求完课之后|名额有限|快点击下方链接报名吧|cycle 自行车|cle自行车|车|英语不敢开口|全国3-10岁的孩子|及字母组合的发音规则|就像学拼音一样自然|还支持无限次回放|还包邮赠送学习成长地图|29元14节课|还可以领取奖学金|自然拼读记单词 坛更高效|比音标更高效|【s]在e、i、y前|【k]|、i、y前|k)|【s]在e、i、y前|AB|字母C|比音|cup杯子|car|cap帽う|cell细胞|细胞|ce|cat 小猫\", \"video_asr\": \"自然拼读记单词，今天学字母C，在单词的发音大多数情况下他撒。|但是在字母E I Y前面的发，比如说杯子卡，帽子CAT，小猫CAT，细胞FEEL，城市CITIES。|你家孩子是不是英语不敢开，口语感特别差，英语成绩跟不上，我给你推荐伴鱼自然拼读课，伴鱼自然拼读课现在面向全国三到十岁的孩子。|出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样，自然，必须让孩子做到见词能读。|应能写，而且课程还支持无限次回放，现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价。|二十九元十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧TEL FISH FOX！\"}\n",
      "multi-modal tagging model forward cost time: 0.016118764877319336 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '配音', '中景', '场景-其他', '单人口播', '办公室', '静态', '室内', '平静', '课件展示', '手机电脑录屏', '动画', '知识讲解', '喜悦', '家', '动态', '特写', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.83', '0.79', '0.77', '0.70', '0.51', '0.22', '0.03', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e475d3197e3ba34bef24cb4f3b52873.mp4\n",
      "{\"video_ocr\": \"一定要看|我想报名作业帮直播课|语文阅读写作提分班|我同学都在用|这个学语文|好|但是啊不需要这么多|来都还给你|只要29块钱就够啦|2/元|真的吗|29块钱能学什么啊|不要小看这29块钱|还有3大作文主题|8种解题大招|哇 好超值啊|这些还不够来 给我|这是我们这三年内|课程无限次回放服务|清北毕业名师带队授课|课后辅导老师|全程一对一答疑|给到大家|还有这个|超值学习大礼盒|免费包邮赠送|里面的教辅教材|都是孩子用得到的|那我怎么报名啊|点击视频下方链接|选择您家孩子对应年级|就可以报名啦|初一机二语文成绩|6个重难题型答题模板|课后辅导老师全程一对一答疑|上不去|74个必备阅读写作大招|12种语文思维能力|￥29=20节名师课|想在初三逆袭的孩子|以及5种语文学习方法|高师有大限|00|新级更高效|上课内容与收到礼盒请以实际为准|5可道高预青题|语文阅读 写作提分班|作业帮直播课|优秀作文集\", \"video_asr\": \"想在初三逆袭的孩子一定要看，我想报名作业，帮直播课语文阅读写作提分班，我同学都在用这学语文好，但是啊，不需要这么多来都还给你，只要二十九块钱就够了，真的吗？二十九块钱能学什么啊？不要小看这二十九块钱。|来六个重难题型答题模版，七十四个必备阅读写作大招，十二种语文思维能力，以及五种语文学习方法，还有三大作文主题，八种解题大招和五十七的高频考题。哇，超级这些呀，还不够来给我，这是我们三年内课程无限次回放服务。|清北毕业名师带队授课！|课后辅导老师全程一对一答疑给到大家，还有这个超值学习大礼盒免费包邮赠送，里面的教辅教材都是孩子得到的，那我怎么报名啊？点击视频下方链接，选择你家孩子这个人。|就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01604628562927246 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '平静', '单人口播', '配音', '静态', '场景-其他', '家', '室内', '动态', '极端特写', '教辅材料', '混剪', '学校', '特写', '情景演绎', '手写解题', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.94', '0.85', '0.83', '0.22', '0.21', '0.20', '0.06', '0.03', '0.01', '0.01', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e54887ca2117fddfa8331f00bce6bed.mp4\n",
      "{\"video_ocr\": \"我在此|仅代表米读极速版|向大家郑重承诺|绝不会像市面上|那些刷视频、走走路|就能赚钱的软件一样|昧着良心|每次提现只有几毛钱|甚至几分钱|这种行为|简直是在欺骗用户|现在|您只要下载米读极速版|就会有100元现金红包|而且你只需像往常 一样|看看小说|也有金额不等的红包奖励|我们会在第 凡A时间|萃时间”|时间|给您兑换成现金|而且提现秒到账|听清楚了|现在通过下方链接下载|登录就能直接领取|100元的现金红包|我们从不搞虚的|如果您不相信|现在就点击屏幕下方链接|下载试试吧|￥|“关糯现|来自您考的红包|史上最强炼气期|方羽其|b，枫认真地观察，发现床|惊不已。 明明昌唐规出巻|言不发，气氛很阴郁。 回去的路上，所有人都一|大老 走到|100.00元|翻倍|立即|羽的师父渡劫成 基期。 多炼补ニ|我说了，夏修之已怪去|“哥！“漂亮女孩尖叫。 那四名保镣反应过来，立|唐小柔黛眉微蹙，喃 我总感觉…..方羽有点眼|“方羽哥哥对我最好了|过去了，方羽仍 筑基期。|直接踏步走进了草房。 后，他就看到躺在床上|击，整个人往后飞去，摔倒在 地也。|专爷子命令，他也长好跟着|时候，一位背着书包的女孩小|个法渡劫成|唐似眼中泛看希望的|自身反倒道安到一股巨力的撞|唐枫虫不日心，但既然|就在万羽给鸡冻刑酱料的|就再没有人关|肯定 皱眉，对于唐枫闯入草|“不准动手！”坐在轮椅上 的唐老爷子用嘶哑的声音命令|“这怎么可能?我们这是 第一次来到西北地区，|地的家门。 这女孩就是|来自作者的包|文该死的炼 他师父也觉得是|的资 闭的夏修之。 上的|在场其他人脸色大变，|“好香啊 方羽 开，我|事，方羽心|门才 你们可以回去了。“方|即往前几步，走到方羽的身前。|熟，好像在哪里随。”|女孩高兴地说道，然后走|站起身来， 的流逝，地球上唐暂哺植药|动有点不满。 很天然想到体6纯专妙|可能跟这个方羽见迹故事纯扈声钢|数的住户， F玥玥，正读高一.本故事纯属虚构|古代言情|层。而 衰老的 继续阅读赚更|终于 夫制|地了?|你在想什么事情？”|”方 的说道。|完成金本阅读即可全部提现 立即领取|看小视频翻倍领取红包|左滑阅读¥钱|离开了地球。|炼到烁|希望破灭，浑身都失去|这少年 反而到|“小柔，|鸡，|0.6元|唐意到一旁的妹妹表|已存入零钱，可转入零钱通赚收益〉|03元 活动现金红包 全击|全部 现代言情 古代言情 都市 玄幻 科|通过看书、互|4天5天|2天 3天 4天5天  天|3天4天5天6天|6天|米读极速版的红包 恭喜发财，大吉大利|阅读指定小说可 立I提现0.3元 当前|阅读指定小说可直接提现|史上最强|已签0/30天，30天领大奖|30天领大奖 签0/30天|中国联通|85%|目余|过要渡劫|离开。|电进院子。|用米读极速版 轻松领现金红包|具体奖励以APP内活动规则为准|当前进度0%|0%|本故事纯属虚构|待久一点。|险色|500|388|金额|都市|动共获得|¥100|58|极速 米读|玄幻\", \"video_asr\": \"我在此谨代表米读极速版向大家郑重承诺，我们米读极速版绝不会像市面上那些刷视频。|走路就能赚钱的软件一样昧着良心，每次提现只有几毛钱，甚至几分钱，这种行为简直在欺骗用户。|现在您只需要下载米读极速版，就会有一百元现金红包，而且你只需要像往常一样看看小说，也有金额不等的红包奖励。|我们会在第一时间给您兑换成现金，而且提现秒到账，听清楚了，现在通过下方链接下载登录，就能直接领取一百元的现金红包，我们从不搞虚的，如果您不相信，现在就点击屏幕下方链接下载试试吧！|快快快快快快。\"}\n",
      "multi-modal tagging model forward cost time: 0.022223949432373047 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '单人口播', '影棚幕布', '手机电脑录屏', '场景-其他', '平静', '配音', '拉远', '办公室', '室内', '工作职场', '喜悦', '全景', '游戏画面', '动画', '红包', '上下级'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.76', '0.51', '0.47', '0.15', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e62d3957736b045ea5d68cc8f50de30.mp4\n",
      "{\"video_ocr\": \"老虎-老虎-tiger|tiger-tiger atiger|tiger tiger tiger|熊猫-熊猫 panda|panda-panda panda|狮子狮子 lion|lion3lion lion|狗熊 狗熊bear|bear bear bear|斑马AI课英语体验课|由猿辅导在线教育出品|适合2-8岁儿童|北美外教趣味教学|综合丰富的场景A互动|锻炼孩子听说读写能力|让孩子脱离枯燥乏味的学习|平板手机都能看|孩子随时随地轻松学习英语|家长更省心|现在报名|78元10节外教AI课|加10节思维课|抓住孩子|学习英语 数学的黄金启蒙期|打好学习基础|报名还包邮赠送两套|价值216元的配套教具礼盒|数量有限先到先得|赶快点击视频报名吧|我专|猿辅导在线教育 出品|猿辅霸y|狼辅事：大教育|导在线积|瘪得导在线秘啊|辅导在线敦蜀|猿辅导 在线飘翼|猿辅导在技驭菁|熊猫|3子|拍熊|逻辑训练|At th A Funny|2-8岁上斑马学思维 学英语|老庆|panda|lion|bear|斑马A课|英语|\\\"bconut|dos|Dad likes climbing.|The Pet Monkey|unny|ORD|A|三一|出品\", \"video_asr\": \"老板老虎TIGER。|TIGER TIGER TIGER熊猫熊猫PANDA。|PANDA，潘达潘达狮子狮子LINE。|LY LY LY狗熊狗熊。|贝尔贝尔贝尔斑马AI课用于甜可有猿辅导，在线教育出品，适合二到八岁儿童，北美外教趣味教学，综合丰富的场景AI互动，锻炼孩子听说读写能力，让孩子脱离枯燥乏味的学习，平板手机都能看，孩子随时随地轻松学习英语，家长更省心，现在报名七十八元，十节外教AI课加时间。|思维课抓住孩子学习英语数学的黄金启蒙期，打好学习基础，报名还包邮赠送两套价值二百一十六元的配套教具礼盒，数量有限，先到先得，赶快点击视频报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016448497772216797 sec\n",
      "{'result': [{'labels': ['推广页', '配音', '现代', '场景-其他', '填充', '教辅材料', '课件展示', '平静', '静态', '极端特写', '动画', '宫格', '手机电脑录屏', '影棚幕布', '中景', '室内', '全景', '知识讲解', '特写', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.75', '0.50', '0.33', '0.19', '0.02', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e7101a1723333c219b57e15a08998c0.mp4\n",
      "{\"video_ocr\": \"果果|这数学呀|要做题的时候一定要 列算式啊|它和它相乘|我跟你说过多少回了|不要用这种老套的方法 教孩子做数学|你不列算式它题目做得 出来吗|106×108等于多少|前尾数加后尾数写前面|答案等于11448|厉害|咱闺女什么时候变得 这么厉害啊|我给她报了学而思网校|秋季数学特训班|清华哈佛毕业的老师|教孩子解题技巧|3类巧算方法加数感训练|不用列竖式就能快速 得出答案|还有小学5大题型的 应用题技巧|让孩子举一反三搞定压轴题|那她课程学这么多|孩子跟得上吗|课后呀还有班主任老师 一对一在线辅导|直到孩子呀学会为止|好好好|这课真好|那|应该不便宜吧|9块钱10课时|还赠送这个超值大礼盒|各位家长朋友们|你们也快点击下方|查看详情报名吧|大ZZIxIE -66x6z =8TxSZr|10课时名师互动直播 4天班主任跟踪答疑|¥390好课+名师+教辅轻松学|好课+名师+教辅=轻松学|(具体教辅礼盒以收到实物为准)|专攻重难点，紧扣考纲高效提分|小初各年级礼盒不同，请以收到实物为准|17年清华毕业教研打磨|学而思网校 一每天进步一点点一|-6XLLL OIxL0T|全国 包邮|一点点|每天进|学而|哈佛大学教育学|一TTx88 =6x999|pC Kr|JC RTH|立即报名|立省 ￥390|-LIXIII|小初高|杨佳|¥9|限时\", \"video_asr\": \"老板这数学呀，要做题时一定要领钻石啊，他就是。|哎，我给你说了多少回啦，不要用这种老套的方法教孩子做数学，你不列算式的题目做的出来吗？不关一百零六乘一百零八个赞，不想稳暑假后尾数写前面，尾做成后，尾数减后面。|答案等于一万一千四百四十八。|什么时候才能真正厉害啊？因为呀，我给他报了学而思网校秋季数学特训班，清华哈佛毕业的老师教孩子解题技巧，三类巧算方法，加速感训练，不用列竖式就能快速得出答案。还有小学五大题型的应用题技巧，让孩子举一反三搞定压轴题，一要科学的多。|什么课后呀，还有辅导老师自己在家不知道孩子呀，学会为止好好好，这可真好，那应该同孩子就买点吃合适。|从这里呀，超值大礼，各位家长朋友们在视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016251325607299805 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '亲子', '喜悦', '家庭伦理', '平静', '家', '愤怒', '夫妻&恋人&相亲', '极端特写', '单人口播', '室内', '特写', '悲伤', '手机电脑录屏', '动态', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.86', '0.84', '0.84', '0.41', '0.34', '0.25', '0.21', '0.11', '0.05', '0.04', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e7eb01481bf6de2586db8a55188a6f5.mp4\n",
      "{\"video_ocr\": \"没点P丝(1。)|点P丝烬(1）有题遮，有|设点P丝七 由A(un|由A(u'心).Bcao|由Alao).B(ao)得k4|由A(x心).B(ao)得《AP-A|投点P3丝奴(1）有题遮，看釜|点P台丝标1)有题嬷，有各+|进点P丝烁(1）有题意|由A(uo),B(ao)得人A-杰品a|是高途课堂周帅老师|北京大学毕业|省级高考状元|高考数学满分|从事高中数学教学15年|439个知识点|167个考点|80个易错点|57个难点失分点|都是有规律可循的|跟对老师|就能掌握高效的学习方法|不管今天周几|每天都是周帅|不管你基础如何|我都能帮到你|立即报名|例1 设福圈。会-la>h>0的左，右顶点分别|例1 设输四一-1@h 0的左、右顶点分别方|例1 设概国。卡1asb0)的左，右顶点分别为人B，点P看 两点，0为坐标潮点，若直线 ：中P与 柳的科率之积为，|两点，0为坐标原点 若直线 4P与 a的料率之积为，一！，水稀服的期|tab o的左、右顶点分别为人，的，点户在换菌上且路于|tab>o的左、右顶点分别为A8超儿在邮|男1 设概四。告1a>6>0的左，右顶点分别为人队，、点P在备国上超身于|例1 设锅国。-a对b>0的左、右预点分别为4B.点P在|ta小>0的左、右质点分别为人，股，点P在极圆上且身于4#|例1 设桶直。会-a>b 0的左、右顶点分别为A8，点P在装伽上且异于4.房|特1 设锅魔。会-1>b>0的左，右顶点分别为人8 点/在能伽上且身于4.R|二10|例1设微国子后ta3bo的左、右顾点分别为48点P在稀国上日穿于A，B|例1 设懒国子子1as的 0的左，右质点分别为4在唤图上目异于A.R|例1 面心率|解题快且清晰|新学员9元专享|不会任|高途课堂 名师特训班|省高考状元带队授课 老师平均教龄11年+|解题慢 思维混乱|K|KBP二|高途\", \"video_asr\": \"高中数学能从六十分提高到一百四十分吗？不用怀疑，你也可以做到。数学其实很简单，只要掌握了方法，我是高途课堂周帅老师。|北京大学毕业，省级高考状元，高考数学满分，从事高中数学教学十五年，高考数学四百三十九个知识点，一百六十七个考点，八十个易错点，五十七个难点，失分点都是有规律可循的，跟对老师就能掌握高效的学习方法。不管今天周几，每天都是周帅，不管你基础如何，我都能帮到你，查看详情，在线报名。|克。\"}\n",
      "multi-modal tagging model forward cost time: 0.01610732078552246 sec\n",
      "{'result': [{'labels': ['填充', '现代', '单人口播', '推广页', '中景', '场景-其他', '配音', '教师(教授)', '影棚幕布', '静态', '手机电脑录屏', '平静', '极端特写', '手写解题', '室内', '教辅材料', '动态', '课件展示', '知识讲解', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.84', '0.44', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e8d40d95e47794bc46bccdab7e02d04.mp4\n",
      "{\"video_ocr\": \"我必须再强调一遍|价值300元的健康咨询体验金|现在免费领|免费领了!|这款由水滴保险商城|推出的图文问诊服务|你千万别不重视|BACTER0A|作为家里的顶梁柱|倘若不幸生病|所有症状|不限病种|随时随地有专业医生|为您在线会诊|不用排队|就享受一对一在线咨询|免排队免请假免挂号|在线图文问诊|为你的健康保驾护航|现在点击视频下方链接|输入手机号就可以免费领取|凭手机号免费领取|水滴 保险商城|CHs|LLNESS|水滴 保院商城|RESEARC|OH|syntheszer ge|emtlt) formatsb)|THREATMENT|CLN|ADVAN|NOVATION|BIOLOGY|MODERN|CHEMSTRY|VRUS|LABORATORY|ADVANCED rebumcad|f (argNameMatches part returm cachedFormater|fon (ntpartinde|(parphdex =nextT|NEUROLOGY|HOSPITAL|protectedSmmng fomatwhtgo ot Suing lertcale,|X-RAY|if (argName|returnnul for lintpartindex=0|if (argNumber sMe sagePatternA|retume|NURSE|ULTRASOU|String templtte ObjectlA aeueq TAexoreuezone|Timezonetnnezone) Object l] arguments|MessageFormiat mf =new MessageF|ormats li]|CAROICLCGYh|CARDIOLOG|CLINC|TECHNMOLO|protected String fomatwt tie|protected String formatwithTimezone|ADV|Int argNumber - MessagePa|nt argNumber -i sagePattern.vali|NF cachedFormatters e nu|IFEATMEN|publt Formuat getFomatByAgumend. Nif (cachedFomatters --nul) 〔|pitFommutgeir omatByiumenta|R2|LAB|SyI|ptbicF|Locaielocaie|TEOH-NOLOGY|HgC|BIOLOGY|SCENCE N|MRI rni set. aese bncaeg|Q,setVorce (voce)|synther?t|bicFommat getFor|retumnmt ptn3t 信|十|免费领取300元|MEDICAL|健康咨询体验金|MessagsFeomatmfe new Mess mf setLocae (locae)|mf appyPattern (templa|serQUEtE EMTY) synt|bd String fontatwtn tmeZone template|argumentName)|H0|mat) fommats h]|partindex =!|shesktext nil)|DE|DENTIST|retum caehedFormatters get|vt n orpktersget lpartird|ehur!|ormat)\", \"video_asr\": \"我必须再强调一遍，价值三百元的健康咨询体验金，现在免费领，免费领了这款由水滴保险商城。|出的图文问诊服务，你千万别不重视，作为家里的顶梁柱，倘若不幸生病，所有症状无限病重，随时随地有专业医生为您在线会员，不用排队修！|享受一对一在线咨询！|免排队，免抵押，免挂号！|见图文问诊，为已经健康保驾护航，现在点击视频下方链接，输入手机号就可以免费领取！\"}\n",
      "multi-modal tagging model forward cost time: 0.016204118728637695 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '单人口播', '中景', '影棚幕布', '平静', '配音', '全景', '喜悦', '场景-其他', '手机电脑录屏', '特写', '室外', '教辅材料', '极端特写', '拉近', '室内', '单人情景剧', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e9d0b37992bf4ed2b0c577b22131543.mp4\n",
      "{\"video_ocr\": \"少爷 我们最近让西方第一家族损失了十几亿|您可要小心|给我上|痛痛|秦浩|凝霜已经是我妻子|请你放尊重一点|你放心|等我得到她之后|我会好好尊重她|给脸不要脸|你敢动我|还好今天是我们一家人团圆的日子|否购你这只手得断了|凝霜 我回来了|咱们的女儿|自从妈带她见了秦浩之后|她就走丢了|我女儿呢|你不是挺能吗|自已找啊|秦家没有存在的必要了|我秦家数十亿资产|岂是你|少爷早已准备好了|秦家完了|全本免费|立即点击下方 看更多热门小说|6年历练荣耀回 归|老婆孩子遭人欺负|如今的他只手遮天|敢觊觎我老婆的人|本故事纯属虚构|小说\", \"video_asr\": \"消炎，我们最近讲西方第一家族损失了十几亿，你可要小心。|圣。|钱浩明庄已经是我的妻子。|请放尊重点，你放心，等我得到他之后，我会好好尊重他，等你。|要命。|动物疼痛。|好，今天就一单。|否则你这是首领哇。|你说我回来了，等了个女儿。|在当前的情况就等着。|别啊时候。|我女儿呢，你不是挺能耐，你找啊，秦家没有存在的必要了，我秦家数十亿资产。|谁是你少爷早已准备把钱交完了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01576709747314453 sec\n",
      "{'result': [{'labels': ['中景', '现代', '静态', '推广页', '填充', '极端特写', '特写', '多人情景剧', '惊奇', '动态', '愤怒', '喜悦', '室外', '夫妻&恋人&相亲', '悲伤', '厌恶', '室内', '手机电脑录屏', '全景', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.94', '0.83', '0.67', '0.64', '0.56', '0.55', '0.51', '0.28', '0.27', '0.07', '0.04', '0.03', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0e9fdfe45e8bca624c28c30b31863c3e.mp4\n",
      "{\"video_ocr\": \"744%|￥85950.00|14.05% 7.44% 13.30%|图可知，2月份销售额下降，主要因为医院春节放假及城市外来人口返乡因素影响:其它月份销售额较为稳定|10k-20k|标准色|各渠道每月度及总体销售数据情况，包括各渠道销售额占比，同比增长，环比增长，月均增长率，销售新排序等关键数据分析。使用方法:本年度各月数据来 手度相关数据及所有渠道或者代理商名称(要求与基础数据表一致、且渠道数量完整)即可，其他数据及图表为自动生成，请勿随意录入数据，|数据分析，使用方法:本年度各月数据来源|3012662828 236781 2818-81-15.星期五|object 时间|t(salesDF.describe())|intSer-pd.Series(groupby1['实收金额'.sum())|labels.append(key) count.append(value)|网络资源轻松爬取|轻松爬取|网络资源轻松爬取k以上，为了更加方便我们分析，取每个|¥120，00.00 ¥0.00|26.99%|91.50% 129.80%|ort matptotlib.pyptot as plt|ort numpy as np|for i,p in enumerate fwedges) np.cos(np-deg2rad(ang))|医、看一下要求。|设计 设计风格 :涉及 4射击|玉安|营销实战工具—销售业务管理 2017上半年各渠道商总体销售数据对比分析(全自动，第1名醒目提示)|216.8|从上述可以看到，学历要求和工作经验的值比较少且没有缺失值与异常值，可以直接进行分析:但薪资的分布比较多，总计有75种，为了更好地进行分析。|宋体|销售业务管理|View sDF.dtypes|818-01-01|m-Lambda x:x.month|御案 狱轩|/ss3.bdstatic.con/78cFvBShQ1YnxGkpoMK1HFhhy/it/u-2692971166，1224397637f=26gp-0.jpg|2016上半年合 排名|占比|名称|工作经验|十课吧|常规|条件格式|Markcdown|图片设|aikeba|给图 页面布局 公式 数据 审阅 视图|AA|Kemel Widgets|社保卡号|服务|Cell|analyze最后检查:12分钟前未保存改变 啬看 插入 羊元格|yter analyze最后检量:12分钟前 (朱保存改变|歌鄢kba|k哥ikba|82.60%|18jp9|AmountSer[7]|ath.exists(dir):|atRate=TimesSer[query].sum()/|return salary_dic|2.04%|金颁|pe:object fLoat64|Insert|ViewInsertCell|yter|就去1诛0|；Help Kemel  Widgets|学大数据|文金额|count—t|代码|长|卡号|购率\", \"video_asr\": \"三。|三三。|三。|我。|三三三三三三三三。|三。\"}\n",
      "multi-modal tagging model forward cost time: 0.01636219024658203 sec\n",
      "{'result': [{'labels': ['场景-其他', '手机电脑录屏', '现代', '推广页', '才艺展示', '配音', '静态', '图文快闪', '特写', '中景', '商品展示', '极端特写', '重点圈画', '混剪', '情景演绎', '教辅材料', '手写解题', '喜悦', '平静', '幻灯片轮播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0eae30e66a83a65d64fc4b4c8d74f1fd.mp4\n",
      "{\"video_ocr\": \"七猫免费小说APP|爹地|礼物送到|请签收|公司系统全部瘫痪了|并且集团账户少了两亿|什么|查|给我查|我倒要看看是谁在跟我作对|亲爱的爹地|我只能帮你到这儿啦|《天才萌宝:爹地债主我来啦》|萌 地债我来啦》|七猫免费小说|免费看书100年\", \"video_asr\": \"D礼物送到请轻松，公司系统全部瘫痪了，并且集团账号少了两个，什么给我查。|倒要看看是谁在跟我作对。|我只能帮你到这了。\"}\n",
      "multi-modal tagging model forward cost time: 0.0162351131439209 sec\n",
      "{'result': [{'labels': ['现代', '中景', '特写', '静态', '推广页', '平静', '多人情景剧', '愤怒', '极端特写', '单人口播', '填充', '室内', '配音', '动态', '室外', '朋友&同事(平级)', '手机电脑录屏', '亲子', '悲伤', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.93', '0.90', '0.83', '0.50', '0.22', '0.15', '0.08', '0.04', '0.02', '0.02', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0eb269dd0cd08921b16acf04afee8d20.mp4\n",
      "{\"video_ocr\": \"你确定不来扶一下|你的小可爱吗|大长腿|这个月停车费交一下|420|刷我的卡牛瑞贷额度吧|你刚刚刷的是什么呀|卡牛瑞贷啊|最高可借20万|最快3分钟放款|最长可以分12期|慢慢还|那我也能申请吗|可以啊|点击视频下方链接|凭信用卡就可以|申请你的额度啦|息费减免优 本活动规则为准|息费减免优惠以具现则为准|同要减气优惠以具体活动规则为准|知微惠以具体活动规则为准|4年196天 1172030|实际额度以批为准|实际额度以银行审 |正以银行审批为准|文际额度以银行审批为准|际莉良以银行审此为淮|四你都鹰以银行审此为准|真体额度费莘际情况审批为准|具体额度费率根担厂情况审批为准|黑休额费率根据实际情况审批为准|很据实况审批为准|惆如际…况审批为准|你度率恨据实际情况审批为准|是仙度变字恨椐英际情况审批为准|Drag|200.000.00|我没有信用卡 借不到 我来赔|去拿钱|核载人数55人|Dragon Sa|借一万量日材息低整2元，随借随压|被拒目期付188元|信用管家|月管家|信瑜家|信用|租|PAS46|Dragon Safety|付款码|H，您当前最高可借额度|我的|抢宝|慈当|凭信用卡申请/最高可借20万|趴您回|28卫81G|凯您回眸|卡牛|瑞贷|韩BAS￥46|#生|牙齿|1包4|试试你|蘸岁\", \"video_asr\": \"我吃。|你确定不叫你小可爱吗？大长腿这个月停车费交一下四百二刷我的卡牛人大额度。|你刚刚说的是什么呀？卡牛瑞，大家最高可借二十万，最快三分钟放款，最长可以分十二期慢慢还，那我也能申请吗？可以啊，点击视频下方链接，凭信用卡就可以申请你的额度啊。\"}\n",
      "multi-modal tagging model forward cost time: 0.016570568084716797 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '手机电脑录屏', '多人情景剧', '极端特写', '特写', '喜悦', '夫妻&恋人&相亲', '朋友&同事(平级)', '单人口播', '愤怒', '惊奇', '(马路边的)人行道', '室外', '配音', '平静', '悲伤', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.90', '0.88', '0.43', '0.28', '0.24', '0.20', '0.16', '0.02', '0.02', '0.02', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0eb3a02e1623fdab24acbe64a61eb83c.mp4\n",
      "{\"video_ocr\": \"术|在线少儿美术|1对1\", \"video_asr\": \"\"}\n",
      "multi-modal tagging model forward cost time: 0.015892744064331055 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '绘画展示', '场景-其他', '极端特写', '静态', '才艺展示', '教辅材料', '配音', '填充', '幻灯片轮播', '商品展示', '拉近', '过渡页', '室内', '手写解题', '手机电脑录屏', '图文快闪', '喜悦', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.94', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ec2f62970c02e0e5353c81a66dad8d8.mp4\n",
      "{\"video_ocr\": \"为什么上了高中之后|很多孩子突然跟不上了|其实|大多是数学拉了后腿|小题全靠蒙|大题没思路|有时候光数学这一科|就能拉开30-50分的差距|其实高中数学真的很简单|如果只靠自己揣摩|不仅会走很多弯路|还会消磨兴趣!|为何不让孩学一开始|就跟着专业的人学呢?|我是高途课堂张宇老师|专注高中数学一线教育 9年|总结了高中数学|识|带你轻松涨分!|只需|只需9元 还在等什么|点击视频下方|报名吧!|RAPT|LAUNCH|高中数学资深主讲老师|CH|6大|知识板块|52个|知识点|32|提分技巧|浙江卫视指定在线教育品牌|新用户专享 立即体验|[查看详情〗|浙江卫视|高途课堂 ，浙江卫视|高途课堂|全国百佳教师带队教学 平均教龄11年|￥9|M，|夕n|加啤|名师特训班|华少|张宇\", \"video_asr\": \"为什么上了高中之后，很多孩子突然跟不上了？其实大多是数学拉了后腿，小题全靠蒙，大题没思路。|时候光数学这一科就能拉开三十到五十分的差距，其实高中数学真的很简单，如果只靠自己揣摩，不仅会走很多弯路，还会消磨兴趣，为何不让孩子一开始就跟着专业的人学呢？我是高途课堂张宇老师，专注高中数学一线教育九年。|总结了高中数学六大知识板块，五十二个核心知识点，以及三十二积分技巧，带你轻松涨分。|只需九元，还在等什么，点击视频下方查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016049623489379883 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '中景', '静态', '平静', '多人情景剧', '单人口播', '教师(教授)', '室内', '极端特写', '家庭伦理', '喜悦', '惊奇', '影棚幕布', '全景', '拉近', '情景演绎', '转场', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.92', '0.89', '0.72', '0.70', '0.10', '0.06', '0.05', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ecadb7a7818f04f9cbc98a7621fccb9.mp4\n",
      "{\"video_ocr\": \"吃这个吧|莉总|不吃饭|怎么能好好工作呢|就是前段时间|刚买了房|每个月的工资|除了日常开销|就全都还房贷了|钱不是攒出来的|你要学会让钱生钱|钱生钱|我也想啊|可是我收入。|本身就不高|也没敢奢望去理财|其实啊无论收入多少|都能通过|科学的理财方法|学习用钱赚钱|来|推荐你报名|微淼商学院的|小白理财课|学会利用复利|学会分散投资|科学购买|国债基金和股票|获得非工资收入|可是|我怕我学不会啊|这么好的课程|肯定不便宜吧|放心|不贵的|现在只需要12元|可享12天线上课程|而且呀|有专业名师|手把手教你学理财|所以啊|0基础也能学会的|那快告诉我怎么报名|如果你也想学习理财|赶快点击查看详情|报名吧|+86|获取社证锅|你首吃台用层你还信不过一场大病7|你省吃位用居然还撑不过一场大病?？ 你要存多少年才能凑够买房的首付？|理财小白 基础为零，希望花了时间就能学会|没存双，想裴拥有自己的小全区|想提升家庭财力的全职妈妈 家庭主妇|月薪3000%何开启财务音由之路 为什么越穷越要理财|限量抢购 为什么要学理财?|你能承受几次间人要钱建到的白限?|你翠量奇斗却赶不上爸妈日新老去?|适合人群|踩坑老手|如何从等开始增加非工资收入 如何解别投费抢阱|12元|特价12元|微森小白训练营手把手带你 轻松学理财|月光一族|零基础学习并不难|课程设计师:封贸|从理财小白到精有人生第一杨金|上班太忙，需要快速掌握理财思路 工作狂人|选择找们，12天可以学到以下知识|对课程不演意，无条件速款|你能杀爱几次间人要技速到购a租？|盲目投资，急需扭转亏多赚少局面|专为零基础同学设计，不需要数学基础|训练营|小白理豺训练爱|特价|17:16|适合小白的理财工具有害烧|提高财商学习富人服|檀高财费学习富人恩耀|训缔|44|抢购\", \"video_asr\": \"ZZZZ ZZZZ。|吃这个吧，那总不吃饭，咱们得好了。工作呢，就是前段时间刚买了房，每个月的工资除了日常开销就全都还房贷了，钱不是攒出来的，你要学会让钱生钱，钱生钱我也想啊，可是我收入本身就不高，也没赶上。|学理财啊，其实啊，无论收入多少，都能通过科学的理财方法学习，用钱赚钱来。|推荐你报名微秒商学院的小白理财课，学会利用复利，学会分散投资，科学购买国债，基金和股票，获得非工资收入，可是我怕我学不会啊。|这么好的课程肯定不便宜吧，放心不会的。|现在只需要十二元可享十二天的线上课程，而且呀，有专业名师手把手教你学理财，所以啊，零基础也能学会的那个告诉怎么报名啊？如果你也想学习理财，赶快点击查看详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016314029693603516 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '多人情景剧', '静态', '推广页', '极端特写', '喜悦', '特写', '夫妻&恋人&相亲', '愤怒', '惊奇', '填充', '单人口播', '平静', '家', '朋友&同事(平级)', '动态', '商场', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.91', '0.77', '0.58', '0.57', '0.28', '0.27', '0.13', '0.09', '0.08', '0.05', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0eda8f0f518f9feeb6ca1a84e2645592.mp4\n",
      "{\"video_ocr\": \"我是一个家庭的顶梁柱|今天|给你们看看我的账本|每个月的工资5500元|每个月要还房贷2000元|孩子的学费|家庭生活费|等等开支2300元|都说养儿防老|每个月给父母的生活费 也不能少|没有存款的家庭|拿什么来抵挡意外风险|9月份在同事的推荐下|我报名了快财商学院的 小白理财训练营|学会了如何增加非工资收入|9月份目标收入7500元|11月份 目标收入12000元|现在我的生活压力 减轻了许多|市场价299元的|现在0元就能学习 6天的理财课程|赶紧点击视频下方链接|跟我一起学习吧|1月份报名 快高学院小的财训练营|快高学小的硬|快高学院小的神》|肚高学 院小的袒财|快高|9 快财高学院|1月 快高学 院小|快财高学院小百神时训东|训东营|肚财高学院小面|肤高学院小日神财东营|顶梁茌的账本|欢梁茌的业|顶梁|顶梁档的贴|顶染档的账本！TE|顶染档的胀本!AT|顶梁档的账大！|顶梁构的账本!ATE|矶、元|让赶元|认持元|挑起元|训村0元|现あ元|训海元|让、|让 在元|元 班知元|效元 班颊礼|元礼|赡养父母|赡养|心海养|隆养夕|自的收入|奶收入|自的川|学理财 上快财|目奶收|创的山|*投资有风险选择需谨慎|风险责任由购买者自行承担|2000元|ZU、|2000D|'0え|日的|且的业|7300 元|75|9300 w0|7300 元|7500ィ|00元|固定工资|固足工 店贷|固定|5500 え|550O|--0え|l1A|月1|月份分|视频为演绎情节|O0|211 元|211 元|9自份|7月份|DATE|家庭支出|存款|练营\", \"video_asr\": \"我是一个家庭的顶梁柱，今天给你们看看我的账本。|每个月的工资五千五百元，每个月要还房贷两千元，孩子的学费，家庭生活费等等开支两千三百元。都说养儿防老，每个月给父母的生活费也不能少。|没有存款的家庭拿什么来抵挡意外风险？|九月份，在同事的推荐下，我报名了快财商学院的小白理财训练营，学会了如何增加非工资收入。|九月份目标收入七千五百元，十月份目标收入九千三百元。|十一月份目标收入一万两千元，现在我的生活压力减轻了许多，市场价二百九十九元的小白理财训练营，现在零元就能学习六天的理财课程，赶紧点击视频下方链接跟我一起学习吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022267818450927734 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '手写解题', '极端特写', '静态', '配音', '重点圈画', '平静', '填充', '情景演绎', '幻灯片轮播', '悲伤', '动态', '中景', '惊奇', '手机电脑录屏', '特写', '混剪', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.81', '0.79', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0edeafabc71ec83e0f3160247d871bef.mp4\n",
      "{\"video_ocr\": \"听说最近有一款刷视频|就能赚零花钱的软件挺火的|你们知道不|嗯|刷视频就能赚钱|你觉得有可能嘛|我在跟你们俩讲话呢|你们两个在干嘛呀|没..没干嘛啊|那你告诉我这是什么呀|这个就是你刚才说的刷视频|就能赚零花钱的趣铃声啊|这一会你就赚了这么多钱啊|我还赚了比他多呢|好啊你|为了藏零花钱|居然开始跟哥哥|来欺骗我|对不起媳妇儿|如果你现在能告诉我|在哪里下载的话|我就勉为其难原谅你吧|点击视频下方链接|就可以直接下载啦|300 领取时段奖励|勋章殿堂|金币余额10.3万|疯抢红包雨|幸运金币|元>|2:44 提现记录|完成阶段任务解锁10元 +100000 成就越高，赚得越多|+2000|领金币|官方红包雨来袭，拼手速赚钱!|看小视频越久赚越多|每天设置视频铃声稳赚金币|日常任务 解锁10万金币红包|128|微信提现(阿飞6)|抢红包雨赚金币|首次提现|提现拿钱还能再领金币!|仅需几步设置铃声赚2000金币 去完成|看小视频赚金币|立即瓜分金币 开启流量补贴权限|补贴权限|每日设置视频铃声(0/2)|20元|128  158  168|领取|领取 2天 3天 4天 5天  6天 7天|手任务 口ヮ设置视频铃声|0.3元|已到账|提现|888|每天|具体金额以实际活动为准”|每日签到|2天3天 4天5天 6天 7天|190|3元|最高|即赚即提|赚钱攻略>>\", \"video_asr\": \"听说最近有一款刷视频就能赚零花钱的软件挺火的，你们知道不是刷刷视频就能赚钱呢？你觉得可能吗？我再给你们两讲话呢，你们两个在干嘛呀？没干嘛呀，哎，那你告诉我这是什么呀，这个就是你刚才说的刷视频就能赚零花钱的趣铃声啊。|这一会就赚了这么多钱啊，我还赚的比他多呢。|好哇，你柜台零花钱就老是跟那个欺骗我，对不起，如果你先告诉我在哪里下载的话，我就勉为其难原谅你吧，点击视频下方链接就可以直接下载了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016336917877197266 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '手机电脑录屏', '静态', '家', '喜悦', '单人口播', '亲子', '场景-其他', '配音', '平静', '愤怒', '惊奇', '悲伤', '家庭伦理', '全景', '夫妻&恋人&相亲', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.62', '0.55', '0.50', '0.28', '0.06', '0.04', '0.04', '0.04', '0.03', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0edf45c35ee12ed3c515f657909add49.mp4\n",
      "{\"video_ocr\": \"理财思维!|你没长眼睛啊|对不起对不起...|刚子|怎么是你啊|这就是你那好朋友刚子啊|你不是说他开公司的嘛|怎么是个乞...|你闭嘴|我兄弟用不着你说三道四|你|这就是你跟我说的开公司|对不住了兄弟|给你丢面了|你还当我是兄弟吗|遇见这么大困难 都不联系我|公司破产了|房子也抵押出去了|别说了|这张卡你先拿着应个急|把你手机给我|我帮你报名了 快财商学院|你在上面学学理财|每天打开手机 就有额外的收入|这不会是骗人的吧|兄弟我能骗你吗|我身边好多人都在学理财|用理财赚的钱|比每个月工资都多|真的呀|那这报名费贵不贵啊|点击视频下方链接|0元就能免费报名了|0元抢购|适合人群|000|直播 学理财上快财|爆款理财小白 训练营|1分钟告诉你不会理财有多吃|快财|亏!现12元12天，带你学习|查看详情\", \"video_asr\": \"你没长眼睛啊，对不起，对不起，对不起。|刚刚怎么是你啊，这就是你的好朋友刚子啊，你不知道他开公司的吗？怎么升级闭嘴。|我兄弟用到你不敢告诉你，这就是你跟我说的，开公司啊，对不住了，兄弟给你丢面了，你还当我是兄弟吗？遇见这么大困难都不联系我，公司破产了，房子也抵押出去了，别说了，这张卡你先拿着用给几。|把你手机给我。|给我帮你报名了，快财商学院，你在上面学学理财，每天打开手机就有额外的收入，这不会是骗人的吧？兄弟，我能骗你吗？我身边好多人都在学理财，用理财赚的钱比每个月工资都多，真的呀，那这报名费贵不贵啊？点击视频下方链接，零元就能免费报名了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016443967819213867 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '填充', '全景', '静态', '平静', '悲伤', '朋友&同事(平级)', '室外', '惊奇', '路人', '动态', '喜悦', '极端特写', '夫妻&恋人&相亲', '(马路边的)人行道', '手机电脑录屏', '远景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.87', '0.78', '0.33', '0.27', '0.23', '0.23', '0.18', '0.13', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ee565c192c0874e71b96929fc61b5ce.mp4\n",
      "{\"video_ocr\": \"如果你想在高考英语中取得好成绩|刷题一大堆|成绩没提高|问题不在于刷题|而在于第|刷的题目可能不对|第二你刷题的过程中没有走心|第三你没有概括出刷题应该概括出的方法与技巧|很多努力不能变成分数问题在于|不是努力不对|而在于方法不得当|没有在最该努力的地方努力|那么必须学考点|学方法|学技巧|找方向|我们才能在高考的时候|真正的见到题目有思路|解题有模板|难题有巧招|定要认认真真的解决好|背什么单词|刷什么习题|用什么方法|怎样努力的问题|这是我们磊神专属的学习方法|你渴望力量吗|你想有方法途径实现自己的目标吗|如果你正在背英语困扰着|不要在犹豫了|上车报名|走起|跟谁学|ENGAISH|:|掌握高效学习法，快人一步成黑马!|单词记不住|你背的是不是|高考必考的单词|胼决你的英语困惑|磊神专属学习方法|高考英语|数学 重难点题型高分突破 英语 单词+语法高效记忆|高频词汇|而在于没理解|没有抓住重点|高中全科培优特训营|节直播课|15|这很重要|跟谁学|在线学习更高效|Hi.geod|Porning|you|meee|互元\", \"video_asr\": \"单词记不住，问题不在于记忆力，而在于没理解，没有抓住重点，你背的是不是高考必考的单词，这很重要，刷题一大堆，成绩没提高。|题不在于刷题，而在于第一，刷的题目可能不对，第二，你刷题的过程中没有走心。第三，你没有概括出刷题应该概括出的方法与技巧很多，努力不能变成分数，问题在于不是努力不对，而在于方法不得当，没有在最该努力的地方努力。如果你想在高考英语中取得好成绩。|那么必须学考点，学方法，学技巧，找方向，我们才能够在高考的时候真正的见到题目有思路，解题有模板，难题，有巧招，一定要认认真真的解决好为什么单词，刷什么习题，用什么方法，怎样努力的问题，这是我们雷神专属的学习方法。|你渴望力量吗？你想有方法途径实现自己的目标吗？如果你正在被英语困扰着，不要在犹豫了，上车报名走起。\"}\n",
      "multi-modal tagging model forward cost time: 0.01618194580078125 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '单人口播', '中景', '静态', '教师(教授)', '平静', '配音', '场景-其他', '影棚幕布', '室内', '课件展示', '特写', '极端特写', '宫格', '教辅材料', '手写解题', '混剪', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.88', '0.87', '0.40', '0.06', '0.06', '0.04', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0efa030b8083930a094d3998241127d2.mp4\n",
      "{\"video_ocr\": \"学思维学英语2-8岁上斑马 现在报名加送 全套英语豪华教材礼盒|We made some banana bread. 我们做了一些香蕉面包。|We made some orange juice. 我们做了一些橙汁。|We made some apple pie.|Let's alleat together. 一起来吃吧。|送|(包邮到家，港澳台除外)|49元10节课|斑马AI课|Whats\", \"video_asr\": \"你没上班？|咩咩咩咩咩咩咩咩咩咩咩咩。|不卖。|今天。\"}\n",
      "multi-modal tagging model forward cost time: 0.016192913055419922 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '才艺展示', '场景-其他', '情景演绎', '中景', '全景', '影棚幕布', '绘画展示', '填充', '喜悦', '特写', '动画', '极端特写', '配音', '室内', '转场', '平静', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.94', '0.81', '0.05', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f174fc8bd41c7798168e89bef11bf66.mp4\n",
      "{\"video_ocr\": \"在中国|有超过90%的|亿万富翁|都是白手起家|所以成功的必要条件|并不是背景|而是富人思维|他们不会用|体力劳动去生钱|而是通过理财|没有一个有钱人|会用自己的钱做生意|而是贷款|利用别人的钱|白国|让钱进入|生生不息的状态|才是财富升值的秘密|我们每个人|都可以拥有|哪怕你月薪3000|当你学会|专业的理财课程后|每月拿出|三分之一的工资|就能实现财富升值|0元抢推荐你学习|快财商学院的|小白理财训练营|6天理财达人|实战直播课|18个财富增值实用技巧|18个财富增值实用技巧|22种投资必备避坑指南|理财名师|教你掌握精准投资|通过理财工具|就能让你额外收入|超过工资|就现在|点击视频|下方链接报名|一起加入|富人的队伍吧|0元抢购 惊爆价!|快财商学院院长:九坤|+86 请输入手机号|请输入验证码 获取验证码|本人已阅读并同意 《用户使用协议》|适合人群|月光一族|系统的学习，掌握理财 想通过|没有投资技巧的人 的全职妈妈|想提升家庭财力 盲目投资|课在介绐|穷与富的秘密:如何打造富人思维? 1|理财的秘密:甩开同龄人，适合普通人 的投资工具?|荐股骗局大揭秘:适合小白的“低风险”投 资策略|复利法则:小白如何选指数基金?|无保障不投资:投资如何进行风险管理？|2020年投资分析:家庭应该如何配置财产?|老师介绍|课程特色|理财名师直播授课 手把手带你轻松学理财|实用的快速入门课程|社群学习，方便学习随时回看|班主任督学|学习理财，增加额外收入|家庭主妇|作业实时批改反馈，不让问题过夜|踩坑老手|吕珍老师|基金、股票、低风险投资理财干货|社群学习|前200名|工资不够花，想通过 基础为0，希望通过|擅长低风险搭配收益|学理财上快财|6天理财名师爆款直播课|专业的理财学习平台|靠谱的平台 专业的老师|国家高级理财规划师|快财商学院金牌理财课讲师|真播|直指|快财|没存款?|快财工资，小白理财如何少走弯路!|抢购|作业实时批改反馈， 不让问题过夜|全国知名理财专家|款理财小日|升家底财力|训练营\", \"video_asr\": \"在中国，有超过百分之九十的亿万富翁都是白手起家，所以成功的必要条件并不是背景，而是富人思维。他们不会用体力劳动去生钱，而是通过理财。没有一个有钱人会用自己的钱做生意，而是贷款利用别人的钱，这就是富人思维。|让钱进入生生不息的状态，才是财富升值的秘密。富人思维，我们每个人都可以拥有，哪怕你月薪三千，当你学会专业的理财课程后，每月拿出三分之一的工资。|就能实现财富升值！推荐你学习快财商学院的小白理财训练营，六天理财达人实战直播课，十八个财富增值实用技巧，二十二种投资必备避坑指南！|才名师教你掌握精准投资，通过理财工具就能让你额外收入超过工资，就现在点击视频下方链接报名，一起加入富人的队伍吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016695261001586914 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '平静', '配音', '混剪', '场景-其他', '远景', '城市景观', '手机电脑录屏', '动态', '全景', '天空', '重点圈画', '中景', '幻灯片轮播', '静态', '手写解题', '特写', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.91', '0.65', '0.28', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f1e21793a962134273b33d944e858ab.mp4\n",
      "{\"video_ocr\": \"首先第一个我们会教给学生|比方说|比如说这个|因为考试当中最需要体现|你文采的部分|而且很多学生|把我们课上讲的作文开头|拿到他考试当中之后|确实在学校得到了很高的分数|跟对老师你就能早一点|享受语文学习的乐趣|如何快速提高作文分数|非常多的开头的方式|排比举例引话题|简单对比出哲理|就是作文的开头|我是作业帮直播课|姜玥老师|学会语文学习的|正确方式|华大学中 学生一个月成线|清华大学中文系毕业 学生个月成绩升至年级前三|立即抢课|小初语文 阅读写作提分班 8元13节课|【作业帮累计用户超8亿]|全国包邮 掌握必考点快速提升阅读写作能力|课|中国女排|优秀作文集|姜玥\", \"video_asr\": \"如何快速提高作文分数的？首先，第一个我们会教给学生非常多的开头的方式，比方说排比，举例，引话题，比如说这个简单对比出哲理，因为考试当中最需要体现你文采的部分就是作文的开头，而且很多学生把我们课上讲的作文开头拿到他考试当中去之后，确实在学校得到了很高的。|分数我是作业帮直播课，江月老师跟对老师，你就能早一点学会语文学习的正确方式，享受语文学习的乐趣。\"}\n",
      "multi-modal tagging model forward cost time: 0.016346216201782227 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '单人口播', '平静', '室内', '教师(教授)', '配音', '特写', '极端特写', '知识讲解', '场景-其他', '教辅材料', '动态', '情景演绎', '办公室', '混剪', '影棚幕布', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.77', '0.69', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f21bd5e154ce1ca91573cb09b390f0f.mp4\n",
      "{\"video_ocr\": \"报名就送超值教材礼盒 想家 龙现台除外)|(包邮到家，港澳台除外)|Little flish, littlefish, doyou need some water? 小鱼，小鱼，你们需要一些水吗?|It's too hot. 太热了。|Letsmix water together. 让我们把水混合在一起。|送 立即报名|斑马A/课 学思维学英语2-8岁上斑马|49元10节课\", \"video_asr\": \"我喜欢的。|没有，你想我喜欢你喜欢的话。|二。\"}\n",
      "multi-modal tagging model forward cost time: 0.016184091567993164 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '情景演绎', '静态', '中景', '影棚幕布', '全景', '才艺展示', '喜悦', '特写', '多人情景剧', '场景-其他', '平静', '室内', '单人口播', '填充', '教辅材料', '动画', '拉近', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.96', '0.21', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f3592093414f51db3afa8296f606663.mp4\n",
      "{\"video_ocr\": \"学费全返|panda|cake|flower|我给发你工资|是让你来干活的|不是让你在这|陪你孩子学习的|老板|妈妈我写好啦|就你这水平|孩子能写出什么|pink|这|这真是她自己写的|她能记住这么多单词啊|我只是花了热恐句型6个|给孩子报名了|伴鱼自然拼读课|叫节真人外教录播课|教孩子自然拼读法|这是孩子学习英文单词|熟悉英文单词发音规则|力求做到|14|原来是这样啊|我先说声抱歉|刚才是我态度不好|你能告诉我|这个课在哪报名吗|点击下方链接就能报名|还赠送配套学习成长地图|按要求完课后 还能领取奖学金呢|tomGto|mouth|自信说英语 累计开口次数400+|培养语感|夯实见词能读基础|语言能力综合提高 阅读畅销原版绘本6本，累计熟悉句型6个|掌握Aa-Ff的发音及书写规则|掌握Aa-Ff字母相关单词18个|记录学习轨迹，培养宝宝坚持学习的好习惯。|meat|学习目标|购课即送学习成长地图|自然拼读|悉句型6个|TMONIC|笛一周|29元|29块钱|发音规则|拼读技巧|看词能读|听音能写|flov|grape|orange|习轨迹，培养宝宝坚持学习的 惯。|每天约20分钟|下单送成长地图|Da|pahdo air|Co|Cake|pahcla|29元14节外教课，帮3-8岁孩|立即报名 学完得 奖学金|子，提高听说读写能力，学完|finger|nqer|查看详情|AB\", \"video_asr\": \"真的。|TAKE。|不了。|哇。|我给你发工资是让你来干活的，不是让你在这陪你孩子学习的。|老板刚下载好了。|就你这水平，孩子能写出什么？|这。|这真是他自己写的。|不能进入这么多单词，我只是花了二十九块钱给孩子报名了半年自然拼读课，十四节真人外教录播课，教孩子自然拼读法，这是孩子学习英文单词，欢迎孤独技巧的方法，熟悉英文单词发音规则，并做到看见能读，拼音能写了。原来是这样的，新人，抱歉，刚才是我态度不好。|能告诉我这个课在哪报名啊，点击下方链接就能报名成功推到学习成长地图，按要求完课后还要预习奖学金呢。\"}\n",
      "multi-modal tagging model forward cost time: 0.02215266227722168 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '平静', '静态', '多人情景剧', '场景-其他', '手机电脑录屏', '室内', '极端特写', '亲子', '单人口播', '配音', '宫格', '家', '家庭伦理', '动态', '课件展示', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.91', '0.87', '0.64', '0.44', '0.44', '0.36', '0.17', '0.06', '0.05', '0.05', '0.04', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f41b78d16f7bb6c6395a482ce9bb20f.mp4\n",
      "{\"video_ocr\": \"我母亲治病急需五万块|你就把工钱给我结了吧|我也没钱啊|大哥|又狠赚了三十万呢|您行行好吧|我母亲等钱治病呢|少跟我装可怜 0.O|赶紧滚|你妈妈快不行了|什么|接我古医仙门传承|尔当悬壶济世|你没事吧|古医传承|我可以救我妈了|诶你跑什么|哟穷鬼来|Q-O|你妈的病可不能再耽误了|我自己救|哼装神弄鬼|被撞了还那么能跑|秦小姐你怎么到这来了|我找他|您刚才可是用的华佗金方里的回魂九针|是又如何|你可知道她是谁|叶神医求您救我爷爷|无论什么条件我都答应|(本故事纯属虚构)|七猫兔费小说|战猫免费+说|《都市古仙医》 说|市击仙医》说|七福集费说|《都市|《都送》|七海小说|市苦免国况|七猫免费小说|市古仙医X|立即下载 全本免费读|免费看书100年|七猫免费小说APP|叶母|瞧不起他的人都将俯首称臣|穷小子意外传承古医仙门|*阅读中含有广告内容|OO|救活命悬一线的母亲|俘获富家千金的心房|00|故事纯属虚构|秦楚楚|APP|小弟|元百|35\", \"video_asr\": \"我母亲治病急需五万块钱，你就把工钱给我结了吧，我也没钱呢，那就赚三十万。|大哥你心情好吧，我母亲给他钱治病啊，上课上课了赶紧滚回来。|你妈妈快不行啊。|借我古一仙门传承，而当悬壶济世。|一分钟我可以救我妈了。|二零一六穷鬼来了连收起了吧。|你妈的病可不能再耽误了我自己就。|抢什么别冲动还能不能跑。|秦小姐怎么到这来了，我找他。|什么还要管用的，可是滑头清华你的回归九真是有一个赵胖子变成你，求你救救我爷爷，无论什么条件我都答应。\"}\n",
      "multi-modal tagging model forward cost time: 0.016078472137451172 sec\n",
      "{'result': [{'labels': ['多人情景剧', '中景', '推广页', '静态', '现代', '填充', '愤怒', '特写', '家', '夫妻&恋人&相亲', '惊奇', '家庭伦理', '亲子', '极端特写', '悲伤', '动态', '亲戚(亲情)', '拉远', '上下级', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.95', '0.93', '0.91', '0.89', '0.83', '0.79', '0.77', '0.62', '0.25', '0.23', '0.17']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f4da00e5bfae20a1b75c2cc6e8cbd55.mp4\n",
      "{\"video_ocr\": \"痔明|今天见我姐|你可得好好表现|你放心|走|姐，这是痔明|你们先唠|我去泡茶|我去|这不就一个人吗|这剧组也太省钱了|小伙子|你在玖富万卡|额度是多少|都不知道|我怎么放心|把妹妹交给你|到底什么是|最高能申请|20万的借款额度|最快3分钟|就能放款|你们这些年轻人|有了这个额度|要是以后遇到急事|我也放心\\\\啊|额度还能侧面|反应一个人|信用程度|姐，您说得对|我这就测试一下|上面的额度|您看我有20万|小伙子信用不错啊|我同意了|我就说痔明|很靠谱吧|这个特效也是|算了|你们也来测测|你们的额度吧|最高可借20万元|贷款额度，放款时间等以实际审批为准|19:2021|1920|14151|5678|贷款遗，放款时间际审批为准|久，放款时间等以实际审，准|好款额 以一神批为准|贷款额虚放款间等火实际ロ他为准|贷们付批准|欢款事以实际审为准|贷款 额度般款时间实际电批准|请根据个人能力合理贷款|1213|痛很惦个人能力款|贷款有风险，借款需谨慎|贷款声风惜款需谨慎|贷款有风脸惜慎|贷款有风瞪，借款镇|贷款有风嘘，信真|1.2|19复|2345|151|1U7181920?|101八12|101121321|10小1213142|t71819M2|t7nBiw2222|1t810M2222|17t8jA2122|178 i222z|1tgnA222|17wB noA222|1071|七7礼8|TU|忆21314|13|纳斯达克上市企业旗下品牌|12345678|最终以持牌金融机构或具备 放款资质的审核标准为准|ONECARD|测测|测测你能借多少钱|HOMI\", \"video_asr\": \"志明今天将会给你和你好好表现啊，你放心。|姐这是志明，你们先唠，我去泡茶哈。|这不就有一个人追剧，总裁是假了吧，三个人你在酒桌上的额度是多少？|九万咯。|你连九富万卡都不知道我怎么帮你房间里交给你了。|到底什么事情不敢通过玖富万卡最高能申请二十万的借款额度？|最快三分钟就能放款，你们这些年轻人有了这个额度，要是以后遇到急事了我也放心啊，而且九万卡的额度还能侧面反映一个人的信用程度。|姐，你说的对，我这个做学校的就不要看上面的额度哎，你看我有二十万。|哟，小伙子信用不错啊，我同意了。|姐，我就说清明你靠谱吧，这头像是。|唉算。|你们也来测测你们的额度啊，最高二十万来求你，不问他这次你有多少额度吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016704797744750977 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '静态', '喜悦', '家', '亲子', '家庭伦理', '平静', '夫妻&恋人&相亲', '特写', '愤怒', '惊奇', '极端特写', '单人口播', '悲伤', '动态', '室内', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.93', '0.73', '0.58', '0.55', '0.18', '0.16', '0.09', '0.06', '0.05', '0.04', '0.02', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f62c1e53c777bc966b6058c09966bea.mp4\n",
      "{\"video_ocr\": \"是9527的顾客吗|对去百货大厦|对不起对不起啊|我家里有点事|我分心了|没事没事|你这是怎么了呀|这不孩子5岁了嘛|在上这个数学辅导班|马上就要缴学费了|你说别人家的孩子|都在学|我们也是想着让孩子|早点接触数学|你可以给孩子报名|一个斑马A课 思维体验课呀|10节课才49块钱|我身边的宝爸宝妈|还赠送一个 教具礼盒呢|特别划算|这么便宜的课|能有效果吗|这就是你的偏见了吧|这个课程啊|是专门为3-6岁孩子|定制的 逻辑思维课程|由拥有10年以上|经验的资深教研团队|精心打磨而成|采用动画|儿歌游戏的形式|培养孩子的思维能力|我们家孩子跟着|学了一年多了|现在呀 100以内的加减法|都能轻松算出答案|太好了|这个是在哪报的名啊|点击视频下方 查看详情|就可以报名了|斑马A|课 猿辅目|猿辅导 |猿辅导在线教育出品|猿辅导在教育|行辘导在线教育|声|这是|在线教育 出品|猿辅导田给教|猿辅导在线狐 业|猿辅导起4道育|痃骊寻|痃铺寻急|猿输景|老公f.子的思住学，又该交 共6，赶忡我啊|老公，孩子的思维学费又该交了， 一共6000，赶快打给我啊|¥6000|提前学思维，学数学变容易|该视频为演绎情节，请安全驾驶|2-8岁上斑马 学思维学英语|适合3-B数学学前准备|转账金额 添加转账说明|转账 倾慕已斑驳(**强)|趣味数学思维课|49元10节|斑马思维|斑员维|送全套教具|微信支付\", \"video_asr\": \"ZZZZ。|是去花时间补课吗？对，去百货大厦。|对不起，对不起啊，我家里有点事，我分心了，没事没事，您这是怎么了呀？我这不孩子五岁了吗？|在上这个数学辅导班，马上就要交学费了，你说别人家的孩子都在学，我们也是想着让孩子早点接触数学。|您可以给孩子报名就干吗AI科思维体验课呀，实践课才四十九块钱，我身边的宝爸宝妈都抢着给孩子报名，还赠送一个教辅礼盒呢，特别划算，这么便宜的课能有效。|就是你的偏见了吧，这个课程啊，是专门为三到六岁孩子定制的国际思维课程，由拥有十年以上经验的资深教练团队精心打磨而成，采用动画，儿歌，游戏的形式培养孩子的问题。|你家孩子跟你生了一年多了，现在一百以内的加减。|轻松算出答案，太好了，唉，这个是在哪报名啊？点击视频下方查看详情就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.015966176986694336 sec\n",
      "{'result': [{'labels': ['推广页', '汽车内', '现代', '中景', '多人情景剧', '静态', '喜悦', '特写', '悲伤', '亲子', '极端特写', '平静', '手机电脑录屏', '愤怒', '动态', '夫妻&恋人&相亲', '惊奇', '家庭伦理', '路人', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.95', '0.95', '0.55', '0.35', '0.26', '0.22', '0.13', '0.12', '0.04', '0.03', '0.03', '0.03', '0.02', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f67097595b33b9b925523633144fde1.mp4\n",
      "{\"video_ocr\": \"亮哥 你在看什么|我在看我们家孩子的这个|斑马A课的学习报告|都累计开口说英文|三千七百多次了|对|斑马A课|是不是那个|2.8岁上斑马|学思维 学英语|孩子特别爱学|因为它是用这种|小朋友特别喜欢的动画元素|而且可以根据孩子的这种|学习的这种进度|然后自由地去调整|孩子主动学 家长就不会操心|你看 思维课也是三颗星|我觉得你也可以学|真的吗|对 来|那我继续去摆盘子了 亮哥|我也要去干活了|10节趣味英语课+趣味大礼包 点击视频|28岁赶马 孩子贷学家长省心|0基础学英语|就来斑鸟A课英语体验课|A1个性化定制|北美外教动画课|无限次回放|KRML 每天15分钟|KA M|让孩子主动开口|现在10节课仅需49元|读绘本|随堂测|Ja35qo1 还包邮赠送全套教具礼盒|赶紧点击视频下方|给你的孩子抢课吧|我家2无排英语忽星|R计上|云1.上津 239|京计上像|找拿20E动讲英语激程|我家ZOE讲英还放界|Look at the jce!|WOIHBS DIANEYION|WORKBOOK|crab lobster|猿辅导|奶/金|线脯导在线教育 出品|英语+|sing|猿辅导行刺御 出品|提升识记效率|有声绘本提升阅读 培养口语表达力|Ful D|ul Dog|Yummy Pog|Helpful Dog|记单词 AI评分、外教领读|2-8岁上斑马学思维 学英语|在线教害 出品|uPovan ar UDro|3730|AC|sheep|230册趣味在线绘本|100 个外教拓展视频|mas .GoodM House MyF|10 节AI互动课 28 件教辅随材|KDH|升阅读|芒果tV 独播|芒果tV 独播|-英语-|地段约|发布价品地|发布到价城|发有到品魂|内k|M House|WORD|完脯号|下方报名|COW|送配套教材礼盒|CARDS Fun at Schooi|S3|12|horse|sealion|L9￥|随党学小测|烬怅揪こm|LETS|“6!d犯|re|2イ\", \"video_asr\": \"哎，亮哥你在干嘛呢？我在看我们家孩子这个斑马AI课的学习报告呢，哇哇，都累计开口说，因为三千七百多次了，对啊，斑马AI课，哦，是不是那个二到八岁上斑马趣味学英语？|这个斑马AI课还特别爱学，因为它是用这种，小朋友特别喜欢那种动画元素，而且可以根据孩子的这种学习的这种进度，然后自由地去调整，因为孩子。|洞穴家长就不会操心，思维课也是三颗星，我觉得你也可以试试哦，对了OK那我继续失败，盘子二大了，我要去干活了，零基础学英语就来斑马AI课英语体验课AI个性化定制北美外教动画课，无限次回放，每天十五分钟，让孩子主动开口，现在十节课仅需四十九元！|包邮赠送全套教具礼盒，赶紧点击视频下方给你的孩子抢课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01615762710571289 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '平静', '静态', '餐厅', '手机电脑录屏', '场景-其他', '朋友&同事(平级)', '多人情景剧', '动画', '教辅材料', '家庭伦理', '配音', '特写', '极端特写', '喜悦', '惊奇', '动态', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.02', '0.01', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f761df508226b81371111a51e340586.mp4\n",
      "{\"video_ocr\": \"不管你语法什么基础|哪怕|什么你都不懂|这些概念我们都不用|我保证你跟我学习|彻底地理解英语语法|一个全新的语法体系|徐磊英语语法|将会带你|搞定语法问题|同学们|不要犹豫了|报名吧|跟谁学首席高中英语讲师|跟谁学 在线学习更高 效|十四年一线教学经验|在线学习剪高效|?|徐磊|跟壁\", \"video_asr\": \"不管你语法什么基础，哪怕主渭滨定状补什么，你都不懂这些概念我们都不用，我保证你跟我学习彻底的理解英语语法，一个全新的语法体系时的英语语法将会带你搞定语法问题，同学们不要犹豫了，报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015735149383544922 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '推广页', '静态', '教师(教授)', '平静', '室内', '填充', '影棚幕布', '知识讲解', '拉近', '喜悦', '特写', '惊奇', '教辅材料', '场景-其他', '情景演绎', '宫格', '拉远'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.92', '0.13', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0f8609194977be4abe2cb4c94d1911b4.mp4\n",
      "{\"video_ocr\": \"Mom 妈妈|this is grass 这是草|tree 树|flower 花朵|thisis leaf agreat这是树叶|CA双这是树叶|这些都是 plants 已台这些都是植物|哇宝贝你真棒|今天又背会了200个单词|小学生在学习英语过程中|最大的问题就是背单词|单词学得好 英语没烦恼|单词学得差 考试总害怕|中小学生的英语课本中|每个单元都有自己的主题|利用思维导图、速记主题和单词的顺序|再通过音标和发音规律拼读单词|Sam老师会告诉你在英语的学习过程中|背单词只是第一步|那么接下来还要在阅读和文章之中 速记单词的使用方法和意思|在4天的学习训练营中|我将会带领同学们一起来挑战高效地背单|价值699元的拼读课程|现在免费学习|大家可以点击屏幕下方的链接|跟着Sam老师一起来高效地学习英语|单词速记训练营 扫码入群听课 领取学习资料|注册成功后 截图保存图片|在线学习更高效|在线莘|LoNN|适用1-6年级|stop|点a归 $t0P|“A要P咨|eat beang|a great excuse to|跟谁学|自词|staving\", \"video_asr\": \"THIS PRICE。|FLOWER B其实都是PLANT宝贝，你真棒，今天又背会了二百个单词，单词，单词，单词，小学生在学习英语过程中最大的问题就是背单词，单词学得好，英语没烦恼，单词学得差考。|总害怕中小学生的英语课本中，每个单元都有自己的主题，利用思维导图速记主题和单词的顺序，再通过音标和发音规律。|拼读单词三老师会告诉你，在英语的学习过程中，背单词只是第一步，那么接下来还要在阅读和文章之中速记单词的使用方法和意思。在四天的学习训练营中，我将会带领同学们一起来挑战高效的背单词。原价六百九十九元的拼读课程现在免费学习，大家可以点击屏幕下方的。|链接，跟着三老师一起来高效的学习英语，扫码入群领取学习资料！\"}\n",
      "multi-modal tagging model forward cost time: 0.017115354537963867 sec\n",
      "{'result': [{'labels': ['现代', '中景', '单人口播', '推广页', '室内', '多人情景剧', '静态', '转场', '平静', '影棚幕布', '动态', '教师(教授)', '家', '喜悦', '悲伤', '夫妻&恋人&相亲', '家庭伦理', '室外', '朋友&同事(平级)', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.90', '0.70', '0.50', '0.29', '0.11', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0fb1fc1d74b062347178981eaa9ef900.mp4\n",
      "{\"video_ocr\": \"终于找到你们不报 补习班的原因|报班吗|高中数学集中突破|不用了|我报名了|高途课堂|高中全科名师班|里面高考439个知识点|老师啊|都帮我梳理了一遍|报班吗7?|高中语文强化训练|你那有高途课堂|清华北大毕业的名师|教的好吗|我儿子啊|已经掌握了|日类诗歌鉴赏技巧|万能作文模板啦|FBVV213|不需要|高途课堂语数英物|四科旧节直播课|只要块钱|课后还有辅导老师|1对|答疑|学不会？|3年内还能免费|无限次回看课程|这么好的课|真的存在吗？|不信?？|那你就|点击视频下方链接|花块钱|给孩子试试吧!|华少|新用户专享 立即体验 浙江卫视指定在线教育品牌|全科笞|梳理了|月旧节直|仅需 ¥9|浙江卫视|名师特训班|全国百佳教师带队教学 平均教龄11年|PB.VVX|PB\", \"video_asr\": \"终于找到你们不报补习班的原因报班吗？集中突破不用了，我报名了高途课堂高中全科名师班，里面高考四百三十九个知识点，老师啊都帮我梳理了一遍，哎，爆斑马高中语文强化训练不用了，你们有高度课堂，清华北大毕业的名师教的好吗？|我儿子已经掌握了六轮适合建厂技巧万能作文模版吧，报班吗？不需要高途课堂语数，英物，四科十六节直播课，只要九块钱，课后还有辅导老师一对一答疑，一绝不会，三年内还能免费无限次回看课程。好课真的存在吗？不信，那你就点击视频下方链接，花九块钱给孩子试试吧！|我。\"}\n",
      "multi-modal tagging model forward cost time: 0.01602339744567871 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '填充', '多人情景剧', '全景', '动态', '静态', '平静', '室外', '亲子', '家庭伦理', '喜悦', '惊奇', '朋友&同事(平级)', '特写', '悲伤', '愤怒', '极端特写', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.96', '0.95', '0.94', '0.93', '0.90', '0.17', '0.05', '0.02', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0fdd8a8e4d7c5020c0a497459a7a0008.mp4\n",
      "{\"video_ocr\": \"RM ANINMA|M ANIMALS|不行我得找个人问问|您好|请问张婷婷的快递到了吗|到了到了这就你的|诶姐|这叽里呱啦课怎么样啊|特别好|这可是北美外教AI 英语课|孩子学了特别感兴趣|课程通过真人外教演绎|加动画视频|加互动游戏的学习形式|让孩子边玩边学|培养英语思维|引导孩子主动说英语|特别适合2-8岁的骇子|这课才9块9可真划算|但万一孩子听不懂怎么办|不用担心|课后有老师社群辅导|孩子有什么问题啊|都能及时解决|现在报名啊|NIMALS 这套英语启蒙礼盒|还免费送|里面有34件|配套教具呢|那这个课怎么报名啊|现在点击视频下方|就可以报名了|￥9.9|这叽里呱啦是疯了吧|9块9就有|7节北美外教AI英语课|还送这么大个|英语启蒙|叽尘%啦|i.es|dntees|叽￥啦|叽尘呱啦 m|节体验课 天社群铺导|包邮送精美学习大礼盒|适合2-8岁宝贝 加赠80张儿童拼拼卡|Look! A tiger|Tiger, tiger|gpnfees|害方价199元|AI强停课|RM\", \"video_asr\": \"这期里呱啦是疯了吧，九块九就有七节碑的外交AI英语课还送这么大个英语启蒙礼盒，不行我得找个人问问，您好，请问张婷婷的快递到了吗？|到了到了这就是你的哎，姐啊，这激励挂了课怎么样啊？特别好，这可是北美外教英语课，孩子学俩特别感兴趣，课程通过真人外教演绎加动画视频加互动游戏的学习形式，让孩子边玩边学，培养英语思维。|引导孩子主动说英语，特别适合二到八岁的孩子，英语启蒙，这课才九块九啊，可真划算，三万亿孩子听不懂怎么办啊？|不用担心课后有老师社群辅导，孩子有什么问题都能及时解决，现在报名啊，这套英语启蒙礼盒还是免费送，里面有三十四件配套教具，那这个课怎么报名啊？现在点击视频下方就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.015890121459960938 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '平静', '配音', '静态', '场景-其他', '动态', '教辅材料', '多人情景剧', '亲子', '家庭伦理', '极端特写', '室外', '手机电脑录屏', '情景演绎', '动画', '课件展示', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.96', '0.72', '0.24', '0.19', '0.18', '0.16', '0.14', '0.11', '0.10', '0.09', '0.09']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0fe5c884929d69e06ecc8a730cb64b23.mp4\n",
      "{\"video_ocr\": \"为4-12岁孩子量身定制 .1对1教学.美院师资·.家长可旁听 立即报名|9元9节 在线少儿美术课|1对1|美术宝1对1|美米云\", \"video_asr\": \"真的。|绝对能开始的。|鸽子好好过，我好过。|自作多情了好吧，我认了。|至少能换来释怀，洒脱，没丢失了自我。|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.016327857971191406 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '推广页', '绘画展示', '极端特写', '静态', '才艺展示', '教辅材料', '幻灯片轮播', '商品展示', '配音', '全景', '拉近', '宫格', '课件展示', '动画', '手机电脑录屏', '转场', '中景', '图文快闪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.76', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0fea93d543f92228bab6e36696dedc64.mp4\n",
      "{\"video_ocr\": \"张总|您好这里是包厢|非会员只能坐大堂|瞧不起谁啊|不就是会员吗|多少钱|月会员888元|季度会员2388元|好|来个年会员|来扫我的零钱吧|张总好帅啊|你这钱不太够啊|没钱装什么大头蒜|扫我的京东金条|您本次消费8888|剩余额度为|141112元|张总这京东金条是什么|怎么这么高额度啊|这是京东数科旗下的借贷大平色典率最低9%|这是京东数科旗下的借贷大平台|最高可借20万|借1万元日息最低2块5|还能分12期慢慢还|那我能申请吗|当然可以啊|0抵押0担保|全程手机申请|最快1分钟到账|快告诉我怎么申请吧|点击视频下方链接|来测测你能借多少吧|111 节1.2节热力狂欢 !得4888元大奖|别狂补10亿京贴|放款时间等以实际审批为准|贷款额度、放款时间等以实际审批为准|负款欲度、|货豪款额度、|负款人额度|女录欲攴、|际审批为准 贷款欲度、放款时同等以买|货示视、放款时问寺以头|女人颌反|4888元兔维大翼 白条激活得3000元服眼免意御|白条激活得3009元前道业原看|会员新人最高量10000金带|赢48名8元务带大账0|请根据个人能力合理贷款|贷款有风险，借款需谨慎，请根据个人能力合理贷款|力合理贷款|立即参与|我们为你难备了险手花|京东金融|年化费率最低9%|有风|贷款有风险，|京东金融APP|7:12\", \"video_asr\": \"张总。|你好，这里是包厢，非会员只能做大堂，瞧不起谁呀，不就是会员吗？多少钱优惠元八百八十八，季度会员两千三百八十八你也度过元八千八百八十八。好来的年会员扫我的零钱包，中国好帅啊。|你之前不是跟我没钱装什么大头蒜，看不起谁呀，来扫我的京东金条，您每次消费八千八百八十八延伸的对位。|十四万一千一百一十元，张总侍，京东金条是什么呀？怎么这么高额度啊？这是京东数科旗下的借贷大平台，最高可借二十万，借一万元日息最低两块五，还能分十二期慢慢还。那那我能申请吗？当然可以啊，零抵押，零担保，全程手机申请，最快一分钟到账。|快快怎么申请吧，点击视频下方链接，来测测你能借多少吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016685009002685547 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '特写', '平静', '手机电脑录屏', '极端特写', '动态', '朋友&同事(平级)', '悲伤', '办公室', '单人口播', '喜悦', '工作职场', '夫妻&恋人&相亲', '家', '室内', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.95', '0.77', '0.73', '0.42', '0.30', '0.29', '0.24', '0.13', '0.02', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ff4d5dcaf8d9ae9b7ddb44fea23b2dd.mp4\n",
      "{\"video_ocr\": \"你家孩子是不是也在这样写作文|秋风 一吹树叶发出沙沙的响声|而用拟人式写作法写出来的作文|秋风拂过 金灿灿的落叶随风舞动|像可爱的小公主穿着金黄色的裙子|跟着美妙的音乐翩翩起舞|想学习更多写作方法|来作业帮直播课|语文阅读写作提分班|由清北毕业的名师带队教学|总结出74个必备阅读写作大招|教孩子轻松写好各类作文|冲刺高分|现在点击视频|价值499的课程|限时只要29元|还包邮赠送全套教辅礼盒|越飞越高 卡友是 刺连|t公剧取.每一I 烈卡b影个美|t心，世没那么热y， 烈友是个美而的委书，小鸟坊|大阳,，也没有那么热， 利真影个美加的丢书，小鸟松变得热|t阳,U股有ョ公热， t公剧取.-INお加度N限，树车|视频为滴绎情节|到|优秀作文集|双直指训|字钻本|成卡|越κ越高，我ъ创远、|中国女排|播课 作帮登|29元20元万|语雳提劈班 1节|和NAh腕的书，小鸟以变得热|上课内容与收到礼盒请以实际为准|掌握重难点|快速提升阅读写作能力|不合福|立即报名>>|送全套教辅材料|送全套教耕|致直播课|上的|Arletta\", \"video_asr\": \"你家孩子是否也在这样写作文？秋风一吹，树叶发出沙沙的响声，而又女人是写做法写出来的作文。秋风拂过金灿灿的落叶，随风舞动，像可爱的小公主穿着金黄色的裙子，跟着美妙的音乐翩翩起舞，想学习更多写作方法来做。|语文阅读写作提分班不能，清北毕业的名师带队教学，总结出七十四个必备阅读写作大招，让孩子轻松写好策略，作文冲刺高分。现在点击视频价值四百九十九的课程，限时只要二十九元，还包邮赠送全套教辅礼盒。\"}\n",
      "multi-modal tagging model forward cost time: 0.016751766204833984 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '单人口播', '静态', '场景-其他', '平静', '特写', '配音', '喜悦', '教辅材料', '室内', '手写解题', '教师(教授)', '拉近', '家', '单人情景剧', '极端特写', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.79', '0.40', '0.28', '0.28', '0.19', '0.18', '0.09', '0.02', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ff5d569a4586f1a3868f397f05f360b.mp4\n",
      "{\"video_ocr\": \"当你将心中的梦想与别人分享时 When you share your dream with someone，|有时即使是身边最关心你的人 sometimes even people with the best intentions|也会给你泼来一盆凉水 are prone to saying, No, you can't do that.\\\"|但你知道吗 But you know what|你并不需要太在意别人对你梦想的否定 you don't have to take \\\"no\\\" for an answer.|只需找到能实现梦想的途径 You just need to stick to your goal|一步步坚定地向前走 and do it in right way|所以，如果你想成为一名摄影师 Soif you wanna be a photographer,|那就找到对的摄影学习资料，并且开始不断练习 find the right learning materials and start practicing.|如果你想成为一名英语演讲者 If you wanna be an English public speaker,|那就写下练习英语的计划，持之以地练习 make an English practice routine and stick to it|开言英语是一款多功能英语学习应用 OpenLanguage is an all-in-one app|适合各个阶段英语学习者 that teaches English to learners of all leves.|其课程以对话为主 Its lessons are based on real dialogues|以及实用的日常对话 and practical daily conversations|赶紧试试开言英语吧 Just give it a shot|谨记 and remember|不要被别人对你梦想的否定击倒 never ever let anybody tell you that you can't do anything.|你是唯一决定你人生走向的人 After all, you are the author of your own story.|Bargainingin the markiet|共12个调汇|开言英语|6500|bargaining|、|KP|7H\", \"video_asr\": \"FOR YOU SURE YOUR DREAM WITH SOMEONE SOMETIMES EVEN PEOPLE WITH THE BEST INTENTION UPON TO SAYING NO YOU CAN DO THAT。|YOU KNOW WHAT YOU DONT HAVE TO TAKE NO FOR AN ANSWER YOU JUST NEED TO STICK TO YOUR GOAL AND DO IT IN THE RIGHT MAN SO IF YOU WANT BE A PHOTOGRAPHER FINE RIGHT LEARNING MATERIALS AND START PRACTICE ING。|IF YOU WANNA BE AN ENGLISH PUBLIC SPEAKER MAKING ENGLISH PRACTICE ROUTINE AND STICK TO IT OPEN LANGUAGE IS AN ALL IN ONE THAT THAT TEACHES ENGLISH TO LEARN AS OF ALL LEVELS IS LESSONS BASED ON REAL DIALOGUES AND PRACTICAL DAILY CONVERSATIONS JUST GIVE IT A SHOT AND REMEMBER NEVER。|EVER LET ANYBODY TELL YOU THAT YOU CANT DO ANYTHING AFTER ALL YOU ARE THE AUTHOR OF YOUR OWN STORY。\"}\n",
      "multi-modal tagging model forward cost time: 0.016640424728393555 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '多人情景剧', '静态', '平静', '单人口播', '家', '室内', '手机电脑录屏', '喜悦', '动态', '极端特写', '办公室', '特写', '惊奇', '朋友&同事(平级)', '室外', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.90', '0.30', '0.11', '0.08', '0.06', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/0ffe610098e87b43f2724d1905b38e76.mp4\n",
      "{\"video_ocr\": \"哥哥|我嫂子|喝多了吧|借我点钱呗|我最近|我哪有钱呀|钱都在你嫂子那儿呢|哎哎哎|我在安逸花上|还有十好几万呢|十好几万|真的吗|我身边很多朋友|都在用这个安逸花|最高可借20万|最快1分钟就放款|那我也要测测|15万|我竟然有15万额度|老公|我有多少额度啊|赶快点击|视频下方链接|就能在线测试|你的额度了|凭本人身份证|马上消贾|申请额度 一选择安逸花的六个理由.一|利息低|安逸花申请 最长可分12顺最快1分钟放款额度可错环|年化利率最低7.2%|年化利率最低|填写资料|完成 2|200.000|额度高|安逸|安诊花|逸花|贷款额度、放款时间等以实际审批为准 贷款有风险，借款需谨慎，请根据个人能力合理贷款|4:22 4G|A8C JKL|PQRS|MNO|十头# TUV|即可申请|DEF|WXYZ\", \"video_asr\": \"哥哥嫂子喝多了吧。|哥，借我点钱吧，我最近还哪还有钱呀，钱都在你嫂子那呢啊，我在安逸花上还有十好几万呢，十好几万真的吗？我身边很多朋友都在用这个安逸花，最高可借二十万，最快一分钟就能放款，那我也下载。|我十五万，我县有十五万额度，老公，我有多少额度？赶快点击视频下方链接，就能在线测试你的多了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01607489585876465 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '多人情景剧', '推广页', '特写', '喜悦', '平静', '单人口播', '惊奇', '夫妻&恋人&相亲', '愤怒', '动态', '餐厅', '手机电脑录屏', '朋友&同事(平级)', '极端特写', '室内', '路人', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.82', '0.72', '0.51', '0.50', '0.41', '0.06', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/100e23e0b5fbb8652e9313662858399a.mp4\n",
      "{\"video_ocr\": \"我们生活在一个|速食爱情的时代|长相|年龄|经济状况|大家好像都习惯于|通过外在|来认识一个人|外在|就那么重要吗|好看的皮囊固然珍贵|有趣的灵魂才最难得|后来|我意识到|我对他抱有了|新的期待|原来所谓爱情|不是才貌者的得分游戏|嘉琪|而是遇上对的人|即使人海喧嚣|也能听见|你的心跳|来瞧瞧|聊得来再了险|原来你也这样想|我想跟你打个赌|周三晚上中央广场见|我赌 我会在不知道你 外貌的情况下 找到你|2-1913|完善个人资科玩，我野合语的策友|完香个人资料页，找到合适的朋友~|完蕾个人资料页，找到会选约服友|关于艺来，可以立样介能有|关于艺术，可以这样介绍自己|薛定谔的喵w 21岁.处女座|21岁经女度|中重等动 6|中国移动 4G|距赢6.97km|你在这好|丽立 理里李离力利|关于艺术，事明之城们经身e|＃灵魂有趣|eod|la . vpclua|123|o0|宜宾|本科\", \"video_asr\": \"我们生活在一个速食爱情的时代。|长相，年龄，经济状况。|大家好像都习惯于通过外在来认识一个人。|外在就那么重要吗？|好看的皮囊固然珍贵，有趣的灵魂也这样想。后来我意识到。|我都看到有了新的期待，我想跟你打个睹，周三晚上中央广场见我睹，我会在不知道你外貌的情况下找到你。|原来所谓爱情不是才有着不同的东西。|速度。|而是遇上对的人，即使人海喧嚣也能听见。|你的新娘。|来瞧瞧聊得来再看脸的社交APP。\"}\n",
      "multi-modal tagging model forward cost time: 0.01595783233642578 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '手机电脑录屏', '喜悦', '动态', '平静', '悲伤', '惊奇', '单人口播', '全景', '特写', '愤怒', '家', '拉近', '亲子', '路人', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.80', '0.69', '0.32', '0.17', '0.17', '0.08', '0.08', '0.06', '0.04', '0.04', '0.03', '0.03', '0.03']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/100e893ea9a97ddb1031d8d3a5d486c2.mp4\n",
      "{\"video_ocr\": \"别怀疑 他们都在用|这个我要了|诶 没钱了?|扫这个吧!|这顿饭我请了|来 扫吧!|嘿柔什北呢|我没想干什么|你们误...|你有什么目的|快说|我就想知道|她哪来那么多钱|你说的是这个呀|这个是玖富万卡|只要输入 手机号和身份证号|就能测测你的额度啦|最高有20万呢|那我有没有啊|每个人都有|你当然也有了|把手机给我|天哪!|我居然有|我有钱了|点这里|输入你的|实际额度以审批结果为准，请珍视信用 技还勿过幸极|实际额度以审批结果为准 请珍视信用，按时还款，勿过度举债|以审批结果为准，请珍视信用。按时还款勿过度举|实际御们雷以韩社结果为准，信用，按时还款，勿过度举债|实际额度以就天黄视信用，按时还款，勿过度举债|实|言用， 按时还欢，勿过度举|村还款，勿过度举惯|20W|最高额度(元)|测测你能借多少|ONECARD|注册即表示您同意 《(平台用户注带协议)|注册部表示您网意(平包限户注爆特议)|你h声美 主食来果腹|你帅江尚|测测你能借多少钱|玖富万卡 Q欢直万卡|中国移动 4G 18:06|恭喜黄·，获得35000元额度!|三步极速借款|请输入您本人办理的手机号|实际额度以审批结界|与额度以审批结果为准，请|5w|10w|惹活额度|大以富方卡|填写贲料|审核额度、放款时间 最终以持牌金融机构或具备 放款资质的审核标准为准|付款码|9202|200000|快速提现|ihes. 如odt sw|150l|26%|米线|饮品|4G\", \"video_asr\": \"老板这个我要了。|哎，没钱了。|扫这个吧，这个饭我请了。|来扫吧。|那你干什么呢。|我没想干什么，你你有什么目的？快说我就想知道他哪来那么多钱啊，哎，你说的是这个呀，这个是久负万卡，只要输入手机号和身份证号就能测测你的额度啦，最高有二十万呢，那我有没有啊？每个人都有，你当然也有了，把手机给我，唉，行啊，我这手机我都二十万，我有钱了，点这里输入你的手机号和身份证，测测你的额度吧。|最高二十万来，请你不问他这次你有多少额度吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016026973724365234 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '手机电脑录屏', '中景', '静态', '城市道路', '喜悦', '多人情景剧', '特写', '极端特写', '餐厅', '惊奇', '平静', '朋友&同事(平级)', '单人口播', '动态', '汽车内', '路人', '单人情景剧', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1018fea26f36005f70d027790c4efff9.mp4\n",
      "{\"video_ocr\": \"只需要几分钟就能处理上百个表格|我一个月给你两万|两万还不行吗|经理|她刚毕业|凭什么一个月两万啊|她会用Python|用Python写几行代码|你们行吗|Python我们都会啊|那么难|你们怎么可能会呢?|上扇贝编程Python课啊|0基础就能轻松学|特别简单|名师倾力打造的精华课|4天带你轻松上手|现在报名只要6块9|都用不上一顿饭钱|你们怎么报名的啊|点击视频就可以报名啦|￥6.9立即体验|涨薪季 6.9|学什么 Python基础课+认知课|Python认知课|教学特色 交互式课堂+作业实操+每日一练|学完后将收获|用户评价|天·带你从0入门|专业教研团队|认识5个常见函数|专|啦得|扇贝编程|融限|扇贝|く◇扇 【编程|学会3大数据类型|视频为演绎情节|宾干|之路|了解基础语法|掌握编程思维 完成10+编程作业\", \"video_asr\": \"我一个月给你两万，两万还不行吗？经理，他钢笔凭什么一个月两万，他会用PYTHON用PATH写几行代码，只需要几分钟就能处理上百个表格，不行吗？拍下我们都会呀，那么难你们怎么可能会呢？上上上开心呀，零几手就能轻松学。|特别简单，名师倾力打造的精华课，四天带你轻松上手，现在报名只要六块九道，用不上一顿饭钱，你们怎么报名啊？点击视频就可以报名了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016526460647583008 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '工作职场', '手机电脑录屏', '动态', '多人情景剧', '办公室', '中景', '上下级', '朋友&同事(平级)', '喜悦', '极端特写', '愤怒', '特写', '静态', '惊奇', '悲伤', '单人口播', '平静', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/101d1e9613a3f196ebc1faea12bd65e1.mp4\n",
      "{\"video_ocr\": \"理想前程如何写到作文里面呢|在这么多年教学当中发现|作文永远是秒变|你应该先找方法|一定记住语言最重要|邀请您加入我的|9元 6项全能训练营|我们会在|作文阅读文言诗词|基础和人文素养|6个方面|给您一个|全面的提升学习效率的机会|網易NETEASE 有道精品课|找|找名师，就来有道精品课|Dentro dite\", \"video_asr\": \"理想前程如何写到作文里面的？在这么多年教学当中发现作文永远是秒变敏感，先找方法，一定记住语言最重要，邀请您加入我的九元六项全能训练营，我们会在作文阅读，文言诗词基础。|和人文素养六个方面给您一个全面的提升学习效率的机会。|你想试试啊。\"}\n",
      "multi-modal tagging model forward cost time: 0.016196489334106445 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '室内', '平静', '静态', '特写', '教师(教授)', '办公室', '课件展示', '填充', '配音', '知识讲解', '动态', '极端特写', '场景-其他', '家', '多人情景剧', '朋友&同事(平级)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.09', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/101f1720bb30aaebd6d5b8469e50635e.mp4\n",
      "{\"video_ocr\": \"哇~ 这么多|这些、这些、这些|全部都是免费送的|腾讯企鹅辅导|这是报了哪个课程|这么划算啊|小学数学思维能力提高班|清北毕业名师教您的孩子|提升速算能力|培养逻辑思维|49元·12节课|60个模块 120个知识点|班主任1对1辅导答疑|49元让您家孩子成为|计算小超人|应用题小达人|点击视频下方报名吧|目形小专家|四巧板|山|立好|腾讯自营品质保障 包邮送11项教辅大礼包|清北毕业名师授课 双师教学1对1答疑|小学思维能力课|秒杀特价 价值499|3年级|一年级—五年级\", \"video_asr\": \"哇，这么多这些这些这些，全部都是免费送的腾讯企鹅辅导，这是报了哪个课程这么划算？|腾讯企鹅辅导小学数学思维能力提高班，清北毕业名师教您的孩子提升速算能力，培养逻辑思维四十九元，十二节课。|六十个模块，一百二十个知识点，班主任一对一辅导答疑四十九元，让您的孩子成为计算小超人，应用题小达人，点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016405582427978516 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '单人口播', '静态', '室内', '平静', '场景-其他', '教辅材料', '配音', '极端特写', '特写', '多人情景剧', '教师(教授)', '手写解题', '愤怒', '家', '手机电脑录屏', '家庭伦理', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.94', '0.16', '0.10', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10426cade5ad33dba95960ae810b5d35.mp4\n",
      "{\"video_ocr\": \"到底发生了什么?|您好|请问您跟他说了什么|他问我|为什么看小说 换不7手机碎片|然后我就跟他说|因为你看的不是正版的|疯读极速版|那他为什么瞬间崩溃呢|因为正版的疯读极速版|可以免费看小说|并且连续签到7天|就可以获得九个碎片|只要看看小说在|西多1I 也能获得碎片|M21王|我给他看了我刚刚|包邮免费送的P30手机|他就崩溃了|在哪里可以下载呢|现在点击视频下方链接|就可以下载到正版的|明日签到继续领碎片 签到提醒|刘家老太太只好另谋他策， 请来了一个仙风道骨的道人|医，在整个徐州为刘妍招婿 ，而陈平恰好是符合条件的|08:08|你已签到1天，别中断哟|已获得手机碎片+1 明日签到可继续获得碎片哦~|道人看过之后说刘妍不|一个九九重阳出生的人成婚 冲喜。|刘妍康复后，刘家对于|B|她管理下蒸蒸日上，可谓是|待见了，没钱没势没背景，|病，看遍了所有名医都治不 好，眼看就要香消玉损了，|刘家只能死马当作活马 事。|规则|福利中心|一年前刘妍得了一场怪|积分买|恼利|是被G害， 是病了 想要康复只有一个办法，找|真的慢康复|13:15|B0书|Q0书E|碎片成婚|9安|极速版|小区监控意外 拍到惊人一幕|赢手机|免费兑换P30|开始收集你的|4月11日程期四|刘家的摇钱树，她可不能出|了，而且变得更加美丽动人|具体活动以实际规则为准|8月24日 星期一|我的奖品|4天|知道了|后，刈，|签到领碎片|BA8160|8I6|IG|1天 ，2天|分类|书城|0.19% 设置|女碎片|3天4天5天|17:21|那一个。|星期一|新禹|疯读|赢大奖|领取|陈平\", \"video_asr\": \"您好，请问您和他说了什么？他问我为什么看小说换了手机碎片，然后我就跟他说，因为你看的不是正版机和极速版，跟他为什么瞬间崩溃呢？因为正版的疯读极速版可以免费看小说，并且连续签到七天就可以获得九个碎片。|要看看小说也能获得碎片，我给他看了，我刚刚包邮免费送到了P三零手机，他就崩溃了。那么正版的疯读极速版在哪里可以下载呢？|现在点击视频下方链接就可以下载到正版的疯极速版了，快到家的！\"}\n",
      "multi-modal tagging model forward cost time: 0.020075082778930664 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '中景', '推广页', '静态', '填充', '配音', '场景-其他', '单人口播', '平静', '愤怒', '室内', '惊奇', '多人情景剧', '喜悦', '情景演绎', '工作职场', '朋友&同事(平级)', '极端特写', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.86', '0.52', '0.32', '0.27', '0.20', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/104a603b046f03fb3c5ea3fe8fe631b1.mp4\n",
      "{\"video_ocr\": \"自然·天气|阳|识字互动游戏书|第四册:水果|针对3-6岁读书识字敏感期|*非赠品|课s3课程|啡赠品|3课程|手机/平板在家学|拼音为系统课s3课程|拼音为系统课s3课程|桃瓜椰 蕉莓果|人头 目|鼻耳|头目|教具礼盒 全国包邮(港澳台地区除外)|yang|yue|00|xuě|shan|反义词二|¥49 10节A1动画课|专业老师全程辅导|白云|雪 花|重开关|梨橙|眉牙|趣味动画语文课|热冷 快|人 鼻|#首为|拼音为|汉字画出来|斑马A/课|雨雪|笑 哭轻|慢空满\", \"video_asr\": \"好。\"}\n",
      "multi-modal tagging model forward cost time: 0.02194952964782715 sec\n",
      "{'result': [{'labels': ['场景-其他', '推广页', '现代', '极端特写', '教辅材料', '静态', '配音', '课件展示', '宫格', '动画', '绘画展示', '手写解题', '商品展示', '喜悦', '中景', '拉近', '才艺展示', '动态', '惊奇', '医生'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1051d85f9a9f6fc69532f5e9826bbba5.mp4\n",
      "{\"video_ocr\": \"小姨小姨|怎么了圆圆慢慢说|小姨救救我妈妈|圆圆别乱说|姐夫我姐怎么了|没事儿|妈妈被查出了乳腺癌|手术费要三十万|你姐不让我说|不对啊|我前一阵子不是跟你俩说|让你们在健康的时候免费领取平安i动保吗|这家里没病没灾的|还要填一大堆的信息|我这不嫌麻烦嘛|就几分钟的事|填写个信息|就能领最高100万医疗保障|还有啊|现在免费领取|就能有最高10万重疾保险|能保25种高发重疾呢|你说你们当时要是领了|现在不就不用这么着急了嘛|这个拿去救急|别等意外来了再后悔|赶紧点击下方链接|查看详情|给自己和家人领取一份健康保障吧|免费领保额(限新用户)|就上平安健康APP|亚安健康保脸险|平车安健月份余脸|平安健品果险|平安健股|平安健月保果险险|(限新用户)|(走路换保颤)|本产品由平安健康险公司承保 投保需如实健康告知保障内容以保险合同为准|AWAI|RWST|平安i动保|平 安动 保|住院医疗保障 重大疾病保障 最高100万元 最高10万元|平安 健康 买保险\", \"video_asr\": \"小姨怎么了？媛媛妈妈说谁叫叫我妈妈。|别呀，别乱说姐夫，我姐怎么了？没事吗？为啥出了乳腺癌。|手术费要三十万，你姐不让我说。|不对啊，我前一阵子不是跟你们俩说让你们在健康的时候免费领取平安I动保吗？这家里没病没灾的还要贴一大堆的信息，我这不是嫌麻烦吗？就几分钟的事，填写个信息就能领最高一百万的医疗保障。|还有啊，现在免费领取就能有最高十万的重疾保险，能保二十五种高发重疾呢，你说你们当时要是领了，现在不就不用这么着急了。|这个拿去救急。|别等意外来了再后悔！|赶紧点击下方链接查看详情，给自己和家人领取一份健康保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015913724899291992 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '多人情景剧', '推广页', '特写', '平静', '惊奇', '动态', '室外', '愤怒', '悲伤', '朋友&同事(平级)', '极端特写', '喜悦', '单人口播', '手机电脑录屏', '夫妻&恋人&相亲', '亲戚(亲情)', '厌恶'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.86', '0.74', '0.56', '0.43', '0.32', '0.16', '0.11', '0.05', '0.03', '0.03', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/105e8e46388498e87de32d45ff9759af.mp4\n",
      "{\"video_ocr\": \"成长打卡地图|你想让你家孩子|说出一口流利的口语吗|那就报名|伴鱼自然拼读课吧|3到8岁是孩子|学习语言的关键期|14节自然拼读体验课|让孩子找到|学英语的好方法|通过掌握26个英文字母|及字母组合的发音规律|用学拼音的方法学习英语|力求在潜移默化中ub|见词能读 听音能写|14节课仅需29元|还赠送配套的|只要按要求完课|还能领取等额奖学金|学好英语其实不难|赶紧左滑视频|给你家孩子报名听课吧|伴鱼|WeLlke|未充课|NUtS Story - ut|DAY51|适合3-8岁孩子|Basic Skills - ub|手机平板随时学 社群老师专业辅导|ALittle Cub|2人拼团|伴急自然拼读学习成长地眉|uO|29元/14节|猪pig|狐狸fox|狮子 lion|鸡 chicken|9|PHONICS 伴鱼自然拼读课学习成长记录表|自然拼读的方式学英语|ABC|自然拼读 Al动画英语课|专享价|耒完课|无限回放|A日\", \"video_asr\": \"猪PIG PIG。|狐狸FOX FOX。|狮子LINE LINE。|七。|CHICKEN CHICKEN你想要你家孩子说出一口流利的口语吗？那就报名伴鱼自然拼读课吧！三到八岁是孩子学习语言的关键期，十四节自然拼读课，让孩子找到学英语的好方法。|通过掌握二十六个英文字母及字母组合的发音规律，用学拼音的方法学英语的仇在潜移默化照片孩子做到见词能读，拼音能写，现在报名十四节课，仅需二十九。|赠送配套的成长，打开地图，只要按要求完课，还能领取等额奖学金，知道英语其实不难，赶紧左滑视频给你家孩子报名听课吧！|FISH PHONICS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016311168670654297 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '填充', '中景', '场景-其他', '配音', '静态', '单人口播', '平静', '课件展示', '喜悦', '教辅材料', '手机电脑录屏', '室内', '家', '动态', '动画', '宫格', '知识讲解', '多人情景剧'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.78', '0.04', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1065b4ce0ddce6af6f5559cd1ad06f68.mp4\n",
      "{\"video_ocr\": \"你还在为孩子|数学成绩|而着急吗|你还在为孩子的数学成绩|提不上去而焦虑吗|现在机会来了|作业帮直播课现推出|小初高数学名师提分班|仅需30元即可拥有|18节提分课|清北毕业名师带队授课|全程线上直播|3年内无限次回放|让您的孩子随时随地 想学就学|课后辅导老师|1对1答疑|加超值学习礼盒|加深孩子对知识点的印象|更有22个计算题解题大招|25个应用题经典解法|12个几何代数专项突破|7大数学思维能力|帮助孩子稳定解决|提分难的问题|这些问题|作业帮直播课|帮你解决|还在等什么|赶紧点击视频下方链接|报名吧|6|名师有大指 解糖更高效|中国女排为作业帮直播课代言|Smlee|计算能力弱|上课内容与收到礼盒请以实际为准|中国女排|应用题不会写|lhNlWn主R古+买田-+|10+日L田|30元 18节课|不理想|不稳定|做题没方法|报多 即送\", \"video_asr\": \"你还在为孩子数学成绩不理想，不稳定而着急吗？你还在为孩子的数学成绩提不上去而焦虑？现在？|回来了作业帮直播课现推出小初高数学名师提分班，仅需三十元即可拥有十八点几分课，清北毕业名师带队授课。|全程线上直播，三年内无限次回放，让您的孩子随时随地！|学习课后，辅导老师会给大家超值。|家深海最知识点的印象只有二十二个汽车机器的大招，二十五个应用题。|二十二级的赛事，专项突破七大数学思维能力，帮助孩子稳定。|计算能力弱，应用题，不会做题没方法，这些问题作业帮直播课，小初高数学名师提分班帮你解决，还在等什么，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016121864318847656 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '平静', '室内', '静态', '单人口播', '动态', '极端特写', '手机电脑录屏', '特写', '配音', '拉远', '教辅材料', '多人情景剧', '场景-其他', '手写解题', '拉近', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.88', '0.45', '0.19', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/106f9047e8f180aa1445e5c6e8b51a8f.mp4\n",
      "{\"video_ocr\": \"怎么|考试又跪了|跪了 没进前十|你妈没给你报 高途课堂啊|9块钱的直播课 浪费时间|那可是|北大清华 毕业的老师|直播授课|能学什么啊|语文130个 高频考点|数学503个 必考知识点|英语6大套路模板|物理108个 答题技巧|语数英物4科 全面提升|还有吗|课程支持3年内 无限次回放|课后还有 辅导老师1对1答疑|唉 早知道就报了|点击视频下方 查看详情|现在还可以报|新学员9元专享 立即报名|视频为演绎情节|凯频为没蜂惯节|视频为滴|力质情节|省高考状元带队授课老师平均教龄11年+|名师特训班|85NH99\", \"video_asr\": \"怎么考试又跪了，跪了没进前十，你妈没给你报高度的九块钱直播课，浪费时间，那可是北大清华毕业老师直播授课学什么？语文一百三十个高频考点，数学五百零三的必考知识点，英语六大套路模板，物理一百零八个答题技。|语语数英物四科全面提升还有吗？课程支持三年内无限次回放课后啊，还有辅导老师一对一答疑。|早知道就报了，点击视频下方查看详情，现在还可以报。\"}\n",
      "multi-modal tagging model forward cost time: 0.016206741333007812 sec\n",
      "{'result': [{'labels': ['填充', '现代', '多人情景剧', '中景', '汽车内', '推广页', '室外', '平静', '亲戚(亲情)', '动态', '全景', '特写', '静态', '惊奇', '家庭伦理', '喜悦', '愤怒', '亲子', '路人', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10726e967537af2eaa74fc7212289b54.mp4\n",
      "{\"video_ocr\": \"选来选去|还是选择|我最信赖的精锐教育|作为一名精锐老师|专心对孩子一对一辅导|我们为每一位孩子|量身定制针对性的方案|面对面答疑解惑|帮助孩子及时查漏补缺|来精锐|您的孩子也可以做学霸|让作文更精影|分式的加减|VIP1VE|VP1W1|这置|放智|小\", \"video_asr\": \"选来选去，还是选择救心，来分析一下，作为一名几位老师，专心对孩子一对一辅导，我们为每一为孩子量身定制针对性的方案。|面对面答疑解惑，帮助孩子及时查漏补缺，来京为您的孩子也可以做学霸！\"}\n",
      "multi-modal tagging model forward cost time: 0.016172170639038086 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '平静', '多人情景剧', '室内', '亲子', '单人口播', '喜悦', '特写', '动态', '极端特写', '家', '家庭伦理', '情景演绎', '朋友&同事(平级)', '手写解题', '教辅材料', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.93', '0.79', '0.48', '0.38', '0.22', '0.08', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10798b465527786dcc7fc20337d3dd2e.mp4\n",
      "{\"video_ocr\": \"在家学习 适合3-8岁 1对1辅导|随材礼盒为课程配套物品 不同级别的礼盒略有差异|熊美术|美术宝出品 包邮|包邮赠送绘画大礼包|BEARARE|小限美术|儿童绘画启蒙课|49元10节|立即报名|S2\", \"video_asr\": \"哦哦，我画画的北美，画画的北美，奔驰的小野马和大四的微微，我说不开心也拍手拍手，伤痕累累。|又干涩的眼泪酿出香醇肥美C茅台般啦，却有温柔的汉奸出温馨的话，有相的多金的吗？多斤的芭蕾，你去巴黎度假，让凡尔赛宫的艺术嫁给毛笔作画，送我大品纪念，把你娶回家吊吊他的这个欣赏了，孩子缺个妈。\"}\n",
      "multi-modal tagging model forward cost time: 0.016037464141845703 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '绘画展示', '场景-其他', '极端特写', '静态', '才艺展示', '室内', '配音', '商品展示', '情景演绎', '教辅材料', '特写', '室外', '多人情景剧', '喜悦', '平静', '手机电脑录屏', '重点圈画', '中景'], 'scores': ['1.00', '1.00', '1.00', '0.98', '0.94', '0.92', '0.04', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1087d14008360ccc6350dc805bb89efb.mp4\n",
      "{\"video_ocr\": \"腾讯开心鼠英语|限时特惠活动|49.9元|15节课的|价格已经确定|我想把这|点读笔|也放进|免费礼包送出去|啊|礼盒里面|已经有贴纸|挂图和单词卡了|还有10本|英文绘本|一本纪念册|成长纪念|再送点读笔|那咱们成本|没关系|这些不变|再加送一支|既然是|仅此一次的|就要把优惠|做到最大|也能更好地辅助|孩子线下学习|尽快安排下去|赶快点击|视频下方链接|报名吧|礼盒包邮送到家|Unit ① 有声哒|启蒙英语，就上腾讯开心鼠|fish|LOok|I Have|398元 超值教材大礼包 包邮 老师专业全程辅导|点击视频下方链接报名|ABCmouse|mouse|15节趣味英语课|38岁孩子|ABCmouse\", \"video_asr\": \"腾讯开心鼠英语这次限时特惠活动四十九点九元，十五节课的价格已经确定。|我想把这点读笔也放进免费礼包送出去啊，你和里面已经有贴纸，画图，单词卡啦，还有十本英文绘本，一本纪念册，再送点读笔，那咱成本没关系，直接不变。|加送一支点读笔，既然是仅此一次的限时特惠，就要把优惠做到最大，点读笔也能更好的服务孩子，线下学习尽快安排下去，赶快点击视频下方链接报名吧，可以和包邮！|送到家！\"}\n",
      "multi-modal tagging model forward cost time: 0.016428709030151367 sec\n",
      "{'result': [{'labels': ['填充', '现代', '多人情景剧', '中景', '推广页', '工作职场', '平静', '上下级', '动态', '宫格', '家庭伦理', '办公室', '喜悦', '惊奇', '企业家', '静态', '极端特写', '特写', '亲子', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.86', '0.07', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/108b9ae4810b510442abf9bb4b844c78.mp4\n",
      "{\"video_ocr\": \"我从来没有后悔的事 |“因为我始终”|都是有把握后|川才有选择|我很庆幸|我在人海中选择了他|我也很庆幸婚纱照 |我选择了|太原幸福V摄影|用客照说话|太原专属|内外号景拍摄基地|自建小景海景|拍摄基地|全程12对1服务|\\\"12年品质保障|婚纱照不满意!|免费重拍!|底片全送|川产品终身保修|省心|省钱!|POPOGR|PHOTOGCR|幸福薇摄影待嫁女人心|薇|幸福微待嫁女人。|易斑直线y=kx+b(k千o]多过lCU.o).且把巴AAOB分A 两部分1茇厶A0B 很分成的两部分面祀比为15.求b的值 すK+b|xib|o-.0cb 水ib=ラ|EA|+blek*o1与直线/--x+2相之内|(2八.’SaAoBx2222|(2八.SaAOB X2Y2-2|SAAOB TX2Y2-2|老aloN周a1PA|PHOTCO|THOOOR|VPHT|童直射影|刚aIPA\", \"video_asr\": \"我从来没有后悔的是，因为我始终都是有把握后才有选择，我很庆幸我在人海中选择了他，我也很庆幸婚纱照我选择了太原幸福薇摄影，太原幸福微摄影，用客照说话，太原专属内外景拍摄基地，自建外景海景拍摄基地，全程十二对一服务。|二年品质保障，婚纱照不满意免费重拍，底片全送，产品终身保修，省心省钱，拍的好就去幸福薇摄影！\"}\n",
      "multi-modal tagging model forward cost time: 0.016405344009399414 sec\n",
      "{'result': [{'labels': ['现代', '平静', '推广页', '静态', '配音', '中景', '特写', '全景', '填充', '单人口播', '动态', '场景-其他', '情景演绎', '室外', '喜悦', '多人情景剧', '混剪', '室内', '拉近', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '0.96', '0.91', '0.88', '0.86', '0.82', '0.58', '0.24', '0.22', '0.05', '0.04', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/108ed2079729c0ed8bb1995325f81fed.mp4\n",
      "{\"video_ocr\": \"语文英语双提升|九块钱十六节课|五种单词记忆法|课后老师1对1辅导答疑 教会为止|初高全科名师班|错过这期，后悔一年|抢购吧!|即刻出发 逆袭学霸|狂欢来袭|高途课堂，|带队授课 倾囊相授|语文|高分作 十大|108种个重难点解题大招|名额有限|快点击 视频下方链接|2020科名师班|中小学生在线教育平台|尚大清华毕业名师|答题模板|答题模檐高分作文|36技|7大必考模块 42个几何模型|中荛购育平台|名师出高徒·网课选高途|极限会挑战|英语|《极限挑战》第六季 官方推荐|分|先到先得|精理|EN FIGHTINE|数学|大必模坊\", \"video_asr\": \"高途课堂狂欢来袭，九块钱十六节课，语文英语双提升！|清北毕业名师带队授课，倾囊相授，语文十大答题模板，高分作文三十六计，数学七大必考模块，四十二个几何模型，英语五种单词记忆法。|物理一百零八个重难点解题大招，课后老师一对一辅导答疑教会为止！初高全科名师班，错过这期后悔一年，名额有限，先到先得，快点击视频下方链接抢购吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016970396041870117 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '配音', '动画', '平静', '静态', '中景', '教辅材料', '混剪', '图文快闪', '手机电脑录屏', '极端特写', '幻灯片轮播', '过渡页', '动态', '室内', '情景演绎', '喜悦', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.17', '0.15', '0.05', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10948055317cebe2be2b0f6f8cf7e369.mp4\n",
      "{\"video_ocr\": \"高|4科全面提升|11年教龄|双师 模式|G大|北大清华 毕业名师|带队授课|1对个辅导答|14诱亲技巧|80个易错点|5大物理能力 14种|高途课堂|领跑新学期|先到凭限|抢购吧|暑期特|课堂暑期特惠|9块钱16节课 语数英物|课上|班主们老师|语文|57|口语听力|物理|暑期高中全科名师班|名师 有秘籍|块钱1|14大阅读技巧|数学|名额有限 赶紧点击视频下方链接|高途课堂名师特训班|156不馨点|57塑失分点|暑赢逄课尝班|作文方能模板|新学员9元专享 立即报名|难点失分点|14种思维方 35个模型规律|英语|省高考状元带队授课 老师平均教龄11年+|名师出高徒·网课选高途|35\", \"video_asr\": \"高途课堂暑期特惠，九块钱十六节课，语数，英物四科全面提升，双师模式，课上十一年教龄，北大清华毕业名师带队授课，课后班主任老师一对一辅导答疑，语文十四大阅读技巧，作文五十六分，秘诀一百三十六个考点，数学五十七个难点，失分点。|八十个易错点，英语口语，听力，作文万能模板，物理五大物理能力，十四种思维方法，三十五个模型规律。|高途课堂暑期高中全科名师班，名师有秘籍领跑，新学期，名额有限，先到先得，赶紧点击视频下方链接抢购吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016546964645385742 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '配音', '平静', '动画', '中景', '单人口播', '课件展示', '幻灯片轮播', '静态', '过渡页', '图文快闪', '情景演绎', '室内', '教师(教授)', '影棚幕布', '知识讲解', '特写', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.17', '0.14', '0.10', '0.04', '0.03', '0.03', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1094eac5d7beaa110013bb57df99bb11.mp4\n",
      "{\"video_ocr\": \"上上上...|儿子|累不累呀|来吃个橘子|哎 一你烦不烦呀|你妈都生病了|你还这样对她|你是不是人呐|你来干嘛啊|咱俩已经离婚了|就是|他天天工作那么辛苦|玩会手机怎么啦|我是来给你送钱的|钱?|之前让你免费 领了一个保险|最高有110万保额呢|我那个保险是走路换的|走走路就能换保险|还免费的?|真的假的啊?|这中国平安推出的|平安动保|走路换保额|不花1分钱|就能免费领取 最高100万医疗险|和最高10万重疾险|你这保险怎么领啊|我也想给自己领一份|现在点击视频下方链接|最高110万医疗保障了|住院医疗保障 100万元 重大疾病保障|就上平安健康APP|查看活动规则>|免费享最高|投保需如实健康告知 保障内容以保险合同为准|请输入手机号 请输只验出|中国平安 PINGAN 专业让生活更简单|获取验证码|证一NY验证码|买保险|110万元保障|(限新用户)|智能客服|升级保障|PINGAN|平安 健康\", \"video_asr\": \"儿子累不累啊，来吃个橘子你烦不烦呀。|你妈都生病了，你还这样对你是不是人，那你来干嘛呀？咱俩已经离婚了就是。|他天天工作那么辛苦，玩玩手机怎么了哎。|我是来给您送，有钱的，之前我让您免费领一个保险，最高一百一十万，你医疗保障吗？我那个保险就是走路换的走走路就能换，保险还是免费的，这是卖假的呀，这是中国的这个。|爱动宝走路就能换保额，不花一分钱就能免费领取最高一百万的医疗险和最高十万的重疾险，你这保险怎么领啊？我也想给我自己领领。|现在点击视频下方链接，就能免费领取最高一百一十万的医疗保障了！\"}\n",
      "multi-modal tagging model forward cost time: 0.016284942626953125 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '家', '全景', '单人口播', '动态', '喜悦', '夫妻&恋人&相亲', '亲子', '拉近', '平静', '手机电脑录屏', '惊奇', '家庭伦理', '愤怒', '极端特写', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.91', '0.80', '0.65', '0.51', '0.48', '0.38', '0.14', '0.13', '0.13', '0.08', '0.08', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10c8bd1930ac248efa14187cd6a1ae06.mp4\n",
      "{\"video_ocr\": \"这里放个3|哎呦|乐乐才多大呀|这么难的数独棋|都能独立完成啦|你平时怎么教她的呀|我哪有时间教她这些啊|这些|都是跟着|斑马A课 思维体验课上|的内容学的|对啊|专门针对 3~6岁的孩子|进行数感思维培养|是游戏加动画的|形式教学|趣味性强|孩子一边玩儿|一边就把知识记住了|每天15分钟|时间短效率高|特别符合|这个年龄段孩子的|学习习惯|那这课贵不贵啊|不贵|10节思维体验课|才49块钱|还包邮赠送这个|教辅大礼包呢|原来这些都是送的呀|快告诉我|在哪可以报名啊|点击视频下方|查看详情|就可以报名了|痕辅导 在移教m|撼辅导a+教|限骊是 技算 un|狼辅导在1我需 商|狼辅导在线教算 出品|在钱教算 出品|招号|狼铺号|抵辅号|狼辅导在a|猿辅导q|猿辅m|I|IIll|2-8岁上斑马 学思维 学英语|同婚导，|斑马Al评9|斑马ΑlP|数量变多就是加 哒哒哒 哒哒哒|减号出现减少啦 减少啦|思维\", \"video_asr\": \"这里放个三，家里放个二，哎哟了，我才多大呀，这么难的数独题都能独立完成的。|你平时怎么教她的呀，我哪有时间教他这些呀，这些都是跟着斑马AI课思维体验课上的内容学的斑马AI课。|对啊，斑马AI课思维体验课专门针对三到六岁的孩子进行数感思维培养，是游戏加更快的形式教学，趣味性强，孩子一边玩一边就把知识记住了。|每天十五分钟，时间短，效率高，特别符合这个年龄段孩子的学习习惯，那这个贵不贵啊，不贵，十节思维体验课才四十九块钱，还包邮赠送这个教辅大礼包呢，原来这些都是送的呀，要快告诉我在哪可以报名啊，点击视频下方查看详情就能报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01623702049255371 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '静态', '平静', '单人口播', '场景-其他', '家', '教辅材料', '配音', '特写', '极端特写', '室内', '单人情景剧', '动画', '亲子', '家庭伦理', '手写解题', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.98', '0.93', '0.29', '0.11', '0.04', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10d35ad98d5c4a9cf86ba03cef76931e.mp4\n",
      "{\"video_ocr\": \"老爷突袭检查|十分钟内就到|谁能在10分钟内|把公司年度报表|和办公室需要的|所有网络资源交给我|奖励一个月带薪休假|怎么可能|就是|搞定|没想到公司|还真有人才啊|你是怎么做到的|我下班时间|在线上学的Python编程|这些都是小菜一碟|哇塞|那这个难学吗|不难|扇贝针对小白人群|推出了Python编程课|现在点击视频|只要6块9|就能报名啦|4天·带你从0入门|6.9|涨薪季|学什么|Python基础课+认知课|专业教研团队|典.组合|教学特色|该视频为情景演绎|6|扇贝编程\", \"video_asr\": \"少爷不好了，老爷退役再打十分钟内就到。|谁能在十分钟内把公司年度报表和办公室需要的所有网络资源交给我，奖励一个月带薪休假可能。|搞定。|没想到公司还真有人才呀。|你是怎么做到的？我下班时间在线小学的潘森编程，这些都是小菜一碟，这个你拿学吗？不难，扇贝针对小白人群推出了拍摄编程课。|现在点击视频只要六块九就能报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.01638507843017578 sec\n",
      "{'result': [{'labels': ['现代', '特写', '推广页', '工作职场', '多人情景剧', '办公室', '静态', '手机电脑录屏', '上下级', '单人口播', '朋友&同事(平级)', '平静', '喜悦', '中景', '愤怒', '企业家', '悲伤', '动态', '极端特写', '惊奇'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.95', '0.38', '0.33', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10e1e3fd7d7cefa06a3c6b14e2b81ba0.mp4\n",
      "{\"video_ocr\": \"总裁不好了|咱们360借条|调整成借4万内免息后|就是为了提供给|大家疯狂借款|是否马上调整|不但不能调整|我还要把额度增加|最高增加到20万|把放款时间也要加快|最快五分钟到账|即使过了免息期|利息也给我降到最低|降到万元日息2块7起|这样亏死了 我不干|有资金需求的人|让他们在第一时间|解决燃眉之急|帮助他们渡过难关|如果不免息了|不是违背了360借条|实惠大众的宗旨了吗|对不起 是我狭隘了|点击屏幕下方链接|输入你的手机号|就可以测出|你有多少额度了|输入手机号Q|实际额度以银而|实际额度以银行审批为准|借贷有风险选择需谨慎 具体贷款额度/时间以实际审数为准|￥ヨ60借条|￥ 360,w|￥彐60信杀|¥ ヨ60|查看6约额多|查看你的额度|B-D)E|借4万最长免息30天|我高可省20万|借条 ￥B60|亢\", \"video_asr\": \"总裁不好了。|凌借条调整成借四万内免息后封面，求大家疯狂借款是否马上调整？新人借条不但不能调整，我还要把额度增加，最高增加到二十万，把放款时间也要加快，最快五分钟到账，即使过了免息期，利息也给我降到最低，降到万元，日息两块七起。|亏死了怎么办？我们推出三六零借条，就是为了提供给有自己需求的人，让他们在第一时间解决燃眉之急，帮助他们渡过难关，如果不免息了。|不是违背了三六零借条实惠大众的宗旨了吗？|是我狭隘了。|点击屏幕下方链接，输入你的手机号，就可以测出你有多少额度了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01658797264099121 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '平静', '手机电脑录屏', '喜悦', '路人', '极端特写', '室外', '单人口播', '全景', '配音', '特写', '动态', '惊奇', '场景-其他', '悲伤', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.77', '0.74', '0.49', '0.40', '0.36', '0.29', '0.18', '0.18', '0.14', '0.10', '0.04', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10eb4d55f3c2e31b782fe5a5f958e1ad.mp4\n",
      "{\"video_ocr\": \"小豆|你画的这是自己和妈妈吗|是的|给小豆报了美术宝1对1|他进步很大|在家就能学专业的美术课了|我的老师特别厉害|我赶紧给我孩子报名去\", \"video_asr\": \"小东怎么坏的，这是你自己和妈妈妈蛋给小哥画吧，美术宝一零一他基本都很大。|在家就能学专业的，你出克了，我的老师知道学历，我赶紧给我孩子报名去。\"}\n",
      "multi-modal tagging model forward cost time: 0.015905380249023438 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '特写', '静态', '喜悦', '家', '平静', '朋友&同事(平级)', '家庭伦理', '极端特写', '动态', '惊奇', '亲子', '夫妻&恋人&相亲', '悲伤', '愤怒', '配音', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.87', '0.82', '0.71', '0.67', '0.49', '0.44', '0.39', '0.29', '0.15', '0.08', '0.04', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10efe6ae06cb432e87758cec04540532.mp4\n",
      "{\"video_ocr\": \"你家孩子算数|是不是只会 掰手指、列竖式|100以内的加减法|怎么教都学不会|孩子不会加减法|算数靠掰手指头|那是因为没有 掌握数学逻辑思维|为此|猿辅导在线教育推出了|斑马AI课思维体验课|10年以上研究 孩子数学启蒙的|教研团队倾力打造|系统培养 孩子6大知识模块|9大思维能力|16种思维方法|250多种知识点|以及10000多次 可交互挑战练习|孩子在玩中 就能打好数学基础|课程无限次回放|不怕孩子学不会|现在报名|10课时只要49块钱|还包邮赠送 教具大礼盒|家长们 抓紧时间报名吧|斑马AI课|8-2=|8|出品|685|10-6=\", \"video_asr\": \"你家孩子算数是不是只会掰手指列竖式一百以内的加减法怎么教都学不会，孩子不会加减法算数靠掰！|生指头，那是因为没有掌握数学逻辑思维。为此，猿辅导在线教育推出了斑马AI课思维体验课，十年以上研究孩子数学启蒙的教研团队，倾力打造，系统培养孩子六大知识模块，九大思维能力，十六种思维方法，二百五十多种知识点，以及一万多次可交互挑战练。|孩子在玩中就能打好数学基础课程，无限次回放，不怕孩子学不会，现在报名十课时只要四十九块钱。|包邮赠送教具大礼盒，家长们抓紧时间报名吧！|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.01680302619934082 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '静态', '单人口播', '平静', '室内', '极端特写', '配音', '场景-其他', '教师(教授)', '手写解题', '情景演绎', '教辅材料', '单人情景剧', '特写', '拉近', '家', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.20', '0.19', '0.06', '0.04', '0.04', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10f35a5ae2f1a996f973f11bc5d5e7f6.mp4\n",
      "{\"video_ocr\": \"学习数学的基础|而算数尤为重要|如果掌握不了正确的方法|孩子以后学习数|将会举步维艰|猿辅导小学数学|清北毕业老师|讲解速算巧算方法|在家就能帮助孩子|夯实数学基础|巩固数学知识|暑期数学 特训班|成老师 小学数学是|邓诚老师|猿辅导|九年教龄 高考状元 清华大学 本科|暑期圈|昌期数|E\", \"video_asr\": \"嗯。|小学数学是学习数学的基础，而算数尤为重要，如果掌握不了正确的方法，孩子以后学习数学将会举步维艰。猿辅导小学数学，清北毕业老师讲解速算巧算方法。|就能帮助孩子夯实数学基础，巩固数学。\"}\n",
      "multi-modal tagging model forward cost time: 0.01598334312438965 sec\n",
      "{'result': [{'labels': ['现代', '静态', '单人口播', '家', '平静', '推广页', '中景', '极端特写', '特写', '手写解题', '教师(教授)', '室内', '多人情景剧', '手机电脑录屏', '情景演绎', '亲子', '填充', '家庭伦理', '愤怒', '场景-其他'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.98', '0.92', '0.92', '0.83', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/10fee543622e7e62c0a80df600f35bf2.mp4\n",
      "{\"video_ocr\": \"假装努力正在毁了你!|感动了自己却得不了高分!|很多孩子每天早上5点起|晚上1点睡|数学还是一塌糊涂|因为不会学习!|你想想|努力的人那么多|但为什么满分的人却那么少!|差的就是方法!|我是高途课堂周帅老师|省级高考状元|高考数学满分|在15年教学生涯中|我总结了|无论你基础如何|我们都能帮到你!|[查看详情]就能报名!|学会硬核解题技巧|这道题|5秒就能出答案|看到三角形内有一点|加起来就等于0|直接想到奔驰定理|三者系数相加|3+4+5=12|于是呢也就是|3:12|也就是1:4|直接选择选项A|点击视频下方链接 即可报名|只需9元|为什么?|阳|高连课堂|439个知识点|57个难点失分点|省高考状元带队授课老师平均教龄11年+|B.4:5|A.1:4  B.4:5|A.1:4|高途|167个考点|80个易错点|数学 语文 英语 物理|高途课堂5秒解题技巧|名师出高徒·网课选高途|C.2:3|9元=|名师特训营|dABC内有一点◎，满足30A+40B+50C=o 则AOBC与ΔABC的面积之比为\", \"video_asr\": \"学会任何解题技巧，这道题五秒就能出答案。看到三角形内有一点加起来就等于零，直接想到奔驰定理，三者系数相加，三加四加五等于十二。于是呢，也就是三比十二，也就是一比四，直接选择选项A，假装努力正在毁了你。|多了自己却得不了高分，很多孩子每天早上五点起，晚上一点睡，数学还是一塌糊涂，为什么？因为不会学习，你想想。|努力的人那么多，但为什么满分的人却那么少？差的就是方法。我是高途课堂周帅老师，省级高考状元，高考数学满分。在十五年教学生涯中，我总结了四百三十九个知识点。|百六十七个考点，八十个易错点，以及五十七个难点失分点，无论你基础如何，我们都能帮到你，查看详情就能报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.016428232192993164 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '单人口播', '静态', '教师(教授)', '配音', '场景-其他', '室内', '平静', '影棚幕布', '手写解题', '特写', '喜悦', '知识讲解', '办公室', '极端特写', '混剪', '幻灯片轮播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.99', '0.09', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1101d9d2434d38894f4d97b90eba9f70.mp4\n",
      "{\"video_ocr\": \"这些阅读题 我能直接写出答案|不需要反复读题|不需要生搬硬套|大量阅读|但是你心中 还是会有疑惑|啊|怎么就写出来了|有点快啊|我题还没理解呢|这节奏有点快啊|完全不懂|你家孩子是这个感觉吗|作业帮直播课|帮直播襦|聘请清华北大毕业 的老师带队教学|在教学的过程中|会配备 专业班主任精心辅导|让孩子学习有保障|听不懂的|3年内还可免费回放|课程教援的74个 必备阅读写作大招|可以直接帮助孩子|迅速攻政破阅读难题|让阅读理解 不再成为拉分项|此外还有 12种语文思维能力|敏语文思维能九|6个 重难题型答题模板|等等技巧统统教给孩子|名额有限|你也赶快 点击视频下方|报名吧|00:19.65|00:20.12|00:43.97|001455906|01:26.36|01:37.78|02:08.41|02:10.86|02132.32|中国女群|屠女幸|快速提升阅读写作能力|00 名师有大宿|镇大补|清华北大毕业名师带队教学 曲 全国包邮送教辅大礼包|上课内容与收到礼盒请以实际为准|语文阅读写作提分班|20节重难点提分课|立即报名 特惠|29\", \"video_asr\": \"这些阅读题我能直接写出答案，不需要反复读题，不需要生搬硬套大量阅读，但是你心中还是会有一只啊，你就写出来了，有点快呀，我题还没理解呢，这节奏有点快呀，完全不懂，你家孩子是这个感觉吗？|作业帮直播课聘请清华北大毕业的老师带队教学，在教学的过程中会配备专业班主任。|辅导让孩子学习有保障，听不懂的三年内还可免费回放课程教授的七十四个必备阅读写作大招，可以直接帮助孩子迅速攻破阅读难题。|将阅读理解不再成为了分享，另外还有十二种语文思维能力，六个重难题型，答题模版等等技巧通通教给孩子。|额有限，你也赶快点击视频下方报名吧！|的英文。\"}\n",
      "multi-modal tagging model forward cost time: 0.015976905822753906 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '填充', '单人口播', '静态', '平静', '室内', '配音', '手机电脑录屏', '场景-其他', '极端特写', '手写解题', '喜悦', '教辅材料', '动态', '单人情景剧', '学校', '办公室', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.96', '0.96', '0.21', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1107686847c1a28e1decb5583a3b2f5c.mp4\n",
      "{\"video_ocr\": \"斑马A课 猿辅导么么教厅出品|猿辅导在线教育 出品|猿辅导在线教育 出品|奶乃|女1|友日|姐|支女|妈马|娃主|shu\", \"video_asr\": \"嗯嗯嗯。|嗯。|拜拜。|ZZZZ。|嗯。|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.01605391502380371 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '场景-其他', '平静', '极端特写', '特写', '配音', '影棚幕布', '喜悦', '室内', '教辅材料', '动态', '多人情景剧', '情景演绎', '单人口播', '课件展示', '才艺展示', '家'], 'scores': ['1.00', '1.00', '0.98', '0.94', '0.66', '0.27', '0.07', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11079b8fd64328bbf20edcf1a1914b38.mp4\n",
      "{\"video_ocr\": \"上月伤投岛快摘学院理财小白孙筠营|上日伤投快学院王财小白到绮营|上月伤投快帆摘当院黟财小白到练营|上日伤投名快摘学院鳄财|上月伤投知快摘肖防黑财|上月伤投知快蚬摘学院理|上月伤招知快帆撞防ア财小白2|上月伤投快摘学|上伤投名快|上日伤ャ快学 bln|上月伤招匀快撞学院|bl7u|上月伤投快摘学院王财小白?|日伤投岛快摘肖院财小白到练营|2l.现在0元免费学|2．现在0元|2. 现在0|21现在|2，观在0元免费|上|6日台|い台|E伤 い日价|り日|l200u|tp00元|41资s000元 ))00|41资o0|4日1资o00Z|4日了|4国7资000そ|4日工资50003|Io0|<00|IS-00|ISoo|[o|b0吃|h200元L|108元|b20元|10u|b(元|f200a元|飞0|70元|blo07 P00元u|7飞600|存款|La07|L000Z|)0a007|【aoZ|信用卡|彦知|日常开破|日弟破|日弟不形|最石只剩下|最后兄剩下|纯理财|bwu|投资有风险选择需谨慎 风险责任由购买者自行承担|7日伤|1月伤|1风份|7价|NOTEEOOK|QTEBOO|rOTEBooN|rOTEO0、|noTerbo|DDaies|Data|Dae|Ma|16|Duio|Lates|AMo ate|舛理T|埏理|快财|直播\", \"video_asr\": \"今天给你们看一个北漂女孩的账本，四月份工资只有五千，房租两千二，信用卡要还一千五，日常开支要八百。|最后只剩下五百存款只有零，一分钱都剩不下，还好五月份在朋友推荐下报名了快财商学院小白理财训练营，当月理财收入就有六千六百元元，六月理财收入七千五百元，原价二百九十九元的快财商学院小白理财训练营，现在零元就可以免费学，赶快点击下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.0162961483001709 sec\n",
      "{'result': [{'labels': ['现代', '场景-其他', '配音', '静态', '平静', '极端特写', '重点圈画', '推广页', '特写', '手写解题', '情景演绎', '室内', '幻灯片轮播', '悲伤', '填充', '单人口播', '中景', '教辅材料', '教师(教授)', '动态'], 'scores': ['1.00', '1.00', '0.99', '0.98', '0.97', '0.97', '0.97', '0.47', '0.33', '0.26', '0.18', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/110bd7640b6b316d637e5965c8e0d4ee.mp4\n",
      "{\"video_ocr\": \"李大伟|5年的感情|你说没就没了|你爸这病|都已经花了几十万了|现在一天治疗费|就得8千|难道让我当冤大头吗|哎|我记得你说|叔叔自己买过|微医保百万医疗险|你赶紧去申请理赔呀|你是说那个 START|保费首月1元|次月14元起的保险吗|医生说了|我爸这病要七八十万|能赔吗|保额高达600万|覆盖100种高发重疾|保障责任范围内|因大小病|意外产生的住院医疗费用|都能报销|那我妈今年62能买吗|可以|出生满30天（含)-65 周岁（含)|健康人群|都可投保|符合续保条件|可续保至100岁|你也快点击视频下方链接|用微信零钱|即可为自己和家人|添一份保额高达|600万的医疗保障了|一定要给父母添一份保障|BETETN MAr的Weo|MArsWMEe|(保费随不同年龄、有无社保等情况变化而不同)|修BAem MAT rWWEo|MAf厅|(具体保障的重大疾病详情参见保险条款)|(一般住院医疗1万免赔额/年、100种重疾住院0免赔)|(阅读保险条款和投保须知，确认被保人符合健康告知和投保条件)|M0|M1b|11|DN6|等生病就晚晩了|MArWWE|MA WNE。|片上被THW MIAT厅SWME|泰康在线 TK.CN|泰康t|DOING i9W|具体费率及保险金额以实际金额为准 本产品由泰康在线财产保险股份有限公司承保|ROE8|COMW|5T\", \"video_asr\": \"别太累了。|我的感觉你是。|你爸这病都已经花了几十万，现在一天治疗费都在八千，难道让我当冤大头吗？哎，我记得你说叔叔自己买过微医保百万医疗险，你赶紧去申请理赔啊，你说的那个保费首月一元起，次月十四元起的保险吗？你说我败家不要紧，万能万能，可能无医保百万医疗险保额高达六百万呢。|覆盖一百种高发重疾，保障责任范围内，因大小病意外产生的住院医疗费用都能报销。哎，我妈六十二万能买吗？可以微医保百万医疗险，出生满三十天至六十五周岁的健康人群都可投保，符合续保条件可续保至一百岁。|你也快点击视频下方链接，用微信零钱即可为自己和家人添一份保额高达六百万的医疗保障了！|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.01603865623474121 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '喜悦', '室外', '愤怒', '惊奇', '平静', '亲子', '特写', '路人', '单人口播', '动态', '汽车内', '极端特写', '夫妻&恋人&相亲', '全景', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.80', '0.77', '0.43', '0.32', '0.18', '0.14', '0.10', '0.09', '0.08', '0.04', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/111e634c65ce0873735dc6870caaba47.mp4\n",
      "{\"video_ocr\": \"品牌雪糕|你吃了吗|毫不夸张地说|这是我吃过|最好吃的|瓦片状的形状|充满了中国风|从上面看有一个回字|最爱特级牛乳味的|一口下去|太爽了|奶香浓郁|但是不腻|注意看|配料表排名第一的|稀奶油|所以说他贵|是有道理的|现在福利来咯|换季大活动|快来挑选|更多的口味吧|点击下方|就可以购买|点击“超级秒杀”参与活动 1元抢购30支雪糕|司(A)|瓦江苏省南通市 受委托方：江苏养|食品|受委托方:南通欣晨乳业有限公司(B) 地址|地址:江苏省扬A 食品生产许可证|嘉定区南翔铺 扬州市鼎兴路|鼎兴路33号(|滨兴路33号（扬州市！ 翔路515号|5号5层501 21物州市食品工|品工业园内 501室|电话:4008 H102100200388|贮存条件:-18|配科表:稀奶油，牛奶，海藻糖复合糖浆(海藻料|产品标准代号:GB/T31119 贮存条件:-18|藻糖复合糖浆(海藻糖 萄口味)雪糕|塘复合糖浆(海莱糖，麦 贮存条件:-18°C以T|他址:上海市嘉|产品类型:维台|产品类型:组合型雪糕 保质期:903|保质期:90天 生产日期:见包装 末增字号代|委托方:钟萨裹食品 受委托方:江 产地：江苏省#|产地:江苏省扬州市 食品生产资|方:江苏美伦 苏省扬州市|食品有限|银限公司(A) 限公司(8|公司|地址:上海市嘉定区南团|证编号:SC|品股份有限公司(C)|受委托方:米开朗食品股份有限公司(C)|南通市如东县|SC1103210 爆东县袁庄镇|电话:0513|网红瓦片雪糕|弓标准代号|佳代号:G|香精|点击首页“限时秒杀”|产地:新江省富兴市|受委托方:米开朗食品股份有限公司(C)|忠：浙江省嘉兴市|档省南通市|外观专利|编亏.3C11032100200388|证编号:SC11032062301556 南通市如东县袁庄镇人民路35号|欢晨乳业有限公司(8)|(业有限公司(B)|业有限公|地址:：江苏省南通市如东县袁庄镇人民路35号|兴市经济技术开发区白云桥路317号|食用方法:开封即食|福标准代号:GB/T3119 贮存条件:-18~C以下冷冻贮存|GB/T31119 贮存条件:-18~C以下冷冻贮存 保质 含麸质的谷|3119 存条件:-18C以下冷冻贮存 全麸质的谷物及其制品|贮存条件:-18~C以下冷冻贮存  保质期:90天|条件:-18~C以下冷冻贮存|糖，饮用水，水，全脂乳粉，甜炼乳，葡莓千，加题冰蛋黄，|墨起不是水司|无色素|无防腐剂|市赢宠至惠是有道理的|1元30支|拼多多APP专享|注:本包装及产品外观已申请外观专利，专利号:2018300|地址:浙江省嘉兴减|食品生产许可证编号:SC1103206230155|产品名称:酿红提(牛乳葡萄口味)雪糕 产品标准代号:GB/T31119|[和表:稀奶油，牛奶，海藻糖复合糖浆(海藻糖，麦芽糖，饮|馆红提(牛乳葡药口味)雪糕 热，牛奶，海溪糖复合糖案（海藻糖，麦芽糖，饮用水，水|防，海莱糖复合糖浆(海藻糖，麦芽糖，饮用水，水，全脂乳|糖复合糖浆(海藻糖，麦芽糖，饮用水)，水，全脂乳粉，甜钱|暖糖，麦芽糖，饮用水)，水，全脂乳粉，甜炼乳，葡莓干，加|XJOWIX|物质提示:本产|天天5.5起|致敏物质提示:本产品含麸质的谷物及其制品、|委托方:钟薛高食品上海)有限|质的谷物及其制品、蛋类 委托方:萧霹离良品(上海有限公|飞:钟薛高食品 蛋类及其制品|品、坚果及装 上海有|果及其果仁类|的谷物及其制品、蛋类及其制品、坚果及其果仁类制品|品、蛋类及其制品、坚果及其果仁类制品、乳及乳制品|1元限时抢购|利，专利号:201830001534.5;201830001572.0|注:本包装及产品外观已申请外观专利，专利号:2018）|及产品外观已申请外观专利，专利号:201830001534.5|地址:浙江省嘉兴市经济技术 吧已申请外观专利，专利号:201830001534.5201830|术开发区|食品生产许可证编号:SC10833048402052 地址:浙江省嘉兴市经济技术开发区白云桥路317|534.5;201830001572.0|每周田23:00.每次两件|08330484 20183000157|兰步放|聚看客|系刷|名称:酿|系霸字 称:酿红捷|下载拼多多|中表:稀奶|寻:稀奶油，|无|首页找到限时秒杀，直接点击进入|雪糕 [款式品牌丰富，随机|距开始5434 13 共2件|30支雪糕|等17.8万人想买|品含麸质的谷物及|拼多多\", \"video_asr\": \"红花配雪糕你吃了吗？好不夸张的说，这是我吃过最好吃的雪糕。哇，天上的形状充满了中国风，从上面看有一个回字最爱特特级牛肉味的，第一口下去他太少了，满三毛与，但是独立这一看配料表，排名第一的不是水，是期待有，而且无香精，无色素，无防腐剂，所以说它贵是有道理的。|福利来咯！换季打破动物为三十时，在挑选更多的口味吧，点击下方就可以购买！\"}\n",
      "multi-modal tagging model forward cost time: 0.016399145126342773 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '配音', '填充', '手机电脑录屏', '平静', '中景', '静态', '场景-其他', '动态', '喜悦', '极端特写', '室内', '单人口播', '情景演绎', '拉近', '家', '特写', '宫格', '教辅材料'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.99', '0.99', '0.99', '0.71', '0.23', '0.23', '0.20', '0.16', '0.15', '0.03', '0.03', '0.02', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11265fef29d92121227756e85b21acf4.mp4\n",
      "{\"video_ocr\": \"爸妈|你们怎么来了|我们和刘老师沟通一下|你的学习情况|她说你很努力|考个好大学|根本不在话下|不用安慰我了|我模拟考特别差|我们已经找到了|让你快速提分的方法|只看你愿不愿意|只要能提分|做什么都行|就是这个|途课堂全科名师班|里面有北太清华毕业|毕业名师团队授课|主讲老师平均教龄|10年以上|并且针对|课前、课中、课后|制定不同的授课方式|只要9元?|是的|现在报名|就能够拥有16节|讲直播互动设|让孩子和老师|在|在线互|专攻语、数、英、物4科|课后还有辅导老师|一对一答疑|okexciting news|B.还支持3年内|无限次观看课程回放|在哪里报名|你们快去|点击屏幕下方链接即可报名|请选择孩子9月升入年级|到手价|知新，冲刺领跑新学期|抓住暑假关键时期|名师出高徒·网课选高途|口诀一: D. How an|香教学，快速全面提升|D.|新用户专享 立即体验|名师有秘籍 领跑新学期 9元16节课|D. What an|4. 已知S,=-2n'+n，求a.|3. 已知S,一v+ 2n,水a,eut 4.已知S, = 2n＋n，求a|4. 已知S,=-2n2+n， 求a二-(pnt|秒涂 求a,二-(nt|3. Look!|全国百佳教师带队教学 平均教龄11年|让你秒杀全校|口诀，来秒杀! happily the baby is playing in the garden!|浙江卫视|what alan注意到!|浙江卫视指定在线教育品牌|全部做完 高途课堂|形名what 反之how|2.已知Se费yn，求a,F 3. 己知S,=n2+2n, 求a、|2.已知S秒杀|3.已知S，-厂秒2杀|3.已知S，-P秒2奔求a，uNt|秒2杀|1. 已知S，|B. What|exciting news it is!|2. 己知S 3.已知S, =|A. How|名师特训班 口华少|只看横线后面两个词|高途课堂 冰姐英语满分育你|C. What a happily A. How happily|Amy语法宝典:|7-10秒，|求a二n|2.--All of my classmates have passed the English test.|the girls are makingpaper roses in the club!|￥9|How|仅需\", \"video_asr\": \"妈，你们怎么来了？我们来跟刘老师沟通一下你的学习情况，你很努力，考个好大学啊，根本不在话下，不用安慰我了。|模拟考特别差，我们已经找到了让你快速提分的方法，去看你啊，愿不愿意，只要能提分，做什么都行，就是这个高途课堂全科名师班里面有北大清华毕业名师团队授课，主讲老师平均教龄十年以上呢，并且针对课前，课中，课后。|不同的授课方式只用九元，是的，现在报名高途课堂全科名师班，只要九元，就能够拥有十六节精讲直播互动。|让孩子和老师在线互动，专攻语数，英物四科，课后啊，还有辅导老师。|一的答疑还支持三年内无限次观看，课程回放在哪里报名？你们快去点击屏幕下方链接即可报名。\"}\n",
      "multi-modal tagging model forward cost time: 0.016443252563476562 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '平静', '多人情景剧', '静态', '亲子', '动态', '室外', '家庭伦理', '喜悦', '路人', '手机电脑录屏', '特写', '场景-其他', '惊奇', '极端特写', '全景', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.86', '0.78', '0.72', '0.68', '0.54', '0.37', '0.15', '0.13', '0.10', '0.07', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1128bfe515b1c4b5104c477d675b3b1a.mp4\n",
      "{\"video_ocr\": \"你们也太黑了吧|我去找别家|没用的|我们黑网贷都是一样的|你好我想借10万|你好啊|我们又见面了|怎么又是你|我说过|我们这些黑网贷|都是一样的|黑|好我借|不管多高的利息我接受|急用钱别找黑网贷|来随手记|什么|凭身份证在随手记上|凭身份证在|最高可获20万借款额度|最快15分钟放款|哇我有15万额度|别走啊|我们有话好说|新人享20万额鹰|用身份证就能借钱|新人专享|额度最高20万|200000 :\", \"video_asr\": \"ZZZZ。|你们也太黑了吧，我去找别家。|我们黑网贷都是一样的黑。|你好，我想借十万。|你要啊。|我们又见面了，怎么又是你呀？我说过我们这些黑网贷都是一样的。|嗨，好友间。|我保证保险不接受。|急用钱别找推广带来选手器什么。|随手记，凭身份证在随手记上最高可获得二十万借款额度，最快十五分钟放款，哇，好，有十五万额度。|哎，别走啊。|我们有话好说。\"}\n",
      "multi-modal tagging model forward cost time: 0.016147613525390625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '静态', '推广页', '朋友&同事(平级)', '办公室', '平静', '手机电脑录屏', '动态', '特写', '单人口播', '拉近', '愤怒', '喜悦', '极端特写', '惊奇', '工作职场', '室内', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.92', '0.89', '0.74', '0.62', '0.57', '0.49', '0.43', '0.35', '0.30', '0.24', '0.16', '0.09', '0.04', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1132ff60b771b19b03c0f6e2868dd196.mp4\n",
      "{\"video_ocr\": \"奶奶|你看他与我同岁|白色布衣难看死了|我才不要嫁给他|清荷|你可知三年前|因他一件布衣|就震慑境外八十万|虎狼之师|这个男孩|到底长什么样|十三年前|我四叔惨死你宁沧海之手|幸好遇见了她|不然|我可能早就惨死街头了|你们宁家|没有无辜|倩倩|不用你管|你|就是布衣男孩|我猜你就是|北境战神|霸王宁北吧|你怎么知道|奶奶已将我许配给你|我这未来老公|到底有多大本事|十五年前那个女孩|原来就是你|更多精彩内容下载米读极速版 搜索《都市最强战神》继续阅读|《都市最强战神》|本故事纯属虚构|B300:201060026|极速|米读\", \"video_asr\": \"奶奶，你看到与我同岁白色我估计难看死了，我才不要嫁给他。|清河你可知，三年前因为他一件布衣，就这个事，千万八十万狼，只是这男孩到底长什么样？十年前惨死。|沧海之手，幸好遇见了她，不然我可能早就惨死街头了，你们宁家。|没有。|亲亲哟你还。|你就是故意男孩。|我猜你就是北京战神霸王命给的吧。|来你教我学会的。|读者，未来老公到底有多大？|想。|AS AS。|十五年前的那个女孩。|原来就是你。\"}\n",
      "multi-modal tagging model forward cost time: 0.016189098358154297 sec\n",
      "{'result': [{'labels': ['中景', '多人情景剧', '现代', '静态', '推广页', '动态', '特写', '夫妻&恋人&相亲', '愤怒', '惊奇', '家', '平静', '悲伤', '全景', '朋友&同事(平级)', '情景演绎', '拉近', '工作职场', '配音', '厌恶'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.94', '0.93', '0.91', '0.83', '0.75', '0.29', '0.25', '0.24', '0.21', '0.21', '0.19', '0.04', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11361fdb2dda32b267b01d95a4ed40cb.mp4\n",
      "{\"video_ocr\": \"你的孩子|在刚接触英语的时候|是不是也存在坐不住|用汉字拼音来念英语|经常读错改不过来|导致越来越没有信心|越来越不爱学习|斑马英语特推出|北美外教趣味动画课程|专为2-8岁的孩子设计|北美外教动画教学|提供语言环境|每天只需15分钟|让小朋友坐得住|同时A互动环节|提高小朋友学习英语的信心|还在等什么赶快报名吧|送|现在报名加送: 全套英语豪华教材礼盒 (包邮到家)|猿辅导在线教肓 出品|学思维 学英语2-8岁上斑马|89元20节课 专注2-8岁少儿英语|SOgo|soso|NICE TO MEET YOU!|Your Name?|What's|班马AI课 英语|BOOKS Unit|WO0|WORD|CARDS|alue\", \"video_asr\": \"你的孩子在刚接触英语的时候，是不是也存在坐不住用汉语拼音来念英语，经常读错，改不过来，导致越来越没有信心越来越？|爱学习斑马英语特推出北美外教趣味动画课程，专为二到八岁的孩子设计北美外教动画教学提供。|环境每天只需十五分钟，让小朋友坐得住，同时AI互动环节，提高小朋友学习英语的信心，还在等什么，赶快报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016346216201782227 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '单人口播', '静态', '平静', '特写', '动态', '教师(教授)', '拉近', '室内', '喜悦', '家', '影棚幕布', '配音', '远景', '多人情景剧', '教辅材料', '场景-其他', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.69', '0.31', '0.08', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/113d68cb6194d2546647fe62e1b1db4c.mp4\n",
      "{\"video_ocr\": \"搜索店铺 拍照搜同款|立即领取抢购|周日23:00开抢|pp专享 [款式丰富，随机发货]iPhone11等品|特种兵手表|￥9.9 ￥101 查看更多>|【款式丰富，随机发货]iPad等平板电 脑【2月5日发完】|移动联通电信话费充值100元 【1月28日发完|距结束33.2|距结束09:28:34|限量2件|01月12日 23:00开始 提醒我|商品售完时未能拼单者视为抢购失败，将发起退款|nfc小米手环|立即领取抢购9.90ppo 立即领取查看更多|￥9.9 ￥5499|￥9.9￥101限量2件|19|204|墙壁花瓶 评书内存卡|限时秒杀|TUV WXYZ 0|14:00正在疯抢|动充值[1月28日发完】|生态鱼缸 搜索 更多搜索方式|即将开抢|JKL|憨豆熊坚果礼盒 17秒前参与拼单|立即领取9块9手机|正在疯抢 更多预告|01月12日 23:00即将开抢|&年货节 拼多多|移动、联通、电信通用|分词 ABC DEF|¥20.6 万人团价 德芙盒装丝滑巧克|MNO 重输|共10款商|距开始 80:28:30|秒杀万人团|年货品质|中石化加油卡充值1000 全国通用自|¥11.9起|秒杀100元话费 9.9包邮送到家|疯抢|¥36.g万人团价|如何低成本过年？|日发完】 限量特惠|闪电直充 话费快充|100元|充值|下载拼多多|预售 App专享|9.9秒杀|金士顿车载u盘|表带圈|PQRS|123 空格|GHI\", \"video_asr\": \"涨工资你给我出来涨工资优优当然妹子我想回咱宁愿出花费，是不是我出一百让我团五十四。|宠物是宠物是被骗你了，是不是在配合上给我出来开头九块九你都知道了，那你就我四十块钱还不给你弄。|哎呦哎呦妹子哎呦，你打开屏幕组搜索立即领取，点击查看更多就能找到九块九秒杀一百元话费了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016837358474731445 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '填充', '场景-其他', '多人情景剧', '手机电脑录屏', '喜悦', '配音', '朋友&同事(平级)', '平静', '惊奇', '单人口播', '特写', '室内', '极端特写', '亲戚(亲情)', '全景', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.90', '0.76', '0.44', '0.37', '0.15', '0.05', '0.05', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/114fe2edb40dc652a4be207a48fdfabe.mp4\n",
      "{\"video_ocr\": \"李雅|你太过分了|我把闺女抚养权给你|不是让你带她去送水的|我|把女儿抚养权给我|不然等着被起诉吧你|爸爸不要吵妈妈了|妈妈得了癌症|想多攒点钱给我|癌症|你怎么不去医院啊|光手术费都要30多万|还不如不治|我之前不是让你领过|平安i动保吗|可那是免费的呀|你都没看条款吗|每天走走路就能换保额|住院医疗保障最高100万|那癌症也能理赔吗|保障范围内不限疾病|不限社保用药|还有重疾保障最高10万呢|走跟我去医院去|你也快点击下方链接|输入手机号|免费领取你的保障吧|华豫三札|就上平安健康APP|买保险|投保需如实健康告知|保障内容以保险合同为准|(限新用户领取)|平安 健康|PINGAN\", \"video_asr\": \"你呀，你太过分了，我把闺女抚养权给你，不如让你带她去送水的，把女儿抚养权给我。|不等了，费奇都不理爸爸，不要再找我妈妈了，妈妈得了癌症，想多攒点钱给我。|你怎么不去医院啊，我手术费得三十多万，还不如不吃，我之前不是让你领过平安夺宝吗？可能是免费的呀，不过手术费得三十多万，你都没看条款吗？|平安爱动宝每天走走路就能换保额，住院医疗保障最高一百万，那还是能理赔吗？保障范围内不限疾病，不限社保用药，还有重疾保障，最高十万呢，走，跟我去医院去。|你也快点击下方链接，输入手机号，免费领取你的保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.017601490020751953 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '多人情景剧', '推广页', '愤怒', '特写', '喜悦', '平静', '极端特写', '室外', '惊奇', '动态', '路人', '全景', '朋友&同事(平级)', '悲伤', '夫妻&恋人&相亲', '工作职场', '上下级'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.49', '0.49', '0.27', '0.13', '0.12', '0.11', '0.08', '0.06', '0.05', '0.04', '0.03', '0.03', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1155df31df714f5b0d49df66e978aca4.mp4\n",
      "{\"video_ocr\": \"您的免费P30|真的能免费领|P30手机吗|对呀千真万确|免费P30给我来一个|10个碎片集齐了吗|啥碎片啊|DED 我们是疯读极速版|只要集齐10个P30碎片|BROLNDE|就送免费的P30哦|你们这碎片好集吗|好集得很|新用户只要看5章小说|OLNDED|立送1块碎片|剩下的9个碎片只要|连续签到7天|也白送给你|10个手机碎片就可换|还会包邮送到家哦|这完全是|白送咱们P30手机呀|对呀|赶快点击下方链接|上线读免费小说|集齐10个手机碎片|立马送你|签到领碎?片 赢5G手机和更多好礼|阅读领取|更新于今天>|正在拼命加载中…..|胖橘|而中年贵妇身边坐着一个年轻漂亮 的女人|后，她就前往了医院。 您已进入了流产手术|您阅读太快，碎片领取要有耐心，|超给力 喜欢就加入书架吧|目录 连载至703章|一胎三宝，总裁爹地超给力|问了起来。|璃在自己母亲墓碑前待了三个|モ饭 总モ副下|废后归来:嫡女狠角...|明日签到继续领碎片|签到提醒|MCILAE|7|医妃倾宠世无双|“赵医生，检查结果怎么样，我媳妇这个身体可以做人|TOP1 本周现代言情榜|赵医生皱着眉头看着中年贵妇，声|19元/月=首月免费用+200G高速流量+100分|0分 确定|4月11日星期四|新人福利 每认真阅读五章得1枚，最高可得20+碎片 距结束仅剩|萌宝|豪门总裁|这个身体可以做人工受孕吗？”|肚子里的孩子来的不是时候，|胎三宝|华为手机碎片 剩余27份|0/10|“顾太...”.|皮着眉 妇已经|却没有想到，孩子的父亲竟然是他! 那个自恋狂!|S酬晃|她生下三个天才宝贝，发展事业，一路打脸虐渣。|换一批己|其妙怀孕。她生下三个天才.|认真阅读文章 阅读15分钟 +1碎片|继续阅读>|加入书架随时阅读 合法授权·违者必究|看视频免30分钟广告>|的棋子。庶妹辱她，就撕破... 古代言情|免费送 p30手|第1章 扫地出门|我是为了你才嫁到顾家的，如 了这个世界，我已经找不到活|赢手机|PROW|读享快乐 每日阅读轻松赚碎片|“你儿媳妇已经怀孕一个月...”|疯读极速版(原扎堆小说):已加入书架|RCONH PRO|AE|UA|LD ONM PROwC|ROTIND|赵医生皱着眉头看着中年贵妇，声音带着一种为难。 这话一说完，沈琉璃惊愕地看着赵医生，手下意识地放|医生，手下意识地放在了肚子上面。 这话一说完，沈琉璃惊愕地看着赵|-去共空师正在拍着一个可爱的男孩子|下意识地放|嫁给植物人之后，沈琉璃莫名其妙怀孕。|帝都中心医院的一个办公室里面，|‘母亲，我不想让我的孩子和我|工受孕吗？”|咄门 检查线|9.8 评分商商商女肉|星卡 取消|总裁爹地|“赵医生，检查结果怎么样，我媳妇|有气了。” 金璃红着眼睛跪在了墓碑面前。|里日 C桑园|重归当年，她不再是仇人手中|您已进入 新人认真阅读五章即可得碎片~|请认真阅读哦~|领取 你已签到1天，别中断哟|免费阅读|“赵医生，你确定没有检查错吗？”|个男孩子长得很是精致，他是刚|一个中年贵妇对着面前的赵医生着急地|-样没有父亲..”.|福利中心 规则|2619万人气|现代言情·连载137万字|在了肚子上面|完，所|饭宝三|这话|刚刚穿越，就被泼了一头冷|6天23时58分|iPhone碎片|开宝箱得碎片|碎片|路力脸虐道。|笑无单|下载疯读|703章|她怀孕了，这怎么可能!|读享|具体奖励金额以实际为准|人不召变|礼传有不香|1天|干芏三|并我的奖品|书城|下载 加书架|设音|20:19|每认真阅|MCTA|NAV|0/3|晚上8:19|碎片明细|疯读|1212王\", \"video_asr\": \"您的免费七三零真的能免费领P三零手机吗？对呀，千真万确！|免费测，你给我来一个十个碎片集齐了吗？啥碎片？我是疯读极速版，只要集齐十个碎片。|品就是免费。|你这碎片好集吗？新用户只要考五张小说立送一块碎片，剩下的九个碎片只要连续签到七天就白送给你。|十个手机碎片就可以换免费的P三零了，还会包邮，送到家就完全是白色。|必须三零手机呀。|想要免费P三，赶快点击下方链接下载疯读极速版上线总免费小说，集齐十个手机碎片立马收紧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01629018783569336 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '手机电脑录屏', '静态', '多人情景剧', '喜悦', '平静', '单人口播', '惊奇', '配音', '全景', '场景-其他', '夫妻&恋人&相亲', '家', '动态', '愤怒', '拉近', '悲伤', '路人'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.83', '0.67', '0.63', '0.61', '0.51', '0.29', '0.19', '0.11', '0.08', '0.06', '0.04', '0.03', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/115f87d031e1b4576273a350e06c5174.mp4\n",
      "{\"video_ocr\": \"医奇是被冤枉椏的 皇上|传朕指令|屐皇后|打八冷宫|是你|我的好姐|现在就轻易的得到你的一切|这种感觉真是好呢|喝盏茶补补身子吧|(这是怎么回事)|(连诗雅）|滩道|我重生了|又是这茶|原来xn比|连诗雅|你给我等着|谢谢味味|不过这么好的茶|还是留给味味自己喝吧|搜荣《一品有女》 催续阅读|品婚女>|《一品婚女司|<一品有女>|得间|《一品有安》|一品确女》|有女》|得间小说|欲知卮事 小说|10\", \"video_asr\": \"臣妾是被冤枉的，皇上传朕指令菲皇后。|打扰了勾。|是我的好姐姐，现在就轻易的得到了你的一切，这种感觉真是好。|和检查和补身子。|棒子。|呵呵。|放开。|韩振，直隶皇后是怎么回事？若冷冻我的好姐姐现在就轻易的得到了你的一切，这套感觉真是好了。|我重生了。|又是这张玉兰如此。|也是呀。|给我顶盒盏茶补补身子吧。|谢谢妹妹，不过这么好的茶还是留给妹妹自己喝吧。|番荔枝故事，请下载得间小说搜索一品嫡女继续阅读。\"}\n",
      "multi-modal tagging model forward cost time: 0.016031980514526367 sec\n",
      "{'result': [{'labels': ['中景', '多人情景剧', '推广页', '静态', '特写', '现代', '动态', '愤怒', '夫妻&恋人&相亲', '悲伤', '极端特写', '全景', '平静', '室外', '喜悦', '古代', '惊奇', '拉近', '古装/武侠', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.89', '0.84', '0.61', '0.39', '0.37', '0.32', '0.23', '0.13', '0.06', '0.06', '0.05', '0.03', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/116d684a7f9449a78405b0ad1fb1c7bb.mp4\n",
      "{\"video_ocr\": \"立即报名|你家孩子是不是 英语不敢开口|语感特别差|英语成绩跟不上|我给你推荐 伴鱼自然拼读课|现在面向全国|推出14节自然拼读课|让孩子掌握26个字母 及字母组合的发音规则|教孩子学英语 就像学拼音一样自然|力求让孩子做到见词能读|听音能写|而且课程 还支持无限次回放|还包邮赠送学习成长地图|2698元的全套课程|现在仅需体验价 29元14节课|按要求完课之后 还可以领取奖学金|名额有限|快点击下方链接报名吧|plane 飞机|d爸爸|ne飞机 dad爸爸|pla. 飞机 dad爸爸|plane dad爸爸|飞了|plane 飞机 dd爸爸|孩子开心学 启蒙更有效|t帽子|hat帽子|cake 蛋梳|好子|cake“糕 ht帽子|蛋糕|cake 蛋糕|h|da|用自然拼读法学习单词发音|让小朋友养成 英语学习兴趣和习惯|动画趣味授课 孩子喜欢的英文课|Palfish Phonics|name 名字 bag袋子|字母A|d爸爸字母A|[ei]|ABC\", \"video_asr\": \"例如。|NAME。|PLAIN。|CAKE。|BAGHDAD。|HOT。|你家孩子是不是英语不敢开口语感特别差，英语成绩跟不上？我给你推荐伴鱼自然拼读课，伴鱼自然拼读课现在面向全国推出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样自然，必须让孩子做到见词能读，听音能写，而且课程还支持无限次回放。|现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价二十九元，十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧TFBYS PHONICS！\"}\n",
      "multi-modal tagging model forward cost time: 0.021358251571655273 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '单人口播', '推广页', '场景-其他', '配音', '静态', '平静', '室内', '办公室', '教师(教授)', '动画', '喜悦', '手写解题', '课件展示', '手机电脑录屏', '家', '知识讲解', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.69', '0.46', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/117fe29bef7ea71119ca178ec7817e9c.mp4\n",
      "{\"video_ocr\": \"家里有15-18岁孩子的 家长们|注意啦|高途课堂现在推出|暑期高中全科名师班|价值499元的课程|现在报名只要9块钱啦|哎哎|我家孩子开学|就上高二了|数学和物理都不好|可以报名吗|可以的这位家长|高途课堂的老师|为您家孩子总结了|数学高考7大秒杀技巧|和物理108个大招 秒杀重难点|我家孩子快上高三了|基础差|逼他刷再多题也没用|还有救吗|基础差是因为没有|掌握正确的学习方法|盲目刷题只能越学越差|北大清华毕业的老师|为孩子直播授课|课后还有班主任|1对1辅导答疑|孩子一周就能挑战高分|那这课怎么报名啊|现在点击视频下方|选择您家孩子所 对应的年级|名额有限|抓紧抢购吧|新学员9元专享 立即报名|省高考状元带队授课 老师平均教龄11年+|名师出高徒.网课选高途|高途课堂 名师特训班|视频为演绎情节\", \"video_asr\": \"家里有十五到十八岁孩子的家长们注意了，高途课堂现在推出暑期高中全科名师班，价值四百九十九元的课程，现在报名。|只要九块钱了，唉，我家孩子开学就上高二了，数学和物理都不好，可以报名吗？可以的，这位家长高途课堂的老师为您家孩子总结了数学高考七大秒杀技巧和物理一百零八个大招，秒杀重难点，我家孩子快上高三了，基础差B三刷再多题也没用，还有用啊，基础差是因为没有掌握正确的学习方法。|萌萌刷题只能说学越差，北大清华毕业的老师为孩子直播授课，课后还有班主任一对一辅导答疑，孩子一周就能挑战高分，那这课怎么报名啊？现在点击视频下方选择你家孩子所对应的年级就可以报名了，名额有限，抓紧抢购吧！|哎。\"}\n",
      "multi-modal tagging model forward cost time: 0.016330718994140625 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '室外', '喜悦', '全景', '惊奇', '平静', '亲子', '路人', '家庭伦理', '单人口播', '动态', '教师(教授)', '愤怒', '极端特写', '悲伤', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.95', '0.95', '0.50', '0.36', '0.36', '0.33', '0.32', '0.11', '0.05', '0.02', '0.02', '0.02', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11858305c18a53db615e48dbbb30e1ce.mp4\n",
      "{\"video_ocr\": \"初中生的家长 你错了|你还在认为孩子的成绩差|是因为不听话太叛逆吗|错了呀|孩子正是因为成绩差|才会不听话才会更叛逆|我教了11年的书|这种情况我之前也遇到过|孩子怎么学都学不明白|我一直会责怪他|没有上课好好听讲|但是后来呢|我仔细的反省了一下|其实孩子们会比老师比家长|都更想学好|但是苦于没有方法没有技巧|真的学不会|我开始吧学生们的难点易错点|进行总结|归纳整个初中数学|考试所出现的规律|还有考试难点|提炼出了503个必考的知识点|173个重难点|还有63套几何模型|直接应用技巧和方法做题|让孩子们先考出高分|然后建立学习自信|再去探究背后的逻辑方法原形|吃透题目本身|孩子有了成就感自然爱学习|更愿意主动的去学习|别犹豫了|赶紧查看详情|帮孩子把分数提上去吧|9元特惠|?施佳辰 1年一教学经验 最谁学首库抢分名师|掌握高效学习法 快人一步成黑马 考点整合+12大技巧=提分快 难点解析+23个模型=冲重点|跟谁学 在线学习更高效|跟谁学|在线学习更高效|初中数物 高分训练营|1688元|年|11\", \"video_asr\": \"初中生的家长，你做了，你还在认为孩子的成绩差的视频不听话太叛逆吗？错了呀，孩子正是因为成绩差才会不听话，才会更叛逆。我教了十一年的时候，这种情况我之前也遇到过，孩子怎么学都学不明白，我一直会责怪他没有上课好好听讲，但是后来呢，我仔细的反省一下，其实孩子们会比老师比家长都更想学好。|但是苦于没有方法，没有技巧，真的学不会，我开始把学生们的难点，易错点进行总结，归纳整个初中数学考试所出现的规律。|有考试难点，提炼出了五百零三个必考的知识点，一百七十三个重难点，还有六十三套几何模型，直接应用技巧和方法做题，让孩子们先考出高分，然后建立学习自信。|去探究背后的逻辑方法，原型，湿透题目本身，孩子有了成就感，自然爱学习，更愿意主动的去学习，别犹豫了，赶紧查看详情！|帮孩子把分数提高。\"}\n",
      "multi-modal tagging model forward cost time: 0.016076087951660156 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '静态', '平静', '填充', '教师(教授)', '室内', '特写', '场景-其他', '影棚幕布', '配音', '图文快闪', '家', '课件展示', '教辅材料', '手机电脑录屏', '动画', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.93', '0.05', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1185c06e135a17e3e7b0012f94ccc1a0.mp4\n",
      "{\"video_ocr\": \"这么晚回来干嘛去了呀|我累了一天了|连口热乎饭是不是也吃不上|生病这么大事为什么不跟我说呀|这光药费就十几万|我不想拖累你|老婆你别担心|你忘了我之前给你买的平安e生保2020|只要是在在保障范围内|不限疾病 不限社保用药|进药自费药均能报销|可我这是恶性肿瘤|能行吗|平安e生保2020|能保120种特定疾病|而且每年啊|有最高 400万的医疗保障呢|老婆对不起|是我没好好照顾你|你安心养病|家里有我呢|平安健康保险|平安仁保呤|平有保险|平安健唐 呆险|菜安健が棕|菜安健|莱安健珠|平女催就保卖|平卖健就保|这国结果：您性肿瘤|疾病诊断证明书|买保险 就上平安健康APP|诊断结果:恶性醉雇|本产品平时健康险公后承保 投保需如实健康告知保障内容以保险合同为准|本产品百中啡雕险公司承保|本产品曲平瘘健减险公司揉保|市人民医院|平天|进口药 自费药均可报销(免赔顿1万)|(免赔额1万)|(年缴)|最高400万保障 保障范围内 不限疾病 不限社保用药|4|健康\", \"video_asr\": \"今晚回来干嘛去了呀。|我累了一天了，累口热乎饭什么也吃不上。|生病这么大事，为什么不跟我说呀，这光医药费就十几万。|我不想拖累老婆，你别担心，你忘了我之前给你买的平安一生保，二零二零，只要是在保障范围内，不限疾病，不限社保用药，进口药，自费药均能报销。|可我这是恶性肿瘤，能行吗？平安一生保二零二零，能保一百二十种特定疾病，而且每年啊，有最高四百万的医疗保障呢，老婆，对不起。|是我没有照顾好你，你安心养病。|家里有我呢。|现在点击下方链接，为自己和家人购买一份安心保障吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015877246856689453 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '家', '静态', '动态', '喜悦', '悲伤', '亲子', '夫妻&恋人&相亲', '平静', '惊奇', '极端特写', '家庭伦理', '特写', '愤怒', '单人口播', '配音', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.91', '0.81', '0.79', '0.78', '0.70', '0.48', '0.40', '0.39', '0.32', '0.19', '0.12', '0.05', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/118954c10a9c40b4c4b356398fdbad6a.mp4\n",
      "{\"video_ocr\": \"你为什么不给我报名|作业帮直播课 SUMI|语文阅读写作提分班|其他人写完|都检查两遍了 UMM|我连题都没写完 R|SUMMER|太好了|这次语文又有提高了|哎呀妈妈你看|人家就是跟着|老师学习|现在作文写的又快又好|成绩很快就提高了|而我只会死记硬背|这位家长|你没有给孩子报名|29元就能上 20节的名师课吗|这试试也不贵啊|你快给我说说|它由清华北大毕业名师|带队教学|一次教授孩子学会|3大作文主题|教孩子轻轻松松|写出高分作文|还有74个必备 阅读写作大招|摸清语文阅读写作技巧|专注考试方法|课程还支持|3年内无限次回放|而且课后还有辅导老师|1对1答疑解惑呢|都怪我|哎这课怎么报名啊|我现在报上|现在点击视频下方链接|就可以报名了|这课程啊太火了|上课内容与收到礼盒请以实际为准|清北毕业名师带队授课|考场|写作提分班 语文阅读|￥29=20节名师课|AAJHFFIEM BAILUNSHIJLA|JA|优秀作文集|字帖本 致直播课 的我|精选古诗40首|基础知识手册|AAJHFFIER|AAJF|AAJHFFIEM BAILUNSHIJLA|成长|4ER|UIMME|HAI|险记\", \"video_asr\": \"不给我报名作业，帮你如何语文阅读写作提分班。|家人写完都检查两遍了，我连题都没写完。|又有提高了到那里。|作业帮直播课老师学习，现在作文写得有。|会有好成绩很快就提高了，而我只会死记硬背。|这位家长，你有给孩子报名二十九元就生二十九名刺客吗？真是是也不贵呀，这课程啊太火了。|你快跟我说说，这作业帮直播课语文阅读写作提分班，适用清华北大毕业名师带队教学一次教授孩子修。|三大河主题教孩子。|松写出高分作文还有七十四个必备阅读写作大招没，请语文阅读写作技巧专注考试方法课程还支持三。|年内无限次回放，而且课后还有辅导老师一对一答疑解惑的都怪我，这个怎么报名啊？我现在就报上，现在点击视频下方链接就可以报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016049623489379883 sec\n",
      "{'result': [{'labels': ['多人情景剧', '现代', '推广页', '中景', '亲子', '静态', '愤怒', '家庭伦理', '喜悦', '特写', '平静', '夫妻&恋人&相亲', '悲伤', '惊奇', '全景', '朋友&同事(平级)', '家', '动态', '室内', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.88', '0.69', '0.40', '0.39', '0.19', '0.13', '0.11', '0.07', '0.07', '0.04', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/118aaa83c82ce338180d6fa8596523aa.mp4\n",
      "{\"video_ocr\": \"我要嘛|好好好|不许闹了|那就再看一会|就回家好吗|我还没见过哪个孩子|这么爱学英语的|那你是不知道啊|3000多本精选品质绘本|丰富有趣地激发孩子兴趣|让孩子爱不释手|A智能纠音|锻炼纯正英语口语|还配有英文动画TV|闯关赛|分级阅读等趣味教学|这么多的肉容|不用我说|她白己就主动吵着要学|这个不错哎|回去我也下载一个|让我家孩子学学|只需点击屏幕下方链接|就可以下载伴鱼绘本啦|伴鱼绘本|欧美外教配音|中教老师讲解|TL2|A|听说能力|0000~|学乐-神奇校车系列|学乐民间童话故事|David (Gets in Trouble|英语听说启蒙 亲子阅读专家|Litfle Pigs|更多|By David Shannon|88分|29元抢14节课|13-15岁 三天一本|亲子生活|BIG PARADE|VIP|Loses a Tooth PumpsltP|Folk&Fairy Tale|100-200L G1|学乐-大卫不可以|avid!|Good Morning|O0*00|学乐-大红狗系列|The Little|Hal|Walking Walking|106.2万|听故事|Red Hen The Three|D。 vid|专项提升发音课|Clifford's SPRING CLEAN-UP|Magic SchodlBus|Al's Pdls|了20 早国秘动 40|NORMAN BRIDWELL|泡婚鱼|The|Pumps .t Up|Shannon|YLE|学乐-Decodable Readers|快乐小剧场|Cockroach|Level Α 第1本|7-9岁 Movers|Starters/|LittleRed Riding Ho|自然拼读|Martinq the|Tckes g Nap|aIO|aVId|AND|Today|7岁以下|学习报告|动画TV|8-10岁|Cat|Aroheling by Vioilel Findey · lustrobed by lyme Crovoh|HSCHOLASTIC|By|推荐|I See|每日学|级别说明|¥203.2万|AND THE BIG STORM|Book 1. short a|BR-100L|录绘本|381.4万 00:58|381.4万  00:58|G3-G4 PET-FCE|thc Phonics Reading program|alesaNap|g0 to schod|EASY READERS|伴鱼绘本话故事|J K|FOlk8 FajpyTale|Big A|VIP终身打分|Yamns /t(I LCD|YTale|Areling by Komo anhoen · Muatrdhed by olia Gicuoed Arowellng by Violet lindey \\\" luiowed by Kalkto Mofoyora|HSCHOLASTId VIP|15|推荐年龄 阅读建议|D.|01:05|Book12.th|pI|I see b. I see h.|趣味乐园|SES|andtheBIG PARADE|Arelollng by Vilot Findly · luwtraled by lynoe Crovoh|School|and the|batesc Map|and the DIG r2RIVUJ SPHIIVG ULEAIV-U|LMOHLCCLIOCLL LMO LUCLCLLC|ROMEO JULIET|G2-G3 KET-PET|Aretellng by Koma Einhorn · liustroted by Potick Ghrouord A roteling by Violet Finditey · Mlutraed by Keikto Moloyama|.th|D.vi d|Areleling by Violet Findley · lutiroiod by yne Crovolh|Sonn|A retellng by Violel Findley ·lmlutoted by Kelko Moloyormo|Arceweling by Violo Findley · lushrolod by lyne Crovoth Aretelng by Violel Findley · lushrahed by Kelko Moloyama|LaqiGS|Areteling by Violet Findiey · lutrorted by Kelko Moloyamd|去PK|米谷|/6|剑桥\", \"video_asr\": \"要啊。|嗯。|好好好不喜欢我就在看一会就回家好吗？|好的家风。|我还没见过哪个孩子这么爱学英语的，那你是不知道啊，你鱼绘本的魅力，这里有三千多本精选品质绘本。|丰富有趣的激发孩子兴趣，让孩子爱不释手。欧美外教，一名中教老师讲解未来智能纠音，纯正英语口语的。|能力还配有英文动画闯关赛，本期的酵素了，这么多内容，不用我说，他自己就主动吵着要学着，不错，回去我也下载一个，让我家孩子学学，只需点击屏幕下方链接就可以下载伴鱼绘本吧，伴鱼绘本英语听说启蒙亲子阅读专家。\"}\n",
      "multi-modal tagging model forward cost time: 0.016454219818115234 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '场景-其他', '手机电脑录屏', '配音', '中景', '平静', '静态', '多人情景剧', '喜悦', '动态', '室外', '亲子', '特写', '单人口播', '惊奇', '单人情景剧', '愤怒', '家', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.59', '0.57', '0.35', '0.07', '0.04', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11951ff9c7756c801bbd6de285b152f2.mp4\n",
      "{\"video_ocr\": \"聪明的高中生|会把高三的困难|在高二全部解决|如果你现在英语偏科严重|一科和其他人拉开了|30分-50分的差距|那你一定要重视了|因为方法远比努力更重要|谁能在步入高三的前百旧之内|也就是高二下学期这段时间|用最短的时间|拥有最高效的学习方法|谁就能实现在高三黑马逆袭|我是徐磊老师|我会教你刷新认知的|高中英语学习方法|合理规划高三一轮复习|无论你当前什么基础|只要上完这节课|一定会有收获|时间极其宝贵|我们一定要在关键的时候|得到实实在在的提升|平均教龄11年以上 猪导价1688|数英专项 提分训练营|老师教的好，孩子才能考的好 清北毕业名师带队教学|新同学专享9元|立即报名|跟谁学|心\", \"video_asr\": \"聪明的高中生会把高三的困难在高二全部解决，如果你现在英语偏科严重，一科和其他人拉开了三十分到五十分的差距，那你一定要重视了，因为方法远比努力更重要，谁能在步入高三的前百日之内，也就是高二下学期。|段时间，用最短的时间拥有最高效的学习方法，谁就能在高三实现黑马逆袭。|我是徐磊老师，我会教你刷新认知的高中英语学习方法，合理规划高三一轮复习，无论你当前什么基础，只要上完这节课。|一定会有收获，时间极其宝贵，我们一定要在关键的时候得到实实在在的提升，网课就选跟谁学，孩子考上好大学。\"}\n",
      "multi-modal tagging model forward cost time: 0.015967130661010742 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '单人口播', '静态', '室内', '平静', '配音', '场景-其他', '教师(教授)', '极端特写', '转场', '特写', '手写解题', '填充', '喜悦', '手机电脑录屏', '拉近', '影棚幕布', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.96', '0.32', '0.11', '0.07', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11a202eacdd4f3323be7e6f2c6c0976a.mp4\n",
      "{\"video_ocr\": \"时间宝贵|开门见山吧两位|你们在360借条的|额度是多少|360借条是什么啊|我|有20万的额度|我想我们可以|更深入的了解一下|凭啥选他不选我|现在是信用社会|360借条作为|正规贷款平台|你在上面额度|很大程度代表了|你的信用程度|和你的个人能力|360借条最高可借20万|最快5分钟就能到账|说半天了|你也没说在哪申请啊|点击视频下方链接|就可以申请啦|最快5分钟放款 点击视频下方立即申请|￥彐60借条|¥ ヨ6口借条|￥目日口借条|￥写目口 借条|￥写冒回借条|¥200001.86|立即领取|活动规则>>|236B/s令“12%|全民免息狂欢|选择360借条的四个理由 额度高 审批快|我的额度|最长30天免息|最高可借20万|贷款额度，放款时间等以实乐审邶为准 贷款有风险，惜款需谨慎，请根据个人力台理贷款|常见问题 本服务由360借条提供|最长 30天息费优惠券|借一年，慢慢还|额度明细|6:07|0000|提现 转账\", \"video_asr\": \"时间宝贵，开门见山吧，两位你们在三六零借条的额度是多少？三六零借条是什么？我？我在三六零借条有二十万呢。|我想我们可以更深入的了解一下，平常他不行，我现在是信用社会，三六零借条，作为正规贷款平台，你在上面，额度很大程度代表了你的信用程度和你的个人能力。|三六零借条最高可借二十万，最快五分钟就能到账，说半天了你也没说在哪申请啊，点击视频下方链接就可以申请啦！|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.016492843627929688 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '推广页', '喜悦', '静态', '多人情景剧', '平静', '动态', '惊奇', '极端特写', '悲伤', '朋友&同事(平级)', '配音', '单人口播', '特写', '餐厅', '室外', '夫妻&恋人&相亲', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.94', '0.54', '0.28', '0.17', '0.16', '0.13', '0.11', '0.03', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11acf694bdd55526bab14ae08b7b291b.mp4\n",
      "{\"video_ocr\": \"45+28-49|Sin& + 等于|24|17+65-30|这算数算的这么好|都怎么学的呀|这都是在|腾讯企鹅辅导暑假班学的|效果你也看见了|在家就能学|8次16节精品名师课|哇哦~ 只要49|帮助孩子快速掌握|小学数学高效解题法|清华北大毕业名师直播授课|还有一对一老师在线答疑|像这种速算小题|可以直接秒出答案|现在报名还包邮赠送|精美学习大礼包|赶快点击视频下方链接报名吧|孩子将收获|清北毕业老师实力保障|6大课程服务全方位伴学|腾讯企鹅辅导|练一练2|UNMYSW|OF|口高能预警|本发|灵感本发|品质保苏 满意服争|多种新解题思路|(x-u|(x-x)+(|(0可)3|00后|IMO|90\", \"video_asr\": \"四十五加二十八等四十九等于二十七加六十五点七九十二，这算术算的这么好，都怎么学的呀，这都是在腾讯企鹅辅导暑假班上学的效果，你也看见了腾讯企鹅辅导暑假班在家就能学八四十六级，报名时刻只要四十九，帮助孩子快速掌握小学数学高效解题法，清华北大毕业名师直播授课。|一对一老师在线答疑，像这种计算小题可以直接讲出答案，现在报名还包邮赠送精美学习大礼包，赶快点击视频下方链接报名吧！|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.019345521926879883 sec\n",
      "{'result': [{'labels': ['现代', '极端特写', '推广页', '中景', '手机电脑录屏', '特写', '手写解题', '家', '亲子', '单人口播', '平静', '动态', '教辅材料', '静态', '情景演绎', '朋友&同事(平级)', '喜悦', '多人情景剧', '惊奇', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11b37786e655a4e3a0e0d25705dc1d31.mp4\n",
      "{\"video_ocr\": \"Listen carefully. 仔细听。|Please find \\\"cow. 请找到“牛”。|chick mouse|Found it! 找到了!|Good job, Tom!|You areso fast! 你真快!|Bird.|Ball.|The winner is Tom!|Congratulations!|iceberg 适合2-8岁宝贝|goat|pig|做得好，Tom!|球。|冠军就是 Tom!|祝贺他!|penguin 冰山|7|节体验课｜ 5天社群辅导 包邮送精美学习大礼盒|加赠80张儿童拼拼卡|sheep|Activity|BOok|frog|Bal|Bouncy|哼!|北美外教AlI英语课|我又 输了|34件配套教具|lomb|donkey \\\"obhit|duek|叽里呱啦|官方价199元|hen|FARM|￥9.9|生气\", \"video_asr\": \"LISTEN CAREFULLY PLEASE FIND CAD。|GOOD JOB。|优秀拜拜。|OO OOO WHAT。|VUE卫生。\"}\n",
      "multi-modal tagging model forward cost time: 0.01613330841064453 sec\n",
      "{'result': [{'labels': ['现代', '静态', '推广页', '中景', '配音', '平静', '场景-其他', '教辅材料', '喜悦', '填充', '情景演绎', '手机电脑录屏', '特写', '宫格', '单人口播', '多人情景剧', '家', '亲子', '极端特写', '课件展示'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.98', '0.83', '0.82', '0.35', '0.12', '0.08', '0.07', '0.05', '0.04', '0.03', '0.03', '0.02', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11be4a5edaa9e37beba0adb44318360e.mp4\n",
      "{\"video_ocr\": \"爸我都检查3遍了|没什么问题我就交卷了|这题太简单了|我3分钟一道小题|这些都是上过咱们|&高 高途课堂的学生|能够帮助孩子们 提升成绩啊|咱们课程就有意义|你为什么不给我报|高途课堂全科名师班|我连后面3道大题 都没做完|我哪知道9块钱的课|能有这么好的效果啊|我就注定是个差生了|9块钱的课|为什么不给孩子试试啊 52I|21|高途课堂都是|北大清华毕业的 老师带队教学|有多年的教学经验|总结出解题技巧 和解题方法|都直击考点|现在已经来不及了 U62|已经没有名额了|我们会跟领导申请|继续开通9元课的名额|各位家长们|价值499元的课程|现在只要9块钱|就有4科16节课|赶快点击下方链接|报名吧|新用户专享立即体脸 浙优亚视借定在线粽育品牌|高谧佩堂 S红工视|浙江卫视|名师特训班|全国百佳教师带队教学 平均教龄11年|062l|Z15|华少|仅需 ￥9\", \"video_asr\": \"没了那我就只能干掉了，没什么问题，我的胶卷了，这题太简单了，我三分钟一道小题，五分钟一道大，这些都是上过咱们高速课堂的学生，能够帮助孩子们提升成绩，咱们课堂就一，你为什么不报告高速做到学的名师班？他们都检查三遍了，我连后面做大题都没做完。|我哪知道这个群的课能有这么好的效果啊，你们注定是个差生啊哎。|有颗结石，可为什么给她的是高途课堂，都是北大清华毕业的老师带队教学，有多年的教学经验，总结的解题技巧和解题方法都直击考点哎，现在已经来不及了，已经没有名额了，我们会跟领导申请继续开通九元课的名额。各位家长们，价值四百九十九元的课程，现在只要九块钱就有四科十六节课，赶快点击下方链接报名吧！|AS。\"}\n",
      "multi-modal tagging model forward cost time: 0.016204357147216797 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '推广页', '多人情景剧', '平静', '静态', '亲子', '全景', '室外', '朋友&同事(平级)', '动态', '喜悦', '特写', '惊奇', '家庭伦理', '单人口播', '手机电脑录屏', '悲伤', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.97', '0.90', '0.84', '0.33', '0.25', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11c1e3b2b64183b6ffb121a4c7a72c87.mp4\n",
      "{\"video_ocr\": \"每天就知道玩手机|小了|什么小了|你想出门不得花钱|不赚钱哪来的钱花|你在家看看视频|人家还能给你开工资不成|真能|用这个 刷宝视频|每天看看视频|就能领金币|金币可以兑换成现金|随时提现|邀请好友最高|还能领37.6元|还能领 37.6元|真的啊|那你怎么不早告诉我|我也是才发现|你赶紧点击下方链接|下载一个|我邀请你|看到减了.去朦钱吧|微信|￥0.30元|我的邀请码:Q5PY9X|轻轻松松赚零花|8|现代科技真是日新月异啊|立即邀请好友|+10|我不敢休息|100|4750|提现微信账号将与刷宝账号绑定|我的收益|又邀请了1位好友，已提现15元|具体金额以实际活动为主|具亿 际活动为主|收益说明|可提现金额(元 赚兼更多元宝|抢趣|余额流水|选抖提现金额 提现到|复制|@神奇逗趣办公室 你说说他一天天在想什么?？？|穿搭|邀请技巧|更多邀请方式 微信好友|2G 10:47|2G:1l|78.7Ks|活动规则|面对面邀请|下载刷宝短视频app 观看更多精彩视频|看视频 领元宝|刷宝|Giill|11:09\", \"video_asr\": \"每天就是玩手机小了。|什么小格局小了。|陛下了，出门不得花钱。|不赚钱哪来的钱花啊你。|真难用这个刷宝视频，每天看看视频就能领金币，金币可以兑换成现金，随时提现，邀请好友最高还能领三十七块六的男人就知道了阿，我也是才发现你开始点下方链接下载一个我邀请你。\"}\n",
      "multi-modal tagging model forward cost time: 0.019161224365234375 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '平静', '手机电脑录屏', '单人口播', '静态', '填充', '配音', '场景-其他', '动态', '室内', '多人情景剧', '喜悦', '全景', '特写', '拉近', '室外', '家', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.67', '0.42', '0.34', '0.16', '0.15', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11cce0fd1994419b000c21cfb7bc6d53.mp4\n",
      "{\"video_ocr\": \"你连30块钱都不 舍得给孩子花吗?|现在有一个机会 让你花30块钱|就可以改变孩子 的末来|不要怀疑|作业帮直播课 现推出|小初高 数学名师提分班|30元18节 名师课|教会孩子数学|21个重难考点|18个解题大招|还免费包邮赠送 教辅大礼包|清华北大毕业的名师 带队教学|专攻孩子解题思路 和做题技巧|让孩子做一道题 掌握一类题|一次学不会也不要紧|因为课程支持 3年内无限次回放|你放心买 孩子安心学|各位家长们|给你得孩子一个 改变的机会吧|点击视频下方 报名吧|作业帮累计激活用户超8亿 清北毕业名师带队授课 班主任老师1对1答疑|小初高数学 30元18节名师课 抢|作业帮旗下产品累计用户量|小学数学公式|30DAYS|展示礼包为小学礼包 【上课内容与收到礼盒以实际为准】|草稿本 成长笔记|致直播课|课|免费 赠送\", \"video_asr\": \"你连三十块钱都不舍得给孩子花吗？现在有一个机会，让你花三十块钱就可以改变孩子的未来，不要怀疑，作业帮直播课现推出小初高数学名师提分班，三十元十八节名师课，教会孩子数学二十一个重难考点，十八个解题大招，还免费包邮，赠送教辅大礼包，清华北大的。|的名师带队教学，专攻孩子解题思路和做题技巧，让孩子做一道题，掌握一类题，一次学不会也不要紧，因为课程支持三年内无限次回。|你放心买，孩子安心学，各位家长们给你的孩子一个改变的机会吧！点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.015962600708007812 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '室外', '中景', '单人口播', '平静', '动态', '静态', '配音', '特写', '室内', '转场', '教辅材料', '场景-其他', '全景', '公园', '填充', '商品展示', '混剪', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.98', '0.60', '0.29', '0.27', '0.12', '0.04', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11d8b550241011ba75387231c6e28f31.mp4\n",
      "{\"video_ocr\": \"欠回放|3年内无限次F|银次回放|3年内天|3年内无/欠回放|课程安排|课程精|我是高途课堂周帅老师|北京大学毕业|省高考状元|研究了近十年高考真题|总结出高考数学|带孩子思维开窍|直取高分|毕业名师授课|强大，清北毕业名师授课|小学二年级|439个知块点|167个考点|80个易错点|适合学员|理精 立即报名|自年打基础年分水的 [年上雄场|新用户专享|请选择孩子所在年级 幼升小|小升初|小季|16 首学 英语|手机号|个划只点|全国百佳教师带队教学 平均教龄11年|1对1随时答疑|疑|辅导老师1对1随时答疑|课后辅导1随时答疑|499元名师直播课|机三|周帅|易错点|浙江卫视|您将收获|X 高途课堂全科特调营|高途课里全科特训营|高途课堂|为什么要给孩子报名|高途课堂名师特训班 浙江卫视指定在线教育品牌|名师特训班|价值499元|个难点夫分点|圣年级|确认支付 ¥0|新用户专享立即体验|名师直播课|9元即可体验|师资强大，清北“授课|华少|便宜，“6节名师直播课（语、数英、物)|便宜，9元16节名师直播课（语、数、英、物)|物理|小初高全科|课后辅导老师!|课后辅导老师1对1随|9元即享|课堂2020全科名师班?|名师班|￥9|仅需|初一\", \"video_asr\": \"为什么要给孩子报名高途课堂二零二零全科名师班，一便宜九元，十六节名师直播课。二，师资强大，清北毕业名师授课，三课后辅导老师一对一随时答疑，四三年内无限次回放。|来来来。|我是高途课堂周帅老师，北京大学毕业，省高考状元，研究了近十年高考真题，总结出高考数学四百三十九知识点，一百六十七个考点。|八十个易错点，五十七个难点失分点，带孩子思维开窍，直取高分，现在报名九元即可体验价值四百九十九元名师直播课！\"}\n",
      "multi-modal tagging model forward cost time: 0.016726970672607422 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '静态', '单人口播', '平静', '教师(教授)', '场景-其他', '配音', '室内', '极端特写', '手机电脑录屏', '影棚幕布', '特写', '学校', '家', '全景', '手写解题', '幻灯片轮播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.86', '0.81', '0.43', '0.06', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11de7a7ee918cc9a19d83a51c1827b5c.mp4\n",
      "{\"video_ocr\": \"dete|let|de|你只需背|完形填空啊|不需要把每句话|都翻译的清清楚楚|做到|就可以了ssary|都在考场上|现打草稿|到考场上灵活运用|就可以了|那么必须|帮助你摆脱低分的困扰|轻松打开英语学习的新世界！|同学们学到就是赚到|立即报名|eS|Jes|其实啊同学们|背单词你不要背3500|+|题题有线索|作文你不需要每篇文章|你只需背住十个核心模板|如果你想在高考英语中取得好成绩|把精力放在真正的核心考点上|赶紧来听课吧!|高中英语阅读理解看不懂|完形填空不会写|英语词汇积累少|怎么办?08.7|怎么办？|高考英语 高频词汇氵|高中数英培优特训营 新学员9元专享|用户340159 用户992426 用户776785|用户776785908: 青青葡萄:将要|用户863911708：将要|为你打嗨:正式刚刚学完，|D. name|跟谁学首席高中英语讲师|anreNetor(H动4\\\\…ke|但信，信  the presence ofsueh a shdlent|用户966|江雷:现在 用户528818|用户966504958|小谷粒儿:现在完成|用户612089708:OK|用户340159938：完成时|用户992426658:现完|用户863911 为你打嗨|为你打嗨:正式 D. advocate|东韵漫歌:将来|用户671027038:同义替换|1tI0|ngle|Bwspaper|I; when may be|TUuKI/ImINOd.Cvu webs may be|nle, the uld have|other end would have|issip vary depending on the situation.（2016年高考菜语新|the world would|用户612089|connect|2015 依T 体赖|you eaten|首席高中英语讲 线教学经验|14年 线教学经验|tace|serious|C. all|B. skeptical D. CIaZ|decide B. argue|十二大解题方法|horitys解题技巧|兰犬解题方法 uthri;解题技巧|Puo解题技巧|解题技巧|学考法|学方向|用户992988678:将要|D. stones|在 线 学 习更高 效|在线学习更高效|glaciers(冰川广|le gaciers(冰川)|natural experiences.|untntfrig更TI .The glaciers|as  purer\\\" natural experiences.|A. discover|46. A. equipment B. grass C. camps|用户528818348:|用户405150898:过去时|小谷粒儿:现在|1th toilets at camps and along the|起来，它们试图理蔚新单 delight 的过去式及过去|闪理解新单 去式及过去|ENGLISH|美国纽交所上市公司|nd do|to papers|ys eats|へnrodatへr(合动物`always eats|n. For example, the|元ets at  amps and along the|英语 单词+语法高效记忆|4Q|为你打酶：正式网刚刚学完，啸嘿|nhap|scammerswill ave moved ontocleverer means.Inthe nearfuture isntN|ha|arl|ng|unh ling|800|感情色彩一致性|eet sales of t was able to speak to a receptionist and boo|4After rnning hundreds oftects the researchers noted that the monkevs|兄币定。|D. clean C. sianificant|stories C. crowds|B. bunldings|43. A. position|D. reporters|C.|I年一教学经验|erable|intended to preventscammers from getting through. Unfortunately,ifstoelrs|rab|10|lera|disappearing, changury the 43 oFKilimanjaro.|are disappearing, changming the 43 oFKilimanjaro|2019全国1卷完形填空 90% 前后呼应|现在完|用户671027038: discover等于find?|verles|1ap rle|going to be the number you see on your screen that willbe in doubt Swa|ver|oweI|voice technology able to produce such a convincing human-sounding voice that|for anyone to use. At this year's IO Conference, a company showed a new|for anyone to use. At this year's IlO Conference, a compay i s usually sea copy|not easy) to buy single copies of newspapers before 1830, but this usually|3. This development did not take place overnight. It had been possible (bu|extinction(灭绝) of one of them. And if a predator can move on to another species that is easier to find when a prey species becomes rare, the switch|would be rewarded with the sum of the numbers—17 in this example.|rewarded with seven drops of water or juice; if they went for the circle, they|而定；取决于 tuation.(2016年高考英语-|16年高考英语-|breakfas?|D, cany|D.crazy|D. necessary|点击下方 查看详情|yed|toolate.Bythe time these\\\"solutions”（解决方案) become widelyaveian|oye|40.000|2019全国1卷完形填空|“谁学|them lots of wasteThe|Thealages，N/ )are disappearing, changing the43 orFKilimanjaro.|SED|理 ) and automation technologies that are about to become a|SSE|蓝语讲师|copies on the street.|徐看|ority seem to be 49|除磊|nbelie|ejeo|Jell|eliev|Afeha?跟谁常|B. bu|ion|sition|A. equipment|卫9ロ三D|卫一0の|汇Sた|[OWwOS|pointo|00Int|ISCO|SC oIn|ScOu|JCO|Cou|I experiences.|B. mix|ries|keep|A. keep|41. A_ keep 42.A. storie|42. A.sto|在线学习更高效|美国纽交所上市公司|Cover|A. remote|47. A. remote 48.A. new|ent|oment|uipment|remote|A. new|acicrs|bout|rica 7n|y yea|going to be the number you see on your screen that will be in doubt. Soon you|revolution that was taking place in the 1830s would change all that.|operating in food webs. Most food webs, for instance, consist of many weak links rathe-than 2 fへow sfr- |links ratherk~n a fau rfrcr ones|links rather hora fou cfrora cnas .Whon|provided with touch screens. On one part of the screen, a symbol would appear,|provided with touch s-ens On rne part che scren a symbol would appear,|the days of disturbing reports of mental challenges are 48 but the erorts made by the Tanzania|environmental challenges are 48 but the efforts made by the Tanzania|数学 重难点题型高分突破|place.|B. grass|B. quiet|2.T1 made w|2. The|huge n a preda|huge numh|and or numbe|and on the othe number 7 woul|ark Authority seem to be|ional Park Authority seem to be 49|paths. The environmental+challenges are 48 but the efforts made by the Tanzania|44:A. siemnt|too late. By the time these\\\"solutions”（解决方案)become widely available.|politics or the trades. In addition, most newspapers had little in them that would|2. With such models, scientists have found out some key principles|3. Here's how Harvard Medical School scientist Margaret Livingstone, who|stories, I' m 44 about the place — other destinations are described|in Airica. Tey41with them lots of waste Then2might damage the beauty e.Zh线glhacje更 高划l) are disappearing, changmg the 43 oFKilimanjaro.|ain in Africa. They 41 with them lots of waste The12 might damage the beauty ola在 辅egta晃r冰川l) are disappearing, changmg the 43 oFKilimanjaro.|with them lots of waste The￥2might damage the heauty 冰川ll) are disappearing, changu g the 43 orFKilimanjaro.|of the place. The qlaciers (冰川) are disappearing, changigthe43 oFKilimanjaro. mo在t线n学 前fr更a高hy41 with them lots of waste Ther2.might damage the heauty|lie|ThB|a. h|year|problem by supporting and developing a group of tools, apps and approaches intended to prevent scammers from getting through. Unfortunately, it's too little,|small amount of money, but at that time these amounts were forbidding to most citizens. Accordingly, newspapers were read almost only by rich people in|smallamount ot money, but at that tme these amounts were forbidding|wolves? To find an answer, scientists have built mathematical models of food webs, noting who eats whom and how much each one eats.|water or juice as a reward. The researchers then tested how the eys combined—or added—the symbols to get the reward.|drops of water or juice as a reward. The researchers then tested how the|跟谁学|在线学身 到新单饲的声音可能会|miaht damage the heauty|hem lots of waste Then2 might damage the beauty attempt to climb Kilimanjaro, the highest|90% 朋后呼应|000 pe ple attempt to climb Kilimanjaro, the highest|49. A. paying off B. spreading out|C. sig|discol|ste|nte'|Nto|悦主人 愉快|Every year about 40，000 peple attempt to climb Kilimanjaro, the highest|scammers will have moved onto cleverer means. In the near future, it's not just|appeal to a mass audience. They were dull and visually forbidding. But the|led the team, described the experiment In their cages the monkeys were|about the place — other destinations are described|Hearing these stories, I'm 44 about the place - other destinations are described|agg|itag|SIr|disir|dis|nta|高频|核心词汇|it was able to speak to a receptionist and book a reservation without detection|voice technology able to produce such a convincing humansd|meant the reader had to go down to the printer's office to purchase a copy. Street sales were almost unknown. However, within a few years, street sales of|allows the original prey to recover. The weak links may thus keep species from|would go for the higher values more than half the time, indicating that they were|C. blowing up D. fading away|理 )and automation technologies that are about to become widely available|ble (but|perhaps more importantly it meant newspapers that could be bought in single|dominated by many weak links because that arrangement is more stable over the long term. If a predator can eat several species, it can survive the|9 and 8. If the monkeys touched the left side of the screen they would be|兄而庭 ng about what was said, as if the salyation of the world would|D bring|ct|tew|tE\", \"video_asr\": \"高中英语阅读理解看不懂完形填空，不会写，英语词汇积累少怎么办？其实啊，同学们背单词不需要背三千五，你只需要背八百高频核心词，完形填空，不需要把每句话都翻译的轻轻涂，你只需要一句十二大解题方法和解题技巧，做题时有线索。|一G就可以了，作文你不需要每篇文章都在考场上见打草稿，你只需要备注十个核心模版到考场上灵活运用。|可以啊，如果你想在高考英语中取得好成绩，那么必须学考点，学考法学方向，把精力放在真正核心的考点上来，帮助你摆脱低分的困扰，轻松打开英语学习的新世界。同学们学到就是赚到，赶紧来听课吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01643204689025879 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '单人口播', '中景', '平静', '场景-其他', '静态', '教师(教授)', '手机电脑录屏', '特写', '配音', '室内', '拉近', '影棚幕布', '课件展示', '动态', '教辅材料', '过渡页', '极端特写', '拉远'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.44', '0.22', '0.08', '0.08', '0.03', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11f16d56395e53524c920c7c4cb086f9.mp4\n",
      "{\"video_ocr\": \"喂|您好|这里是360借条客服|请问有什么|可以帮助您的|我想问一下|我在360借条上|额度有多少|那您手机号|用了多久了呢|3年多了|那您可以|输入您的手机号|以及身份证号|查询一下呢|我这|额度有10万|我这什么时候|能到我卡上|最快呀|5分钟就能放款|那这利息高吗|我这一时半会儿|还不上怎么办|没关系的先生|利息不高|借4万内|最长免息30天|最长还能分|12期慢慢还款|减缓您的还款压力|谢谢谢谢|太好了|以后急用钱|再也不用愁了|急用钱的你|也赶紧点击屏幕下方链接|申请属于你的借款额度吧|贷款有风险，|借款需谨慎，请根据个人能力合理贷款|贷款有风险，借款需道谨慎，请根据个人能力合理贷款|贷款有风险，借款需谨慎；借款额度、放款时间以实际审批结果为准 息费减免优惠以具体活动规则为准|贷款额度、放款时间等以实际审批为准|请根据个人能力合理贷款、理性消费、避免逾期|纯线上申请，自动审批|1分钟申请，最快5分钟出额度|最高额度20万|返回 开始|(以审批结果为准)|借1万最低日息27元|年化利率9.8%起|5390口|欢迎注册/登录360借条|额度高 一次授信，循环可用|￥ヨ60借条 可用额度|安全的贷款产品|额度不使用，不收费|输入手机号查看你的额度|¥6口借条|利息低|100，000|最高200000 最快5分钟到账|下一步|审批快|放心贷\", \"video_asr\": \"喂，您好，您好，这里是三六零借条客服，请问有什么可以帮助您的？我想问一下我在三六零借条上额度有多少。|那你手机号用了多久了呢？三年多了，那您可以输入您的手机号以及身份证号查询一下呢。|我这在三六零借条上，额度有十万，哎，你好你好，请问一下，我这什么时候能到我卡上，最快呀，五分钟就能放款，那利息高吗？|我这一时半会还不上怎么办？没关系的先生，我们三六零借条压力媳妇高借四万内最长可以免息三十天，最长还能分十二期慢慢还款，两千年的还款压力。谢谢，谢谢，谢谢。|太好了，以后急用钱再也不用愁了，急用钱的你也赶快点击屏幕下方链接，申请属于你的借款额度吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01670241355895996 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '平静', '多人情景剧', '手机电脑录屏', '单人口播', '家', '特写', '夫妻&恋人&相亲', '配音', '朋友&同事(平级)', '动态', '喜悦', '惊奇', '悲伤', '办公室', '家庭伦理', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.77', '0.74', '0.65', '0.21', '0.12', '0.08', '0.05', '0.05', '0.04', '0.04', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11f87a0c5162a4a0ad3949ce74191341.mp4\n",
      "{\"video_ocr\": \"即刻增加10000个名额|还是丰富的益智教具礼盒 免费送到家|还是十年教研团队|专门为3-6岁的孩子|研发的系统性的进阶课程|还是以 动画、游戏、儿歌形式|搭配A1互动技术|培养孩子 对数学学习的兴趣|我们斑马Al课|最希望做的|就是在3-6岁这个|孩子思维养成的关键时期|给到孩子优质的教育资源|让孩子拥有 观察力和逻辑推理能力|各位家长们|现在我们 整个的购课流程呢|已经全部开启了|点击视频下方链接 即刻报名|49元10节课|还包邮赠送|千万别错过 孩子思维培养的黄金期|快点击视频下方报名吧|斑马AI课|数量变多就是加|哒哒哒哒哒哒|减号出现减少了|5和7|1对1|5个苹果7个梨|手拉手呀找朋友|狼辅导a1R司|l马Al课果|剑马A|现马A1|马A课|狼辅导在a教间|S2|二|2-8岁上斑马 学思维 学英语|猿辅导在线教育出品|紧急通知|咱们斑马A课思维体验课|5A芫甲|思维\", \"video_asr\": \"紧急通知啊，咱们斑马AI课思维体验课即刻增加一万的名额，还是丰富的益智教具礼盒免费送到家，还是十年教研团队专门为三到六岁孩子研发的系统性的进阶课程，还是以动画，游戏，儿歌形式。|搭配AI互动技术，培养孩子对数学学习的兴趣，我们斑马AI课最希望做的就是在三到六岁这个孩子思维养成的关键时期。|给到孩子优质的教育资源，让孩子拥有观察力和逻辑推理能力。各位家长们，现在我们整个的购课流程呢已经全部开启了，点击视频下方链接即可报名。|四十九元十节课还包邮，赠送丰富的益智教具礼盒，千万别错过孩子思维培养的黄金期，快点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.018220901489257812 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '静态', '中景', '室内', '填充', '单人口播', '平静', '极端特写', '课件展示', '教辅材料', '配音', '场景-其他', '单人情景剧', '动态', '影棚幕布', '手机电脑录屏', '转场', '宫格', '教师(教授)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.85', '0.82', '0.08', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11fd7fae4a2000bc1ef8c0e16dd8760a.mp4\n",
      "{\"video_ocr\": \"孩子坐不住|反反复复教不会|加减计算不会做|举一反三更不行|数学是更加抽象的一门学科|更需要具备抽象思维能力|而8岁前的孩子|还处于从具象思维|到抽象思维过渡的阶段|所以就更需要顺应他们的|发展规律|用一种更适合的方式|进行思维训练|打好数理基础|我是斑马AI课|思维教研老师王金玉|专注儿童互动课堂研究|逅3年|我们将孩子提升数学思维|必备的9大思维能力|和16种思维方法|融入到了有趣的|动画儿歌和游戏当中|每天仅需15分钟|课上AI智能互动学|课下专业老师|进行针对性辅导|双师教学|保证孩子学得会|现在报名|仅需49元10节课|还包邮赠送价值|108元的教具礼盒|猿辅导在数教肯出品|猿辅导在教|猿辅导 在线教首|猿辅导在金装台|猿辅导在金牧育 出品|猿辅导线就育|猿辅导幽维我育|猿魎号金你|猿辅导a金合|斑马/ 互动 3年幺|互动课策划 3年幼教课堂经验|S3|2-8岁上斑马 学思维学英语|斑马Al课|猿辅导 在线数省出品|斑马AI课思维教研老师|思雄|2|2-8岁上斑马|上斑马|A果|王金3|出品\", \"video_asr\": \"孩子坐不住，反反复复教，不会价钱计算，不会做，举一反三更不行，数学是更加抽象的一门学科，更需要具备抽象思维能力。儿八岁前的孩子还处于从具象思维到抽象思维过渡的阶段，所以就更需要顺应他们的发展规律，用一种更合适的方式。|行思维训练，打好数理基础，我是斑马AI艾克思维教研老师王新宇，专注儿童互动课堂研究近三年，我们将孩子提升数学思维必备的九大思维能力和十六种思维方法融入到了有趣的动画，儿歌和游戏当中，每天仅需十五分钟，孩子坐得住。|课上AI智能互动学科下专业老师进行针对性辅导，双师教学，保证孩子学得会，现在报名只需四十九元。|节课还包邮赠送价值一百零八元的教具礼盒。\"}\n",
      "multi-modal tagging model forward cost time: 0.01618051528930664 sec\n",
      "{'result': [{'labels': ['推广页', '中景', '现代', '静态', '单人口播', '平静', '室内', '动态', '喜悦', '手机电脑录屏', '室外', '配音', '教辅材料', '亲子', '家庭伦理', '多人情景剧', '家', '特写', '极端特写', '情景演绎'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.89', '0.51', '0.31', '0.27', '0.19', '0.08', '0.08', '0.07', '0.05', '0.04', '0.03', '0.03', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/11ff8a8af96416f63936dc6938cd962c.mp4\n",
      "{\"video_ocr\": \"老公|你怎么垂头丧气的呀?|公司刚成立个个新项目|正在招人|可是我对编程不熟悉|没机会了|你赶紧报名风变编程啊|8.9元优惠体验四天|Python实操课和人工智能认知课|大家都开始学Py1h|大家都开始学9yhon疯i|Python编程入门学什么|风变编程|立即名\", \"video_asr\": \"老公你怎么垂头丧气的呀，公司刚成立了一个新项目，正在招人，可是我对编程不熟悉，没机会了，你赶紧报名风变编程啊，八点九元优惠体验，四天拍摄实操课和人工智能认知课。\"}\n",
      "multi-modal tagging model forward cost time: 0.016220808029174805 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '手机电脑录屏', '静态', '推广页', '家', '喜悦', '朋友&同事(平级)', '平静', '动态', '特写', '极端特写', '惊奇', '单人口播', '夫妻&恋人&相亲', '办公室', '工作职场', '配音', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.97', '0.78', '0.66', '0.53', '0.32', '0.19', '0.11', '0.10', '0.06', '0.04', '0.03', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/120da9d721f494f31f7afa15af28a732.mp4\n",
      "{\"video_ocr\": \"高途课堂发布重要通知|我是高途课堂厉程远老师|初一学完初中数学|保送北京大学数学科学学院|在过去的10年中|用自创的|教学生轻松解决几何问题|让孩子10s一个小题|5分钟一个大题|冲刺中考数学120!|初一|初二/高中数学|42个几何模型|名师特训班|首批课程限量名额100名|点击下方查看详情|新用户专享 立即体验|课|课程:语数英物四科全面辅导|师资:北大清华毕业|全国百佳教师带队教学平均教龄11年|只需|只需9元 就能体验16课时|师资:北大清华毕业|厉程远 们年教龄|高途课堂|掌握:数学42个几何模型 语文作文5大模块36个技巧|物理|物理108个口诀大招|浙江卫视|即日起看到这条视频，报名高途课堂初高名师班|英语|英语3分钟 搞定|英语3分钟搞定CD阅读篇|浙江卫视指定在线教育品牌|教龄11年以上名师教学|保送北京大学|￥9\", \"video_asr\": \"高途课堂发布重要通知，即日起，看到这条视频，报名高途课堂初高名师班，只需九元就能体验十六课时！|狮子，北大清华毕业，教龄十一年以上，名师教学课程，语数，英物，四科全面辅导。|掌握数学四十二个几何模型，语文作文五大模块三十六个技巧，物理一百零八个口诀大招，英语三分钟搞定CD阅读篇。|首批课程限量名额一百名，点击下方查看详情。|我是高途课堂厉程源老师，我初一学完初中数学，学完高中数学，保送北京大学数学科学学院，在过去的十年中，用自创的四十二个几何模型。|教学生轻松解决几何问题，让孩子十秒一个小题，五分钟一个大题，冲刺中考题。\"}\n",
      "multi-modal tagging model forward cost time: 0.01628851890563965 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '平静', '中景', '场景-其他', '静态', '单人口播', '配音', '特写', '室外', '混剪', '影棚幕布', '教师(教授)', '转场', '室内', '动态', '过渡页', '情景演绎', '城市景观'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.97', '0.96', '0.93', '0.93', '0.69', '0.66', '0.15', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1228eb9e97572aae7edebd4b6feb8160.mp4\n",
      "{\"video_ocr\": \"哇!|哇!苹果|西|苹男 菠墓|雪梨 橙子|橙子|樱桃|猿辅导在线教育出品|斑马A课|读一读|非赠品|西瓜 草莓|草莓|雪梨|果 菠萝|荔枝|7果|使萝|出品\", \"video_asr\": \"一直在听呢。|无语。|他在游泳哦。|不离开酒店。|那我有老公的。|费用。|阿里木。|MIGOO。|西瓜，橙子，荔枝，苹果，草莓。|你。\"}\n",
      "multi-modal tagging model forward cost time: 0.015991926193237305 sec\n",
      "{'result': [{'labels': ['现代', '极端特写', '推广页', '静态', '场景-其他', '教辅材料', '配音', '手写解题', '平静', '中景', '室内', '商品展示', '课件展示', '特写', '动态', '宫格', '绘画展示', '拉近', '转场', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.08', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/122d637c5efbe879c6b0858e0b1ba7e0.mp4\n",
      "{\"video_ocr\": \"你跟我离婚 就因为她|房子都给你了|你还想怎样|我不同意|啊|你怎么来了|你生病的事|为什么瞒着我|还找人在我面前演戏|我得的是癌症|不能再拖累你了|你忘了|我早在你健康的时候|就给你买了平安e生保2020啊|那个保险首月才3元起|我这医疗费要几十万呢|怕是不能报销|你放心|平安e生保2020|只要在保障范围内|不限疾病|不限社保用药|进口药 自费药都能报销|最高有400万的医疗保障呢|老公|你安心治病|我会一直陪着你的|首月只要3元起|次月18元起|就有最高400万的保障|赶快点击视频下方链接|在线投保吧|免赔额一万)|(保费随不同年龄有无社保等情况变化)|保障范围内 不限疾病 不限社保用药|就上平安健康APP|投保需如实健康告知 保障内容以保险合同为准| 乒牢|平安 健康 买保险|VAC\", \"video_asr\": \"跟我离婚就是因为她。|房子都给你了，你还想怎样？我不讨厌。|啊。|你怎么来了？你生病的事为什么瞒着我？|还找人在我面前演戏，我觉得是癌症。|不能再拖累你了，你忘了，我早在你健康的时候就给你买了平安一生保二零二零啊，那个保险首月才三元起，我这医疗费要几十万呢，怕是不能报销，你放心，平安一生保二零二零只要在保障范围内，不限疾病，不限社保用药，进口药，自费药都能报销，最高有四百万的医疗保障呢。|老公，你安心治病，我会一直陪着你的。|平安一生保二零二零首月只要三元起，四月十八元起就有最高四百万的保障，赶快点击视频下方链接在线投保吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016119003295898438 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '多人情景剧', '中景', '静态', '喜悦', '特写', '夫妻&恋人&相亲', '悲伤', '惊奇', '动态', '平静', '朋友&同事(平级)', '亲子', '极端特写', '家庭伦理', '愤怒', '手机电脑录屏', '单人口播', '家'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.89', '0.88', '0.84', '0.71', '0.68', '0.68', '0.21', '0.10', '0.07', '0.03', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/123bf81599fb1c2f95858b283c5cef77.mp4\n",
      "{\"video_ocr\": \"主要是你嫂子啊|钱管得比较紧|你能不能先|你总这样也不行啊|要不你试试小米贷款吧|小米贷款0抵押0担保|最高额度30万|最快1分钟放款|点击视频下方链接就可以直接申请啦|贷款额度 放款时间等以实际审批为准|货款有风险 借款需谨慎请根据个人能力合理贷款|小米金融消费信贷服务|小米信款 最高借款额瘦(元)|300.000|查看额度|7.2%|MI|3\", \"video_asr\": \"要是你嫂子管的比较紧，你那么担心你总算也不行呀，要不你试试小米贷款吧，零抵押，零担保，最高额度三十万，最快一分钟放款，点击视频下方链接就可以直接申请了。\"}\n",
      "multi-modal tagging model forward cost time: 0.017863750457763672 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '手机电脑录屏', '单人口播', '办公室', '平静', '喜悦', '动态', '惊奇', '工作职场', '配音', '极端特写', '特写', '朋友&同事(平级)', '悲伤', '上下级', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.87', '0.85', '0.74', '0.68', '0.57', '0.47', '0.34', '0.31', '0.28', '0.22', '0.04', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/12475d033ae702ab71660b8196fedece.mp4\n",
      "{\"video_ocr\": \"外|如果在上小学前|可以帮孩子更好的适应小学生活|我们家孩子的启蒙秘籍就是它|斑马A课 语文体验课|适合3-6岁宝宝|由清华北大毕业的老师带队教研|通过A动画的形式激发孩子学习兴趣|yu宝宝识|3-6岁宝宝识字妙招|能有800-1000的识字量|而3-6岁是孩子读书识字的敏感期|好的启蒙可以帮助孩子轻松识字|培养孩子识字和阅读能力|斑马AI课|da|JATG|斑三A几茉|y幼u|德加一字 CHAMPION|xiao|语文-|shang|wai|gao|S1\", \"video_asr\": \"三到六岁宝宝识字妙招，坐在上小学前，能有八百到一千的识字量，可以帮助孩子更好地适应小学生活，而三到六岁是孩子读书识字的敏感期。|好的启蒙可以帮助孩子轻松识字，我家孩子的启蒙秘笈就是她三吗AI课，语文体验课，适合三到六岁宝宝，培养孩子识字和阅读能力。|由清华北大毕业的老师带队教研，通过AI动画的形式激发孩子学习兴趣。\"}\n",
      "multi-modal tagging model forward cost time: 0.01629471778869629 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '喜悦', '单人口播', '家', '亲子', '手机电脑录屏', '场景-其他', '家庭伦理', '动态', '极端特写', '平静', '配音', '特写', '教辅材料', '多人情景剧', '拉近', '动画'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.79', '0.79', '0.62', '0.49', '0.39', '0.39', '0.34', '0.29', '0.29', '0.06', '0.05', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/124a8294a0a3258a5dd41678bcca9f74.mp4\n",
      "{\"video_ocr\": \"你能不能好好坐着|s姐姐|这个数太大了|我两只手都数不过来|这题一点都不难|首尾相加放两边|两边相加放中间|答案等于|你怎么那么厉害啊|因为妈妈给我报了|斑马Al课思维体验课|在动画中学数学|不知不觉我就有了数感|这些题目当然都会了|妈妈妈妈|我也想学斑马Al课 jasharrentt|jashacrrt|不就是49块钱嘛|妈妈给你报|斑马A1课|思维体验课|专为3-6岁小朋友|数学启蒙设计|趣味动画场景|孩子听得懂 好实践|还有智能Al互动|启发孩子手脑并用|充分培养孩子 逻辑思维能力|0基础也能学|课程还支持 免费无限次回放|直到孩子学会为止|屏幕前的宝爸宝妈们|现在报名啊|49块钱就有10节课|还免费包邮赠送|这个超值教具礼盒|赶紧点击视频详情 报名吧|猿辅导在线教置 出是|姨事在线教高 出品|猿辅导在线教售|猿辅导在经霾 出|特轲是 。|165|三三222 三品三24|7A2|你真棒|思维-|2-8岁上斑马学思维 学英语|Accral|Poshccrul|PeskArcrel pos crdl|picrnl|ws.crul|78+87=|斑马思维|1276|yo|yosh Accrml|Dos|ictent|Arcent|10节课49元|1ash ad pnsh arrenl|pyashar|pagh?|g|三0w2|E2A52|kent Aet|sttentl|yaslceg|accertt|nnch ar|s2|yosh pothkral|sh Actenl|Duse|ttt|+3\", \"video_asr\": \"乖乖乖。|你能不能抓住姐姐，真的是太大了，我的手都数不过来，这一点都不来，下载过两天小编在绽放六点等等一百六十五，姐姐你怎么了？厉害！|你让妈妈给我报了，我们靠自己。|我的化妆前后钱，不知不觉我个人不敢全部当然都。|你。|妈妈妈妈我。|不就是四十九块钱吗？妈妈给你报了斑马AI课思维体验课，专为三到六岁小朋友数学启蒙，设计趣味动画场景，孩子听得懂，好实践，还有智能AI互动，启发孩子手脑并用，充分培养孩子的逻辑思维能力，零基础也能学，课程还支持免费无限次回放，直到孩子。|学会为止，以目前的宝爸宝妈们现在报名啊，四十九块钱就有十节课，还包邮赠送这个超值教具礼盒，赶紧点击视频详情报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022642850875854492 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '多人情景剧', '静态', '亲子', '平静', '家', '配音', '场景-其他', '喜悦', '单人口播', '教辅材料', '家庭伦理', '惊奇', '特写', '极端特写', '课件展示', '手写解题', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.91', '0.91', '0.87', '0.56', '0.30', '0.12', '0.12', '0.08', '0.03', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/126202a42ba3b200545438bbe5353f2f.mp4\n",
      "{\"video_ocr\": \"当她被姐姐算计陷害|为了孩子而向他打电话求救的时候|战少却从始至终都没有出现|过去的五年|是她人生最屈辱痛苦的五年|她从云端跌入泥潭|为了活下去|她用尽力气|重来一次|她定要所向披靡|下载弋猫小说|看她如何逆风翻盘!|看她何逆风翻|本故事纯属虚构) 免费看书100年|七猫免费小说|《坑爹萌宝已上线》|Q、O|阅读中含有广告内容|她九死一生终回归|带娃逆风翻盘!|渣姐算计|战少抛弃\", \"video_asr\": \"当她被姐姐算计，陷害，为了孩子而向着打电话求救的时候，站上去从始至。|都没有出现过，过去的五年是他人生最屈辱，痛苦的。|他从云端。|为了活下去，他用尽力气。|重来一次，他定要所向披靡。|下载七猫小说，看他如何逆风翻盘，免费看书一百年七猫免费小说。\"}\n",
      "multi-modal tagging model forward cost time: 0.016091585159301758 sec\n",
      "{'result': [{'labels': ['填充', '推广页', '现代', '静态', '中景', '全景', '平静', '场景-其他', '特写', '室外', '混剪', '情景演绎', '配音', '动态', '室内', '古装/武侠', '极端特写', '古代', '转场', '拉远'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.84', '0.78', '0.76', '0.35', '0.24', '0.04', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/126901d00ee13fb4486ad9ceaf1f8bb1.mp4\n",
      "{\"video_ocr\": \"你就能在高考时轻松考到120分以上|你是不是还在背写作的万能开头|剩下的东西就交付命运|NONO NO|真相很残酷|你吃了学习的苦|但是最终却没有得到学习的回报|这是为什么呀|我们先从方法说起|完形填空|你只需要掌握12大解题方法和|解题技巧|阅读理解你只需要掌握|10大解题方法|作文|你只需要背10个万能的写作模板|以上方法加上800个核心高频单词|如果你想在高考时|高中英语拿个好分数|那么我3节的高中英语精华课|你一定不能错过|想新学期逆袭的同学们|来课堂上见吧|高中英语偏科的|如何利用|迅速的|在新学期|实现黑马逆袭呢|你是不是还在背|把阅读理解的每句话|从头翻到尾|?|风谁学|银准学|限谁字|掌握高效学习法，快人一步成黑马!|同学们|很短的时间|赶超他人|3500词|节直播课|15|教学:|数学:100|数学：1了”|高考英语 高频词汇|英语 单词+语法高效记忆 立即报名|英培:|英语:1l|英诘:110|语：|芝话:|芝语：135|3元|限谨学|数学 重难点题型高分突破|跟谁学|在线学习更高效|高中全科培优特训营|新学期黑马逆袭|目标：二本|掌握核心方法|Hi nice\", \"video_asr\": \"高中英语偏科的同学们，如何利用很短的时间迅速的赶超他人，在新学期实现黑马逆袭呢？你是不是还在被搬迁五百词？你是不是还在不停地把阅读理解的每句话从头翻到尾？你是不是还在被写作的万能开头，剩下的东西就交付英语？NO NO NO真相很残酷。|你吃了学习的苦，但最终却没有得到学习的回报，这是为什么呀？我们先从方法说起，完形填空你只需要掌握十二大解题方法和解题技巧，阅读理解你只需要掌握十大解题方法。|作文你只需要背十个万能的写作模版，以上方法，加上八百个核心高频单词，你就能在高考的时候轻松考到一百二十分以上。如果你想在高考时高中英语拿个好分数，那么我三节的高中英语精华课你一定不能错过！|想在新学期逆袭的同学们，来课堂上见吧，网课就选跟谁学，孩子考上好大学。\"}\n",
      "multi-modal tagging model forward cost time: 0.016298294067382812 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '单人口播', '现代', '静态', '中景', '配音', '教师(教授)', '平静', '影棚幕布', '场景-其他', '教辅材料', '课件展示', '极端特写', '手写解题', '特写', '室内', '混剪', '情景演绎', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.81', '0.03', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/126dd43859c209dea31c098944670f9e.mp4\n",
      "{\"video_ocr\": \"大家都觉得|小学数学很简单|为什么咱家孩子|就是上不了95呢?|很多孩子喜欢说 1+2)6|考试考不好|(12+3+妇)川(12）子3。|是粗心了、马虎了 (12++财)メ1+2)-3。|(I2++妙)Xl12)=3。|错啦!|数学的学习没有粗心|没有马虎|只有知识掌握不牢固|习惯不标准|我是胡涛|北大人培养北大好苗子|在我的课上我会刷新孩子|数学学习的认知|cEAMT不仅让孩子|知道怎么做|更知道如何举一反三|培养数学思维|还在等什么呢?|点击视频下方报名吧!|一数线段 二数角|三数涌形|六数多成长方形|五券够层三角书|八.数多层四边形|高途课堂 名师特训班|5+7=12 8+12=20，12+24-36|明6个面。|名师出高徒网课选高途|名师出高徒网课选高|HENTURY|CENTURY|义务教育教科书|2|我把问题表示 成这样……|我晨开了一个长 方体的纸盒|所以，奇数+偶数-奇数，奇数+奇数-—，|偶数+偶数-|(2)每个面的长和宽与长方体的长、宽、高有什么关系？ 观察长方体展开图，回答下面的问题。|新用户专享 立即体验 浙江卫视指定在线教育品牌|浙江卫视|奇数与偶数的和是奇数还是偶数?奇数与奇数的和是奇数还是偶数?|十么形状的呢?|534+319-853|所以，奇数+偶数-奇数。|折叠后，哪些图形能围成左侧的正方体?在括号中画|奇数:5.7.9.11.… 偶数:8.12.20.24、…. 奇数:|五年级|1+2+3-6|一招改掉孩子数学粗心 胡涛|分数的意义和性质 45|1+2+3十ygl0|下册|长方体和正方体|探索图形|北京大学硕士|华少|回顾与反思 这个结论正确吗?|做一做 (1)哪些面的面积相等?|请在上面的展开图中，分别用“上”“下”“前”“后”“左”“右”标|eEHoR|从题目中你知道了什么?|要活着校剪开!|因数与倍数|1十ン十ン十y=(o|11+2+う)x2|l1+2+う)x2:12|1+ン+ン=し|分析与解签|把长方体和正方体的6个面分别展开，如下图。|图形的运动(三) 83|偶数?|全国百佳教师带队教学 平均教龄11年|题目让我们对 奇教、偶教的 和作一些探索|2UI3|阅读与理解|我随便找几个 奇、偶数， 加起来看一看|华杯赛い希望杯等优秀教练|物体(三)|不打不骂|18|奇教除以2余1，偶 奇数加偶数的和除以|2 还余1，所以……|后是这样的|毁除以2没有余教，|￥9|还有其他方法吗？你觉得哪种方法好?|(x+的)( -x(a+b)x+ab|(a+0)(Q-0)=a2-62|(a+b)(ab)=0Q-62|(x+a)(是2+(十b)x+ab|(x+a)(x -xa+(+b)x+ab|(x+a)(-x2+(a十句)x十的|我可以再找一 些大数试一试|正方体展开|(a+b)2=q2十2的b+比 (a-b)2=02-20b+h2|(a+b)2=a2+2nb十b2|(十b)2=92十2ab+包2|(a-02=n2-2a0+62|偶数与偶数的和呢?|5+8-13|5+|请在|奇数+奇数|表面积|数学\", \"video_asr\": \"大家都觉得小学数学很简单，为什么咱家孩子就是上不了九十五呢？都孩子喜欢说考试考不好是粗心了。|哭了，错了，数学的学习没有粗心，没有马虎，只有知识掌握不牢固，习惯不标准。我是胡涛，北京大学硕士。|北大人培养北大好苗子，在我的课上，我会刷新孩子数学学习的认知，不仅让孩子知道怎么做。|更知道如何举一反三培养数学思维，还在等什么呢？点击视频下方报名吧，欢迎来到高途课堂，正式成为高途学生！|祝大家金榜题名，一路高歌！\"}\n",
      "multi-modal tagging model forward cost time: 0.015980005264282227 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '单人口播', '教师(教授)', '静态', '室内', '场景-其他', '平静', '配音', '教辅材料', '家', '拉近', '特写', '幻灯片轮播', '极端特写', '手机电脑录屏', '手写解题', '影棚幕布'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.79', '0.74', '0.45', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/127514f281043f3af92e443344aeddfd.mp4\n",
      "{\"video_ocr\": \"我每天在家上辅导班|在家里让老师讲题|进步可快了|什么班啊|腾讯佥鹅辅导|清北毕业名师授课|一对一答疑|有问必答|0元免费领取一线名师课程|官方价3元 特惠0元|腾讯自营 顶尖教师团队|中小学全科在线辅导|让孩子夏上掌习\", \"video_asr\": \"我每天在家上辅导班，在家里让老师讲题，进步可快了，什么班啊？腾讯企鹅辅导，清北毕业名师授课。|一对一答疑，有问必答，零元免费领取一线名师课程！\"}\n",
      "multi-modal tagging model forward cost time: 0.01642632484436035 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '多人情景剧', '中景', '静态', '动态', '惊奇', '拉远', '拉近', '平静', '亲子', '家', '全景', '室外', '填充', '喜悦', '特写', '配音', '夫妻&恋人&相亲', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.95', '0.80', '0.29', '0.25', '0.18', '0.17', '0.14', '0.13', '0.10', '0.06', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1277039e49fa244c875f7b6395f82a98.mp4\n",
      "{\"video_ocr\": \"3-6岁是培养孩子|数学思维的黄金期|错过了就弥补不了了|赶紧试试|想照处术排思照IA|斑马A1课思维体验课吧|现在49元10节课|报名还送超豪华教具礼盒|赶紧点击视频下方|给你的孩子报名吧|9+8= 18-9-|猿辅导在线教育 出品|15-8=|2-8岁上斑马学思维 学英语|10+9=9|守护小镇大作战|思|13-7=。o|11-4=7|速算口算题|斑马A课|7+4=l(|8+5=13|6+9=|5+6=1l|8+2=0|l0|7+|三国\", \"video_asr\": \"AS。|三到六岁是培养孩子数学思维的黄金期，错过了就弥补不了了，赶紧是斑马AI课思维体验课吧！现在四十九元十节课报名还送超豪华教具礼盒，赶紧点击视频下方给你的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01631617546081543 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '单人口播', '平静', '配音', '教辅材料', '场景-其他', '室内', '家', '极端特写', '情景演绎', '喜悦', '特写', '拉近', '动态', '手写解题', '动画', '宫格'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.67', '0.58', '0.54', '0.48', '0.23', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/127bdebfa49367fb44141ea230177747.mp4\n",
      "{\"video_ocr\": \"李老师|我想报您的|金牌名师火箭特惠班|不用这么多|现在只要18元|就可以得初中数学|和物理双科|再加1元|还可以获得|语文加英语22课时的|全方位辅导|现在报名啊|直接优惠480元|这优惠力度这么大?|教学质量它有保障么|你放心|这课程还是由|北大清华毕业的名师|带队教学|22个课时的学习|让你学到这些内容|你看|数学168个必考模块|以及考点重难点易错点|语文作文五大模板|和文言文古诗词|解读技巧|英语单词四小时记忆法|和十大完形填空技巧|物理的108个解题技巧|物理 在这里你都能学到|108 个解题技|所有课程三年内|无限次回放|课后还有专属老师|根据学生的情况|一对一答疑辅导|让孩子学得会|记得牢!|优惠名额有限|别犹豫了|赶紧点击视频下方链接|报名吧|考点、重难点、易错占|语文 作文五大模|英语单词 四小时记忆法|数学 -58 个必考模!|高途课堂|499|18|5\", \"video_asr\": \"我想报名的金牌名师火箭特惠吧，不用这么多，现在只要十八元就可以了，初中数学和物理双科。|加一元还可以获得语文加英语二十二课时的全方位辅导，现在报名啊，直接优惠四百八十元。|就是这么牛，网上吗？你放心这课程吗？还是由北大清华毕业的名师带队教学，二十二个课时的学习让你学到这些内容。|你看数学一百六十八个必考模块，以及考点重难点，易错点，语文作文五大模板和文言文古诗词解读介绍。|英语单词四小时记忆法和十大完形填空技巧，物理的一百零八个解题技巧，在这里你都能学到，所有课程三年内无限次回放。|还有专属老师根据学生的情况一对一答疑辅导，让孩子学的会记得牢牢，优惠名额有限，别犹豫了，赶紧点击下方链接！|名吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.016259431838989258 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '单人口播', '办公室', '平静', '动态', '配音', '手机电脑录屏', '特写', '室内', '喜悦', '极端特写', '场景-其他', '情景演绎', '拉远', '家', '工作职场', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.46', '0.45', '0.19', '0.03', '0.02', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/127ef61664c05f2427dbcc4c7d30d9b7.mp4\n",
      "{\"video_ocr\": \"妈妈|你为什么还没给我报名|作业帮直播课|小初高数学名师 提分班啊|不就30块钱的事吗|现在给你报还不行吗|我们班同学李乐 和其他的|一些同学都已经报名了|他的名次都超过我了|就连一些平时拿倒数的 同学都赶上来了|你说他们报的是|这谁能想到效果 能有这么好呢|你这是偏见|清北毕业名师带队教学|专业高效|课后还有班主任老师 1对1辅导答疑|不懂就问 直到会为止|课程还能 3年内无限次回放|能让我随时随地的 巩固复习|好|我现在就给你报|让你也能掌握数学难题 的解题技巧|少走弯路|冲刺高分|让你也反超他们|你现在也点击视频下方 报名吧|立即报名>|名师提分班 30元18节课|上课内容与收到礼盒请以实际为准|赠送12件套教辅礼盒|哇!\", \"video_asr\": \"妈妈，你为什么还没给我报名作业帮直播课？小初高数学名师提分班不就三十块钱的事吗？现在给你报还不行吗？我们班同学理论和其他的一些同学都已经报名了，它的名词都超过我了，就连一些平时连到处都同学都赶上了。你说他们报的是作业帮直播课。|小初高数学名师提分班吗？这谁能想到效果能有这么好呢？你这是偏见，清北毕业名师带队教学专业高效，课后还有班主任老师一对一辅导，答疑，不懂就问，直到废为止，课程还能三年内无限次回放，能让我随时随地的跟我没复习好，我现在就给你报让你。|也能掌握数学难题的解题技巧，少走弯路，冲刺高分一份抽他们，你现在也点击视频下方报名吧！|下载。\"}\n",
      "multi-modal tagging model forward cost time: 0.01654505729675293 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '推广页', '静态', '多人情景剧', '平静', '亲子', '家', '家庭伦理', '全景', '特写', '喜悦', '动态', '愤怒', '单人口播', '室外', '极端特写', '惊奇', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.96', '0.64', '0.19', '0.11', '0.07', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1282dbc9783a520769e97f29445e97d0.mp4\n",
      "{\"video_ocr\": \"差点酿成悲剧|哎你这是怎么开车的|啊不好意思不好意思|后面有娃看不|妙你|您真是我的救命恩人|要是被压到了|我都不知道|该怎么办术好了|不客气|你的外卖都酒了|我赔给你吧|哎哎不用了不用了|喂老婆|我刚才救了一小孩|外卖全酒了|我还得回去取一趟呢|你自己都救不了|迹救别人|马小云|你能不能有点出息呀|你老要我孩子都生不起了|这日子别过了|真不好意思小哥|你把你的手机给我一下|我给你在手机上|报了一个快财商学院|这是免费的课程|你在上面学学理财|每天打开手机|就会有额外的收入|理财学好了|收入可能比你|送外卖都高呢|可是|我读书少|这理财我真的能学会吗|咋不能|我们小区的|退休老头老太太|都在学理财|外卖小哥见义勇为 狂飙枚下婴儿|究竟是车主的失误？|还是家长的责任？|快财 直播|外卖|五月\", \"video_asr\": \"哎，不明确呗。|你是怎么看上的？|怪我看不见。|你真是我的救命恩人，你要是被压到了，我都不知道该怎么办才好了，不客气，你的外卖都撒了，我赔给你吧，不要了。|喂，老婆，我刚才救了一小孩，外卖全洒了，我还得回去取一趟呢，你自己都救不了还救别人。|小云，你能不能有点出息啊，你老婆我孩子都生不起了，这日子别过了。|真不好意思，小哥，你把你的手机给我一下，我给你在手机上报了一个快财商学院，这是免费的课程，你在上面学学理财，每天打开手机就会有额外的收入，理财学好了收入。|能比你送外卖都高呢，可是我读书少，这理财我真的能学会吗？咋不能我们小区的退休老头老太太都在学理财课程会。\"}\n",
      "multi-modal tagging model forward cost time: 0.016308069229125977 sec\n",
      "{'result': [{'labels': ['现代', '填充', '多人情景剧', '中景', '静态', '推广页', '平静', '全景', '愤怒', '室外', '动态', '惊奇', '夫妻&恋人&相亲', '路人', '特写', '喜悦', '悲伤', '汽车内', '停车场', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.94', '0.88', '0.83', '0.77', '0.72', '0.31', '0.21', '0.12', '0.08', '0.06', '0.02', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/128d9e22ee1937ad0c47c3c088f0e78b.mp4\n",
      "{\"video_ocr\": \"两万|我这只有一万三|爸我就这么多点钱呢|姑娘 你是不是遇到什么困难了|这不家里需要点钱|我这攒了这么久|还是差点|这样|我送你一个大礼包|你用手机啊|报名一个快财商学院|哥|这理财|咱不懂也没钱报名|这个啊|不需要花钱|299元的课程|现在0元就能报名|天理财名师爆款直播课|18个财富增值实用技巧|还有22种投资必备避坑指南|你呀好好去学一学|以后利用空闲时间|也能积累一笔不小的财富|谢谢哥|这是一点心您拿着|不用了/姑娘|你自己留着|屏幕前的你们 赶紧点击视频下方链接|一起报名吧|告别死工资，小白理财如何少走弯略?|0元抢购|适合人群|理财小白 月光一族|速程分绍|的投资工具?|前200名|AD|战力 月光|群坑老手|家度主妇|直播 学理财上快财|快财商学院|15:28|训练营|快财|视频为演绎情节|限量0元抢购\", \"video_asr\": \"两万，我这只有一万三，还把我就这么多点钱了。|姑娘，你是不是遇到什么困难的？这不家里需要点钱，我这攒了这么久还是差点啊，我送你一个大礼包。|你用手机呀，报名一个快财商学院。|这理财咱不懂，我也没钱报名，这个不需要花钱二百九十九元的课程啊，现在零元就能报名六天理财名师爆款直播课，十八个财富增值实用技巧。|还有网二十二种投资必备避坑指南！|你呀，好好学学。|以后利用空闲的时间啊，也能积累不小的财富。|谢谢哥这一点心意，你拿着我用了公家。|你自己留着吧。|屏幕前的你们，赶紧点击视频下方链接，一起报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016222238540649414 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '填充', '静态', '平静', '手机电脑录屏', '动态', '特写', '悲伤', '惊奇', '愤怒', '喜悦', '全景', '朋友&同事(平级)', '单人口播', '路人', '夫妻&恋人&相亲', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.72', '0.37', '0.05', '0.05', '0.04', '0.03', '0.02', '0.02', '0.02', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/129620cd2fa2474c54d305b61db22e3e.mp4\n",
      "{\"video_ocr\": \"你怎么开车的|不知道货车有盲区啊|加塞什么啊|明明是你|诶 老王是你啊|诶可以啊|你都买大货车啦|这些年挣了不少钱吧|呀是小佳啊|这是公司的车|我是司机|自己买车成本太高了|给公司开车|不用成本|工资还很高呢|可是我听说|这么好的工作可不好找啊|我这呀也是在58同城上找的|你看|打开58同城|点击司机招聘|上面有很多公司招聘司机|学历不限|工资高|福利待遇好|到岗速度快|还可以直接视频面试|赶快点击视频下方链接|应聘吧|让生活简单美好|饭补话钢圆环|6000-8000元|限个在超职位|保健按摩|3-12000元/月|招若干人|经验不限|高中以上学历|一位描述|岗位职贵 展开|北京市丰台区和义西里一区三号楼伊利量奶|和义西里小|1957.6|您的竟争力分析|62个在招职位 5000- 四倒|8000 1h000|房产中介 家政保洁/安保|公交廉务|列车乘务 2年经验C本团|2年经验C本司机或自带车|职位发布人|经理 今日清联|公司信息|北京德邦直招B2司机包住|生产管理/研发|北京江启科技有限公司|i个在招聊位|德邦快通 者发|酒店 超市/百货/零售|出租车司机|G5N400 63N515|广*4S国|高新工作好福利 搜索职位或企业|sil 4G|人事/行政/后勤 餐饮|全部 商务|物业管理|急聘网约车司机周结包吃住 在线 包住|时长计算中|李丽爆/理7|普工/技工|德邦快递名企|5到8K招C1本诺货司机|账校教练|货语司机(0a|重通|货运|网约车司机10k|世约是争力排名|李丽娜:经理|CIMC u4g国|u 4日团|顺义招大件送货员+高油补 司机/...|女货运司机|100|建筑|化奔 联合|门维修|【北京司机招聘网|北京交|.ll4G|职位|990-8000无/月|G|联名卡年|12:45|诚40|区域|6:39 m4G 期望职位|出 4G|详情|3N40D 8RN515|8R3N5)5|北京|北奔里|北奔重卡|们维|Weichi Engagem 0859-381481|申请|发布|薪资|来58APP|Wei|更多|53同城|58 同城|联念本骐车П|特约维|我的|立即打开|CNViC|CNC|企业 最热\", \"video_asr\": \"你嫂子开枪了，不知道大货车有盲区太强吗？感觉是同事你老忘事你。|你都买大货车了，这些年挣了不少钱吧，要是小张这是公司的时候，我是司机自己买车成本太高了，给公司开车没有成本，工资还很高呢。|呀，可是我基础这么好的工作可不好找呀哎，我这也是在五八同城上找的，你看打开五八同城，点击司机招聘。|上面有很多公司招聘司机，学历不限，工资高，福利待遇好，到岗速度快，还可以直接视频面试，赶快点击视频下方链接音频。\"}\n",
      "multi-modal tagging model forward cost time: 0.016849994659423828 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '手机电脑录屏', '特写', '路人', '惊奇', '喜悦', '动态', '极端特写', '单人口播', '平静', '愤怒', '朋友&同事(平级)', '室外', '悲伤', '全景', '汽车内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.91', '0.82', '0.69', '0.48', '0.20', '0.15', '0.12', '0.08', '0.02', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/12ab00d7279bb1b8f0ec0af86ee2008b.mp4\n",
      "{\"video_ocr\": \"35(头1-12=23|47-35头1=12|47|用对了方法再难的题目|掌握了技巧也会变得很简单|我是高途课堂侯志腾老师|山东省高考状元|北京大学学士|清华大学硕士|我的三位一体满分教学法|让孩子形成|最终形成高分数学的良性循环|点击下方报名|培养数学思维 让孩子受益终身|新学员9元专享 立即报名|小学数学鸡兔同笼的经典题|用侯哥的大招5秒钟直接出答案|有一天我把鸡和兔放到一个笼子里面|我数了一数|问 有多少只鸡和小兔子呢|侯哥大招: 腿数一半减头得免|94腿|94(腿)|94腿]÷2|94腿）÷2=47|47-35头)|47-38= 94/R|17-34大=|北大清华毕业师资 平均教龄11年以上|侯志|俣侯志腾勝|志腾|高途课堂名师特训班|笼子里一共有35个头，94条腿。 请问有多少只鸡和免子?|三位一体教学法|三位一体满分数学|I月1MS1|方法大招|思维体系|只需9元|小学数学主讲老师 北京大学学士清华大掌4|学会则全科可用，且受益终身!|三者缺一不可|高途课堂|习惯 当前没有人发言|学习习惯|名师出高徒·网课选高途|清北|满分 数学|学习|个人|思维 体系|大招|方法\", \"video_asr\": \"小学数学鸡兔同笼的经典题，用猴哥的大招五秒钟直接出答案。有一天我把鸡和兔子放在一个笼子里面，我数了一数，一共是有三十五个头，九十四条腿，问多少只鸡和小兔子呢？猴哥的大招就叫腿数一半，剪头的兔就是用九十四条腿直接除以二，得到的是。|十七四十七剪头的兔减去三十五个头，直接秒出答案，十二就是兔子的数量，那鸡的数量是不是也就可以直接求出来，三十五减十二就是二十三只鸡。大家有没有发现，用对了方法，再难的题目掌握技巧也会变的很简单，我是高途课堂候智腾老师，山东省高考状元，北京大学学士。|大学硕士，我的三位一体满分教学法，让孩子形成方法大招，思维体系，学习习惯，最终形成高分数学的良性循环。|去九元，点击下方报名，培养数学思维，让孩子受益终身！\"}\n",
      "multi-modal tagging model forward cost time: 0.016326189041137695 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '单人口播', '静态', '场景-其他', '平静', '教师(教授)', '影棚幕布', '配音', '填充', '特写', '室内', '教辅材料', '情景演绎', '课件展示', '喜悦', '知识讲解', '宫格', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.88', '0.84', '0.66', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/12b66945fbed2c559ff804ba8d9c809a.mp4\n",
      "{\"video_ocr\": \"想要换手机的朋友们注意了|千万不要再花冤枉钱 去买手机了|这里有现成的免费拿 P40手机的APP|它不香吗|没错|今天我就来教教大家|在疯读极速版上薅羊毛|如何在疯读极速版上|通过看小说的方式|赢得全新的华为P40手机|其实很简单|只需要在手机里|下载疯读极速版APP|通过看小说集碎片即可|签到7天能获得9个碎片|看5章小说能获得1个碎片|10个P40碎片|就能免费兑换一部P40手机|如果你是新用户|看50章小说就能免费 获得10个碎片|10个P40碎片免费兑换 P40手机|屏幕前的你还在等什么|这么好机会千万不能放过|赶紧点击视频下方链接|看小说赢手机啦|疯读极速版|签到提醒 明日签到继续领碎片|4天|你已签到0天，别中断城 新人福利|每认真阅读五章得1枚，最高可得20+碎片 超级上门女婿|3收研片|起的上门女婿，门后却是连|【验家精品] 门前是被人瞧不|换一抢|每日阅读位松照碎片|认真阅读文章|+1碎片|手，为寻找未婚妻，来到繁…|陆逸，一个身怀医术的超级高|绝品神医|阅|阅读120分钟|中奖晒单|很惊讶收到了疯读上抽奖送的华为P40手机是真 错感谢|书友 481216|至尊。年少时被神秘少女救|第一刺客女婿|基市|世界第一杀手，血刃修罗|换一抵口|阅读领取|6天23时58分|姓名:伊“黑龙江省泰来县永|读享快乐|剩余109份 iPhone1l碎片|0/10|剩余55份|世界第一杀手，血刃修罗|开宝箱得碎片|黑龙江省泰来县永|我的奖品 碎片明细|距结束仅剩|读享快乐每日阅读轻松赚碎片|阅读1|手，|日门女娇|时娇|时门婚|6天23|20|领家|认真阅请|去阅读|0/3|查看更多> 2020-08-28|疯读|陆逸，|认直阅请立音|明日签到红以|每认真阅读五|华为|剩余|很惊讶收 真的很不|上女婚|碎片|姓名:伊|全站小说免费看|第灵制|上奶|口叩质大作 免费畅读|奖励金额视具体活动而定|疯读 疯读极速版|88|箩女款|(原扎堆小说)|免费赢手机|上门设娟|赢手机\", \"video_asr\": \"想要换手机的朋友们注意了，千万不要再花冤枉钱去买手机了，这里有现成免费拿P四零手机的APP它不香吗？没错，今天我就来教教大家在疯读极速版上好羊毛，如何在疯读极速版上通过看小说的方式赢得全新的华为P四零手机，其实很简单。|只需要在手机里下载疯读极速版APP通过看小说集碎片即可签到七天能获得九个碎片，看五章小说能获得一个碎片，十个P四零碎片就能免费兑换一部P四零手机。如果你是新用户，看五十章小说就能免费获得十个碎片。|十个P四零碎片免费兑换，P四零手机屏幕前的你还在等什么？这么好的机会千万不能放过，赶紧点击视频下方链接！|下载疯读极速版APP看小说赢手机了！|哎。\"}\n",
      "multi-modal tagging model forward cost time: 0.017114877700805664 sec\n",
      "{'result': [{'labels': ['推广页', '配音', '手机电脑录屏', '现代', '中景', '场景-其他', '静态', '喜悦', '单人口播', '拉近', '平静', '多人情景剧', '全景', '动态', '愤怒', '惊奇', '工作职场', '室外', '上下级', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.92', '0.86', '0.85', '0.82', '0.73', '0.17', '0.17', '0.03', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/12b76c0df5ed14fb9dada676bf2a8666.mp4\n",
      "{\"video_ocr\": \"为什么说初中阶段 是人生的转折点|你听过这样一句话吗|初一不分上下|初二两极分化|初三天上地下|如果你初二成绩一般|还想在初三一年时间里|考上理想高中|那你一定要掌握 高效的学习方法|才能让你的学习事半功倍|你知道数学中考必备的 42个几何模型吗|你知道初中物理 105个解题技巧吗?|了解这些解题技巧|带你的孩子挑战中考高分|高途课堂 2020全科名师班|9元4科直播课|平均教龄11年的|北大清华毕业名师 带队教学|教解题思路|帮助学生系统学习巩固|训练常考题易错题|课后老师1对1辅导|3年内无限次回放|名额有限|赶紧点击视频下方链接 报名吧|6 电场强度与电势差的关系|4.静电平衡|挑战极限|7.利用数学法求解|对称法|纱抢物理108技|1.利用|电场强度求法|1三挑战|加 挑战|水艺礼战|1尽二礼战|UUT|中小学生在线教育平台 《极限挑战》第六季官方推荐|3.电场的婚加|2利用k|利用|距帮指手线间迪钱级的风魔 无蔽制|水尽管热战|即刻出发 逆袭学霸|名师在线\", \"video_asr\": \"为什么说初中阶段是人生的转折点？你听过这样一句话吗？初一不分上下，初二两极分化，初三天上地下。如果你初二成绩一般，还想在初三一年时间里考上理想高中，那你一定要掌握高效的学习方法，才能让你的学习事半功倍，你知道，数学中考必备的。|四十二个几何模型吗？你知道初中物理一百零五个解题技巧吗？了解这些解题技巧，带你的孩子挑战中考高分，高途课堂二零二零全科名师班，九元四科直播课，平均教龄十一年的北大清华毕业名师带队教学，教解题思路，帮助学生系统学习。|训练常考题，易错题，课后老师一对一辅导，三年内无限次回放，名额有限，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.022552490234375 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '平静', '单人口播', '配音', '静态', '室内', '场景-其他', '极端特写', '家', '拉远', '情景演绎', '手写解题', '手机电脑录屏', '拉近', '动态', '特写', '混剪'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.94', '0.30', '0.27', '0.11', '0.07', '0.06', '0.06', '0.05', '0.03', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1300204a477abb0a504fd341bd60fd38.mp4\n",
      "{\"video_ocr\": \"王阿姨|刚才怎么了|丽丽为什么先走了|这不就马上初中了嘛|她这语文成绩上不去|你说作文才得了几分|还怪我不给她报辅导班|可是你说这辅导班 才29块钱|这谁能相信啊|你说的是这个|作业帮直播课推出的|语文阅读写作提分班吗|就是这个啊!|29块钱就有20节名师课|这怎么可能啊!|那您可就错了|这个作业帮直播课|由清华北大毕业的老师|带队授课|20课时|掌握了90%必考重难点|课后还有班主任老师|一对一辅导答疑|而且现在报名还免费包邮|12件套大礼包|我们班好多同学都报名了|那这课这么好|在哪报名啊!|点击视频下方链接|就可以报名了!|￥29=20节名师课|孩子不知道什么原因把书撕了|孩子好妈在什旁系足无搭撕了|妈妈在一旁手足无措|作为家长你怎么看|清北毕业名师带队授课|16.44.999|优秀作文集|上课内容与收到礼盒请以实际为准|监控下发生了这一幕|语文阅读 写作提分班|课|F2.81901000.0E\", \"video_asr\": \"蚂蚁刚才怎么了？丽丽没什么先走了，这不就马上初中了吗？她在语文成绩也上不去，你说是做完才做了几分，还怪我不给他报了辅导班，可是你说这辅导班才二十九块钱，这谁能相信啊，你说的是这个作业帮直播课推出的语文说。|写作提分班吗？就是这个呀，二十九块钱就有二十节名师课，这怎么可能啊，那你可就错了，这个作业帮直播课语文阅读写作提分班由清华北大毕业的老师带队授课，二十课时掌握了百分之九十的必考重难点，课后班主任老师一对一辅导答疑，而且现在报名还免费包邮十二件套。|点炮，我们班好多同学都报名了呢，那这课这么好，在哪报名啊？点击视频下方链接就可以报名啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.01673269271850586 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '平静', '多人情景剧', '家庭伦理', '静态', '亲子', '全景', '动态', '惊奇', '喜悦', '愤怒', '路人', '家', '室外', '拉近', '特写', '极端特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.92', '0.20', '0.11', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1304f5861ddeaa9381fef437e659ffac.mp4\n",
      "{\"video_ocr\": \"这可比你用Excel快多了|整理了3天的报表你还会出错|你是来混工资的是吧|我不想干了|每天那么多报表要整理|那么多数据要分析|天天加班到12点|出了点小错老板不体谅也就算了|居然还觉得我在混工资|我帮你啊|搞定啦|哇塞，你怎么做到的|用Python自动化办公呀|敲几行代码就能搞定批量报表|你在哪学的呀？|扇贝编程Python课|6.9元就能带你轻松入门|0基础也能学哦|快点击视频报名吧|涨薪季|专业教研团队|用户评价|用扇贝编程，既高效又有趣 ¥50-800|Python编程课|经.典.组.合|教学特色|Python基础课+认知课|交互式课堂+作业实操+每日一练|初识Python 基础语法|扁贝编程|扇编程|扇贝编程|视频力演绎情节|4 天.带你从0入门|¥6.9|制丰舰惠|中机\", \"video_asr\": \"整理商机的报表要吃素啊，你怎会不知道，是吧？我不想干了，每天那么多报表要处理，那么多数据要分析。|天天加班到十二点，出了点小错，老板不体谅你就算了，居然还追的我在分公司我帮你啊。|搞定了，哇塞，你怎么做到的？用PYTHON自动化办公呀，敲几行代码就能轻松搞定批量报表，这可比你用EXCEL快多了，你在哪学的呀？扇贝编程PYTHON课六块九就能带你轻松入门，零基础也能学会，快点击视频报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01710224151611328 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '工作职场', '手机电脑录屏', '办公室', '中景', '多人情景剧', '朋友&同事(平级)', '上下级', '动态', '单人口播', '静态', '特写', '愤怒', '极端特写', '悲伤', '企业家', '喜悦', '惊奇', '配音'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.97', '0.17', '0.09', '0.05', '0.03', '0.02', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/132a2f61698edd2515fd36ccecf302ed.mp4\n",
      "{\"video_ocr\": \"7.4元购买30包竹浆本色抽纸|小心|快闪开|你这吸水又柔润的纸巾在哪里买的|天然竹浆|本色纸巾|丝滑柔润|湿水不破|就在拼多多哟|原生木桨卫生纸巾|已拼10万+件 旗舰店30包/8包丝飘天然竹浆本色纸巾抽纸批发整箱家用|妇婴适用 足量300张|¥7.4起￥66.9|拼多多 下载拼多多 购买同款|卫生纸面巾纸抽|川击毛|848|0|sipiao|30包|丝飘\", \"video_asr\": \"不给。|小心。|来了。|快上课。|哎。|明天明天明天。|你在吸血绿绿绿的平安，你买的是标天然竹浆本色水晶，丝滑柔韧，湿水不破，现在的都有。\"}\n",
      "multi-modal tagging model forward cost time: 0.016353845596313477 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '静态', '填充', '喜悦', '手机电脑录屏', '惊奇', '愤怒', '悲伤', '路人', '特写', '单人口播', '动态', '夫妻&恋人&相亲', '极端特写', '朋友&同事(平级)', '拉近', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.86', '0.82', '0.79', '0.74', '0.09', '0.06', '0.05', '0.02', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1332d3a206da11c214cfd955b1b0295b.mp4\n",
      "{\"video_ocr\": \"和秒杀解题技巧|高效方法|高考目标 都是冲刺名校|我这么努力的学习|连考个二本都难|那谁也没想到|9块钱就能听|北大清华毕业老师的 直播课|效果能这么好呀|他们这次做题|都是1分钟一小题|5分钟一大题|就算是最难的压轴题|他们用解题方法|几步就搞定了|这次考试|我题都没做完|他们都检查三遍了|你做题不能只求快|你得学会呀|你看班里的乐乐|就学会了一道题|老师方法还没讲呢|同类型的题 他都会做了|那你去问问乐乐|他报的是这个吗|就是这个|高途课堂 初高中全科名师班|9块钱|就有语数英物|各4节|16节课|不仅能掌握|高中数学的|167个考点|代数9大考法|几何42个模型啊|而且学到了 好多名师总结的|重难点和解题技巧|做题轻松学会 举一反三|考试拿高分|这课这么好啊|那妈给你报名|现在|只要点击 视频下方链接|就能报名啦|小姐小姐|不就是9块钱吗|我给你报|我们班好多同学|跟着高途课堂的名师|从高一就开始学习|08\", \"video_asr\": \"李小姐，我就是九块钱吗？我给你报。|我们班好多同学跟着高途课堂的名师从高一就开始学习高效方法和秒杀解题技巧，高考目标都是冲刺名校，我这么努力的学习，连考个二本都难，但谁也没想到九块钱就能听北大清华毕业老师的直播课，效果能这么好呀！|只做题都是一分钟一小题，五分钟一大题，就算是最难的压轴题，他们用积极棒子底部就搞定了。这次考试我题都没做完，他们都检查三遍了，你做题不能只求快，你得学会呀，你看班里的乐乐就学会了一道题，老师方法还没讲呢，同类型的题她都会做了，那你去问问乐乐，她报的是这个吗？就是这个高途课堂，初高中全科名师班。|九块钱九五与数英物各四节，十六节课，不仅能掌握高中时学的一百六十七个考点，代数九大，考法，几何四十二。|模型，而且学到了好多名师总结的重难点和解题技巧，做题轻松学会举一反三，考试拿高分，这课这么好，能把给你报名，现在只要点击视频下方链接就能报名啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.0176849365234375 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '室外', '填充', '多人情景剧', '静态', '全景', '愤怒', '平静', '动态', '亲子', '汽车内', '喜悦', '特写', '惊奇', '家庭伦理', '极端特写', '路人', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.93', '0.88', '0.81', '0.53', '0.31', '0.22', '0.19', '0.11', '0.05', '0.05', '0.05', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/133f76a36aac48b25fc4a5ea353cdd95.mp4\n",
      "{\"video_ocr\": \"我就把你东西给你扔了|看见这条三八线没|你再过线|何络络|这次考试..|你是不是作弊了|谁作弊了|这都是我自己|一道题一道题算出来的|不可能?!|你哪次考试不是倒数|这次试卷这么难|数学课代表|才考了132分|我可是有名校老师|辅导作业的|我上的课是由|省高考状元|北大清华毕业的|全国百佳教师带队教学|主讲老师来自于|每年能输送上百名的|清华北大学子的|一线名校|平均教龄11年|这么厉害|那他们都教啥啊|还让你刷题不|刷题那都是|旧方法笨方法了|我们教的是学习技巧|高中数学168个常考点|你知道吗?|78个秒杀模型|我掌握了这些解题技巧|我拿到题目啊|就知道套用哪个模型|那这是什么课啊|我也让我妈|赶紧给我报一个|你可别跟别人说啊|高途课堂|9块钱16课时呢|直接点击视频下方按钮|就可以报名啦|高途课堂｜，浙江卫视|浙江卫视|浙江卫视指定在线教育品牌|新用户专享 立即体验|全国百佳教师带队教学 平均教龄11年|名师特训班|华少|仅需|￥9|夕|p，|加r\", \"video_asr\": \"看自己一三八见没在过去就买东西疯了。|何乐乐，这次考试你是不是作弊？谁作弊了？这都是我自己一道题，一道题算出来的，不可能你哪一次考试不是倒数，这次是带这么难，所以就课代表再考了一百三十二吧，我可是有名校老师辅导作业的，我上的课是由省高考状元，北大清华毕业的全国百家教师带队教学，暑假老师来自于每年能输送上百名的。|清华北大学子的一线名校，平均教龄十一年，这么厉害，那他们都叫啥呀，还让你刷题，不刷题那都是旧方法，笨方法了。我们教的是学习技巧，高中数学一百六十八个常考点，你知道吗？七十八个秒杀模型，你会吗？我掌握了这些解题技巧，我拿到题目啊，就知道喷雾哪个模型。|的厉害，那这是什么课呀？我也要我妈赶紧给我报一个，别跟别人说啊，高中课堂九块钱十六课时呢，直接点击视频下方按钮就可以报名了。\"}\n",
      "multi-modal tagging model forward cost time: 0.01705336570739746 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '填充', '家', '静态', '平静', '家庭伦理', '喜悦', '亲子', '极端特写', '特写', '悲伤', '朋友&同事(平级)', '学校', '教辅材料', '惊奇', '全景', '单人口播'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.75', '0.70', '0.56', '0.30', '0.25', '0.13', '0.06', '0.01', '0.01', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13414c7d153b8d9145b47c1f88699180.mp4\n",
      "{\"video_ocr\": \"您好|这里是360借条客服|我就想问一下|我在360借条上的|额度是多少|请问您的手机号|使用了多久呢|3年啦|那您可以自己|输入手机号|和身份证号查询呢|额度有15万|这个什么时候|可以打到我卡上呀|最快呀|5分钟就能放款|那这利息高吗|利息不高|借4万内|最长免息30天|“而且我们最长可以|分12期慢慢还|减缓您的还款压力|真的吗谢谢谢谢|太好了|我以后再也不用|为钱发愁了|查看你的额度|¥ 360借条|￥ 宣宜口借条|￥ 三◎借条|莓借条|P1＠借条|冥回借条|目50借条|”际审批力|具体额度一等很集际审批为准 息费成身体沽观为准|借贷有风险 选择需谨慎|具体额度费率等根据实际审批为准 息费减免优惠以具体活动规则为准|具体额 据|息费|具体霸|享要|息事减以具 具体度率等|具体领度 息费咸气|30天息费优惠券|150000元|点击视频下方链接|立即领取 活动规则>>|快前往APP领取最高20万额度免息借钱 更有88元现金红包等你来拿|现则为准|审批为， 叫为准|具体贷款额度/时间以实际审批为准|借一年，慢慢还|恭喜您成功领取免息券|最快5分钟到账 36口借条360金融集团旗下消费信贷品牌|确民点后金次|激活额度 打开A|免息|免费(免息)借款30天指借款12期时，免除首期借秋部分 利息，提前还款需支付全部剩余息费。|迎请好友再送一 绿球速请好友，有|￥B6借条 最高可借20万|日息0027%起|全民免息节|HUAMNEL|打J开APP|星体额|息借钱|房伴|开始|分享\", \"video_asr\": \"您好，这里是三六零借条客服，就想问一下，我在三六零借条上的额度是多少，请问您的手机号使用了多久了？三年了，那您可以自己输入手机号和身份证号查询呢？我在三六零借条上额度有十五万，这个什么时候可以打到我卡上呀？最快呀，怎么就能放款？那这利息高吗？我们三六零借条的利息不高，借四万内介绍。|三十天，而且我们最长可以分十二期，慢慢还减缓您的还款压力，真的吗？谢谢，谢，谢，谢，谢，太好了，我以后再也不用为钱发愁了。\"}\n",
      "multi-modal tagging model forward cost time: 0.016422033309936523 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '喜悦', '手机电脑录屏', '惊奇', '朋友&同事(平级)', '平静', '单人口播', '汽车内', '夫妻&恋人&相亲', '特写', '拉近', '悲伤', '办公室', '极端特写', '路人', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.48', '0.40', '0.22', '0.17', '0.11', '0.07', '0.06', '0.03', '0.02', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/134d47fd7f16aee8dc71adc64bdf48a1.mp4\n",
      "{\"video_ocr\": \"回来啦|LM 是啊|最近看你|天天出去溜达|锻炼身体呐|MA|走走路|不仅能锻炼身体|还能赚点小钱|一举两得啊|走路也能赚钱|下载全民健走这个软件|走走路就能领金币了|在哪里下载|我也下载一个|这样啊|我给你发个链接|你用我这个链接下载|你能拿5.88元红包|我最高能拿|33块钱|这软件这么大方啊|除了走路能领金币|玩转盘还能|领88元红包呢|这软件好啊|还有更好的呢|每天签签到|就能领红包|和提现券|签到第3天|第5天和第7天|都能领现金红包|第15天|还可以开大宝箱呢|你看|我都领了五百多了|我现在就下载|MAMI|点击视频下方立即下载|成功邀请1位好友 轻松躺赚ヨヨ元|福耐大转盘|天天转转“赚”|WAITING FOR ANEW TREND|提现来自全民健走 +532.52|添加 其他|>|领取初始步数奖励|TH82YG|邀请好友|连续签到第1天|签到规则|现金红包|THELATESTSTREET T 特28金币|THE LATESTSTR|2天|2天3天|我的邀请|+18 同步微信步数|(0/10) 获取更多抽奖机会|邀请码|去填写|Mate30手机碎片|33元|常见问题|TA看视频|好友连续|去邀请|商户单号|100|更都有收益|看视频，你和好友都有收益|2020-07-1310:08:45 100003990100399013742173353|抢现 每日|最高1元|已获得0/600金币|100007137420200713742173353 7622|累计31元|去观看|观看视频 拍奖机会+1|WAITING FORA NEW TREND|LONGMAI O|UITESTSTREET TREND|标签和备注|查看往来记录 对此订单有疑问|交易成功 提现金额:+565.37|0a10王|903|下载|红包|1a1|日奇王|三王|规则|1253|14:57 账单详情|填写邀请码|1元|复制|抽奖 余1次积金|THE LATEST STREETTRENO|具体活动以实际任务为准|全民健走|订单号|觎律时绸|创建时间|商品说明|2020-07-11 20:01 BO2020042991805007410205|神秘|已存入零钱|商家订单号Djv0wZDw7mzG+i01FTK080+Y60A4Rn|B02020nZ81806007410206 衡家订单号D0wZDw7mzO+0FTYaD+H04Rn|+565.37|账种分类|16:33|1888|NL|￥88\", \"video_asr\": \"回来了啊，是啊，最近看你天天出去溜达锻炼身体呢，是啊，六个六不仅能锻炼身体，还能赚点小钱，一举两得，走路也能赚钱，是啊，下载全民健走这个软件，走走路就能领金币了，在哪下载？我也下载一个，这样我给你发个链接，你用我这个链接下载，你能拿五点八八元红包，我最高能拿三分。|红包这软件这么大方啊，是啊，除了走路能领金币，玩转盘还能领八十八元红包等。这软件好啊，还有更好的呢，每天签到就能领红包的体验券。|签到第三天就能领题，验证的五天和第七天都能领现金红包，这十五天还得开大宝箱呢，你看我都领了五百多了，我现在就下载。|哎，记得点击我给你发个链接哦哎。|哦。\"}\n",
      "multi-modal tagging model forward cost time: 0.016332626342773438 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '手机电脑录屏', '室外', '喜悦', '全景', '惊奇', '动态', '单人口播', '路人', '亲子', '夫妻&恋人&相亲', '极端特写', '拉近', '朋友&同事(平级)', '平静', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.88', '0.57', '0.29', '0.15', '0.09', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/135f7e58dcda930a1a903d854275c3af.mp4\n",
      "{\"video_ocr\": \"哈~哈~哈~|王美琪|我给你买的 练习册你做完了吗|哎~等会等会|等什么呀|赶紧给我 回屋做题去|你以为不停 的让孩子刷题|他的成绩 就会提高吗|这样的学习方法|只会让孩子|越来越抵触|最终对学习|丧失自信心|我推荐你试试|高途课堂 全科名师班|9元就能获取|语数英 物四科|知识点全面解答|清华北大 毕业的老师|带队直播授课|课程支持 3年内无限次回放|点击视频下方链接|下拉点击立即报名|选择年级|输入手机号和验证码|就可以了|赶紧行动吧|新学员9元专享 立即报名|请选择孩子9月升入年级|高二|【4科16节】秋季高二全科名师班|省高考状元带队授课 老师平均教龄11年+|语数英物4科全面提升 高中16节精讲直播课|1对1辅导|券后特惠 高中4科|支付方式|科全面提升|下I么名额 剩余支付时间|清北毕业名师团队授课|09:54.7|师团队授课|注业上业师团队授课|高途课堂 请输入支付密码|周 元业龄|3年回放|随时复习 答疑解惑|湖北省高考 北京大学|名师特训班|共16节课|499|上课时间:9月11日14:00开始|状毕教|考学|抓住暑假关键时期 温故知新，冲刺领跑新学期|清北毕业名师教学，快速全面提升|仅剩29名|豪囊袭|￥9.00|52s|= 课程指导价499-优惠券490|注北比Nl夕师拗尚 她法全高坦#|名师出高徒·网课选高途|实付款:￥9 (已|手机号* 验证码|课程|立省490元|获取验证码|认支付|到手价|￥9|9元|上课\", \"video_asr\": \"来来来。|咳咳。|检测的。|哈哈哈。|王美琪，我给你买的练习册，你做完了吗？等会等会等什么呀，赶紧给我回屋做题。|多好，你以为不停地让孩子刷其他的成绩就会提高吗？这样的学习方法只会让孩子越来越抵触。|最终，对学习丧失自信心，我推荐你试试高途课堂全科名师班，九元就能获取语数，英物，四科知识点全面解答。|清华北大毕业的老师带队直播授课，课程支持三年内无限次回放，点击视频下方链接下拉，点击立即报名，选择年级，输入手机号和验证码就可以了。|赶紧行动吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.020371198654174805 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '单人口播', '静态', '配音', '场景-其他', '平静', '喜悦', '室内', '手机电脑录屏', '多人情景剧', '极端特写', '教师(教授)', '家', '动态', '学校', '情景演绎', '填充', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.94', '0.90', '0.81', '0.79', '0.74', '0.64', '0.47', '0.41', '0.25', '0.02', '0.01', '0.01', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1379db740ea3ed9fa83a2c567a85b8ad.mp4\n",
      "{\"video_ocr\": \"抢到啦|耶!|喂 儿子|我给你抢的|高途课堂 高中全科名师班|大家都在抢什么呢?|我们一起去看一下吧|是高途课堂推出的|9块钱就能买到|由北大清华毕业的名师|带队教的|语数英物4科16节课|语文36技|和作文5大模块|数学7大必考点|英语4小时 搞定阅读写作|物理108个秒杀技巧|统统教给孩子|课后还有辅导老师|一对一答疑|轻松解决孩子|做题基础差|解题没有思路的问题|这个课程支持|3年内无限次回放|再也不用担心|我的成绩问题啦|16节直播课|才只要9块钱|语数英物|全面提升|这么好的课程|各位家长|千万不要错过|赶紧点击 视频下方链接|报名吧|新学员9元专享 立即报名|9元学4科集中攻克重难点|省高考状元带队授课老师平均教龄11年+|仅剩-名 班级剩余名额|剩余支付时间|RECO|课程信息 上课时间|[4料16节课]暑期高一升高二全科名师班 8月28日14:00开始|手机号*手机号将是您听课的唯一先证|手机尊摄是您听课的难一凭证|实付款￥0|¥9 已优惠490]|09:59.8|验证码*请输入您的验证码|获取验证码|高途课堂全科特训营|名师特训班|预商付|小T|示十|开卡|222\", \"video_asr\": \"抢到了抢到了，抢到了，儿子，我给你抢的高途课堂高中全科名师班，抢到了大家都抢了，我们一起去看一下吧，是高途课堂推出的高中全科名师班，九块钱就能买到，由北大清华毕业名师带队教的。|数英物四科十六节课，语文三十六计和作文五大模块，数学七大必考点，英语四小时搞定，阅读写作，物理一百零八个秒杀技巧通通教给孩子，课后还有辅导老师一对一答疑，轻松解决孩子做题基础差解题没有思路的问题，这一课程支持三年内无限制回放，再也不用担心我的成绩问题了。|十六节直播课才只要九块钱，语数英物全面提升，这么好的课程，各位家长千万不要错过，赶紧点击视频下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01642894744873047 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '静态', '路人', '亲子', '室外', '采访', '喜悦', '转场', '全景', '平静', '多人情景剧', '单人口播', '情景演绎', '配音', '混剪', '特写', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1382874b48c5bad1b0eafb3c3d5e537c.mp4\n",
      "{\"video_ocr\": \"问个问题|在线补习班的本质到底是啥|请注意|不是把不值钱的东西变得值钱了|而是把那些本来很贵很有价值的课程|让所有的孩子都有机会用|低成本的方式去学习|就拿《泉灵的语文课》来说|那是和多所重点名校联合研发的|很多重点的中小学都在用她的语文工具|比如说用“讲故事手套”的方式|看图说话|用“故事山模型”来概括文章大意|这些工具学完之后|效果当场可以检验|这可是提分的“黑科技”|张泉灵说|希望通过她的语文课|短期都能提高孩子的语文成绩|而长期能够拓展视野|提升语文能力|点击屏幕下方的按钮|加入我们的《泉灵的语文课》|得到App创始人 罗振宇|领新人礼包|少年|得到\", \"video_asr\": \"问个问题啊，在线补习班的本质到底是啥？请注意，不是把不值钱的东西变得值钱了，而是把那些本来很贵很有价值的课程，让所有的孩子都有机会用低成本的方式去学习，就那泉灵的语文课来说，那是和多所重点名校联合研发的。|很多重点的中小学都在用它的语文工具啊，比如说用讲故事手套的方式看图说话，用故事山模型来概括文章大意。|这些工具学完之后，效果当场可以检验，这可是提分的黑科技啊！张泉灵说，希望通过他的语文课，短期都能提高孩子的语文。|成绩而长期呢，能够拓展视野，提升语文能力，好点击屏幕下方的按钮，加入我们的泉灵的。|语文课文。\"}\n",
      "multi-modal tagging model forward cost time: 0.01690220832824707 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '室内', '推广页', '平静', '单人口播', '教师(教授)', '多人情景剧', '家', '特写', '企业家', '极端特写', '家庭伦理', '喜悦', '愤怒', '手机电脑录屏', '动态', '场景-其他', '拉远'], 'scores': ['1.00', '1.00', '1.00', '0.99', '0.97', '0.95', '0.93', '0.66', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1383f2903432c3b07afeaed33257f10b.mp4\n",
      "{\"video_ocr\": \"我今年四岁了|就能掌握|一百以内的加减法|妈妈从来|不逼迫我学习|我可以自2学习|不需要妈妈在身边|陪着我|我分得清形状|可以辩别大小|识别颜色|你知道是为什么吗|这些都是我在|思维体验课上学的|看动画片的时间|就学会了|出状|运算|互动答题|还可以获得|斑金呢|能在斑马商城兑换|好多益智玩具|斑马A课|思维体验课|10节课49元|点击视频下方链浸|就可以报名啦|AI课|孙鸟AI课|队马AI课果|哪马Ali课|A1v|THE|Dm|数独游戏棋|91|猿辅导在线教育|0 逻辑训练|思维\", \"video_asr\": \"我今年四岁了，就能掌握一百以内的加减法，妈妈出门。|逼迫我学习。|自己学习，不要粑粑在身边陪着我，我分的清形状，可以辨别大小，是变颜色，知道是为什么吗？|我在这样的练册上的原的看动画片的时间就学会了，竖屏中预算互动，答题还可以获得斑马金币呢，要在斑马上称对话好多益智玩具会买可视频，可能十节课四十九元，北京直飞下方链接就可以报名啦。\"}\n",
      "multi-modal tagging model forward cost time: 0.023267745971679688 sec\n",
      "{'result': [{'labels': ['填充', '现代', '中景', '推广页', '静态', '平静', '单人口播', '家', '亲子', '教辅材料', '特写', '多人情景剧', '室内', '场景-其他', '配音', '动态', '极端特写', '家庭伦理', '课件展示', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.90', '0.85', '0.54', '0.39', '0.36', '0.29', '0.20', '0.11', '0.07', '0.02', '0.01', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1387424dca060da8f8b08c976fb15567.mp4\n",
      "{\"video_ocr\": \"硬套|300|却占了整整|也就是说|每一句话都承担着|得分的责任|而有时候|孩子们光作文|就能拉开|的差距|我是高途课堂 吴月光老师|北京师范大学 博士在读|年国内外教学经验|小学语文的|我通通传授给你|只要|两周见证孩子的变化|还在等什么|点击视频下方的 查看详情|报名吧!|就能拉开10-20分|9|三大写作诀客|千七太阴读技巧|9块钱|仅需|你家孩子 是不是还是这样写|有一天， 天气睛朗，|阳光明媚，|万里无云，|华少|这样的开头|要知道 9元名师课|g00=400|难忘的一天|名师特训班|北京师范大学博士在读|培养数十万优秀学员|新用户专享立即体验 浙江卫视指定在线教育品牌|课程3年支持无限次回放|全国百佳教师带队教学 平均教龄11年|浙江卫视|吴月光|生搬\", \"video_asr\": \"作文写难忘的一天，你家孩子是不是还是这样写？有一天天气晴朗，阳光明媚，万里无云，这样的开头生搬硬套。|毫无新意，要知道，小学作文才三四百字，却占了整整三十分，也就是说，每一句话都承担着独分的责任，而有时候孩子们光作文就能拉开一二十分的差距。我是高途课堂吴月光老师，北京师范大学博士，在读九年国内外教学经验，小学语文的五大，写作诀窍七大。|技巧我通通传授给你，只要九块钱，两周见证孩子的变化，还在等什么，点击视频下方的查看详情！|报名吧。|ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.016381502151489258 sec\n",
      "{'result': [{'labels': ['推广页', '填充', '现代', '单人口播', '中景', '静态', '教师(教授)', '配音', '特写', '场景-其他', '平静', '室内', '图文快闪', '动画', '影棚幕布', '手机电脑录屏', '幻灯片轮播', '教辅材料', '转场', '全景'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/139c8ef721e69bdddbaceccf3b59380c.mp4\n",
      "{\"video_ocr\": \"看着和你一起 本课程适大专及以上学房学习|刚毕业的同学|都找到了工作|羡慕吗|看着按时下班的同事|你|看着升职加薪的工作伙伴|别急|你不用羡慕任何人|学会Python后|多么复杂的数据表格|编辑几行代码|就能轻松解决|从此提高工作效率|具备核心竞争力|轻松得到|领导赏识|现在开课吧|Python|三天体验课|只要9块9|别犹豫了|赶快点击视频下方|报名吧|星适大专及以上学币 字习|本课程 专及以上学历用学平|课程适大专及a学习|本课程适大 b商户学习|本课程适大专|本课程适ヘ3上学历用户学可|本课程适大一反以上学历用户学习|大专及以上学历用户 |本课程学 户学习|程适大专及以」 一学习|程适大专及以上国户字|）上学房同习|上学房用白学习|本课可|生用户学习|本课程话大专及 用户学习|本课程股业上学房用户学习|本得爱以上学房用白学习|本调…|本和|本课程地义上学贸国白学习|本调上学房用宇习|Koaikeba 开课吧\", \"video_asr\": \"看着和你一起刚毕业的同学都找到了工作，你羡慕吗？看着按时下班的同事，你羡慕吗？看着升职加薪的工作伙伴，你羡慕吗？别急，你不用羡慕任何人，就会PASS后多么复杂的数据表格，编辑几行代码就能轻松解决，从此提高工作效率，具备核心竞争力，轻松得到领导赏识，现在开！|八，潘森三天体验课只要九块九，别犹豫了，赶快点击视频下方报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01658177375793457 sec\n",
      "{'result': [{'labels': ['现代', '单人口播', '平静', '推广页', '中景', '静态', '特写', '多人情景剧', '办公室', '教师(教授)', '动态', '极端特写', '室内', '拉近', '场景-其他', '工作职场', '朋友&同事(平级)', '拉远', '教辅材料', '手机电脑录屏'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.90', '0.24', '0.18', '0.08', '0.06', '0.05', '0.01', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13bd5d4388b9cbe32497126257d825d8.mp4\n",
      "{\"video_ocr\": \"北京外国语大学毕业|方法不对努力白费|你知道高考单词|你知道阅读理解|如果只靠自己揣测|不仅浪费时间|为何不一开始就跟着|我是高途课堂|剑桥TKT证书持有者|5大单词记忆法|阅读理解题眼定位法|20%的时间|80%的分数|英语学习其实|到底有没有|欢迎点击视频下方|报名体验吧|其实只需要掌握|还会消磨兴趣|专业的老师学习呢|张旭老师|就能带你拿下|真的很简单|这样神奇的课程呢|英语专业八级|5大|887个 核心词汇|常考话题|加 ¥9|仅需|剑桥TKT学认证|新用户专享立即体验 浙江卫视指定在线教育品牌|浙江卫视|全国百佳教师带队教学 平均教龄11年|BEC商务英语高级|张旭|名师特训班|华少\", \"video_asr\": \"方法不对，努力白费，你知道高考单词其实呢，只需要掌握八百八十七个核心词汇吗？你知道阅读理解七大常考话题吗？如果只靠自己揣摩，不仅浪费时间，还会消磨兴趣，为何不从一开始就跟着专业的老师学习呢？我是高途课堂张旭老师，北京外国语大学毕业，英语专业八级，剑桥七。|P证书持有者BEC商务英语高级，我总结出了五大单词记忆方法，理解题意定位法，百分之二十的时间就能代理拿下百分之八十的分数。英语学习其实真的很简单，到底有没有这样神奇的课程呢？欢迎点击视频下方报名体验吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016106843948364258 sec\n",
      "{'result': [{'labels': ['填充', '现代', '单人口播', '中景', '教师(教授)', '推广页', '静态', '室内', '场景-其他', '平静', '配音', '幻灯片轮播', '特写', '混剪', '教辅材料', '图文快闪', '重点圈画', '手机电脑录屏', '课件展示', '过渡页'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.92', '0.73', '0.15', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13be44255871ce531f028240d6d1c2be.mp4\n",
      "{\"video_ocr\": \"你家孩子是不是|语感特别差|英语成绩跟不上|我给你推荐|伴鱼自然拼读课|现在面向全国|推出14节自然拼读课|让孩子掌握26个字母|教孩子学英语|力求让孩子做到见词能读|听音能写|而且课程|现在报名|2698元的全套课程|现在仅需体验价|按要求完课之后|名额有限|快点击下方链接报名吧|英语不敢开口|及字母组合的发音规则|就像学拼音一样自然|还支持无限次回放|还包邮赠送学习成长地图|29元14节课|还可以领取奖学金|butterfly|蝴蝶|af|afternoon|下午|de|department|部门|公寓|forgetful|健忘|用自然拼读法学习单词发音|AB C\", \"video_asr\": \"BUTTERFLY BUTTERFLY AFTERNOON AFTERNOON TEA。|ARTEMIS DEPARTMENT APARTMENTS APARTMENTS FORGETS FO。|FORGET FOR你家孩子是不是英语不敢开口，语感特别差，英语成绩跟不上？我给你推荐伴鱼自然拼读课。伴鱼自然拼读课现在面向全国推出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样自然地球，让孩子做到见词能读，听音能写。|而且课程还支持无限次回放，现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价二十九元，十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧TEL BITCH PHONICS！\"}\n",
      "multi-modal tagging model forward cost time: 0.015914201736450195 sec\n",
      "{'result': [{'labels': ['现代', '中景', '填充', '单人口播', '推广页', '场景-其他', '平静', '配音', '静态', '室内', '动画', '办公室', '课件展示', '教辅材料', '喜悦', '手机电脑录屏', '知识讲解', '过渡页', '宫格', '动态'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.96', '0.47', '0.10', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13beeb134bbeb04ff6deb5a55f293631.mp4\n",
      "{\"video_ocr\": \"妈妈你别生气|这个礼包真的是兔费赠送的|朵朵妈怎么了这是|气死我了这孩子|她语文没学好|现在倒学会撒谎了|你看这么大的礼盒|她告诉我她花了29块钱报名|作业帮直播课|语文阅读写作提分班|免费送的|对了 正好你也报名了|今天你王叔在这呢|我看你怎么狡辩|这么大的礼盒29块钱确实买不着|你听见没 你王叔都说买不来|你别着急 我没说完呢|这么大礼盒不用买是送的|啊可是我看了呀|这里面有优秀作文集|古诗词字帖本等|大大小小十几本呢|这个啊是作业帮直播课最新推出的|清华北大毕业老师带队教学|20课时才29块钱|集中提升孩子的阅读和写作能力|我家熊孩子不就语文不好吗|作文总写流水账|这不我也报名了|家长朋友们|如果你家的孩子也有|写作文流水账|阅读理解没思路等问题|别再犹豫了|赶紧点击视频报名吧|成长金 基础知识手册 精选古诗40首|作业帮累计用户超8亿 清北毕业名师带队授课 班主任老师1对1答疑|字帖本 优秀作文集 的我们|6中联城|重更A|蓝更A效|29元20节直播课抢|展示礼包为小学礼包 【上课内容与收到礼盒请以实际为准】|双师系统班体系围 I|《中国姓|中姓|中国女|致直播课|好习限|免费 赠送\", \"video_asr\": \"你别生气，这个礼包真的是免费赠送！|行了吗？怎么回事啊。|气死我了这孩子。|他语文没学好，现在要学会撒谎。|这么大的礼盒，它告诉我们，花了二十九块钱报名作业，帮直播课语文阅读写作提分班免费送的。|对了，正好你也报名了。|今天你王叔在这，我看你怎么狡辩，这么大的礼盒呀，二十九块钱确实买不着，你听见没？你王叔都说买不来哎，你别着急啊，我没说完呢，这么大礼盒啊，不用买。|可是我看了呀，这里面有优秀作文集，不是磁铁本身。|小小十几本呢，这个呀，作业帮直播课最新推出的语文阅读写作提分班，清华北大毕业老师带队教学，二十课时，三十九块钱，集中提升孩子的阅读和写作。|我家熊孩子有一个不好。|做文章写流水账，这我也报名了，家长朋友们，如果你家的孩子也有写作文流水账，阅读理解没思路等问题，别再犹豫了，赶紧点击视频报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01635456085205078 sec\n",
      "{'result': [{'labels': ['现代', '多人情景剧', '中景', '推广页', '动态', '室外', '喜悦', '静态', '特写', '全景', '惊奇', '亲子', '朋友&同事(平级)', '平静', '极端特写', '路人', '悲伤', '家庭伦理', '拉近', '愤怒'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.93', '0.90', '0.82', '0.80', '0.75', '0.35', '0.33', '0.33', '0.21', '0.13', '0.04', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13c1177c8dec3b7db1ae2f23c42876cb.mp4\n",
      "{\"video_ocr\": \"卖菜啦 新鲜蔬菜|喂你这菜多少钱|两块|什么破菜这么贵|给你5毛得了|靓仔|你买多一点|我就便宜给你咯|老板|这些菜我全要|按两块的算|不好意思|我全都要了|有钱了不起啊|什么破菜|大兄弟|我不能全都卖给你|因为你用不这么多|用得了|我家就是开饭店的|大姐|你算一下这里多少钱|大姐你手机借我下|我看你也挺不容易的|你可以给自己报一个|快财商学院|小白理财课程|这样每天就有|额外的收入了|理财|我卖菜哪有这么多钱|怎么理财呀|只要你有一两百|就可以理财啦|而且这个快财商学院|只教理财方法|不卖理财产品|有专业的老师帮你|快速建立理购|知识体系|6天教会你|富人思维方法|学会用钱滚雪球|不停赚钱|再也不用这松麻烦啦|那报名多少钱|0元就可以报名啦|点击视频下方链接|查看详情吧|18个财富增值实用技巧|请输入验证码 获取验证码|本人已阅读并同意《用户使用协议》|踩坑老手 家庭主妇|复利法则：小白如何选指数基金?|老师介绍|力岫老师|课程特色|理财名师直播授课 手把手带你轻松学理财|请输入手机号|0元抢购|月光一族|理财的秘密:甩开同龄人，适合普通人 的投资工具?|无保障不投资:投资如何进行风险管理？ 2020年投资分析:家庭应该如何配置财产?|实用的快速入门课程|班主任督学|前200名 快财商学院院长:九坤|适合人群|理财小白|穷与富的秘密:如何打造富人思维?|荐股骗局大揭秘:适合小白的“低风险”投 资策略|基金、股票、低风险投资理财干货|社群学习 社群学习，方便学习随时回看|6天理财名师爆款直播课 22种投资必备避坑指南|系统的学习，掌握理财|工资不够花，想通过 学习理财，增加额外收入|想提升家庭财力|30000+家庭的财商教练 《九坤理财经栏目主持人》|专业的理财学习平台|视频为演绎情节|30000+家庭视粞为演绎情节|中国TOP5金融机构明星教师|1|基础为0，希望通过|作业实时批改反馈，不让问题过夜|工资少?没存款?|基础 系统白 想通过|盲目投资 没有投资打扌|没有投资技巧的人|的全职妈妈|多篇金融学学术论文发表者 独创”大白话讲金融”课程体系|东方证券消费行业年度金牌分析师 国内多家知名私募基金LP|2017“百大新兴企业” CEO&集团副总裁|年度金牌分析师 特许注册金融分析师CFA二级|告别死工资，小白理财如何少走弯路?|86|栋老师|脾分析师|2020年投资分析：家庭应谡视袍鎏绦情节|训练营|LP|教师|表者 程体系|A二级|直播\", \"video_asr\": \"卖菜了，蔬菜都是多少钱了？凉快什么破菜这么贵给你们的了，靓仔你买多一点我就便宜给你咯。|老板，这些菜我全都要了，按两块算，不好意思，我全都要了，有钱了不起啊。|什么破菜。|大兄弟啊，我不能全都卖给你，因为你用不了这么多用的了，我家里就是开饭店的，大姐算一下这里多少钱，哎，大姐你手机借我一下。|大姐，我看你也挺不容易的，你可以给自己报一个快财商学院小白理财课程，这样每天就有额外的收入了。理财我卖菜哪有这么多钱怎么理财呀？|大姐，只要你有一两百就可以理财了，而且这个快财商学院只教理财方法，不卖理财产品，有专业的老师帮你快速建立理财知识体系。|天教会你富人思维方法，学会用钱滚雪球，不停赚钱，在也不用这么麻烦了，哪报名多少钱了，零元就可以报名了。|点击视频下方链接查看详情吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01675271987915039 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '多人情景剧', '静态', '平静', '手机电脑录屏', '室外', '悲伤', '全景', '喜悦', '惊奇', '动态', '单人口播', '配音', '路人', '极端特写', '朋友&同事(平级)', '拉近', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.95', '0.89', '0.71', '0.66', '0.62', '0.57', '0.52', '0.43', '0.19', '0.11', '0.04', '0.02', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13c857c0ba29b121f356af3097d681d7.mp4\n",
      "{\"video_ocr\": \"磊磊我先回去了|什么时候回去呢|马上了|磊磊|晚上想吃什么|爸爸给你做|嗯|我回来了|居然不理我|傻小子|看看这是什么|妈妈你怎么会有|我和爸爸知道你发朋友圈|说差卡片|所以我们都去下载|快手极速版|签到分享看视频|抽卡片帮你集齐卡片|瓜分一亿现金|去5下载快手极速版|去赚打开首页侧栏|打开首页侧档|进入活动页面|开奖 幸福永远|来便争乐裂|争乐套|57610 93.24|集 卡集苏一亿|看视频赚零花|金币收益> 现金收益(元) 金币每日凌晨自动兑换成零钱|1月23日20:00开奖|21B20:0(奖|集卡分1(现金|查找|新用户7天内填写，领最高56元红包 填写邀请码|一看见勾种|扫一扫|每日任务|登录抽卡|详情见活动规则|详情见活动规则，奖励金额视任务完成情况而定|青少年|@潮男小|奖励金额视任务完成情况而定|动态 消息 私信|2020狂欢开启|30|集齐“快、手、极、速版”卡，分3000万 1688369|8369 集全|集齐所有卡牌，再分7000万|集齐 |忡手|等待|领现金|限时红包|万能卡 42|月亿现金|未开启|设置|关注2个作者 限时 领奖励|活动规则|>已经有|500金币/次，已完成2/2|春晚分101|人集全|点赞2个视频|青少年模式|去填写|99+|礼亿|不会迟到喔|66金币/|战化女|元吹|去点赞|限时\", \"video_asr\": \"累了，我先回去了，什么时候回去马上啊，再见。|晚上想吃什么，爸爸给你做。|我回来了，磊磊回来了，回家都会迷路。|不打。|ZZZZ ZZZZ。|嗯。|傻小子，看看这是什么？妈妈你怎么会有？我和爸爸就给你发朋友圈，说他卡片，所以我们都去下载快手极速版签到分享，看视频中，卡片帮你集齐卡片瓜分一亿现金，下载快手极速版，打开首页侧栏，进入活动页面，持续卡片瓜分一亿，手机幸福永远不会迟到哦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016744375228881836 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '填充', '多人情景剧', '静态', '配音', '平静', '动态', '手机电脑录屏', '场景-其他', '特写', '惊奇', '极端特写', '全景', '夫妻&恋人&相亲', '喜悦', '亲子', '室内', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.95', '0.79', '0.71', '0.51', '0.39', '0.34', '0.29', '0.25', '0.17', '0.14', '0.07', '0.05', '0.03', '0.02']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13d0fb246909b3aeb677cca426785695.mp4\n",
      "{\"video_ocr\": \"苏兮兮|女|无房无车|普通职员|不过|看上去|挺好生养的嘛|结婚哪里还需要相亲|跟我走|歙歙歙|才认识五个小时|我就和这家伙结婚了|老婆走我们回家|你干嘛|都结婚了|你走开|我一定是疯了|居然和认识不到|半天的家伙|呦这不是我的好姐妹|过的这么落魄|那我祝你们两个渣渣|天长地久|苏兮兮是我的女人|怎么能在外面受欺负|少爷|骨髓|与安小姐完全|匹配|本故事纯属虚构|纯属虚构|中麻星构|男主|苏兮兮前男|本龙费/超|本故事 再成城|《闪婚厚爱: 给给力》|《: 老公超给力》|闪婚厚爱：老公超给力》|闪婚|超给力|《闪婚厚爱:老公超给力风|婚厚爱:老公约学伽|相亲现场|婚介所|沈御风家中|8散心|相亲现场被霸道总裁劫走|点击下载红果小说免费版|兮相封象|乔安|全本小说免费看|我竟与他...|TEAHGDD TEA BAR|相亲者|御风|李豪\", \"video_asr\": \"苏兮兮，女，无房无车，普通职员，不过看上去挺好生养的吗？|结婚哪还需要更新跟我走。|才认识五个小时不就和这家伙结婚了，我走，我们回家，哈哈，我一定是疯了，居然会认识不到半天的家伙结婚了哟，这不是我的好姐妹都信息吗？谢谢。|过的这么落魄。|那我就祝你们两个渣渣天长地久。|哈哈哈哈哈哈。|苏兮兮是我的女人。|怎么能在我的手机。|少爷苏兮兮的骨髓有安小姐完全匹配，下载疯播小说你们搜索小怪兽，唉，老公超给力，从第一章看到最后一章，不花一分钱。\"}\n",
      "multi-modal tagging model forward cost time: 0.01613593101501465 sec\n",
      "{'result': [{'labels': ['填充', '推广页', '中景', '静态', '多人情景剧', '特写', '动态', '现代', '愤怒', '喜悦', '室外', '悲伤', '惊奇', '平静', '古代', '夫妻&恋人&相亲', '极端特写', '拉近', '转场', '亲戚(亲情)'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.94', '0.85', '0.68', '0.51', '0.30', '0.26', '0.19', '0.13', '0.13', '0.11', '0.02', '0.02', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13de139982623b11ab7d374a4a90d8d8.mp4\n",
      "{\"video_ocr\": \"我长那么大头一回见|发流量卡|一天发好几箱的|那是因为我们的 电信星卡呀|每月205超大流量|5G网速|首月0月租|免费包邮送到家|真的假的|这么优惠|还没结束|还送100分钟全国通话|激活就送20元话费|后续每月仅需19元|比市面上那些|流量业话费高的手机卡|实惠太罗了|那我之前得花了多少 冤枉钱|充话费买流量啊|你这卡怎么买啊|给我来一张|不用买|现在 点击视频下链接|填写地址|超划算的电信星卡|免费领取 包邮到家|100分钟通话|20元话费|已选|多款热门APP定向免流量|中国电信|19元/月|不限速 免费领取|>S6|包邮到家|包邮|BAOYOU|月租19元/月|000|OOO000|诛|205G超大流|CHINA TELECOM|还没|205GB|Ph\", \"video_asr\": \"哎呀，你咋那么所有电话流量卡一天发微信了，那是因为我们的电信新卡呀，每月二百零五G超大流量，五G网速，首月零月租，免费包邮送到家，真的假的宋慧还没结束还送一百分钟全国通话，激活就送二十元话费。|后续每月仅需十九元，比市面上的一些流量少，话费高的手机卡实惠太多了，哎呀，逗，我之前花了多少冤枉钱充话费买流量啊回去。|怎么买啊？这么来讲不用买，现在点击视频下方链接填写地址，超划算的电信新卡是免费包邮送到家！\"}\n",
      "multi-modal tagging model forward cost time: 0.016440868377685547 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '多人情景剧', '填充', '特写', '喜悦', '平静', '亲子', '惊奇', '单人口播', '极端特写', '家庭伦理', '场景-其他', '动态', '手机电脑录屏', '愤怒', '配音', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.91', '0.68', '0.41', '0.39', '0.33', '0.19', '0.08', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13dfe28b72c79f81bce4d86c8051ed96.mp4\n",
      "{\"video_ocr\": \"这是我幼儿园画|远是我小学的画|我现在的画|喜欢绘画吗|现在只要8.9|就能体验|州教育原画品课|☆触带你开启游戏原画世界|挖掘你绘画的潜力|专业老师手把手教学|线上直播教学|在家在舍就能学起|课堂视频回放|分配专属助教课后|跟踪辅导答疑|全程陪伴你学习|不怕你学不会|总有适合你的|还在等什么|赶紧点击下方链接|了解详情吧|潭州教育|TANZHOUEDU.COM|原画|原画 插画 漫画 Q版\", \"video_asr\": \"这是我幼儿园的画，这是我小学的话，这是我初中的话，我现在的话。|喜欢绘画吗？现在只要八块九就能体验团购教育，原画精品课带出，带您开启游戏原画世界，挖掘你绘画的潜力，专业老师手把手教学，线上直播教学，在家在宿舍就能学习，课堂视频回放，分配专属助教，课后跟踪辅导答疑，全程陪伴你学习，不怕你学不会有原画插画，漫画Q版总有适合你的，还在等什么，赶紧点击下方链接了解详情吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01623678207397461 sec\n",
      "{'result': [{'labels': ['现代', '静态', '中景', '推广页', '单人口播', '平静', '手机电脑录屏', '室内', '配音', '极端特写', '喜悦', '场景-其他', '多人情景剧', '家', '家庭伦理', '情景演绎', '办公室', '室外', '咖啡厅', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13e429308e79e3deff9145f924ef4ef6.mp4\n",
      "{\"video_ocr\": \"斑马A课英语体验课|猿辅导在线教育出品|课程采用儿歌>动画 游戏的方式|智能A互动教学|把复杂难懂的知识|转化成孩子感兴趣|一听就会一学就懂的内容|孩子爱学家长省心|49元10节课|再送30节英语 、数学、语文|再送30节英语、数学、语文|国庆A互动课|三科一起学 进步更明显|还免费赠送教辅大礼盒|名额有限|赶紧点击视频报名吧|猿辅导 线教肓 出品|教育出品|任线教育品|遠辅导在线|little a|orange|pizza|a|Having|OHOM|2-8岁上斑马 学思维 学英语|HIDDEN WORD|INSTRUCTIONS|kiwi|38N10Id|RIM|加四!.G|斑马Al课|语文|doys 1ed ay1|老虎|tig_|熊猫|pan a|狮子|lion|狗熊|be_r|$2-体租健|Having a Pic|SXOO8|1S10de|a6up.o|¥S|dOHs Bd\", \"video_asr\": \"老虎老虎TIGER TIGER TIGER TIGER，熊猫熊猫PANDA。|PAPAPA狮子狮子LINE LINE LINE LINE。|狗熊狗熊呗。|BBBB斑马AI课，英语体验课，猿辅导在线教育出品课程采用儿歌，动画，游戏的方式，智能AI互动教学，把复杂难懂的知识转化成孩子感兴趣，一听就会，一学就懂的内容，还在学，家长省心，四十九元十节课再送三十节英语数学，语文国庆AI互动课。|三科一起学，进步更明显，还免费赠送教辅大礼盒，名额有限，赶紧点击视频报名吧！|在。\"}\n",
      "multi-modal tagging model forward cost time: 0.01943230628967285 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '配音', '场景-其他', '中景', '动画', '情景演绎', '影棚幕布', '静态', '室内', '宫格', '平静', '教辅材料', '极端特写', '喜悦', '填充', '特写', '手机电脑录屏', '课件展示', '知识讲解'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.79', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/13ffbfd00eba60596ce1033d220d8a4a.mp4\n",
      "{\"video_ocr\": \"所有的学生、家长 老师都知道|想要做题又快又准|方法很重要|考试不会考原题|那如何才能让孩子 学会好方法|还会举一反三呢|现在高途课堂推出 全科名师班|北大清华毕业老师 带队授课|让孩子掌握 全科特惠|503个必考知识点|176个常考易错题|语文130个高频考点|物理10大难点失分点|秒杀 英语6大作文套路模板|你吴英语6天謇路模板|课后有老师1对1辅导答疑|3年内支持无限次回放|只要9元|16节课|就能见证孩子成绩的变化|还在等什么|赶紧点击视频下方 查看详情|给孩子报名课程吧|挑战极限|垂足为A，与另一条渐近线交于B，|漆倒多病的干古诗圣啊，我想提住你的手，让你在落木萧莱凭轩悲秋之际， 再泪酒长檬:牵黄撃苍的豪放派鼻祖啊，我想握住你的手，让你幽梦还乡相|九九归-，一个口诀|what happenedtohim.|清北毕业师资 平均教龄11年-|现在完成时一一影响性|中小学生在线教育平台 (极限挑战》第六季官方推荐|若A，B在F同侧:|酒醒杨柳岸晓风残月之下，千种风情有人诉说:瘦比黄花的女调人啊，我想 [住你的手，让你在乍暖还寒时候，不再独自东篱把酒。|顶渐距:1PQ|= 四过焦点F向一条渐近线作重线，|电场偏转|中考难点)|我想握住你的手，让孤独的人，不再悲伤:我想握住你的手，让漂泊的人，|姐108技，为你助力!|： 与双曲线交于点M，与另一条渐近|e者|高途课堂|极限令挑战|即刻出发 逆袭学霸|名师在线|2020全科名师班|2.80th hisparents look sad becausethey\", \"video_asr\": \"所有的学生，家长，老师都知道，想要做题又快又准，方法很重要，考试不会考原题那。|何才能让孩子学会好方法，还会举一反三呢？现在，高途课堂推出全科名师班，北大清华毕业老师带队授课，让孩子掌握五百零三个。|易考知识点一百七十六个，常考易错题，语文一百三十个高频考点，物理十大，难点失分点，英语六大。|套路模板课后有老师一对一辅导答疑，三年内支持无限次回放，只要九元，十六节课就能见证孩子成绩的话还在等什么？|点击视频下方查看详情，给孩子报名课程吧！|ZZZZ ZZZZ ZZZZ。\"}\n",
      "multi-modal tagging model forward cost time: 0.018417835235595703 sec\n",
      "{'result': [{'labels': ['现代', '填充', '推广页', '中景', '单人口播', '平静', '场景-其他', '静态', '配音', '教师(教授)', '室内', '幻灯片轮播', '动态', '影棚幕布', '特写', '教辅材料', '过渡页', '拉近', '混剪', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.93', '0.45', '0.42', '0.30', '0.03', '0.03', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/14009f9b5daa0827c1f759fd9e8012e7.mp4\n",
      "{\"video_ocr\": \"姗姗，怎么还在看动画片呢|作业写完了吗|我在上直播课呢|老师把动画和课堂 结合在一起|动画式教学 不是动画片|我可爱上这个直播课了|就是你爸爸给你报的那个|北大清华毕业名师 带队教学的|高途课堂 小学数学名师班吗|这课真不错|不用我监督你学习了|对呀，老师教了我|VDrt|大数巧算、图形技术 逻辑推理等等|解题方法|而且课后还有专业辅导老师|1对1辅导答疑呢|It|解决了我课上听不n|课后不会做的问题y|真的呀|你爸爸在哪 给你报名的这么好的课呀|现在点击视频 下方链接就可以报名啦|9元就能抢课啦|才rtO|Dtryo|￥9|Tha|D|新用户专享 立即体验 浙江卫视指定在线教育品牌|浙红卫视|名师特训班|全国百佳教师带队教学 平均教龄11年|zUz|Oz08|gh|yDraph|0zO|Dra|Dr唱go|一¥|华少\", \"video_asr\": \"珊珊怎么还在看动画片呢？作业写完了吗？作业写完了呀，我在看直播课呢，老师把动画和课堂结合在一起，可华师教学不是动画片，我可爱上这个直播课了，就是你爸爸给你报的那个北大清华毕业名师带队教学的。|高途课堂小学数学名师班吗？这课真不错，不用我监督你学习了，对呀，老师教了我大叔巧算，图形技术，逻辑推理等等解题方法，而且课后还有专业辅导老师一对一辅导答疑呢，解决了我课上听不懂，课后不会做的问题。|真的呀，你爸爸在哪给你报名的这么好的课呀！现在点击视频下方链接就可以报名了，九元就能抢课了。|我。\"}\n",
      "multi-modal tagging model forward cost time: 0.015984296798706055 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '静态', '多人情景剧', '家', '平静', '亲子', '家庭伦理', '喜悦', '单人口播', '手机电脑录屏', '特写', '惊奇', '极端特写', '教辅材料', '配音', '学校', '悲伤', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.97', '0.74', '0.22', '0.21', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1405496c344cb67ba4bb2a1f2f77210c.mp4\n",
      "{\"video_ocr\": \"上月伤投名快学院理财小白汕营|上月伤捉名快楦学黑财小ょ白训练营|上伤投名规槁羊院理财小ょ白训练营|月伪投岛快财樯学防王财小ょ白会|上月伤投名快摘羊，|上月伤投名快枫学防み|上月伤投岛快槁学院理财 ょ白短|上月伤投岛快槁学院王黟财ょ白汕结营|上日伤投幻快机院黑财小№|上日伤投名快|上日伤拐名枫槁学，|s日伤|上月伤扔名村|月伤投名快财揎学院黟财小白练|上月伤投岛快槁学院理T 佑营|上月伤投名机学孩黑财小ょ白测|上月伤拐名快槁学防黟财小白结|sE|学理财上快财|2元.现在0元竞费学|2.现在0元免费学|2.现石|2iz|2l1z|21. 现在0元免微士|2，现在07u|2，现在0元先级山|2元.珂|2．⑩在0|p200a|0元免费学|纯理财|100|6日台|舛理T|6份|飞0|り日行|700元|纯世|1087元|7月份|1伤|7R伤|[107 |7风货|7日价|1风份|(日|り筒|1F/份|上日伤投知揞羊院理财小妇绮营|上伤投名快槁羊院理财孙绮营|上伤投羊院理财小白练营|上伤投名快揞肖院理财ょ到练营|上投快楦学院理财 ょ白汕练营|存款|)aoZ|Ilao7|上日份|Ll00|上E伤|p2吃|Dpo0元|blo7u|bor7|blo|bh.|b0007|彦短|4日资s000元|4日工资5|4日7资 5000?|41贵5000|最后只剩下 (oo|最石兄制下 上0|最后又剩下|最后兄剩下 c|最后|最石只制下|t200元u|(207|p0a|日弟形|日弟开不|日常破|投资有风险 选择需谨慎|NOTEEOOK|风险责任由购买者自行承担|4资500元|la0|1日侈|7月你伪|80口|(oo|ko0|(-0|任用卡|考租|高俎|老|Is-00|快财|快财商学院|4T|K0D|h1|直播\", \"video_asr\": \"先给你们看一个北漂女孩的账本，四月份工资只有五千，房租两千二，信用卡要还一千五，日常开支要八百。|最后只剩下五百存款，只有零，一分钱都剩不下，还好五月份在朋友推荐下报名了快财商学院小白理财训练营，当月理财收入就有六千六百元，六月理财收入七千五百元，七月理财收入九千二百元，在八月还没有发工资的情况下就已经有了一万二的存款。|原价二百九十九元的快财商学院小白理财训练营，现在零元就可以免费学，赶快点击下方链接了解详情！|报名吧。\"}\n",
      "multi-modal tagging model forward cost time: 0.01616501808166504 sec\n",
      "{'result': [{'labels': ['现代', '填充', '配音', '推广页', '场景-其他', '重点圈画', '静态', '极端特写', '平静', '手写解题', '情景演绎', '幻灯片轮播', '转场', '中景', '特写', '教辅材料', '单人口播', '室外', '室内', '悲伤'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.89', '0.58', '0.15', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1412019f1e006de32bd750f7bf70825a.mp4\n",
      "{\"video_ocr\": \"你家孩子是不是 英语不敢开口|语感特别差|英语成绩跟不上|我给你推荐 伴鱼自然拼读课|现在面向全国|推出14节自然拼读课|让孩子掌握26个字母 及字母组合的发音规则|教孩子学英语 就像学拼音一样自然|力求让孩子做到见词能读|听音能写|而且课程 还支持无限次回放|现在报名 还包邮赠送学习成长地图|2698元的全套课程|现在仅需体验价 29元14节课|按要求完课之后 还可以领取奖学金|名额有限|快点击下方链接报名吧|butterfly|蝴蝶|of|ofter|afternoon|下午|department|部门|公寓|forgetful|健忘|用自然拼读法学习单词发音|AC\", \"video_asr\": \"BUTTERFLY BUTTERFLY AFTERNOON AFTERNOON TEA。|ARTEMIS DEPARTMENT APARTMENTS APARTMENTS FORGETS FO。|FORGET FOR你家孩子是不是英语不敢开口，语感特别差，英语成绩跟不上？我给你推荐伴鱼自然拼读课。伴鱼自然拼读课现在面向全国推出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样，自然，必须让孩子做到见词能读，听音能写。|而且课程还支持无限次回放，现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价二十九元，十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧！|那。\"}\n",
      "multi-modal tagging model forward cost time: 0.01662755012512207 sec\n",
      "{'result': [{'labels': ['填充', '现代', '推广页', '中景', '单人口播', '课件展示', '办公室', '动态', '配音', '场景-其他', '动画', '室内', '平静', '家', '室外', '静态', '喜悦', '手机电脑录屏', '商品展示', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.40', '0.18', '0.01', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1420af650d5e4a83185222140976bbe3.mp4\n",
      "{\"video_ocr\": \"我在看书|你呢|我们喜欢看的书|是一样的呢|我在吃晚餐呢|看样子很好吃呢|我要回家啦|已经在等地铁了|位置显示|我们只有50米的距离|不如见一面吧|我在陌陌上遇见了他|那个属于你的人呢|我在看书，你呢。|书，你呢。|我|我们喜欢看的书是一样的呢|位置显示我们只有50米的距离，不如 见一面吧？|八爪鱼|1分钟前 0.05km|通输入消息，|看样|位置显示我们只有50米的 距离，不如见一面吧。|好的。|关注对方，可接收对方动态更新|WOy|woyaohuijialad|请输入消息...|已经在den|已经在等diti|haod|我要回家了，已经在 等地铁了。|14:28|我咋了|海岛|第二年|地踢|好点|学习|发送|空格|选定 确认|在线0.05km|陌陌 很高兴|很高兴认识你|低于|123|拼音\", \"video_asr\": \"我在看书你呢。|我们喜欢看的书是一样的呢。|我在吃晚餐呢，看样子很好吃呢。|我要回家了，已经在等地铁了。|立即显示，我们只有五十米的距离了。|不是这个面板。|让不好的。|我在陌陌上遇见了吧，那个属于你的人呢，默默很高兴认识你。\"}\n",
      "multi-modal tagging model forward cost time: 0.016129732131958008 sec\n",
      "{'result': [{'labels': ['现代', '中景', '手机电脑录屏', '静态', '推广页', '动态', '单人口播', '喜悦', '特写', '极端特写', '平静', '多人情景剧', '惊奇', '配音', '场景-其他', '拉近', '情景演绎', '家', '愤怒', '夫妻&恋人&相亲'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.89', '0.89', '0.78', '0.68', '0.48', '0.09', '0.05', '0.03', '0.03', '0.03', '0.02', '0.01', '0.01']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/14228562b04aba01bbfb429686d14ac7.mp4\n",
      "{\"video_ocr\": \"210|胡涛|一说到高考|大多数人都会说|知识点细分|是高考出题的特点|相应的|这也要求高考学子|具备灵活分析|多维思考的学习技能|因此我推荐|高途课堂全科名师班|里面有北大清华 所以立该选哪个A|WOW|只看横线后的俩词|毕业名师团队授课|主讲老师平均教龄 okexciting news|形名what 反之how °都在1年以上|D“并针对于|形容词+名词所以排除掉A排除掉D|制定不同的授课方式|帮助考生抓住每一个知识点|仔仔细细的讲每一道重点题型|不仅可以快速提分|还可以让考生真正理解高考|现在16节|精讲直播互动课|只要9元|惊讶|学生可以和老师在线互动|专攻语数英物四科|课后还有辅导老师一对一答疑|学生高考拼的是时间|家长拼的是手速|赶紧点击屏幕下方链接报名|赶紧点击屏 不方链接报名|学K12在线素小学K12在线礼|[育机构 江卫视指仕缆的牌|学旗下中小学K12在线教育机构 指定仕线教育|A. How 3. Look!|How|0|足仕线教有|旗下中|指正仕教筒市牌|视指足仕统教肯的牌|浙江卫视|202|看选项，定考点，想口诀，来秒杀! B. What|9元16节课 全科名师班|Amy语法宝典:|问诀二:|kina paper roses in the what alan注意到!|浙江卫视指定在线教育品牌|N= No.2|高途课堂|课前|课后*园zhow!|推荐|华少|(只看横线后面两个词)|高途课堂 冰姐英语 满分育你|冰姐英语 满分有你|A. HowhappilyWhat alan注意到|2xc|C-Ur|C. What an D. How an|exciti h|exciting news it is!|全国百佳教师带队教学 平均教龄11年|学K12在线拳|工同 先高途|师出高徒 课造亨途|hat Nh|C. What a happily|立即体验|名师特训班|an|the qirls are mcking papgr rases|news|excit he, news|A. How happily|the girls are making paper roses in the clulWoW|B. What happily|the girls are making paper roses in the club!|in the club!|新用户专享|2. ---All|2元cr tと|2.---All of my classmates have passed the English test.|2. --All of my cla mates|of my cl mates|B. Y|B.y|happily the baby is playing in the garden!|ABC|徒途|仅需|￥9\", \"video_asr\": \"一说到高考，大多数人都会说，知识点细分是高考出题的特点，相应的这也要求高考学子具备灵活分析，多维思考的学习技能。因此啊，我推荐高途课堂全科名师班，里面有北大清华毕业名师团队授课。|蒋老师平均教龄都在十年以上，并且针对目前客户客户制定不同的授课方式，帮助考生抓住每一个知识点，仔仔细细的讲每一道重点题型，不仅可以快速提分，还可以让考生真正理解高考。现在十六节金奖直播互动课，只要九点，学生可以和老师在线互动。|专攻语数英物四课，课后还有辅导老师一对一答疑，学生高考拼的是时间，家长拼的是手速啊，赶紧点击屏幕下方链接报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.019245147705078125 sec\n",
      "{'result': [{'labels': ['现代', '中景', '推广页', '单人口播', '静态', '平静', '影棚幕布', '教师(教授)', '配音', '场景-其他', '手机电脑录屏', '学校', '全景', '知识讲解', '动态', '室内', '转场', '手写解题', '远景', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.93', '0.80', '0.77', '0.05', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/142d1b5f7ec4081c5f663558ffa074f6.mp4\n",
      "{\"video_ocr\": \"小熊蒹术|小熊襄|小能萋|小熊美术|美术宝出品 49元10节|包邮赠送绘画大礼包 在家学习|随材礼盒为课程配套物品 不同级别的礼盒略有差异|适合3-8岁 1对1辅导|儿童绘画启蒙课|包邮|立即报名\", \"video_asr\": \"天天都是八点。|就会一直指责我的心情美美的。|是不是三天。|我深深的。|仔细听听歌曲。|这就是的生活。\"}\n",
      "multi-modal tagging model forward cost time: 0.01616835594177246 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '绘画展示', '极端特写', '静态', '场景-其他', '才艺展示', '室内', '配音', '教辅材料', '情景演绎', '中景', '平静', '喜悦', '全景', '商品展示', '填充', '拉近', '动画', '转场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.98', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/142ef57814091f25e6e037d9d4bc3a00.mp4\n",
      "{\"video_ocr\": \"妈|对不起我考砸了|给你报的辅导班|咱没好好学吗|咱复读|咱再换个辅导班|小晨妈妈|你给小晨呀|报个高途课堂吧|我们家丽丽啊|跟着上了16节课|就掌握了130个高频考点|而且啊还有很多|技巧性的知识|这不|刚才说自己啊|高考发挥的不错呢|真有这么好的课程吗|我们的老师都是|北大清华毕业名师 带队教课|那价格肯定很高吧|不贵|价值499元的课程啊|现在只要9元|一共16节|精讲直播互动课|老师啊还会1对1|课后答疑呢|我现在就去报名|丽丽妈|你这在哪报名啊|现在点击视频下方链接|马上就能报名|高途课堂啦|浙江卫视|视频为演绎情节\", \"video_asr\": \"妈妈的。|我考砸了。|给你报的辅导班，咱们好好学吗？咱复读咱俩换了辅导班，小晨妈妈，你给小陈呀报个高途课堂吧，我们家丽丽呀，跟着上了十六节课，就掌握了一百三十个高频考点，而且还有很多技巧性的知识，这不刚才说自己高考发挥的不错呢，真有这么好的课程吗？当然，我们的老师都是北大清华毕业名师带队教课。|价格肯定很高吧，不贵，价值四百九十九元的课程啊，现在只要九元，一共十六节精讲直播互动课，老师啊，还会一对一课后答疑呢，我现在就去报名的的吗？这在哪报名啊？现在点击视频下方链接，马上就能报名高途课堂啦！\"}\n",
      "multi-modal tagging model forward cost time: 0.016556501388549805 sec\n",
      "{'result': [{'labels': ['现代', '填充', '中景', '多人情景剧', '推广页', '静态', '亲子', '家庭伦理', '全景', '悲伤', '室外', '学校', '平静', '惊奇', '特写', '喜悦', '朋友&同事(平级)', '愤怒', '教师(教授)', '拉近'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.73', '0.51', '0.45', '0.22', '0.20', '0.10', '0.07', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/143107ed6eb7b0077fb6baddf57eb4eb.mp4\n",
      "{\"video_ocr\": \"喂 你别加班了|家里电路又坏了|这大半夜的我找谁修啊|什么58|你回来吧|怎么那么快就回来了|您好|我是58同城的|维修工人|李先生说您家停电了|我是来维修的|不可能|谁家公司|大半夜开门啊|妈 这是我叫的工人|快让别人进家看看|不会乱收费吧|这是大平台|58同城上的维修工人|而且24小时上门服务|服务保障和平台认证|价格透明|已经维修好了|家里有任何需要维修的|都可以在58同城APP上|呼叫一键服务|还真是这么方便|快教教我|怎么联系他们|别再费心|家里的日常维修|打开 58同城|58日|即可在线呼叫维修人员|老人也可便捷使用|成都|#黄河出现期2号洪水|消毒杀蘭:临时小时工 接送孩子！陪护老人做饭阿姨 钟点工|保姆/月姆/陪护|保洁清洗|租房 兼职|开锁换锁|防水打孔 电路检修 灯具安装 已服务5.4万用户|登录58同城享受更多精彩服务 立即登录|立|3同城享受|部落 面试单位问你上一份工作离职的原因，该怎么...|D到豕精远 优质服务|到豕游选|插座维修持证修电电路跳闸维修改造开 关维修|临时小时工1接送孩子陪护老人：做饭阿姨|消毒杀菌|淘金节|超职季|三家比价 专业维修 极速上门|到家 精选|万能求助圈|家政管家|洗衣修鞋|放心搬家|生活配送|开关维修电路维修 红星路附近（水电灯|免费领CBA和中超球票 签到|附近职位 晒工资 打工生活|低价好房出|二手回收|养老院|#三十岁左右的你，目标工资…..|接送孩子！陪护老人做饭阿姨｜育|管道疏通 水电改造|全职招聘|二手房|家庭保洁|家政|上门维修水电-全屋改造家庭维修49元起|超多名企火热招聘中|点这里即可拨打电话|都是仓储人|仓储管理员|已服务39.1万|插座维修 短路跳闸 电表维修 5元/个|推荐 部落 视频 热议 家乡|部落|平台监管|92条评价|家电维修|红星路-邻里邻家维修 具、厨卫洁具、浴霸）安装维修49元起|少女路边摆摊，年入上百万|100|超级名企 人才主场|开关组修 （电路维修 电表组修|标准定价规范服务 专业高效细心服务 单单保险24小时维权|新南门-天府新区成都片区华阳东娃子家.|EXIT|58旗下房屋维修|餐饮一家人 聊美食聊开|已服务29万|管道/马桶疏通|插座维修电表维修)电路维修|路上人生 篮球俱乐部|玩游|领30元|短路跳闸专业水电维修安装及改造，解决|厨房家电维 新南门|修电|Q 超职季|插座维修 电表维修电路维修 红星路附近|商铺写字楼 更多|58同城 网…|同城|58同城四Fjuar|视频 员5K可暑|鲜花绿植|游戏有奖|电路维修 电脑维修 手机维修|新车|司机之家|减45|热议 家乡|热议|一切水电问题|投篮！进了|投篮!|一手车|5折特惠|2:241 4G|搜一搜|w冈exr|众鑫水电|D1|官方保障|银庆|订单|50|1杀|东兴 持证\", \"video_asr\": \"喂，你别加班了，家里电路又坏了，这大半夜的我找谁修啊？什么五百回来。|啊。|怎么那么快就回来了。|你好，我是五八同城的维修工人，你现在说您家停电了，我是来维修的，我可能回家可不是大半夜开门啊啊，这是我叫的工人，别人家看看苏菲吧，是大平台，五八同城上了维修工人，而且二十四小时上门。|服务保障和平台认证，价格透明，您好，已经维修好了，家里任何需要维修的东西，在五八同城APP上呼叫，一键服务。|快教教我怎么联系他们，别在最新家里的日常维修，打开五八同城即可在线呼叫维修人员，老人也可便捷使用。\"}\n",
      "multi-modal tagging model forward cost time: 0.01716160774230957 sec\n",
      "{'result': [{'labels': ['现代', '手机电脑录屏', '中景', '推广页', '静态', '多人情景剧', '配音', '场景-其他', '平静', '喜悦', '动态', '单人口播', '室内', '特写', '极端特写', '朋友&同事(平级)', '家', '悲伤', '愤怒', '办公室'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.98', '0.95', '0.81', '0.78', '0.52', '0.19', '0.18', '0.14', '0.05', '0.02', '0.01', '0.01', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/143efe97f9ff46178cc8e145cb5ababf.mp4\n",
      "{\"video_ocr\": \"大家不要被鸡汤所迷惑|尤其是数学的学习|18汤|ル汤 有明确的模型和方法|找不到学习的方法|也没有改进的方向|随着数学题目难度越来越大|出题方式多样化|很多学生可能以前|记住一套公式|就能做很多题目|取得较好的分数|但是|现在即使你背了很多公式|刷了很多题目|在考试当中|也未必能取得|理想的成绩|因为高中数学是一门|特别需要掌握解题方法|和技巧的科目|我们平均十年教龄的老师|为大家精心准备了一套|解题方法|和答题技巧的课程|跟谁学 美国级交所上市么司|良人：谢谢老师|用户405089228:讲过，但考得比这还难|要进群的同学|用户654821768：谢谢老师|学数学靠的是模型+方法 不是你以为的努力+坚持|清北毕业名师带队教学 平均教龄11年以上|忍着吧!: 666|rainbow : 学过的真牛13|用户669827098:没学模型|小阳光:没有|用户350913738：没有|三目录|?|老师教的好，孩子才能考的好|沈辞没|Abby: 666|用户927576648:OK|用户612880088:没|用户738178118:没|在线学习更高效|美国细交所上市公司|跟谁学｜在线学习更高效|高一讲座第一节新-3|高中全科能力提升班|辅导 张浩(跟谁学-K12高中部):私信张浩老师哈|跟谁学首席数学讲师|在线学习更高效|指导价1299元|新同学专享|展神数学|立即报名|殷方展|女展|9元|第2\", \"video_asr\": \"大家不要被这鸡汤说明吧！|就是我们这个学习，尤其数学的学习。|他是明智的模型和方法的，找不到学习的方法，也没有改进的方向。随着数学题目难度越来越大，出题方式多样化，很多学生可能以前记住一套公式就能做很多题目，取得较好的分数，但是现在即使你背了很多公式，刷了很多的题目，在考试当中也未必能取得理想的成绩。|因为数学是一门特别需要掌握解题方法和技巧的科目。|所以我们平均十年以上教龄的老师为大家精心准备了一套解题方法和答题技巧的课程。\"}\n",
      "multi-modal tagging model forward cost time: 0.01695537567138672 sec\n",
      "{'result': [{'labels': ['推广页', '现代', '中景', '教师(教授)', '单人口播', '静态', '配音', '场景-其他', '平静', '特写', '知识讲解', '学校', '影棚幕布', '室内', '课件展示', '手写解题', '转场', '填充', '情景演绎', '喜悦'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.92', '0.91', '0.38', '0.22', '0.04', '0.03', '0.03', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/144fd3a667da60c241228f181cd200b1.mp4\n",
      "{\"video_ocr\": \"白三16 VAVESJI|OWN TOA>G|JOWN TO/|谢谢啊|你的计算题部分|永远拿不到分|回去再做两套速算题|多列列竖式|教孩子要用对方法|否则刷再多题也没有用|叔叔来帮你看看这道题|lHOSHA|LLOSV|107+8=|tA|聪明|115写前面|那7×8=|56写后面|那么这道题的答案是|ON H|你是从哪学到|这个方法的|你给孩子报个|猿辅导数学名师特训班|9课时才29元|多加1元还额外赠送|9课时的|语文阅读写作课|这两套学习礼盒|都是免费包邮赠送的|我天天在你们小区送|我给我们家孩子|也报了一个|我到了再见啊|哎小哥|这课怎么报名啊|现在点击屏幕下方|选择你家孩子对应的年级|就可以报名啦!|礼盒全国包邮|YUANFUDAO|UA际事UOAO|语文特训班|指辅导|108|11,556|高文将训格|27件原创教辅|MAN 4E|3HL|第2页(共|四数|小四数学|数学特训|计算:(共37分)|猿辅导在线教育|小学秋季|80|ANUO。|ANFu04|103×107=8 107-108=)/ 108X49|21+31=2lt 103×107=/8107×108=1 108×49=网19|117X25= 2721+31= 2州￥ 103×107=l 107x10-快1|加1元可换购9课时语文课|さZ之|三2之|115写前面|56|我9|稳箱导|9刀6|猿癔辅导|108X4一|口算:(5分)|117X25= 1|117×25＝|58X10一|58×10=750X|190|学数学5月+班通用体系挂图|600|视频为演绎情节|14:4|59%|aEPNG 202|ONIHLON J 1HDSVA|1HSり NIHLON|LONL SHA|118+921=l|惹输导|铺导 寺训班|在线教育|コz8]え|78×78= 价|全国|UDAO|语文指|包邮|福号|拓展 古诗文|5分钟|7Y|多8|少二\", \"video_asr\": \"你的计算题不会永远拿不倒峰，回去再做两周赚的多列竖式教孩子要用对方法，我都刷再多题也没有用数学来帮你看看这道题，一百零七加八等于一百一十五度啊，聪明一百一十五写前面那七乘以八等于。|十六五十六写后面，那么这道题的答案是一万一千五百五十六，谢谢你啊，你是从哪学到这个方法的？|你给孩子报个猿辅导数学名师特训班啊，九课时才二十九元，多加一元，还额外赠送九课时的语文阅读，写作课，这两套学习礼盒都是免费包邮赠送的，我天天在你小穴送，所以啊，我给我们家孩子也报一个，还不到了，再见啊哎，这课怎么报名呀？现在点击屏幕下方，选择你家孩子对应的年级就可以报名了。|嗯嗯嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016371965408325195 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '多人情景剧', '路人', '亲子', '静态', '特写', '室内', '平静', '单人口播', '愤怒', '全景', '喜悦', '极端特写', '惊奇', '手机电脑录屏', '悲伤', '教师(教授)', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.95', '0.93', '0.86', '0.85', '0.68', '0.45', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/145246bfbfb903d458bf847b30df1c2c.mp4\n",
      "{\"video_ocr\": \"city城市|你家孩子是不是|语感特别差|英语成绩跟不上|我给你推荐|伴鱼自然拼读课|现在面向全国|推出14节自然拼读课|让孩子掌握26个字母|教孩子学英语|力求让孩子做到见词能读|听音能写|而且课程|现在报名|2698元的全套课程|现在仅需体验价|按要求完课之后|名额有限|快点击下方链接报名吧|cycle自行车|@t小猫|中Je自行车|自行车|英语不敢开口|及字母组合的发音规则|就像学拼音一样自然|还支持无限次回放|还包邮赠送学习成长地图|29元14节课|还可以领取奖学金|[s]在ei、y前 cell细胞|i、y前|1在e、i、y前|孩子开心学 启蒙更有效|[k]|k] [s]在e、i、y前|cup杯子|小细胞|1胞|Ceu|celx|自然拼读记单词 比音标更高效|8|ap帽子|ap帖|at小猫|字母C|AB\", \"video_asr\": \"自然拼读记单词，今天学字母C，在单词的发音大多数情况下他撒。|但是在字母E I Y前面开发，比如说杯子，看帽子K，小猫CAT，细胞C，程序C。|你家孩子是不是英语不敢开口语感特别差，英语成绩跟不上？我给你推荐伴鱼自然拼读课。伴鱼自然拼读课现在面向全国推出十四节自然拼读课，让孩子掌握二十六个字母及字母组合的发音规则，教孩子学英语就像学拼音一样，自然必须让孩子做到见词能读，听音能。|而且课程还支持无限次回放，现在报名还包邮赠送学习成长地图两千六百九十八元的全套课程，现在只需体验价二十九元，十四节课，按要求完课之后还可以领取奖学金，名额有限，快点击下方链接报名吧WELL WITH PHONICS！\"}\n",
      "multi-modal tagging model forward cost time: 0.016397476196289062 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '填充', '中景', '场景-其他', '单人口播', '配音', '课件展示', '静态', '办公室', '手机电脑录屏', '室内', '动画', '平静', '家', '喜悦', '知识讲解', '动态', '宫格', '特写'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.96', '0.68', '0.65', '0.52', '0.26', '0.10', '0.06', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1453b1b44e50ddcf4ae6546bb50ea864.mp4\n",
      "{\"video_ocr\": \"你吃|哎呀|我不喜欢吃鸡腿|太油腻了|太咸了|多好的鸡腿啊|你说这孩子|怎么就吃了一回啊|多可惜啊|你怎么又乱花钱啊|赶紧拿了退了去|快去|妈|这是我在嗨喵喵|玩游戏领红包买的|没花自己一分钱|新用户下载就|送5.88元现金红包|每天签到|合成猫咪|都可以获得|额度不等红包|达50级猫咪获得|五福猫|集齐5个五福猫|就可以获得分红猫了|持有分红猫|持续14天|每天可获得|100到200元的现金奖励|轻轻松松|就能领几百块钱|随时提现到账|赶紧给我也下载一个|我也抽红包|试试手气|点击视频下方链接|就可以下载了|你也来领红包|抽手机|快来呀|集10个碎片|点击视频下方立即下载 赢取P30手机|送你一个新人红包 最高领100元，可提现|5.88元|妮蔻|解锁新喵喵，大红包奉上|1.06元|开心收下|恭喜获得分红猫|156.94元 提现记聚|0.3元 50元|商家订单号 Djv0wZDw7mzG+i01FTK080+Y60A4Rn|沙订|已存入我的钱包>|海河|79 46|50级猫合成可得五福猫，集齐五只，必得分红猫|每日产生收益，持续14天|我的进度|立即提现|10活力值|10活|其他|解锁新猫咪可得更多红包哦|满50元就能提现啦 继续努力|88aa|分红猫令日收慈168.15元|合成五福猫|好的|已解锁2.1%，解锁后必得分红猫|音效 开启|200元|添加|查看我的分红猫|量10万只，先到先得。分红猫可以通过两只50级的猫 持有分红猫即可每天获得100~200元的现金奖励，限|什么是分红猫|大转盘抽奖 去抽奖|当前版本:2.27|1.用户获得现金奖励后，请及时申请提现。每个用户每|合成获得，也可以通过个人努力持续玩游戏解锁，还|抽的越多，进度越快|天最多可申请提现1次(同一终端设备、同一手机号、|同—坦化必劫飞只扫为同一用白)|解锁新噬喵|今日每只分红168.15元|自动合成 收入翻倍 600|商店|7.10dd|可以通过五福猫合成获得。查看更多>|加快解锁进度|每天必得现金奖励 我的分红猫收益|个人中心|嗨喵喵|总收益 568.15元|i0:8569326820|收入翻倍|今日收益|156.94元|提现方式: 支付宝|创建时间|绿动|8UT|1.44aa|还差财就能获得分红猫啦|已产出1040/100000|申请金额:|BO2020042991805007410205|明日可领?2元|查看 用户协议和《险私政策》|X2|今日名额已满|订单号|具体金额以实际活动为准|送大奖!|03|152.03|领奖助|6:33|规则说明|规则|对此订单有疑问|查看|剩余时长|活力值|交易成功|游客|1UU|227dd 7.93aa/秒 签到|么0U|赢京东卡|25.0|+1|明日生效|提现金额:+156.94 2020-06-11 20:01|喜猫|抽奖提醒|账单分类|H日|备注|标签和备注|标签和|研究院|成就|钱包余额|提现|9动合成|任务|银奖励|等级1\", \"video_asr\": \"你吃。|哎呀，我不喜欢吃鸡腿，太油腻了，你吃吧，太闲了。|多好的鸡腿，你说这孩子怎么就吃了一口啊，多可惜啊，多休息让我妈看，哎呀，你怎么又乱花钱呀，赶紧拿着。|妈，这是我的嗨喵喵玩游戏里红包买的没花自己一分钱，新用户下载就送五点八八元现金红包，每天签到合成猫咪都可以获得额度不等红包。|打五十级猫咪获得五福猫，集齐五个五福猫就可以获得红包。|是有收红包持续十四天，每天可获得一百到两百元的薪金奖励，轻轻松松就能领几百块钱，随时提现到账，赶紧给我也下载一个，我也抽红包试试手气，别人期视频下方链接就可以下载了，你也来领红包抽手机，快来呀！|不。\"}\n",
      "multi-modal tagging model forward cost time: 0.016927242279052734 sec\n",
      "{'result': [{'labels': ['现代', '中景', '多人情景剧', '推广页', '静态', '家', '手机电脑录屏', '喜悦', '配音', '夫妻&恋人&相亲', '动态', '愤怒', '平静', '悲伤', '特写', '惊奇', '极端特写', '餐厅', '朋友&同事(平级)', '家庭伦理'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.97', '0.94', '0.69', '0.66', '0.64', '0.57', '0.57', '0.53', '0.52', '0.43', '0.29', '0.17', '0.09']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1455877cfe06600598b0778a37af067f.mp4\n",
      "{\"video_ocr\": \"我从小就喜欢小说|从金庸到古龙还有琼瑶|没有一个我不看的|现在科技发展快了|网络小说的传播速度|甚嚣尘上|我研究了很久大家非常喜欢的|只有疯读小说这一款|这个小说软件完全免费无乱码|其次书库也全|男生喜欢的玄幻武侠|女生喜欢的都市言情|霸道总裁上门女婿|还有重生战神之类的书|这里全都有|来疯读小说|你真的可以不花一分钱|就可以读完全本小说|不像其他小说软件一样|读到关键的地方就需要付费|如果你也需要的话|点击屏幕下方|进行下载|小说研究专家 安迪力荐|具体奖励以实际为准|3800|3760 3800|500|1500|疯读|安迪\", \"video_asr\": \"从小就喜欢小说，从金庸到武隆，还有琼瑶，没有一个我不看，现在科技发展快了，网络小说的传播速度甚嚣尘上，我研究了很久，大家非常喜欢的只有疯读小说这一款，这个小说软件完全免费，不乱码，其次呢，书簿也全男生。|喜欢的玄幻武侠女生，喜欢的都市言情，霸道总裁，上门女婿，还有重修战神之类的书。|全都有在疯读小说，你真的可以不花一分钱就可以读完全本小说，不像其它小说软件一样读到关键的的。|有必要付费，如果你也需要的话，可以点击屏幕下方进行下载。\"}\n",
      "multi-modal tagging model forward cost time: 0.016845703125 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '静态', '填充', '单人口播', '室内', '平静', '配音', '场景-其他', '极端特写', '教师(教授)', '特写', '喜悦', '影棚幕布', '动画', '知识讲解', '转场', '手机电脑录屏', '手写解题'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '0.97', '0.95', '0.24', '0.19', '0.07', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/145b63d91fe9ec37b7c63734c7b4c883.mp4\n",
      "{\"video_ocr\": \"我想问一下|我在京东金条上|额度有多少|登陆京东金融|点击金条|能在线查询了|最高可借|20万元呢|20万|我竟然有 最高额度20万|最快啊|1分钟就能放款了|那这利息高吗|日利率|最低O.019%|也就是说借1万元|曰息最低才l块9|而且现在新用户|借3万|还能免息30天|最长还可分|12期还款呢|急用钱的你|也赶快点击下方链接|测测属于你的额度吧|京东数科旗下|京东全融|京车金|京东|贷款额度、放款时间等以实际审批为准 贷款有风险，借款需谨慎，请根据个人能力合理贷款|3-46 金条借款|金条小贴士 你有30000金条额度待领|30000 借还款攻略|当前可借(元) 1000元值1天制息G室019元|下|200，000\", \"video_asr\": \"我想问一下，我在京东金条的额度有多少？现在您登录京东金融，点击金条就能在线查询啦，最高可借二十万元呢，二十万我今天有最高额度二十万，最快一分钟就能放款了，那这利息高吗？我们京东金条的日利率最低百分之零点零一九，也就是说借一万元日息最低才一块九，而且现在新用户借三万还能？|免息三十天，最长还可分十二期还款呢，精彩的你也赶快点击下方链接下载京东金融激活金条，测测属于你的额度吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01622295379638672 sec\n",
      "{'result': [{'labels': ['现代', '中景', '静态', '推广页', '多人情景剧', '平静', '单人口播', '喜悦', '动态', '家', '特写', '惊奇', '手机电脑录屏', '办公室', '家庭伦理', '全景', '悲伤', '工作职场', '夫妻&恋人&相亲', '室内'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.97', '0.90', '0.84', '0.75', '0.48', '0.43', '0.17', '0.07', '0.02', '0.01', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/14622cb6934edf6fc686082fe92fb862.mp4\n",
      "{\"video_ocr\": \"妈妈|我没骗你|我真的花了9块钱|李总|涵涵这是怎么了|这孩子英语没学好|还学会撒谎了|跟我说这是|花9块钱报的|作业帮直播课送的|这个大礼包|花9块钱确实买不来|现在这个礼包|不能买只能送|啊|可是我都看了呀|这里面大大小小十几本|有这个单词宝典|单词本|还有这个 经典易错100题|这是作业帮直播课|推出的英语 单词语法名师课|9块钱13节直播课|由国内外 毕业名师带队教学|总结了八大单词记忆法|高效解决单词记忆问题|掌握速记技巧|单词过目不忘|脱口而出|家长朋友们|如果你家孩子|也有单词记不牢|语法学不会|等英语学习问题|别再犹豫了|赶紧点击视频下方|给你家的孩子报名吧|单词宝典|单街不|名师有大招 解题更高效|报名就送超值大礼包 上课内容与收到礼盒请以实际为准|超典册铺100解|Ook|小学英语1-6年级|9元/13节课|成长笔记|1827|致直播课 的我|500m|12|ee\", \"video_asr\": \"妈妈我没骗你，妈妈我吃了他的小块钱。|妈妈妈妈。|哈哈哈。|李总啊，这是怎么了？这孩子英语没学好，还学会撒谎了，跟我说这是花九块钱报的作业，帮直播课送的，李总这个大礼包花九块钱确实买不来。|现在这个礼包不能买，只能送。|可是我都看了呀，这里面大大小小十几本，有这个单词宝典，单词本，还有这个经典易错题一百题，这是作业帮直播课推出的英语单词语法名师课。|九块钱十三节直播课，由国内外毕业名师带队教学，总结了八大单词记忆法，高效解决单词记忆问题，掌握速记技巧，单词啊，过目不忘，脱口而出。|家长朋友们，如果你家孩子也有单词记不牢，语法学不会等英语学习问题，别再犹豫了，赶紧点击视频下方给你家的孩子报名吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.016394615173339844 sec\n",
      "{'result': [{'labels': ['现代', '推广页', '中景', '汽车内', '多人情景剧', '静态', '亲子', '喜悦', '愤怒', '全景', '路人', '室外', '单人口播', '平静', '悲伤', '极端特写', '特写', '混剪', '夫妻&恋人&相亲', '工作职场'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.67', '0.23', '0.22', '0.13', '0.11', '0.10', '0.09', '0.06', '0.02', '0.01', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/14740304aad7994424d7e1d760348732.mp4\n",
      "{\"video_ocr\": \"来这个陌生的城市|半年了|真的好累|好想被人照顾|如果你喜欢我|就来连信找我吧|连信|一个专门和|附近的人聊天的软件|打开连信|每天都会收到很多|小哥哥小姐姐的信息|除此之外|你还可以|打开附近的人|最重要的是|兄看男生|者只看女生|比如遇到心仪的Ta|直接点开头像|就可以查看他的动态|12输几打招呼内容|か键添加|就可以开始聊天了|简单快捷|还在等什么|赶紧点击屏幕不方链接|下载注册吧|连接中... Q 搜索|搜索 下午3:16|好友圈|你收到34个打招呼消息>|你收到34个打|山中无老虎|金麸|苏紫紫呀|向TA说句话打个招呼|你已经添加了乐乐么|欢迎来到连信。如果您在使用过程中有任何的...|今天RNG的比赛你看了么? 乐乐|按号码查找 添加手机联系人|清除位置并退出|随缘 每天都更好|中国|做个朋友吧|和他们打招呼|附近有9个人对你感兴趣，快去和他们打招呼！ 12:32|34|单身的男人|跟你说个坏消 息，“什么坏消|查看全部|我的未来不是梦 希望遇到有缘人|黑塔|扫一扫|小姐姐你好啊|下午204 以上是打招呼的内容。|为了保护您的信息安全，请不要在聊天中透露您的 手机号、微信号、银行卡等重要信息|6吧 下午2.23|人无完人请多包|夏末凉|心想事成，事审 腰心，天天好|如果复情可以补 考，我不会再作|你不配做我的朋|WXYZ|连信小助手|连信团队|交朋友 我今天新学会了一首歌，等有时间录一个给你|附近打招呼的人|男|美美|个人相册|你这周瘦了没|捕不适这情|七秒钟的记忆|J、人一定要靠自己|打招呼|TUV PQRS|10|献是套欢交朋系|有品妹，有气质， 有修养，帮婚励|无聊的我来心口这是今下午去拍的|清空|重输|一个莫得感情的杀手|微帅|女入图|han|MNO|我好想泡在水里啊，我真是快要热死了|好大一棵树|开心快乐!|兴趣爱好 这个人很懒..没有什么爱好|下午5:39|认识就是缘分 我跟你说，我今天发现了一个特别好吃的饭店|开子心心过好每|宅男，|这几天工作有点忙，没法上线现 就是喜欢交朋友呀|好啊好啊|幸福人生|真诚等候|兴趣爱好 这个人很懒...没有什么爱好|在忙吗?|我回来了|敢告号干高搞|美女好|好啊|上传通讯录看看哪些好友在玩连信|好清爽的小姑娘!|设置备注和描沭|设监备金|魔魂原创一妆容真的可以改变|小可爱，你有没有想我，我都想你了|北京人在北京|最好看一张|JKL|连信(94)|详细资料|发送|下午2:52|都是这样|友!|投诉|确认|删除|你已经添加了乐乐(，现在可以开始聊天了|创新|一天|地区|在呀|尬嘎蛤尕咖|个性签名|在我回来了啦＿快来找我聊天吧|142:00|消息|123|大一程扬|一制多包|多包|质，|DEF|立即下载 开始聊天|gao|遇见不一样的 Ta|2公里以内|rL|〔]|可你产指时|227月|172日|187|17确|ABC|分词|GHI|无无\", \"video_asr\": \"AS。|来这个陌生的城市半年，真的好累，好想被人照顾，如果你喜欢我就来联信找我吧。|连信，一个专门和附近人聊天的软件，打开连信，每天都会收到很多小哥哥小姐姐的信息，除此之外，你还可以打开附近的人。|最重要的是，你还可以选择只看男生或者只看女生，比如遇到心仪的他，直接点开头像就可以查看他的动态，输入打招呼内容，一键添加就可以开始聊天了，简单快捷，还在等什么，赶紧点击屏幕下方链接，下载注册吧！|嗯。\"}\n",
      "multi-modal tagging model forward cost time: 0.016796588897705078 sec\n",
      "{'result': [{'labels': ['手机电脑录屏', '现代', '推广页', '配音', '场景-其他', '中景', '静态', '平静', '单人口播', '喜悦', '全景', '室外', '商品展示', '动态', '室内', '红包', '多人情景剧', '惊奇', '特写', '(马路边的)人行道'], 'scores': ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '0.99', '0.89', '0.28', '0.25', '0.03', '0.01', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/147ca4f782e6b5adfc80b75fd19a37ba.mp4\n",
      "{\"video_ocr\": \"另外 万本金做|另外＿万4|万|另外—万本金做|天时间，元免费学习 理财名师教你掌握|天时间，＿元免|间， 元费学习|特许注册金融分析师CFA二级|息，还剩万 这就是理财和不理财的区别|这就是理财和不理人|赶紧点击屏幕下方链接 为自己做一笔免费的投资吧|赶紧点击， 为自己做一笔|赶紧点击屏幕下方钱|起击屏莫方链接|直播教学，揭露理财的秘密!|那么年后， 你除了有一套子，还完银行利|那么 你除了有一套 子，|你除了有一套子，还完银行利|个财富增值实用技巧 种投资必备避坑指南|个财富增值实用|个财富墙 种投资必|种投|种备避坑指南|月薪3000也能理财!|贷款 万，贷款期限 年，|万，贷款期限|贷款 万，贷款|贷款7下 贷款期限|贷款70万，贷款期限30年，|快财商学院小白理财课|快财商学院小E|快财商学防|U用m|牧之老师|里财|理财血，|另外了0万年 理财|另外]0万本金做理财道，|那么30午后，|为自己做 兔费的+吧|资吧|为自己做一笔免费的投资吧|掌握“富人思维”开启“睡后收入”|万全款买|款买|O0|假如你有/0万全款买|而是通过科学的方法配置资产|天理财名师爆款直播课|理财名师教你掌握|了名师|东方证券消费行业年度金牌分析师|如果你拿出|万做首付|如果你拿出 万做首|如果|如果你拿出30 首付|推荐你报名|推荐你报名|假如你有/00万全款买|假如你有/00万全款买|我们来算个账|理财不是买基金买股票|摆脱月光 财富增值|套8子，|掌握 生 的方法|的方法|年后，你只有|掌握生|息，还剩|24万|冬必|理财有风险入市需谨慎|+86 请输入手机号|年度金牌分析师|ぅ 南s|5 南  |不怕听不懂|的话\", \"video_asr\": \"我们来算个帐，假如你有一百万全款买房的话，三十年后你只有一套房子，如果你拿出三十万做首付贷款，七十万贷款期限三十年，另外七十万本金做理财，那么三十年后你除了有一套房子，还完银行利息还剩一百二十四万。|这就是理财和不理财的区别，理财不是买基金，买股票，而是通过科学的方法配置资产，掌握钱生钱的方法。|见你报名快财商学院小白理财课，六天时间，零元免费学习，理财名师教你掌握十八个财富增值实用技巧。|二十二种投资必备避坑指南，赶紧点击屏幕下方链接，为自己做一笔免费的投资吧！\"}\n",
      "multi-modal tagging model forward cost time: 0.01613640785217285 sec\n",
      "{'result': [{'labels': ['填充', '现代', '场景-其他', '推广页', '平静', '手写解题', '极端特写', '静态', '配音', '手机电脑录屏', '重点圈画', '幻灯片轮播', '单人口播', '惊奇', '室内', '中景', '拉近', '特写', '知识讲解', '室外'], 'scores': ['1.00', '1.00', '1.00', '1.00', '0.99', '0.99', '0.98', '0.98', '0.93', '0.11', '0.02', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']}]}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "/home/tione/notebook/VideoStructuring/dataset/videos/test_5k_A/1483ee744ce5f9e8f09bcccdd14e14a8.mp4\n"
     ]
    }
   ],
   "source": [
    "# !sudo chmod a+x ./VideoStructuring/run.sh && ./VideoStructuring/run.sh test checkpoints/tagging_train799/export/step_2000_0.5608\n",
    "# 替换自己训练保存的\n",
    "!sudo chmod a+x ./VideoStructuring/run.sh && ./VideoStructuring/run.sh test checkpoints/tagging5k_temp/export/step_7000_0.7468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上传代码带COS上面提交\n",
    "* 把json文件下载到本地\n",
    "* 手动上传到COS存储，把权限改为公共可读\n",
    "* 上传成功后复制在线路劲提交"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
